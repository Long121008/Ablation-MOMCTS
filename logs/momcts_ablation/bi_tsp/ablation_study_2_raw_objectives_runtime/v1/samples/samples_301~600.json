[
    {
        "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects promising solutions from the archive by prioritizing non-dominated, high-crowding-distance solutions, then applies a hybrid local search operator that adaptively blends 4-opt for the primary objective and guided segment inversion for the secondary objective, with operator probabilities dynamically adjusted based on relative improvement potential, while ensuring feasibility through strict node validation. The selection refines the archive to maintain diversity, and operator probabilities periodically reset to prevent stagnation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 30% non-dominated solutions with better crowding distance\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: (-x[1], x[0][1][0] + x[0][1][1]))\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 3)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 5:\n        return new_solution\n\n    # Calculate relative improvement potential\n    obj1_improvement = distance_matrix_1[base_solution[-1], base_solution[0]] - sum(distance_matrix_1[base_solution[i], base_solution[i+1]] for i in range(n-1))\n    obj2_improvement = distance_matrix_2[base_solution[-1], base_solution[0]] - sum(distance_matrix_2[base_solution[i], base_solution[i+1]] for i in range(n-1))\n\n    # Dynamic operator blending (70% 4-opt, 30% guided inversion)\n    if random.random() < 0.7 or obj1_improvement > obj2_improvement:\n        # 4-opt for first objective\n        i, j, k, l = sorted(random.sample(range(n), 4))\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        segment3 = new_solution[k:l]\n        new_solution = np.concatenate([new_solution[:i], segment2, segment1, segment3, new_solution[l:]])\n    else:\n        # Guided segment inversion for second objective\n        i, j = sorted(random.sample(range(n), 2))\n        if obj2_improvement > 0:\n            new_solution[i:j] = new_solution[i:j][::-1]\n        else:\n            # Alternative move if inversion doesn't help\n            k = random.randint(i, j-1)\n            new_solution[i:j] = np.concatenate([new_solution[k:j], new_solution[i:k]])\n\n    # Ensure feasibility\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for i in range(n):\n            if new_solution[i] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[i]) > 1:\n                new_solution[i] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm dynamically selects diverse solutions from the archive based on crowding distance, applies a hybrid k-opt operator prioritizing objective-weighted edge importance, and ensures feasibility through multi-phase validation and repair. It balances exploration (random segment selection) with exploitation (weighted segment prioritization), with higher weight given to the first objective (0.7 vs. 0.3 for the second). The dynamic k-opt (3-5 segments) and adaptive segment operations (reversal/insertion) create novel neighborhoods while maintaining tour validity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: adjust selection pool size based on diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for i in range(len(objectives)):\n        dist = 0\n        for j in range(len(objectives)):\n            if i != j:\n                dist += abs(objectives[i][0] - objectives[j][0]) + abs(objectives[i][1] - objectives[j][1])\n        crowding_distances[i] = dist\n    normalized_crowding = crowding_distances / np.max(crowding_distances)\n    selection_size = max(1, int(0.3 * len(archive) * (1 - normalized_crowding.mean())))\n\n    # Select solutions with high crowding distance (diverse)\n    sorted_indices = np.argsort(-normalized_crowding)\n    candidate_indices = sorted_indices[:selection_size]\n\n    # Select a random solution from the diverse candidates\n    selected_idx = np.random.choice(candidate_indices)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Novel k-opt with dynamic segment insertion\n    k = np.random.randint(3, min(6, n//2))\n    segments = []\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        segments.append((i, j))\n\n    # Sort segments by their weighted edge importance\n    segment_importance = []\n    for i, j in segments:\n        edge_weight1 = distance_matrix_1[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]\n        edge_weight2 = distance_matrix_2[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]\n        segment_importance.append(0.7 * edge_weight1 + 0.3 * edge_weight2)\n    sorted_segments = [seg for _, seg in sorted(zip(segment_importance, segments), key=lambda x: -x[0])]\n\n    # Apply segment operations\n    for i, j in sorted_segments:\n        if np.random.rand() < 0.5:\n            # Dynamic segment reversal\n            new_solution[i:j] = new_solution[i:j][::-1]\n        else:\n            # Dynamic segment insertion\n            inserted_segment = new_solution[i:j]\n            new_solution = np.concatenate([new_solution[:i], inserted_segment, new_solution[j:]])\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates by replacing with missing nodes\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Phase 2: Ensure connectivity by checking for isolated segments\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap if still infeasible\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: *Adaptive, Hybrid, Pareto-Efficient, Novel Operators*\n- **Advice**: Use **6-opt + adaptive edge relinking** (weighted 80-20) on **top 10% non-dominated solutions** with crowding-distance filtering. Dynamically blend operators every 2 iterations (90% 6-opt, 10% relinking).\n- **Avoid**: Static 2-opt/3-opt/4-opt, dominated solutions, or rigid selection. Overlook dynamic operator adaptation.\n- **Explanation**: The top 10% selection ensures elite parents, while 6-opt and adaptive relinking balance local and global search. Dynamic blending prevents stagnation, and crowding distance maintains Pareto diversity. Strict feasibility checks are mandatory.\n\n**Implementation**: Randomly select from top 10% non-dominated solutions, apply hybrid operator, and validate feasibility.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 301,
        "algorithm": "The algorithm selects top 15% non-dominated solutions with high crowding distance, prioritizing diversity, and applies a hybrid operator (70% 6-opt for local refinement, 30% adaptive relinking for global exploration) with weighted edge importance (70% first objective, 30% second), while ensuring feasibility through multi-phase checks and fallback mechanisms.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 15% non-dominated solutions with high crowding distance\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: (-x[1], x[0][1][0] + x[0][1][1]))\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 6)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 6:\n        return new_solution\n\n    # Dynamic operator blending (70% 6-opt, 30% adaptive relinking)\n    if random.random() < 0.7:\n        # 6-opt operator\n        indices = sorted(random.sample(range(n), 6))\n        i, j, k, l, m, o = indices\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        segment3 = new_solution[k:l]\n        segment4 = new_solution[l:m]\n        segment5 = new_solution[m:o]\n        new_solution = np.concatenate([new_solution[:i], segment2, segment1, segment4, segment3, segment5, new_solution[o:]])\n    else:\n        # Adaptive edge relinking\n        def edge_importance(a, b):\n            return 0.7 * distance_matrix_1[a, b] + 0.3 * distance_matrix_2[a, b]\n\n        # Select segments based on edge importance\n        segments = []\n        for _ in range(3):\n            i = random.randint(0, n-1)\n            j = random.randint(0, n-1)\n            if i > j:\n                i, j = j, i\n            segments.append((i, j))\n\n        # Sort segments by importance\n        segments.sort(key=lambda seg: -edge_importance(new_solution[seg[0]], new_solution[seg[1]]))\n\n        for i, j in segments:\n            if random.random() < 0.6:\n                # Reverse segment\n                new_solution[i:j] = new_solution[i:j][::-1]\n            else:\n                # Insert segment elsewhere\n                inserted = new_solution[i:j]\n                new_solution = np.concatenate([new_solution[:i], new_solution[j:], inserted])\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Check for isolated segments\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap\n        i, j = random.sample(range(n), 2)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7495880027703677,
            0.8292376399040222
        ],
        "raw_score": [
            6.847039342242999,
            7.564072014638981
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 15% non-dominated solutions with high crowding distance\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: (-x[1], x[0][1][0] + x[0][1][1]))\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 6)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 6:\n        return new_solution\n\n    # Dynamic operator blending (70% 6-opt, 30% adaptive relinking)\n    if random.random() < 0.7:\n        # 6-opt operator\n        indices = sorted(random.sample(range(n), 6))\n        i, j, k, l, m, o = indices\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        segment3 = new_solution[k:l]\n        segment4 = new_solution[l:m]\n        segment5 = new_solution[m:o]\n        new_solution = np.concatenate([new_solution[:i], segment2, segment1, segment4, segment3, segment5, new_solution[o:]])\n    else:\n        # Adaptive edge relinking\n        def edge_importance(a, b):\n            return 0.7 * distance_matrix_1[a, b] + 0.3 * distance_matrix_2[a, b]\n\n        # Select segments based on edge importance\n        segments = []\n        for _ in range(3):\n            i = random.randint(0, n-1)\n            j = random.randint(0, n-1)\n            if i > j:\n                i, j = j, i\n            segments.append((i, j))\n\n        # Sort segments by importance\n        segments.sort(key=lambda seg: -edge_importance(new_solution[seg[0]], new_solution[seg[1]]))\n\n        for i, j in segments:\n            if random.random() < 0.6:\n                # Reverse segment\n                new_solution[i:j] = new_solution[i:j][::-1]\n            else:\n                # Insert segment elsewhere\n                inserted = new_solution[i:j]\n                new_solution = np.concatenate([new_solution[:i], new_solution[j:], inserted])\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Check for isolated segments\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap\n        i, j = random.sample(range(n), 2)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm dynamically selects diverse solutions from the archive based on crowding distance, applies a hybrid k-opt operator prioritizing objective-weighted edge importance, and ensures feasibility through multi-phase validation and repair. It balances exploration (random segment selection) with exploitation (weighted segment prioritization), with higher weight given to the first objective (0.7 vs. 0.3 for the second). The dynamic k-opt (3-5 segments) and adaptive segment operations (reversal/insertion) create novel neighborhoods while maintaining tour validity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: adjust selection pool size based on diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for i in range(len(objectives)):\n        dist = 0\n        for j in range(len(objectives)):\n            if i != j:\n                dist += abs(objectives[i][0] - objectives[j][0]) + abs(objectives[i][1] - objectives[j][1])\n        crowding_distances[i] = dist\n    normalized_crowding = crowding_distances / np.max(crowding_distances)\n    selection_size = max(1, int(0.3 * len(archive) * (1 - normalized_crowding.mean())))\n\n    # Select solutions with high crowding distance (diverse)\n    sorted_indices = np.argsort(-normalized_crowding)\n    candidate_indices = sorted_indices[:selection_size]\n\n    # Select a random solution from the diverse candidates\n    selected_idx = np.random.choice(candidate_indices)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Novel k-opt with dynamic segment insertion\n    k = np.random.randint(3, min(6, n//2))\n    segments = []\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        segments.append((i, j))\n\n    # Sort segments by their weighted edge importance\n    segment_importance = []\n    for i, j in segments:\n        edge_weight1 = distance_matrix_1[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]\n        edge_weight2 = distance_matrix_2[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]\n        segment_importance.append(0.7 * edge_weight1 + 0.3 * edge_weight2)\n    sorted_segments = [seg for _, seg in sorted(zip(segment_importance, segments), key=lambda x: -x[0])]\n\n    # Apply segment operations\n    for i, j in sorted_segments:\n        if np.random.rand() < 0.5:\n            # Dynamic segment reversal\n            new_solution[i:j] = new_solution[i:j][::-1]\n        else:\n            # Dynamic segment insertion\n            inserted_segment = new_solution[i:j]\n            new_solution = np.concatenate([new_solution[:i], inserted_segment, new_solution[j:]])\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates by replacing with missing nodes\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Phase 2: Ensure connectivity by checking for isolated segments\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap if still infeasible\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 302,
        "algorithm": "The algorithm combines Pareto dominance with adaptive objective weighting to select promising solutions from an archive, then applies a multi-phase local search (weighted edge reversal and probabilistic segment insertion) to generate neighbors, while ensuring feasibility through a multi-stage repair mechanism that prioritizes node uniqueness and connectivity. The selection process dynamically balances exploration and exploitation by considering both objective trade-offs and solution quality, while the local search phases introduce diversity through controlled segment manipulation. The repair stages systematically address infeasibilities by inserting missing nodes and validating tour integrity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto dominance and adaptive weighting\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] <= objectives[i][0] and objectives[j][1] <= objectives[i][1]):\n                if objectives[j][0] < objectives[i][0] or objectives[j][1] < objectives[i][1]:\n                    dominated = True\n                    break\n        if not dominated:\n            pareto_front.append(i)\n\n    # If no Pareto front, use all solutions\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Adaptive weighting based on objective ratio\n    obj_ratio = np.mean(objectives[:, 0]) / (np.mean(objectives[:, 1]) + 1e-8)\n    weight1 = obj_ratio / (obj_ratio + 1)\n    weight2 = 1 - weight1\n\n    # Select solution with highest weighted objective value\n    weighted_scores = []\n    for idx in pareto_front:\n        weighted_score = weight1 * objectives[idx][0] + weight2 * objectives[idx][1]\n        weighted_scores.append(weighted_score)\n\n    selected_idx = pareto_front[np.argmax(weighted_scores)]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Multi-phase local search\n    # Phase 1: Weighted edge reversal\n    for _ in range(2):  # Limited iterations to balance exploration/exploitation\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        edge_weight1 = distance_matrix_1[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]\n        edge_weight2 = distance_matrix_2[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]\n        if (weight1 * edge_weight1 + weight2 * edge_weight2) > (np.random.rand() * (weight1 + weight2)):\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Phase 2: Probabilistic segment insertion\n    k = np.random.randint(2, min(4, n//3))\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        if np.random.rand() < 0.6:  # Higher probability for insertion\n            segment = new_solution[i:j]\n            new_solution = np.concatenate([new_solution[:i], segment, new_solution[j:]])\n        else:\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Multi-stage feasibility repair\n    # Stage 1: Node uniqueness check\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Stage 2: Connectivity check\n    if len(np.unique(new_solution)) != n:\n        # Fallback: Insert missing nodes at random positions\n        missing_nodes = np.setdiff1d(np.arange(n), np.unique(new_solution))\n        for node in missing_nodes:\n            pos = np.random.randint(0, n)\n            new_solution = np.insert(new_solution, pos, node)\n\n    # Stage 3: Final validation\n    if len(new_solution) != n or len(np.unique(new_solution)) != n:\n        # If still infeasible, return the base solution\n        return base_solution\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7404482396515323,
            1.5013388395309448
        ],
        "raw_score": [
            6.87207029013563,
            8.230386155115037
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto dominance and adaptive weighting\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] <= objectives[i][0] and objectives[j][1] <= objectives[i][1]):\n                if objectives[j][0] < objectives[i][0] or objectives[j][1] < objectives[i][1]:\n                    dominated = True\n                    break\n        if not dominated:\n            pareto_front.append(i)\n\n    # If no Pareto front, use all solutions\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Adaptive weighting based on objective ratio\n    obj_ratio = np.mean(objectives[:, 0]) / (np.mean(objectives[:, 1]) + 1e-8)\n    weight1 = obj_ratio / (obj_ratio + 1)\n    weight2 = 1 - weight1\n\n    # Select solution with highest weighted objective value\n    weighted_scores = []\n    for idx in pareto_front:\n        weighted_score = weight1 * objectives[idx][0] + weight2 * objectives[idx][1]\n        weighted_scores.append(weighted_score)\n\n    selected_idx = pareto_front[np.argmax(weighted_scores)]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Multi-phase local search\n    # Phase 1: Weighted edge reversal\n    for _ in range(2):  # Limited iterations to balance exploration/exploitation\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        edge_weight1 = distance_matrix_1[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]\n        edge_weight2 = distance_matrix_2[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]\n        if (weight1 * edge_weight1 + weight2 * edge_weight2) > (np.random.rand() * (weight1 + weight2)):\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Phase 2: Probabilistic segment insertion\n    k = np.random.randint(2, min(4, n//3))\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        if np.random.rand() < 0.6:  # Higher probability for insertion\n            segment = new_solution[i:j]\n            new_solution = np.concatenate([new_solution[:i], segment, new_solution[j:]])\n        else:\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Multi-stage feasibility repair\n    # Stage 1: Node uniqueness check\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Stage 2: Connectivity check\n    if len(np.unique(new_solution)) != n:\n        # Fallback: Insert missing nodes at random positions\n        missing_nodes = np.setdiff1d(np.arange(n), np.unique(new_solution))\n        for node in missing_nodes:\n            pos = np.random.randint(0, n)\n            new_solution = np.insert(new_solution, pos, node)\n\n    # Stage 3: Final validation\n    if len(new_solution) != n or len(np.unique(new_solution)) != n:\n        # If still infeasible, return the base solution\n        return base_solution\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm dynamically selects diverse solutions from the archive based on crowding distance, applies a hybrid k-opt operator prioritizing objective-weighted edge importance, and ensures feasibility through multi-phase validation and repair. It balances exploration (random segment selection) with exploitation (weighted segment prioritization), with higher weight given to the first objective (0.7 vs. 0.3 for the second). The dynamic k-opt (3-5 segments) and adaptive segment operations (reversal/insertion) create novel neighborhoods while maintaining tour validity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: adjust selection pool size based on diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for i in range(len(objectives)):\n        dist = 0\n        for j in range(len(objectives)):\n            if i != j:\n                dist += abs(objectives[i][0] - objectives[j][0]) + abs(objectives[i][1] - objectives[j][1])\n        crowding_distances[i] = dist\n    normalized_crowding = crowding_distances / np.max(crowding_distances)\n    selection_size = max(1, int(0.3 * len(archive) * (1 - normalized_crowding.mean())))\n\n    # Select solutions with high crowding distance (diverse)\n    sorted_indices = np.argsort(-normalized_crowding)\n    candidate_indices = sorted_indices[:selection_size]\n\n    # Select a random solution from the diverse candidates\n    selected_idx = np.random.choice(candidate_indices)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Novel k-opt with dynamic segment insertion\n    k = np.random.randint(3, min(6, n//2))\n    segments = []\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        segments.append((i, j))\n\n    # Sort segments by their weighted edge importance\n    segment_importance = []\n    for i, j in segments:\n        edge_weight1 = distance_matrix_1[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]\n        edge_weight2 = distance_matrix_2[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]\n        segment_importance.append(0.7 * edge_weight1 + 0.3 * edge_weight2)\n    sorted_segments = [seg for _, seg in sorted(zip(segment_importance, segments), key=lambda x: -x[0])]\n\n    # Apply segment operations\n    for i, j in sorted_segments:\n        if np.random.rand() < 0.5:\n            # Dynamic segment reversal\n            new_solution[i:j] = new_solution[i:j][::-1]\n        else:\n            # Dynamic segment insertion\n            inserted_segment = new_solution[i:j]\n            new_solution = np.concatenate([new_solution[:i], inserted_segment, new_solution[j:]])\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates by replacing with missing nodes\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Phase 2: Ensure connectivity by checking for isolated segments\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap if still infeasible\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 303,
        "algorithm": "The algorithm dynamically selects solutions from the archive by combining dominance rank and objective variance to prioritize high-potential candidates, then applies a hybrid 3-opt and edge-swap operator with adaptive segment prioritization based on edge variance, ensuring feasibility through multi-step validation and repair while balancing exploration and exploitation with equal weighting for both objectives. It emphasizes high-variance segments for more impactful changes and uses probabilistic edge reversal for local refinement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic selection based on dominance rank and objective variance\n    objectives = np.array([obj for _, obj in archive])\n    ranks = np.zeros(len(archive))\n    for i in range(len(objectives)):\n        dominated = 0\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] <= objectives[i][0] and objectives[j][1] <= objectives[i][1]) and (objectives[j][0] < objectives[i][0] or objectives[j][1] < objectives[i][1]):\n                dominated += 1\n        ranks[i] = dominated\n\n    # Calculate objective variances\n    variances = np.var(objectives, axis=0)\n    normalized_variances = variances / np.sum(variances)\n\n    # Combine rank and variance for selection\n    selection_scores = ranks + normalized_variances[0] * objectives[:, 0] + normalized_variances[1] * objectives[:, 1]\n    selected_idx = np.argmin(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Hybrid 3-opt with edge-swap\n    k = 3\n    segments = []\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        segments.append((i, j))\n\n    # Sort segments by their weighted edge variance\n    segment_variances = []\n    for i, j in segments:\n        edge_var1 = np.var([distance_matrix_1[base_solution[i], base_solution[(i+1)%n]], distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]])\n        edge_var2 = np.var([distance_matrix_2[base_solution[i], base_solution[(i+1)%n]], distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]])\n        segment_variances.append(0.5 * edge_var1 + 0.5 * edge_var2)\n    sorted_segments = [seg for _, seg in sorted(zip(segment_variances, segments), key=lambda x: -x[0])]\n\n    # Apply segment operations\n    for i, j in sorted_segments:\n        if np.random.rand() < 0.5:\n            # Edge reversal\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n        else:\n            # Segment reversal\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Multi-step feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Phase 2: Ensure connectivity\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7815181381042189,
            1.683144748210907
        ],
        "raw_score": [
            7.710849185464529,
            6.7700002447245
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic selection based on dominance rank and objective variance\n    objectives = np.array([obj for _, obj in archive])\n    ranks = np.zeros(len(archive))\n    for i in range(len(objectives)):\n        dominated = 0\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] <= objectives[i][0] and objectives[j][1] <= objectives[i][1]) and (objectives[j][0] < objectives[i][0] or objectives[j][1] < objectives[i][1]):\n                dominated += 1\n        ranks[i] = dominated\n\n    # Calculate objective variances\n    variances = np.var(objectives, axis=0)\n    normalized_variances = variances / np.sum(variances)\n\n    # Combine rank and variance for selection\n    selection_scores = ranks + normalized_variances[0] * objectives[:, 0] + normalized_variances[1] * objectives[:, 1]\n    selected_idx = np.argmin(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Hybrid 3-opt with edge-swap\n    k = 3\n    segments = []\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        segments.append((i, j))\n\n    # Sort segments by their weighted edge variance\n    segment_variances = []\n    for i, j in segments:\n        edge_var1 = np.var([distance_matrix_1[base_solution[i], base_solution[(i+1)%n]], distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]])\n        edge_var2 = np.var([distance_matrix_2[base_solution[i], base_solution[(i+1)%n]], distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]])\n        segment_variances.append(0.5 * edge_var1 + 0.5 * edge_var2)\n    sorted_segments = [seg for _, seg in sorted(zip(segment_variances, segments), key=lambda x: -x[0])]\n\n    # Apply segment operations\n    for i, j in sorted_segments:\n        if np.random.rand() < 0.5:\n            # Edge reversal\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n        else:\n            # Segment reversal\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Multi-step feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Phase 2: Ensure connectivity\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm dynamically selects diverse solutions from the archive based on crowding distance, applies a hybrid k-opt operator prioritizing objective-weighted edge importance, and ensures feasibility through multi-phase validation and repair. It balances exploration (random segment selection) with exploitation (weighted segment prioritization), with higher weight given to the first objective (0.7 vs. 0.3 for the second). The dynamic k-opt (3-5 segments) and adaptive segment operations (reversal/insertion) create novel neighborhoods while maintaining tour validity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: adjust selection pool size based on diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for i in range(len(objectives)):\n        dist = 0\n        for j in range(len(objectives)):\n            if i != j:\n                dist += abs(objectives[i][0] - objectives[j][0]) + abs(objectives[i][1] - objectives[j][1])\n        crowding_distances[i] = dist\n    normalized_crowding = crowding_distances / np.max(crowding_distances)\n    selection_size = max(1, int(0.3 * len(archive) * (1 - normalized_crowding.mean())))\n\n    # Select solutions with high crowding distance (diverse)\n    sorted_indices = np.argsort(-normalized_crowding)\n    candidate_indices = sorted_indices[:selection_size]\n\n    # Select a random solution from the diverse candidates\n    selected_idx = np.random.choice(candidate_indices)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Novel k-opt with dynamic segment insertion\n    k = np.random.randint(3, min(6, n//2))\n    segments = []\n    for _ in range(k):\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        segments.append((i, j))\n\n    # Sort segments by their weighted edge importance\n    segment_importance = []\n    for i, j in segments:\n        edge_weight1 = distance_matrix_1[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_1[base_solution[j], base_solution[(j+1)%n]]\n        edge_weight2 = distance_matrix_2[base_solution[i], base_solution[(i+1)%n]] + distance_matrix_2[base_solution[j], base_solution[(j+1)%n]]\n        segment_importance.append(0.7 * edge_weight1 + 0.3 * edge_weight2)\n    sorted_segments = [seg for _, seg in sorted(zip(segment_importance, segments), key=lambda x: -x[0])]\n\n    # Apply segment operations\n    for i, j in sorted_segments:\n        if np.random.rand() < 0.5:\n            # Dynamic segment reversal\n            new_solution[i:j] = new_solution[i:j][::-1]\n        else:\n            # Dynamic segment insertion\n            inserted_segment = new_solution[i:j]\n            new_solution = np.concatenate([new_solution[:i], inserted_segment, new_solution[j:]])\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates by replacing with missing nodes\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    # Phase 2: Ensure connectivity by checking for isolated segments\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random swap if still infeasible\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 30% nondominated solutions based on weighted objectives (70-30)\n    objectives = np.array([obj for _, obj in archive])\n    weights = np.array([0.7, 0.3])\n    weighted_scores = objectives @ weights\n    sorted_indices = np.argsort(weighted_scores)\n    top_30_percent = max(1, int(0.3 * len(archive)))\n    candidate_indices = sorted_indices[:top_30_percent]\n\n    # Select a random solution from the top candidates\n    selected_idx = np.random.choice(candidate_indices)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Hybrid local search: 3-opt with segment reversal\n    # Randomly select three distinct positions\n    i, j, k = sorted(np.random.choice(n, 3, replace=False))\n\n    # Apply 3-opt: reconnect the tour by removing and reinserting segments\n    segment1 = new_solution[i:j]\n    segment2 = new_solution[j:k]\n    segment3 = new_solution[k:] if k < n else np.array([])\n\n    # Reconstruct the tour in a new order\n    new_order = np.concatenate([segment1, segment2[::-1], segment3])\n    new_solution = np.concatenate([new_solution[:i], new_order])\n\n    # Ensure feasibility (no duplicates and visits all nodes)\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # If duplicates exist, repair by replacing duplicates with missing nodes\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                # Replace duplicate with a missing node\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on a combined objective score (sum of both costs) and applies a hybrid local search combining random edge swaps and segment reversals to generate a neighbor solution while ensuring feasibility through duplicate repair. It prioritizes solutions with lower combined costs and uses a 50/50 chance between edge swaps and segment reversals for exploration. The solution is repaired if duplicates arise, guaranteeing a valid TSP tour.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with the best combined objective score\n    def combined_score(obj):\n        return obj[0] + obj[1]\n\n    archive_sorted = sorted(archive, key=lambda x: combined_score(x[1]))\n    selected_idx = np.random.choice(min(3, len(archive_sorted)))\n    base_solution = archive_sorted[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: combine edge swaps and segment reversals\n    n = len(new_solution)\n    if n < 4:\n        return new_solution\n\n    # Randomly choose between edge swap or segment reversal\n    if np.random.rand() < 0.5:\n        # Edge swap\n        i, j = np.random.choice(n, 2, replace=False)\n        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n    else:\n        # Segment reversal\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Ensure the solution remains feasible (no duplicates and visits all nodes)\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # If duplicates exist, repair by replacing duplicates with missing nodes\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for i in range(n):\n            if new_solution[i] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[i]) > 1:\n                # Replace duplicate with a missing node\n                new_solution[i] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: *Adaptive, Hybrid, Pareto-Efficient, Novel Operators*\n- **Advice**: Use **6-opt + adaptive edge relinking** (weighted 80-20) on **top 10% non-dominated solutions** with crowding-distance filtering. Dynamically blend operators every 2 iterations (90% 6-opt, 10% relinking).\n- **Avoid**: Static 2-opt/3-opt/4-opt, dominated solutions, or rigid selection. Overlook dynamic operator adaptation.\n- **Explanation**: The top 10% selection ensures elite parents, while 6-opt and adaptive relinking balance local and global search. Dynamic blending prevents stagnation, and crowding distance maintains Pareto diversity. Strict feasibility checks are mandatory.\n\n**Implementation**: Randomly select from top 10% non-dominated solutions, apply hybrid operator, and validate feasibility.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 304,
        "algorithm": "The algorithm selects top 10% non-dominated solutions from the archive, applies crowding distance filtering to maintain diversity, and dynamically alternates between a hybrid 6-opt operator (prioritizing segments by weighted edge importance) and adaptive edge relinking (focusing on 70-30 weighted objectives) every 2 iterations. It ensures feasibility through multi-phase repairs, including duplicate handling and segment reversal fallbacks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 10% non-dominated solutions\n    objectives = np.array([obj for _, obj in archive])\n    is_dominated = np.zeros(len(archive), dtype=bool)\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and (objectives[j][0] <= objectives[i][0] and objectives[j][1] <= objectives[i][1]) and (objectives[j][0] < objectives[i][0] or objectives[j][1] < objectives[i][1]):\n                is_dominated[i] = True\n                break\n    non_dominated_indices = np.where(~is_dominated)[0]\n    if len(non_dominated_indices) == 0:\n        non_dominated_indices = np.arange(len(archive))\n    top_10_percent = max(1, int(0.1 * len(non_dominated_indices)))\n    candidate_indices = np.random.choice(non_dominated_indices, min(top_10_percent, len(non_dominated_indices)), replace=False)\n\n    # Apply crowding distance filtering\n    crowding_distances = np.zeros(len(candidate_indices))\n    for idx, i in enumerate(candidate_indices):\n        dist = 0\n        for j in candidate_indices:\n            if i != j:\n                dist += abs(objectives[i][0] - objectives[j][0]) + abs(objectives[i][1] - objectives[j][1])\n        crowding_distances[idx] = dist\n    selected_idx = candidate_indices[np.argmax(crowding_distances)]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Dynamic operator selection (6-opt or relinking)\n    operator_iteration = np.random.randint(0, 2)  # Alternate every 2 iterations\n    if operator_iteration == 0:\n        # Hybrid 6-opt\n        points = sorted(np.random.choice(n, 6, replace=False))\n        i, j, k, l, m, o = points\n\n        # Create segments\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        segment3 = new_solution[k:l]\n        segment4 = new_solution[l:m]\n        segment5 = new_solution[m:o]\n        segment6 = new_solution[o:] if o < n else np.array([])\n\n        # Reconstruct with weighted segment importance\n        segments = [segment1, segment2, segment3, segment4, segment5, segment6]\n        segment_importance = []\n        for seg in segments:\n            if len(seg) == 0:\n                segment_importance.append(0)\n                continue\n            weight1 = sum(distance_matrix_1[seg[i], seg[(i+1)%len(seg)]] for i in range(len(seg)))\n            weight2 = sum(distance_matrix_2[seg[i], seg[(i+1)%len(seg)]] for i in range(len(seg)))\n            segment_importance.append(0.7 * weight1 + 0.3 * weight2)\n        sorted_segments = [seg for _, seg in sorted(zip(segment_importance, segments), key=lambda x: -x[0])]\n\n        # Reconstruct tour\n        new_order = np.concatenate(sorted_segments)\n        new_solution = np.concatenate([new_solution[:i], new_order])\n    else:\n        # Adaptive edge relinking\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        relink_path = []\n        current_node = new_solution[i]\n        for _ in range(j - i):\n            neighbors = [new_solution[(i + 1) % n], new_solution[(i - 1) % n]]\n            weights = []\n            for neighbor in neighbors:\n                weight1 = distance_matrix_1[current_node, neighbor]\n                weight2 = distance_matrix_2[current_node, neighbor]\n                weights.append(0.7 * weight1 + 0.3 * weight2)\n            next_node = neighbors[np.argmin(weights)]\n            relink_path.append(next_node)\n            current_node = next_node\n        new_solution[i:j] = relink_path\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n                if len(missing_nodes) == 0:\n                    break\n\n    # Phase 2: Ensure connectivity\n    if len(np.unique(new_solution)) != n:\n        # Fallback to segment reversal\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        new_solution[i:j] = new_solution[i:j][::-1]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7844504815365476,
            3.328812539577484
        ],
        "raw_score": [
            7.223144383702869,
            7.1796418723669095
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 10% non-dominated solutions\n    objectives = np.array([obj for _, obj in archive])\n    is_dominated = np.zeros(len(archive), dtype=bool)\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and (objectives[j][0] <= objectives[i][0] and objectives[j][1] <= objectives[i][1]) and (objectives[j][0] < objectives[i][0] or objectives[j][1] < objectives[i][1]):\n                is_dominated[i] = True\n                break\n    non_dominated_indices = np.where(~is_dominated)[0]\n    if len(non_dominated_indices) == 0:\n        non_dominated_indices = np.arange(len(archive))\n    top_10_percent = max(1, int(0.1 * len(non_dominated_indices)))\n    candidate_indices = np.random.choice(non_dominated_indices, min(top_10_percent, len(non_dominated_indices)), replace=False)\n\n    # Apply crowding distance filtering\n    crowding_distances = np.zeros(len(candidate_indices))\n    for idx, i in enumerate(candidate_indices):\n        dist = 0\n        for j in candidate_indices:\n            if i != j:\n                dist += abs(objectives[i][0] - objectives[j][0]) + abs(objectives[i][1] - objectives[j][1])\n        crowding_distances[idx] = dist\n    selected_idx = candidate_indices[np.argmax(crowding_distances)]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Dynamic operator selection (6-opt or relinking)\n    operator_iteration = np.random.randint(0, 2)  # Alternate every 2 iterations\n    if operator_iteration == 0:\n        # Hybrid 6-opt\n        points = sorted(np.random.choice(n, 6, replace=False))\n        i, j, k, l, m, o = points\n\n        # Create segments\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        segment3 = new_solution[k:l]\n        segment4 = new_solution[l:m]\n        segment5 = new_solution[m:o]\n        segment6 = new_solution[o:] if o < n else np.array([])\n\n        # Reconstruct with weighted segment importance\n        segments = [segment1, segment2, segment3, segment4, segment5, segment6]\n        segment_importance = []\n        for seg in segments:\n            if len(seg) == 0:\n                segment_importance.append(0)\n                continue\n            weight1 = sum(distance_matrix_1[seg[i], seg[(i+1)%len(seg)]] for i in range(len(seg)))\n            weight2 = sum(distance_matrix_2[seg[i], seg[(i+1)%len(seg)]] for i in range(len(seg)))\n            segment_importance.append(0.7 * weight1 + 0.3 * weight2)\n        sorted_segments = [seg for _, seg in sorted(zip(segment_importance, segments), key=lambda x: -x[0])]\n\n        # Reconstruct tour\n        new_order = np.concatenate(sorted_segments)\n        new_solution = np.concatenate([new_solution[:i], new_order])\n    else:\n        # Adaptive edge relinking\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        relink_path = []\n        current_node = new_solution[i]\n        for _ in range(j - i):\n            neighbors = [new_solution[(i + 1) % n], new_solution[(i - 1) % n]]\n            weights = []\n            for neighbor in neighbors:\n                weight1 = distance_matrix_1[current_node, neighbor]\n                weight2 = distance_matrix_2[current_node, neighbor]\n                weights.append(0.7 * weight1 + 0.3 * weight2)\n            next_node = neighbors[np.argmin(weights)]\n            relink_path.append(next_node)\n            current_node = next_node\n        new_solution[i:j] = relink_path\n\n    # Multi-phase feasibility check and repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Phase 1: Repair duplicates\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for idx in range(n):\n            if new_solution[idx] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[idx]) > 1:\n                new_solution[idx] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n                if len(missing_nodes) == 0:\n                    break\n\n    # Phase 2: Ensure connectivity\n    if len(np.unique(new_solution)) != n:\n        # Fallback to segment reversal\n        i, j = sorted(np.random.choice(n, 2, replace=False))\n        new_solution[i:j] = new_solution[i:j][::-1]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 8 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with lower combined costs (top 30% of sorted solutions) and applies a hybrid local search that reverses a random segment of the tour and reinserts it elsewhere, with a fallback to simple segment reversal if duplicates occur, ensuring feasibility. The selection emphasizes high-potential solutions, while the local search explores diverse neighborhood structures.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a combination of objective ranking and randomness\n    sorted_archive = sorted(archive, key=lambda x: (x[1][0] + x[1][1]))\n    selected_idx = min(int(len(sorted_archive) * 0.3), len(sorted_archive) - 1)\n    base_solution = sorted_archive[selected_idx][0].copy()\n\n    # Hybrid local search: edge reversal with node insertion\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n > 3:\n        # Randomly select a segment to reverse and insert\n        start = np.random.randint(0, n-2)\n        end = np.random.randint(start+1, n)\n        segment = new_solution[start:end+1]\n        reversed_segment = segment[::-1]\n\n        # Insert the reversed segment at a random position\n        insert_pos = np.random.randint(0, n)\n        new_solution = np.concatenate([\n            new_solution[:insert_pos],\n            reversed_segment,\n            new_solution[insert_pos:start],\n            new_solution[end+1:]\n        ])\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random segment reversal\n        start = np.random.randint(0, n-1)\n        end = np.random.randint(start+1, n)\n        new_solution[start:end+1] = new_solution[start:end+1][::-1]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto-efficient solution filtering (top 20%) with a dynamic operator selection that blends 5-opt segment reversals (weighted 70% by the first objective) and guided node relocations (weighted 30% by the second objective), ensuring feasibility through a minimal structural repair mechanism. It prioritizes solutions with better performance in the first objective while adaptively refining the tour based on both objectives, balancing exploration and exploitation through weighted probabilistic selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify top 20% non-dominated solutions\n    objectives = [obj for (sol, obj) in archive]\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and (other_obj[0] <= obj[0] and other_obj[1] <= obj[1]) and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append((sol, obj))\n\n    if not non_dominated:\n        non_dominated = archive\n\n    top_20_idx = np.random.choice(len(non_dominated), size=max(1, int(0.2 * len(non_dominated))), replace=False)\n    selected_sol, selected_obj = non_dominated[np.random.choice(top_20_idx)]\n\n    # Step 2: Dynamic operator selection based on weighted objectives\n    new_solution = selected_sol.copy()\n    n = len(new_solution)\n\n    if n < 5:\n        return new_solution\n\n    # Calculate weights for operator selection\n    total_cost = selected_obj[0] + selected_obj[1]\n    weight_obj1 = selected_obj[0] / total_cost\n    weight_obj2 = selected_obj[1] / total_cost\n\n    if random.random() < weight_obj1 * 0.7:  # 5-opt segment reversal (weighted by first objective)\n        i, j, k, l, m = sorted(np.random.choice(n, size=5, replace=False))\n        new_solution[i:j+1] = new_solution[i:j+1][::-1]\n        new_solution[k:l+1] = new_solution[k:l+1][::-1]\n        new_solution[m:] = new_solution[m:][::-1]\n    else:  # Guided node relocation (weighted by second objective)\n        i, j = np.random.choice(n, size=2, replace=False)\n        if i > j:\n            i, j = j, i\n        if (distance_matrix_1[new_solution[i-1], new_solution[i+1]] +\n            distance_matrix_2[new_solution[i-1], new_solution[i+1]]) < \\\n           (distance_matrix_1[new_solution[i-1], new_solution[i]] + distance_matrix_1[new_solution[i], new_solution[i+1]] +\n            distance_matrix_2[new_solution[i-1], new_solution[i]] + distance_matrix_2[new_solution[i], new_solution[i+1]]):\n            node = new_solution[i]\n            new_solution = np.delete(new_solution, i)\n            new_solution = np.insert(new_solution, j, node)\n\n    # Feasibility repair\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing = np.setdiff1d(np.arange(n), unique_nodes)\n        duplicates = [x for x in new_solution if np.sum(new_solution == x) > 1]\n\n        for dup in duplicates:\n            if len(missing) == 0:\n                break\n            idx = np.where(new_solution == dup)[0][-1]\n            new_solution[idx] = missing[0]\n            missing = missing[1:]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm prioritizes segment reversal (70%) over edge insertion (30%) in a hybrid local search, focusing on the bottom 40% of nondominated solutions (weighted 30-70% by objectives) to generate diverse, high-quality neighbors while ensuring feasibility through random swaps as a fallback. It dynamically balances exploration and exploitation by randomly perturbing segments in selected solutions, with validation to maintain tour validity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify bottom 40% nondominated solutions\n    non_dominated = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and\n                (other_obj[0] < obj[0] or other_obj[1] < obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append((sol, obj))\n\n    if not non_dominated:\n        non_dominated = archive\n\n    # Select bottom 40% based on weighted objectives (30% first, 70% second)\n    weighted_scores = [0.3 * obj[0] + 0.7 * obj[1] for _, obj in non_dominated]\n    sorted_solutions = sorted(zip(non_dominated, weighted_scores), key=lambda x: x[1], reverse=True)\n    bottom_n = max(1, int(0.4 * len(sorted_solutions)))\n    bottom_solutions = [sol_obj[0] for sol_obj in sorted_solutions[-bottom_n:]]\n\n    # Randomly select a solution from bottom solutions\n    selected_idx = np.random.randint(0, len(bottom_solutions))\n    base_solution = bottom_solutions[selected_idx][0].copy()\n\n    # Step 2: Hybrid local search (segment reversal + edge insertion)\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Randomly perturb segments\n    for _ in range(3):  # Limit iterations for efficiency\n        if n <= 3:\n            break\n\n        seg_start = np.random.randint(0, n-2)\n        seg_end = np.random.randint(seg_start+1, n)\n\n        if np.random.rand() < 0.7:  # 70% chance for reversal\n            new_solution[seg_start:seg_end] = new_solution[seg_start:seg_end][::-1]\n        else:  # 30% chance for insertion\n            insert_pos = np.random.randint(0, n)\n            if insert_pos < seg_start:\n                new_solution = np.concatenate([\n                    new_solution[:insert_pos],\n                    new_solution[seg_start:seg_end],\n                    new_solution[insert_pos:seg_start],\n                    new_solution[seg_end:]\n                ])\n            else:\n                new_solution = np.concatenate([\n                    new_solution[:seg_start],\n                    new_solution[seg_end:insert_pos],\n                    new_solution[seg_start:seg_end],\n                    new_solution[insert_pos:]\n                ])\n\n        # Validate feasibility\n        if len(np.unique(new_solution)) != n:\n            i, j = np.random.choice(n, 2, replace=False)\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects the most promising solution from the archive by prioritizing those with lower combined objective values (weighted 30% for the first objective and 70% for the second), then applies a hybrid local search combining segment reversals (70% chance) and node swaps (30% chance), ensuring feasibility through dynamic repair with edge balancing and random insertions if needed. It dynamically repairs infeasible solutions by replacing duplicates with missing nodes or performing random insertions, maintaining a valid TSP tour.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the best combined objective value (weighted sum)\n    best_solution = min(archive, key=lambda x: 0.3 * x[1][0] + 0.7 * x[1][1])[0].copy()\n    new_solution = best_solution.copy()\n    n = len(new_solution)\n\n    # Hybrid local search: segment reversal with node swap\n    if np.random.rand() < 0.7:\n        # Segment reversal\n        seg_start = np.random.randint(0, n-2)\n        seg_end = np.random.randint(seg_start+1, n)\n        segment = new_solution[seg_start:seg_end]\n        new_solution[seg_start:seg_end] = segment[::-1]\n    else:\n        # Node swap with edge balancing\n        a, b = sorted(np.random.choice(n, 2, replace=False))\n        new_solution[a], new_solution[b] = new_solution[b], new_solution[a]\n\n    # Ensure feasibility: dynamic repair with edge balancing\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        duplicates = np.setdiff1d(new_solution, unique_nodes)\n\n        # Replace duplicates with missing nodes or random insertions\n        for i in range(n):\n            if new_solution[i] in duplicates:\n                if len(missing_nodes) > 0:\n                    new_solution[i] = missing_nodes[0]\n                    missing_nodes = missing_nodes[1:]\n                else:\n                    # Random insertion if no missing nodes\n                    insert_pos = np.random.randint(0, n)\n                    new_solution = np.insert(new_solution, insert_pos, new_solution[i])\n                    new_solution = np.delete(new_solution, i if i < insert_pos else i+1)\n                    break\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing lower-cost solutions (weighted 70% for the first objective, 30% for the second) and applies a hybrid local search combining segment reversal and node-swap to generate a neighbor solution while ensuring feasibility through validation. It intelligently balances exploration and exploitation by focusing on high-potential solutions and using creative local search operators beyond standard 2-opt.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted combination of objectives\n    weighted_scores = [(x[1][0] * 0.7 + x[1][1] * 0.3, idx) for idx, x in enumerate(archive)]\n    weighted_scores.sort()\n    selected_idx = min(int(len(weighted_scores) * 0.25), len(weighted_scores) - 1)\n    base_solution = archive[weighted_scores[selected_idx][1]][0].copy()\n\n    # Hybrid local search: segment reversal with node-swap\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n > 4:\n        # Randomly select a segment to reverse\n        start = np.random.randint(0, n-3)\n        end = np.random.randint(start+2, n)\n        segment = new_solution[start:end+1]\n        reversed_segment = segment[::-1]\n\n        # Insert the reversed segment at a different position\n        insert_pos = np.random.randint(0, n)\n        if insert_pos >= start and insert_pos <= end:\n            insert_pos = (insert_pos + 2) % n\n\n        new_solution = np.concatenate([\n            new_solution[:insert_pos],\n            reversed_segment,\n            new_solution[insert_pos:start],\n            new_solution[end+1:]\n        ])\n\n        # Apply node-swap to further diversify\n        swap_pos1 = np.random.randint(0, n)\n        swap_pos2 = np.random.randint(0, n)\n        new_solution[swap_pos1], new_solution[swap_pos2] = new_solution[swap_pos2], new_solution[swap_pos1]\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        # Fallback to random segment reversal\n        start = np.random.randint(0, n-1)\n        end = np.random.randint(start+1, n)\n        new_solution[start:end+1] = new_solution[start:end+1][::-1]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive Pareto selection with a hybrid local search operator that dynamically alternates between k-opt-like segment rearrangement (with k adaptively chosen between 3-5) and guided segment insertion, prioritizing the first objective (60% weight) while using weighted crowding distance to select promising solutions. It ensures feasibility through multi-stage validation and repair, including segment reversal and node swapping when duplicates are detected.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select top 30% solutions with weighted crowding distance\n    def weighted_crowding_distance(pop):\n        distances = np.zeros(len(pop))\n        objectives = np.array([obj for _, obj in pop])\n\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n\n            for i in range(1, len(pop)-1):\n                if objectives[sorted_idx[-1], m] == objectives[sorted_idx[0], m]:\n                    continue\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n\n        # Weighted combination (60% obj1, 40% obj2)\n        return 0.6 * distances + 0.4 * np.random.permutation(distances)\n\n    non_dominated = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and\n                (other_obj[0] < obj[0] or other_obj[1] < obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append((sol, obj))\n\n    if not non_dominated:\n        non_dominated = archive\n\n    crowding_dist = weighted_crowding_distance(non_dominated)\n    top_30_idx = np.argsort(crowding_dist)[-max(1, int(0.3 * len(non_dominated))):]\n    selected_sol = non_dominated[np.random.choice(top_30_idx)][0].copy()\n\n    # Step 2: Adaptive hybrid operator (k-opt + guided insertion)\n    new_solution = selected_sol.copy()\n    n = len(new_solution)\n\n    if n > 5:\n        # Adaptive k selection (3-5 based on solution length)\n        k = min(5, max(3, n // 5))\n\n        # k-opt segment rearrangement\n        if random.random() < 0.7:\n            points = sorted(random.sample(range(1, n-1), k))\n            segments = []\n            for i in range(len(points)):\n                if i == 0:\n                    segments.append(new_solution[:points[i]])\n                else:\n                    segments.append(new_solution[points[i-1]:points[i]])\n            segments.append(new_solution[points[-1]:])\n\n            # Randomly permute segments\n            random.shuffle(segments)\n            new_solution = np.concatenate(segments)\n        else:\n            # Guided segment insertion\n            seg_size = random.randint(2, min(4, n//3))\n            seg_start = random.randint(1, n-seg_size-1)\n            segment = new_solution[seg_start:seg_start+seg_size]\n\n            best_pos = seg_start\n            best_improvement = 0\n\n            for pos in range(1, n-seg_size):\n                if pos >= seg_start and pos < seg_start+seg_size:\n                    continue\n\n                current_cost1 = (distance_matrix_1[new_solution[seg_start-1], new_solution[seg_start]] +\n                                distance_matrix_1[new_solution[seg_start+seg_size-1], new_solution[seg_start+seg_size]])\n                new_cost1 = (distance_matrix_1[new_solution[pos-1], segment[0]] +\n                            distance_matrix_1[segment[-1], new_solution[pos]])\n\n                current_cost2 = (distance_matrix_2[new_solution[seg_start-1], new_solution[seg_start]] +\n                                distance_matrix_2[new_solution[seg_start+seg_size-1], new_solution[seg_start+seg_size]])\n                new_cost2 = (distance_matrix_2[new_solution[pos-1], segment[0]] +\n                            distance_matrix_2[segment[-1], new_solution[pos]])\n\n                improvement = 0.6*(current_cost1 - new_cost1) + 0.4*(current_cost2 - new_cost2)\n\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    best_pos = pos\n\n            if best_pos != seg_start:\n                new_solution = np.concatenate([\n                    new_solution[:seg_start],\n                    new_solution[seg_start+seg_size:]\n                ])\n                new_solution = np.insert(new_solution, best_pos, segment)\n\n    # Feasibility repair with multi-stage validation\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        # Stage 1: Try segment reversal\n        if random.random() < 0.5:\n            i, j = sorted(random.sample(range(1, n-1), 2))\n            new_solution[i:j] = new_solution[i:j][::-1]\n\n        # Stage 2: Node swapping if still infeasible\n        if len(np.unique(new_solution)) != n:\n            missing = np.setdiff1d(np.arange(n), unique_nodes)\n            duplicates = [x for x in new_solution if np.sum(new_solution == x) > 1]\n\n            for dup in duplicates:\n                if len(missing) == 0:\n                    break\n                idx = np.where(new_solution == dup)[0][-1]\n                new_solution[idx] = missing[0]\n                missing = missing[1:]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm selects non-dominated solutions from the archive, prioritizes those with better crowding distance and a weighted objective score (70% for the first objective), and applies a hybrid local search (60% 3-opt for the first objective, 40% segment reversal for the second) while ensuring feasibility through duplicate repair. The operator probabilities dynamically adjust based on relative improvement potential, and the selection pool is refined to maintain diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select non-dominated solutions\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    # Calculate crowding distance for non-dominated solutions\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: (-x[1], 0.7 * x[0][1][0] + 0.3 * x[0][1][1]))\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 3)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 4:\n        return new_solution\n\n    # Calculate relative improvement potential for each objective\n    obj1_improvement = distance_matrix_1[base_solution[-1], base_solution[0]] - sum(distance_matrix_1[base_solution[i], base_solution[i+1]] for i in range(n-1))\n    obj2_improvement = distance_matrix_2[base_solution[-1], base_solution[0]] - sum(distance_matrix_2[base_solution[i], base_solution[i+1]] for i in range(n-1))\n\n    # Dynamic operator blending (60% 3-opt, 40% reversal)\n    if random.random() < 0.6 or obj1_improvement > obj2_improvement:\n        # 3-opt for first objective\n        i, j, k = sorted(random.sample(range(n), 3))\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        new_solution = np.concatenate([new_solution[:i], segment2, segment1, new_solution[k:]])\n    else:\n        # Segment reversal for second objective\n        i, j = sorted(random.sample(range(n), 2))\n        new_solution[i:j] = new_solution[i:j][::-1]\n\n    # Ensure the solution remains feasible\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for i in range(n):\n            if new_solution[i] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[i]) > 1:\n                new_solution[i] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects promising solutions from the archive by prioritizing non-dominated, high-crowding-distance solutions, then applies a hybrid local search operator that adaptively blends 4-opt for the primary objective and guided segment inversion for the secondary objective, with operator probabilities dynamically adjusted based on relative improvement potential, while ensuring feasibility through strict node validation. The selection refines the archive to maintain diversity, and operator probabilities periodically reset to prevent stagnation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 30% non-dominated solutions with better crowding distance\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: (-x[1], x[0][1][0] + x[0][1][1]))\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 3)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 5:\n        return new_solution\n\n    # Calculate relative improvement potential\n    obj1_improvement = distance_matrix_1[base_solution[-1], base_solution[0]] - sum(distance_matrix_1[base_solution[i], base_solution[i+1]] for i in range(n-1))\n    obj2_improvement = distance_matrix_2[base_solution[-1], base_solution[0]] - sum(distance_matrix_2[base_solution[i], base_solution[i+1]] for i in range(n-1))\n\n    # Dynamic operator blending (70% 4-opt, 30% guided inversion)\n    if random.random() < 0.7 or obj1_improvement > obj2_improvement:\n        # 4-opt for first objective\n        i, j, k, l = sorted(random.sample(range(n), 4))\n        segment1 = new_solution[i:j]\n        segment2 = new_solution[j:k]\n        segment3 = new_solution[k:l]\n        new_solution = np.concatenate([new_solution[:i], segment2, segment1, segment3, new_solution[l:]])\n    else:\n        # Guided segment inversion for second objective\n        i, j = sorted(random.sample(range(n), 2))\n        if obj2_improvement > 0:\n            new_solution[i:j] = new_solution[i:j][::-1]\n        else:\n            # Alternative move if inversion doesn't help\n            k = random.randint(i, j-1)\n            new_solution[i:j] = np.concatenate([new_solution[k:j], new_solution[i:k]])\n\n    # Ensure feasibility\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for i in range(n):\n            if new_solution[i] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[i]) > 1:\n                new_solution[i] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for `select_neighbor` with a focus on elite selection and dynamic operator blending:\n\n- **Keywords**: *Adaptive, Hybrid, Pareto-Efficient, Novel Operators*\n- **Advice**: Use **7-opt + adaptive segment inversion** (weighted 85-15) on **top 5% non-dominated solutions** with crowding-distance filtering. Dynamically blend operators every 1 iteration (95% 7-opt, 5% inversion).\n- **Avoid**: Static operators (2-opt/3-opt), dominated solutions, or rigid selection. Overlook dynamic operator adaptation.\n- **Explanation**: The top 5% selection ensures elite parents, while 7-opt and adaptive inversion balance exploitation/exploration. Dynamic blending prevents stagnation, and crowding distance maintains Pareto diversity. Strict feasibility checks are mandatory.\n\n**Implementation**: Randomly select from top 5% non-dominated solutions, apply hybrid operator, and validate feasibility.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 305,
        "algorithm": "The algorithm selects the top 5% non-dominated solutions based on crowding distance, then applies a hybrid local search operator that blends 7-opt segment rearrangement (85% probability) and adaptive segment inversion (15% probability), ensuring feasibility through strict node validation and dynamic operator adaptation. The method prioritizes solutions with better crowding distance, dynamically selects between segment rearrangement and inversion based on potential cost improvement, and maintains feasibility by validating node uniqueness.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 5% non-dominated solutions with best crowding distance\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: -x[1])\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 20)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 7:\n        return new_solution\n\n    # Hybrid operator blending (85% 7-opt, 15% adaptive inversion)\n    if random.random() < 0.85:\n        # 7-opt segment rearrangement\n        points = sorted(random.sample(range(1, n-1), 7))\n        segments = []\n        for i in range(len(points)):\n            if i == 0:\n                segments.append(new_solution[:points[i]])\n            else:\n                segments.append(new_solution[points[i-1]:points[i]])\n        segments.append(new_solution[points[-1]:])\n\n        # Randomly permute segments\n        random.shuffle(segments)\n        new_solution = np.concatenate(segments)\n    else:\n        # Adaptive segment inversion\n        i, j = sorted(random.sample(range(n), 2))\n        segment = new_solution[i:j]\n\n        # Calculate potential improvement\n        current_cost1 = distance_matrix_1[new_solution[i-1], new_solution[i]] + distance_matrix_1[new_solution[j-1], new_solution[j]]\n        inverted_cost1 = distance_matrix_1[new_solution[i-1], segment[-1]] + distance_matrix_1[segment[0], new_solution[j]]\n\n        current_cost2 = distance_matrix_2[new_solution[i-1], new_solution[i]] + distance_matrix_2[new_solution[j-1], new_solution[j]]\n        inverted_cost2 = distance_matrix_2[new_solution[i-1], segment[-1]] + distance_matrix_2[segment[0], new_solution[j]]\n\n        if (inverted_cost1 < current_cost1) or (inverted_cost2 < current_cost2):\n            new_solution[i:j] = segment[::-1]\n        else:\n            # Alternative move if inversion doesn't help\n            k = random.randint(i, j-1)\n            new_solution[i:j] = np.concatenate([new_solution[k:j], new_solution[i:k]])\n\n    # Ensure feasibility\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for i in range(n):\n            if new_solution[i] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[i]) > 1:\n                new_solution[i] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8083838467622058,
            1.0659726858139038
        ],
        "raw_score": [
            7.575137225456828,
            7.761028706209008
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select top 5% non-dominated solutions with best crowding distance\n    dominated = set()\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and archive[i][1][0] >= archive[j][1][0] and archive[i][1][1] >= archive[j][1][1] and (archive[i][1][0] > archive[j][1][0] or archive[i][1][1] > archive[j][1][1]):\n                dominated.add(i)\n    nondominated = [archive[i] for i in range(len(archive)) if i not in dominated]\n    if not nondominated:\n        nondominated = archive\n\n    def crowding_distance(solutions):\n        if len(solutions) < 2:\n            return [float('inf')] * len(solutions)\n        objectives = np.array([s[1] for s in solutions])\n        distances = np.zeros(len(solutions))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            distances[sorted_idx[0]] = distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(solutions) - 1):\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m])\n        return distances\n\n    distances = crowding_distance(nondominated)\n    sorted_nondominated = sorted(zip(nondominated, distances), key=lambda x: -x[1])\n    selection_pool = [x[0] for x in sorted_nondominated[:max(1, len(nondominated) // 20)]]\n    selected_idx = random.randint(0, len(selection_pool) - 1)\n    base_solution = selection_pool[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    if n < 7:\n        return new_solution\n\n    # Hybrid operator blending (85% 7-opt, 15% adaptive inversion)\n    if random.random() < 0.85:\n        # 7-opt segment rearrangement\n        points = sorted(random.sample(range(1, n-1), 7))\n        segments = []\n        for i in range(len(points)):\n            if i == 0:\n                segments.append(new_solution[:points[i]])\n            else:\n                segments.append(new_solution[points[i-1]:points[i]])\n        segments.append(new_solution[points[-1]:])\n\n        # Randomly permute segments\n        random.shuffle(segments)\n        new_solution = np.concatenate(segments)\n    else:\n        # Adaptive segment inversion\n        i, j = sorted(random.sample(range(n), 2))\n        segment = new_solution[i:j]\n\n        # Calculate potential improvement\n        current_cost1 = distance_matrix_1[new_solution[i-1], new_solution[i]] + distance_matrix_1[new_solution[j-1], new_solution[j]]\n        inverted_cost1 = distance_matrix_1[new_solution[i-1], segment[-1]] + distance_matrix_1[segment[0], new_solution[j]]\n\n        current_cost2 = distance_matrix_2[new_solution[i-1], new_solution[i]] + distance_matrix_2[new_solution[j-1], new_solution[j]]\n        inverted_cost2 = distance_matrix_2[new_solution[i-1], segment[-1]] + distance_matrix_2[segment[0], new_solution[j]]\n\n        if (inverted_cost1 < current_cost1) or (inverted_cost2 < current_cost2):\n            new_solution[i:j] = segment[::-1]\n        else:\n            # Alternative move if inversion doesn't help\n            k = random.randint(i, j-1)\n            new_solution[i:j] = np.concatenate([new_solution[k:j], new_solution[i:k]])\n\n    # Ensure feasibility\n    unique_nodes = np.unique(new_solution)\n    if len(unique_nodes) != n:\n        missing_nodes = np.setdiff1d(np.arange(n), unique_nodes)\n        for i in range(n):\n            if new_solution[i] in missing_nodes:\n                continue\n            if np.sum(new_solution == new_solution[i]) > 1:\n                new_solution[i] = missing_nodes[0]\n                missing_nodes = missing_nodes[1:]\n\n    return new_solution\n\n",
        "operation": "elitist"
    }
]