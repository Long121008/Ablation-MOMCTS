[
    {
        "algorithm": "The algorithm selects a balanced solution from the archive (prioritizing those with high combined objective product) and applies a hybrid local search combining weighted item addition, adaptive swapping, and probabilistic flipping, while dynamically adjusting weights based on solution quality and ensuring feasibility through capacity checks. It emphasizes objective-aware selection (using weighted value metrics) and diversity maintenance (through probabilistic flips), creating a balance between exploration and exploitation. The key design focuses on adaptive weights, combined value metrics, and solution quality-aware operations to improve multi-objective performance.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with balanced potential for both objectives\n    archive.sort(key=lambda x: -(x[1][0] * x[1][1]))  # Sort by product of objectives\n    selected_idx = min(len(archive) // 3, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Calculate dynamic weights based on current solution quality\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    weight1 = (current_value1 / total_value1) if total_value1 > 0 else 0.5\n    weight2 = (current_value2 / total_value2) if total_value2 > 0 else 0.5\n\n    # Hybrid local search: weighted item addition\n    potential_items = np.where(new_solution == 0)[0]\n    if len(potential_items) > 0:\n        # Calculate combined value metric with adaptive weights\n        combined_values = (weight1 * value1_lst[potential_items] + weight2 * value2_lst[potential_items]) / weight_lst[potential_items]\n        top_items = potential_items[np.argsort(combined_values)[-2:]]  # Select top 2 items\n\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Adaptive swapping based on objective contributions\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) >= 2:\n        # Calculate relative importance of items\n        importance = (weight1 * value1_lst[in_solution] + weight2 * value2_lst[in_solution]) / weight_lst[in_solution]\n        sorted_items = in_solution[np.argsort(importance)]\n\n        # Swap least important with most promising excluded items\n        if len(sorted_items) >= 2:\n            least_important = sorted_items[0]\n            most_promising = np.argmax(combined_values) if len(potential_items) > 0 else -1\n\n            if most_promising != -1 and most_promising in potential_items:\n                temp_weight = current_weight - weight_lst[least_important] + weight_lst[most_promising]\n                if temp_weight <= capacity:\n                    new_solution[least_important] = 0\n                    new_solution[most_promising] = 1\n\n    # Probabilistic flip based on solution quality and diversity\n    flip_prob = 0.4 if (current_value1 + current_value2) / (total_value1 + total_value2) > 0.6 else 0.8\n    if np.random.random() < flip_prob and len(new_solution) > 0:\n        flip_idx = np.random.choice(len(new_solution))\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n        else:\n            if current_weight + weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8522696808180721,
            0.22630241513252258
        ],
        "raw_score": [
            27.121475374476667,
            27.863057367568054
        ]
    },
    {
        "algorithm": "The algorithm selects a promising non-dominated solution from the archive by prioritizing those with high potential for improvement in either objective, then applies a hybrid local search combining biased item removal/addition, balanced swaps, and probabilistic flips to generate high-quality neighbors while ensuring feasibility. The selection is guided by objective-biased probabilities, favoring either value1 or value2 improvement based on the current solution's potential, and the local search dynamically adjusts operations to explore diverse, high-potential regions of the solution space.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate objective improvement potential and select solution\n    max_value1 = np.sum(value1_lst)\n    max_value2 = np.sum(value2_lst)\n    potential_scores = []\n\n    for sol, obj in archive:\n        total_weight = np.sum(weight_lst * sol)\n        if total_weight > capacity:\n            continue\n\n        # Calculate normalized potential for each objective\n        potential1 = (max_value1 - obj[0]) / (max_value1 + 1e-8)\n        potential2 = (max_value2 - obj[1]) / (max_value2 + 1e-8)\n\n        # Combine with objective weights based on relative potential\n        combined_potential = (potential1 + potential2) * (1 + abs(potential1 - potential2))\n        potential_scores.append((sol, combined_potential))\n\n    if not potential_scores:\n        raise ValueError(\"No feasible solutions in archive\")\n\n    # Select solution with highest potential\n    potential_scores.sort(key=lambda x: -x[1])\n    base_solution = potential_scores[0][0].copy()\n\n    # Generate neighbor with novel hybrid local search\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Objective-biased item selection\n    objective_bias = random.random()\n    if objective_bias < 0.5:\n        # Focus on value1 improvement\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) > 0:\n            # Remove item with lowest value1/weight ratio\n            ratios = value1_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            idx = candidates[np.argmin(ratios)]\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n    else:\n        # Focus on value2 improvement\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) > 0:\n            # Remove item with lowest value2/weight ratio\n            ratios = value2_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            idx = candidates[np.argmin(ratios)]\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n\n    # Phase 2: Random addition with objective bias\n    if random.random() < 0.4:\n        candidates = np.where(new_solution == 0)[0]\n        if len(candidates) > 0:\n            if objective_bias < 0.5:\n                # Add item with highest value1/weight ratio\n                ratios = value1_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            else:\n                # Add item with highest value2/weight ratio\n                ratios = value2_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            idx = candidates[np.argmax(ratios)]\n            if total_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 3: Balanced swaps with diversity\n    if np.sum(new_solution) > 1 and random.random() < 0.5:\n        items = np.where(new_solution == 1)[0]\n        i, j = random.sample(list(items), 2)\n        # Check if swap maintains feasibility\n        if total_weight - weight_lst[i] + weight_lst[j] <= capacity:\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Phase 4: Probabilistic flip with objective bias\n    if random.random() < 0.3:\n        idx = random.randint(0, len(new_solution) - 1)\n        if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n        elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9296926897978155,
            0.3622717261314392
        ],
        "raw_score": [
            27.831550612136972,
            28.052400795200953
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive based on its potential for improvement in both objectives, then applies a hybrid local search that prioritizes high-value-to-weight items, uses biased removal/addition, performs balanced swaps, and includes probabilistic flips\u2014with increased probabilities for swaps and flips to explore diverse regions while maintaining feasibility. It focuses on improving either objective 1 or 2 with a 40% bias, and uses value-to-weight ratios to guide item selection, ensuring the neighbor solution stays within capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate objective improvement potential and select solution\n    max_value1 = np.sum(value1_lst)\n    max_value2 = np.sum(value2_lst)\n    potential_scores = []\n\n    for sol, obj in archive:\n        total_weight = np.sum(weight_lst * sol)\n        if total_weight > capacity:\n            continue\n\n        # Calculate normalized potential for each objective\n        potential1 = (max_value1 - obj[0]) / (max_value1 + 1e-8)\n        potential2 = (max_value2 - obj[1]) / (max_value2 + 1e-8)\n\n        # Combine with objective weights based on relative potential\n        combined_potential = (potential1 + potential2) * (1 + abs(potential1 - potential2))\n        potential_scores.append((sol, combined_potential))\n\n    if not potential_scores:\n        raise ValueError(\"No feasible solutions in archive\")\n\n    # Select solution with highest potential\n    potential_scores.sort(key=lambda x: -x[1])\n    base_solution = potential_scores[0][0].copy()\n\n    # Generate neighbor with novel hybrid local search\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Objective-biased item selection with higher value-to-weight focus\n    objective_bias = random.random()\n    if objective_bias < 0.4:\n        # Focus on value1 improvement with higher value-to-weight ratio\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) > 0:\n            ratios = value1_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            idx = candidates[np.argmax(ratios)]  # Changed to max instead of min\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n    else:\n        # Focus on value2 improvement with higher value-to-weight ratio\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) > 0:\n            ratios = value2_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            idx = candidates[np.argmax(ratios)]  # Changed to max instead of min\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n\n    # Phase 2: Random addition with objective bias and higher probability\n    if random.random() < 0.5:  # Increased from 0.4\n        candidates = np.where(new_solution == 0)[0]\n        if len(candidates) > 0:\n            if objective_bias < 0.4:\n                ratios = value1_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            else:\n                ratios = value2_lst[candidates] / (weight_lst[candidates] + 1e-8)\n            idx = candidates[np.argmax(ratios)]\n            if total_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 3: Balanced swaps with higher probability and diversity focus\n    if np.sum(new_solution) > 1 and random.random() < 0.6:  # Increased from 0.5\n        items = np.where(new_solution == 1)[0]\n        i, j = random.sample(list(items), 2)\n        if total_weight - weight_lst[i] + weight_lst[j] <= capacity:\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Phase 4: Probabilistic flip with higher probability and different bias\n    if random.random() < 0.4:  # Increased from 0.3\n        idx = random.randint(0, len(new_solution) - 1)\n        if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n        elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9696108699777306,
            0.3680013418197632
        ],
        "raw_score": [
            27.531472102636563,
            27.722940598702838
        ]
    },
    {
        "algorithm": "The algorithm selects the best solution from the archive based on combined normalized objective scores, then applies a three-phase local search: first removing low-marginal items, then adding high-marginal items, and finally performing a novelty-aware probabilistic flip to potentially improve the solution further while ensuring feasibility. The method dynamically adjusts weights based on underrepresented objectives and incorporates probabilistic exploration to balance exploitation and diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined normalized objective score\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    archive.sort(key=lambda x: -((x[1][0] / total_value1) + (x[1][1] / total_value2)))\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Dynamic weights based on underrepresented objectives\n    weight1 = 1 - (current_value1 / total_value1) if total_value1 > 0 else 0.5\n    weight2 = 1 - (current_value2 / total_value2) if total_value2 > 0 else 0.5\n\n    # Phase 1: Remove low-marginal items\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 0:\n        marginal_values = (weight1 * value1_lst[in_solution] + weight2 * value2_lst[in_solution]) / weight_lst[in_solution]\n        worst_idx = in_solution[np.argmin(marginal_values)]\n        if current_weight - weight_lst[worst_idx] <= capacity:\n            new_solution[worst_idx] = 0\n\n    # Phase 2: Add high-marginal items from archive\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        marginal_values = (weight1 * value1_lst[excluded_items] + weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_idx = excluded_items[np.argmax(marginal_values)]\n        if current_weight + weight_lst[best_idx] <= capacity:\n            new_solution[best_idx] = 1\n\n    # Phase 3: Novelty-aware probabilistic flip\n    flip_prob = 0.4 + 0.4 * (1 - ((current_value1 / total_value1) + (current_value2 / total_value2)) / 2)\n    if np.random.random() < flip_prob and len(new_solution) > 0:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) > 0:\n            novelty_scores = (abs((value1_lst[candidates] / (current_value1 + 1e-6)) - (value2_lst[candidates] / (current_value2 + 1e-6))))\n            flip_idx = candidates[np.argmax(novelty_scores)]\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8205218409832766,
            0.207279771566391
        ],
        "raw_score": [
            27.117276071697262,
            27.68502992495774
        ]
    },
    {
        "algorithm": "The algorithm selects promising solutions from the archive using a hybrid metric combining crowding distance and value density, then applies a novel local search that probabilistically replaces low-efficiency items with high-potential candidates while maintaining feasibility through dynamic capacity checks. It prioritizes items based on efficiency scores calculated from weighted objective values and weights, with adaptive probabilities for replacements and flips to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine crowding distance and value density\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    value_densities = np.zeros(len(archive))\n\n    for i in range(2):\n        sorted_indices = np.argsort(objectives[:, i])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for j in range(1, len(sorted_indices)-1):\n            if objectives[sorted_indices[-1], i] == objectives[sorted_indices[0], i]:\n                continue\n            crowding_distances[sorted_indices[j]] += (objectives[sorted_indices[j+1], i] - objectives[sorted_indices[j-1], i]) / (objectives[sorted_indices[-1], i] - objectives[sorted_indices[0], i])\n\n    # Calculate value density (sum of normalized values per unit weight)\n    total_weight = np.sum(weight_lst)\n    for idx, (sol, _) in enumerate(archive):\n        sol_weight = np.sum(weight_lst * sol)\n        if sol_weight > 0:\n            value_densities[idx] = (np.sum(value1_lst * sol) + np.sum(value2_lst * sol)) / sol_weight\n        else:\n            value_densities[idx] = 0\n\n    # Combine metrics with adaptive weights\n    alpha = 0.6  # Weight for crowding distance\n    combined_scores = alpha * crowding_distances + (1 - alpha) * value_densities\n    selected_idx = np.argmin(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Dynamic objective weights based on current solution's position\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    weight1 = (1 - current_value1 / (total_value1 + 1e-6)) if total_value1 > 0 else 0.5\n    weight2 = (1 - current_value2 / (total_value2 + 1e-6)) if total_value2 > 0 else 0.5\n\n    # Novel local search: probabilistic item replacement with efficiency scoring\n    in_solution = np.where(new_solution == 1)[0]\n    out_solution = np.where(new_solution == 0)[0]\n\n    if len(in_solution) > 0 and len(out_solution) > 0:\n        # Calculate efficiency scores for items in solution\n        efficiency_scores = []\n        for i in in_solution:\n            efficiency = (weight1 * value1_lst[i] + weight2 * value2_lst[i]) / (weight_lst[i] + 1e-6)\n            efficiency_scores.append(efficiency)\n\n        # Calculate potential scores for items not in solution\n        potential_scores = []\n        for i in out_solution:\n            potential = (weight1 * value1_lst[i] + weight2 * value2_lst[i]) / (weight_lst[i] + 1e-6)\n            potential_scores.append(potential)\n\n        # Select candidate for removal (lowest efficiency)\n        worst_idx = in_solution[np.argmin(efficiency_scores)]\n        current_efficiency = efficiency_scores[np.argmin(efficiency_scores)]\n\n        # Select candidate for addition (highest potential)\n        best_add_idx = out_solution[np.argmax(potential_scores)]\n        new_efficiency = potential_scores[np.argmax(potential_scores)]\n\n        # Probabilistic replacement based on efficiency improvement\n        if new_efficiency > current_efficiency:\n            replacement_prob = min(1.0, (new_efficiency - current_efficiency) / (new_efficiency + 1e-6))\n            if np.random.random() < replacement_prob:\n                temp_weight = current_weight - weight_lst[worst_idx] + weight_lst[best_add_idx]\n                if temp_weight <= capacity:\n                    new_solution[worst_idx] = 0\n                    new_solution[best_add_idx] = 1\n                    current_weight = temp_weight\n\n    # Additional probabilistic flip with adaptive probability\n    flip_prob = 0.3 + 0.5 * (crowding_distances[selected_idx] / (np.max(crowding_distances) + 1e-6))\n    if np.random.random() < flip_prob and len(new_solution) > 0:\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            # Select items with highest potential improvement\n            improvement_scores = []\n            for i in flip_candidates:\n                improvement = (weight1 * value1_lst[i] + weight2 * value2_lst[i]) / (weight_lst[i] + 1e-6)\n                improvement_scores.append(improvement)\n\n            flip_idx = flip_candidates[np.argmax(improvement_scores)]\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9474684886669638,
            1.1541956067085266
        ],
        "raw_score": [
            27.28243927545612,
            27.925655257241498
        ]
    },
    {
        "algorithm": "The algorithm selects a high-quality base solution from the archive (sorted by combined objective scores) and applies a four-phase hybrid local search: (1) removing low-marginal-value items, (2) adding high-marginal-value items with novelty bias, (3) swapping items to improve objective balance, and (4) probabilistically flipping items based on feasibility and novelty. All operations ensure feasibility by checking weight constraints, with dynamic weights prioritizing underrepresented objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive dominance-aware selection\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    archive.sort(key=lambda x: -((x[1][0] / total_value1) + (x[1][1] / total_value2)))\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Dynamic weights based on underrepresented objectives\n    weight1 = 1 - (current_value1 / total_value1) if total_value1 > 0 else 0.5\n    weight2 = 1 - (current_value2 / total_value2) if total_value2 > 0 else 0.5\n\n    # Phase 1: Remove low-marginal items with Pareto-constrained exploration\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 0:\n        marginal_values = (weight1 * value1_lst[in_solution] + weight2 * value2_lst[in_solution]) / weight_lst[in_solution]\n        worst_idx = in_solution[np.argmin(marginal_values)]\n        if current_weight - weight_lst[worst_idx] <= capacity:\n            new_solution[worst_idx] = 0\n\n    # Phase 2: Add high-marginal items from archive with novelty bias\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        novelty_scores = abs((value1_lst[excluded_items] / (current_value1 + 1e-6)) - (value2_lst[excluded_items] / (current_value2 + 1e-6)))\n        marginal_values = (weight1 * value1_lst[excluded_items] + weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        combined_scores = marginal_values * novelty_scores\n        best_idx = excluded_items[np.argmax(combined_scores)]\n        if current_weight + weight_lst[best_idx] <= capacity:\n            new_solution[best_idx] = 1\n\n    # Phase 3: Objective-biased novelty swaps\n    if np.sum(new_solution) > 1 and np.random.random() < 0.5:\n        items = np.where(new_solution == 1)[0]\n        i, j = random.sample(list(items), 2)\n        novelty_diff = abs((value1_lst[i] / (current_value1 + 1e-6)) - (value2_lst[i] / (current_value2 + 1e-6))) - \\\n                       abs((value1_lst[j] / (current_value1 + 1e-6)) - (value2_lst[j] / (current_value2 + 1e-6)))\n        if (novelty_diff > 0 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Phase 4: Probabilistic feasibility-aware flips\n    flip_prob = 0.4 + 0.4 * (1 - ((current_value1 / total_value1) + (current_value2 / total_value2)) / 2)\n    if np.random.random() < flip_prob and len(new_solution) > 0:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) > 0:\n            novelty_scores = abs((value1_lst[candidates] / (current_value1 + 1e-6)) - (value2_lst[candidates] / (current_value2 + 1e-6)))\n            flip_idx = candidates[np.argmax(novelty_scores)]\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7727160107799509,
            0.23587819933891296
        ],
        "raw_score": [
            27.119469702377444,
            27.959461267434055
        ]
    },
    {
        "algorithm": "The algorithm combines adaptive objective weighting with a hybrid local search that prioritizes high-marginal items, using dynamic weights to balance objectives, and employs probabilistic item removals/additions while ensuring feasibility. It selects promising solutions based on their potential for improvement in both objectives and applies a three-phase local search (swap, remove, add) with objective-biased marginal contributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate adaptive weights for selection\n    max_value1 = np.sum(value1_lst)\n    max_value2 = np.sum(value2_lst)\n    selection_scores = []\n\n    for sol, obj in archive:\n        total_weight = np.sum(weight_lst * sol)\n        if total_weight > capacity:\n            continue\n\n        # Calculate normalized potential for each objective\n        potential1 = (max_value1 - obj[0]) / (max_value1 + 1e-8)\n        potential2 = (max_value2 - obj[1]) / (max_value2 + 1e-8)\n\n        # Combine with dynamic weights based on potential\n        combined_score = (potential1 + potential2) * (1 + abs(potential1 - potential2))\n        selection_scores.append((sol, combined_score))\n\n    if not selection_scores:\n        raise ValueError(\"No feasible solutions in archive\")\n\n    # Select solution with highest combined score\n    selection_scores.sort(key=lambda x: -x[1])\n    base_solution = selection_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate adaptive weights for objectives\n    total_value1 = np.sum(value1_lst * new_solution)\n    total_value2 = np.sum(value2_lst * new_solution)\n    weight1 = total_value1 / (total_value1 + total_value2 + 1e-8)\n    weight2 = 1 - weight1\n\n    # Phase 1: Replace low-marginal items with high-marginal ones\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Calculate marginal contributions\n        in_marginals = (weight1 * value1_lst[in_items] + weight2 * value2_lst[in_items]) / (weight_lst[in_items] + 1e-8)\n        out_marginals = (weight1 * value1_lst[out_items] + weight2 * value2_lst[out_items]) / (weight_lst[out_items] + 1e-8)\n\n        # Find worst in-item and best out-item\n        worst_in = in_items[np.argmin(in_marginals)]\n        best_out = out_items[np.argmax(out_marginals)]\n\n        # Check if swap maintains feasibility\n        if weight_lst[best_out] <= weight_lst[worst_in] + (capacity - current_weight):\n            new_solution[worst_in] = 0\n            new_solution[best_out] = 1\n            current_weight = current_weight - weight_lst[worst_in] + weight_lst[best_out]\n\n    # Phase 2: Probabilistic item removal with objective bias\n    if np.random.random() < 0.4 and len(in_items) > 0:\n        # Remove item with lowest weighted marginal contribution\n        in_marginals = (weight1 * value1_lst[in_items] + weight2 * value2_lst[in_items]) / (weight_lst[in_items] + 1e-8)\n        remove_idx = in_items[np.argmin(in_marginals)]\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Phase 3: Targeted item addition with objective bias\n    if np.random.random() < 0.5 and current_weight < capacity:\n        out_items = np.where(new_solution == 0)[0]\n        if len(out_items) > 0:\n            # Add item with highest weighted marginal contribution\n            out_marginals = (weight1 * value1_lst[out_items] + weight2 * value2_lst[out_items]) / (weight_lst[out_items] + 1e-8)\n            add_idx = out_items[np.argmax(out_marginals)]\n            if current_weight + weight_lst[add_idx] <= capacity:\n                new_solution[add_idx] = 1\n                current_weight += weight_lst[add_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9350819388673721,
            0.41082093119621277
        ],
        "raw_score": [
            27.61992255945934,
            27.960503075216337
        ]
    },
    {
        "algorithm": "The algorithm employs a dynamic Pareto-aware selection mechanism that prioritizes solutions with high crowding distances and underrepresented objective combinations, followed by a hybrid local search combining objective-biased swaps and probabilistic feasibility-aware operations. It dynamically adjusts objective weights based on dominance and marginal improvements, ensuring feasibility through capacity-constrained item additions/removals, and uses a novelty swap operator to drive multi-objective progress while maintaining diversity. The selection prioritizes solutions with lower dominance counts and higher crowding distances, while the local search focuses on improving underrepresented objectives through weighted marginal contributions and probabilistic flips.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate Pareto dominance and crowding distances\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n    dominance_count = np.zeros(n_solutions)\n    crowding_distances = np.zeros(n_solutions)\n\n    # Compute dominance counts and crowding distances\n    for i in range(n_solutions):\n        for j in range(n_solutions):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] > objectives[j, 1]) or \\\n                   (objectives[i, 0] > objectives[j, 0] and objectives[i, 1] >= objectives[j, 1]):\n                    dominance_count[i] += 1\n\n    # Normalize objectives for crowding distance calculation\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n    for m in range(2):  # For each objective\n        sorted_indices = np.argsort(normalized_obj[:, m])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for j in range(1, len(sorted_indices)-1):\n            crowding_distances[sorted_indices[j]] += (normalized_obj[sorted_indices[j+1], m] - normalized_obj[sorted_indices[j-1], m])\n\n    # Combine dominance and crowding for selection\n    selection_scores = dominance_count + crowding_distances\n    selected_idx = np.argmin(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Dynamic objective weights based on solution dominance\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    weight1 = (1 - current_value1 / total_value1) if total_value1 > 0 else 0.5\n    weight2 = (1 - current_value2 / total_value2) if total_value2 > 0 else 0.5\n\n    # Novel objective-biased novelty swap\n    in_solution = np.where(new_solution == 1)[0]\n    out_solution = np.where(new_solution == 0)[0]\n\n    if len(in_solution) > 0 and len(out_solution) > 0:\n        # Calculate marginal contributions for items in solution\n        marginal_contributions = []\n        for i in in_solution:\n            ratio1 = value1_lst[i] / (current_weight + 1e-6)\n            ratio2 = value2_lst[i] / (current_weight + 1e-6)\n            marginal_contributions.append(weight1 * ratio1 + weight2 * ratio2)\n\n        # Select item with lowest marginal contribution for potential removal\n        worst_idx = in_solution[np.argmin(marginal_contributions)]\n\n        # Calculate potential additions with weighted marginal contribution\n        combined_marginals = (weight1 * value1_lst[out_solution] + weight2 * value2_lst[out_solution]) / (weight_lst[out_solution] + 1e-6)\n        best_add_idx = out_solution[np.argmax(combined_marginals)]\n\n        # Perform swap if feasible\n        temp_weight = current_weight - weight_lst[worst_idx] + weight_lst[best_add_idx]\n        if temp_weight <= capacity:\n            new_solution[worst_idx] = 0\n            new_solution[best_add_idx] = 1\n            current_weight = temp_weight\n\n    # Probabilistic feasibility-aware flip\n    flip_prob = 0.3 + 0.5 * (crowding_distances[selected_idx] / (np.max(crowding_distances) + 1e-6))\n    if np.random.random() < flip_prob and len(new_solution) > 0:\n        # Select items with high potential for improving underrepresented objectives\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            # Calculate objective biases\n            obj1_bias = (total_value1 - current_value1) / (total_value1 + 1e-6)\n            obj2_bias = (total_value2 - current_value2) / (total_value2 + 1e-6)\n\n            # Weighted selection based on objective biases\n            weights = np.zeros(len(flip_candidates))\n            for i, idx in enumerate(flip_candidates):\n                weights[i] = obj1_bias * value1_lst[idx] + obj2_bias * value2_lst[idx]\n\n            flip_idx = flip_candidates[np.argmax(weights)]\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n\n    # Ensure feasibility by removing least valuable items if needed\n    while current_weight > capacity and len(in_solution) > 0:\n        remove_idx = in_solution[np.argmin(weight1 * value1_lst[in_solution] + weight2 * value2_lst[in_solution])]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n        in_solution = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9430602622118498,
            1.0650815963745117
        ],
        "raw_score": [
            27.254307369453045,
            27.94107969160529
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined normalized potential\n    max_value1 = np.sum(value1_lst)\n    max_value2 = np.sum(value2_lst)\n    potential_scores = []\n\n    for sol, obj in archive:\n        total_weight = np.sum(weight_lst * sol)\n        if total_weight > capacity:\n            continue\n\n        # Calculate normalized potential for each objective\n        potential1 = (max_value1 - obj[0]) / (max_value1 + 1e-8)\n        potential2 = (max_value2 - obj[1]) / (max_value2 + 1e-8)\n\n        # Combine with adaptive weights based on relative potential\n        combined_potential = (potential1 + potential2) * (1 + abs(potential1 - potential2))\n        potential_scores.append((sol, combined_potential))\n\n    if not potential_scores:\n        raise ValueError(\"No feasible solutions in archive\")\n\n    potential_scores.sort(key=lambda x: -x[1])\n    base_solution = potential_scores[0][0].copy()\n\n    # Generate neighbor with adaptive local search\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Marginal item replacement\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Calculate marginal contributions\n        in_marginals = (value1_lst[in_items] + value2_lst[in_items]) / weight_lst[in_items]\n        out_marginals = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n\n        # Replace worst in-item with best out-item if feasible\n        worst_in = in_items[np.argmin(in_marginals)]\n        best_out = out_items[np.argmax(out_marginals)]\n\n        if total_weight - weight_lst[worst_in] + weight_lst[best_out] <= capacity:\n            new_solution[worst_in] = 0\n            new_solution[best_out] = 1\n            total_weight = total_weight - weight_lst[worst_in] + weight_lst[best_out]\n\n    # Phase 2: Objective-biased swaps\n    if len(in_items) >= 2 and random.random() < 0.6:\n        # Calculate objective-specific marginals\n        value1_marginals = value1_lst[in_items] / weight_lst[in_items]\n        value2_marginals = value2_lst[in_items] / weight_lst[in_items]\n\n        # Select items with low marginals in one objective\n        if random.random() < 0.5:\n            # Focus on value1 improvement\n            low_value1 = in_items[np.argsort(value1_marginals)[:2]]\n        else:\n            # Focus on value2 improvement\n            low_value1 = in_items[np.argsort(value2_marginals)[:2]]\n\n        # Find best out-item for swap\n        if len(out_items) > 0:\n            out_marginals = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n            best_out = out_items[np.argmax(out_marginals)]\n\n            # Perform swap if feasible\n            if total_weight - weight_lst[low_value1[0]] + weight_lst[best_out] <= capacity:\n                new_solution[low_value1[0]] = 0\n                new_solution[best_out] = 1\n                total_weight = total_weight - weight_lst[low_value1[0]] + weight_lst[best_out]\n\n    # Phase 3: Probabilistic flips with objective bias\n    flip_prob = 0.3 if (np.sum(value1_lst * new_solution) + np.sum(value2_lst * new_solution)) / (max_value1 + max_value2) > 0.7 else 0.5\n    if random.random() < flip_prob:\n        if random.random() < 0.5 and len(in_items) > 0:\n            # Remove low-marginal item\n            marginals = (value1_lst[in_items] + value2_lst[in_items]) / weight_lst[in_items]\n            idx = in_items[np.argmin(marginals)]\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n        elif len(out_items) > 0:\n            # Add high-marginal item\n            marginals = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n            idx = out_items[np.argmax(marginals)]\n            if total_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9019376134198027,
            0.39216312766075134
        ],
        "raw_score": [
            27.423485354036217,
            27.702511738711678
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with balanced potential for both objectives\n    archive.sort(key=lambda x: -(x[1][0] + x[1][1]))\n    selected_idx = min(len(archive) // 2, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current weight and values\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Calculate adaptive weights for objectives\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    weight1 = current_value1 / total_value1 if total_value1 > 0 else 0.5\n    weight2 = current_value2 / total_value2 if total_value2 > 0 else 0.5\n\n    # Identify promising items to consider\n    potential_items = np.where(new_solution == 0)[0]\n    if len(potential_items) > 0:\n        # Calculate weighted values for potential items\n        weighted_values = weight1 * value1_lst[potential_items] + weight2 * value2_lst[potential_items]\n        # Select top 3 items with highest weighted value\n        top_items = potential_items[np.argsort(weighted_values)[-3:]]\n\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Perform targeted swapping with items in solution\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) >= 2:\n        # Select two items to swap based on their contribution to both objectives\n        swap_candidates = sorted(in_solution, key=lambda x: -(weight1 * value1_lst[x] + weight2 * value2_lst[x]))[:2]\n        if len(swap_candidates) == 2:\n            idx1, idx2 = swap_candidates\n            temp_weight = current_weight - weight_lst[idx1] + weight_lst[idx2]\n            if temp_weight <= capacity:\n                new_solution[idx1], new_solution[idx2] = new_solution[idx2], new_solution[idx1]\n\n    # Perform random flip with probability based on current solution quality\n    flip_prob = 0.3 if (current_value1 + current_value2) / (total_value1 + total_value2) > 0.7 else 0.7\n    if np.random.random() < flip_prob and len(new_solution) > 0:\n        flip_idx = np.random.choice(len(new_solution))\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n        else:\n            if current_weight + weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8179565713407433,
            0.27593129873275757
        ],
        "raw_score": [
            27.604553890686965,
            28.184297658157398
        ]
    }
]