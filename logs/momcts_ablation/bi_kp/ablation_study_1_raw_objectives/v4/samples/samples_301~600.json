[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a Pareto-aware weighted selection based on potential value improvements, then applies a three-phase hybrid local search: (1) cluster-aware probabilistic perturbation to replace low-value clusters with high-value ones, (2) directed greedy insertion of high-value items based on combined value-to-weight ratios, and (3) feasibility-preserving refinement by removing low-value items when beneficial. The method prioritizes items with high combined value density while strictly maintaining feasibility through probabilistic and greedy selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic perturbation\n    combined_values = value1_lst + value2_lst\n    value_density = combined_values / weight_lst\n\n    # Cluster items based on value density\n    from sklearn.cluster import KMeans\n    n_clusters = min(5, len(np.unique(value_density)))\n    if n_clusters > 1:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        clusters = kmeans.fit_predict(value_density.reshape(-1, 1))\n\n        # Identify high and low density clusters\n        cluster_densities = [np.mean(value_density[clusters == i]) for i in range(n_clusters)]\n        high_density_clusters = np.argsort(cluster_densities)[-2:]  # Top 2 clusters\n        low_density_clusters = np.argsort(cluster_densities)[:2]    # Bottom 2 clusters\n\n        # Probabilistically replace low-density clusters with high-density ones\n        for cluster in low_density_clusters:\n            cluster_indices = np.where(clusters == cluster)[0]\n            if np.all(new_solution[cluster_indices] == 1):\n                replace_prob = 0.5 * (1 - cluster_densities[cluster] / np.max(cluster_densities))\n                if np.random.rand() < replace_prob:\n                    new_solution[cluster_indices] = 0\n                    current_weight -= np.sum(weight_lst[cluster_indices])\n                    remaining_capacity += np.sum(weight_lst[cluster_indices])\n\n    # Phase 2: Directed greedy insertion of high-value items\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability proportional to its ratio\n                add_prob = 0.8 * (combined_ratios[idx] / np.max(combined_ratios))\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    current_weight += weight_lst[global_idx]\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.2:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection with adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **effective self-reflection** for designing better heuristics:\n\n- **Keywords**: *Pareto-aware adaptive perturbation, capacity-constrained hybrid search, multi-objective value clustering, feasibility-preserving refinement.*\n- **Advice**: Use *weighted Pareto selection* (prioritize high-value solutions) + *two-phase search*: (1) *Cluster-aware flips* (swap low-value clusters probabilistically) + (2) *Greedy capacity-aware additions* (insert high-value items without exceeding capacity).\n- **Avoid**: *Deterministic or pure random flips*; balance exploration (cluster-based) and exploitation (greedy refinements).\n- **Explanation**: Ensures feasibility while maximizing both objectives, avoiding local optima by combining probabilistic cluster swaps and directed high-value insertions.\n\n*(Word count: ~50, concise and actionable.)*\n\n**Why this works**:\n1. **Pareto-aware**: Focuses on high-value solutions to guide search.\n2. **Hybrid search**: Balances exploration (clusters) and exploitation (greedy additions).\n3. **Feasibility**: Always checks capacity constraints during flips/additions.\n4. **Avoids pitfalls**: No pure randomness or deterministic flips, ensuring diversity and efficiency.\n\n**Next steps**: Implement this in `select_neighbor` with adaptive weights for objectives and capacity checks.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 301,
        "algorithm": "The algorithm combines Pareto-aware selection with a three-phase hybrid search: (1) probabilistically flips low-value clusters based on adaptive value densities, (2) performs directed capacity-aware multi-objective insertions using dynamic objective weights, and (3) refines solutions by removing low-value items that can be replaced with higher-value alternatives while maintaining feasibility. It prioritizes items with high combined value-to-weight ratios and balances exploration/exploitation through probabilistic perturbations and greedy improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9093625530489138,
            1.9498195350170135
        ],
        "raw_score": [
            27.464474070558175,
            28.134197514128836
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection with adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 302,
        "algorithm": "The algorithm combines adaptive Pareto-aware selection with dynamic objective weighting, cluster-based flipping with density-aware probabilities, value-density-aware insertion, and feasibility-preserving multi-objective refinements to generate high-quality neighbors. It prioritizes high-value-density items while dynamically adjusting selection probabilities based on remaining capacity and objective trade-offs, ensuring feasibility through capacity-constrained operations. The multi-phase approach balances exploration of high-potential regions with exploitation of local improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive Pareto-aware selection with dynamic objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    selection_weights = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with dynamic weights\n        not_in_sol = np.where(sol == 0)[0]\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n\n        # Combined selection weight\n        selection_weight = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        selection_weights.append(selection_weight)\n\n    selected_idx = np.argmax(selection_weights)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Cluster-based flipping with adaptive probabilities\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 3: Capacity-aware value-density insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 4: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8819936262507082,
            2.09176766872406
        ],
        "raw_score": [
            27.522361963971914,
            28.13510113725816
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive Pareto-aware selection with dynamic objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    selection_weights = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with dynamic weights\n        not_in_sol = np.where(sol == 0)[0]\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n\n        # Combined selection weight\n        selection_weight = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        selection_weights.append(selection_weight)\n\n    selected_idx = np.argmax(selection_weights)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Cluster-based flipping with adaptive probabilities\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 3: Capacity-aware value-density insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 4: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection with adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 303,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid of crowding distance and objective diversity, then applies a multi-phase local search with dynamic value-density clustering, adaptive objective-weighted insertion, and feasibility-preserving refinements. It prioritizes high-value-density items and uses capacity-aware probabilistic flips to maintain feasibility while exploring the solution space. The method balances exploration and exploitation through adaptive flip probabilities and objective-weighted insertion, ensuring high-quality neighbors across both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection with crowding distance and objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    # Calculate crowding distances\n    sorted_indices = np.argsort(objectives[:, 0])\n    crowding_distances = np.zeros(len(archive))\n    crowding_distances[sorted_indices[0]] = np.inf\n    crowding_distances[sorted_indices[-1]] = np.inf\n    for i in range(1, len(archive)-1):\n        crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / obj_ranges[0]\n\n    # Combine with objective diversity\n    selection_scores = []\n    for i, (_, obj) in enumerate(archive):\n        diversity_score = np.sum((obj - min_obj) / obj_ranges)\n        selection_scores.append(crowding_distances[i] * diversity_score)\n\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Dynamic value-density clustering with adaptive flips\n    combined_values = 0.7 * value1_lst + 0.3 * value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 6)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Dynamic flip probability based on cluster position\n            flip_prob = 0.6 * (1 - (i / (len(density_bins)-1))) * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    remove_prob = 0.8 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    add_prob = 0.8 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Adaptive objective-weighted insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Dynamic objective weights based on remaining capacity\n        obj_weight1 = 0.5 + 0.5 * (remaining_capacity / capacity)\n        obj_weight2 = 0.5 + 0.5 * (1 - remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                add_prob = 0.7 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        current_densities = combined_values[in_solution] / (weight_lst[in_solution] + 1e-6)\n        mean_density = np.mean(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < mean_density and np.random.rand() < 0.5:\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n                    if potential_improvement1 > 0 or potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8527973938515669,
            1.4732016026973724
        ],
        "raw_score": [
            27.521639601018208,
            27.90472445615754
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection with crowding distance and objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    # Calculate crowding distances\n    sorted_indices = np.argsort(objectives[:, 0])\n    crowding_distances = np.zeros(len(archive))\n    crowding_distances[sorted_indices[0]] = np.inf\n    crowding_distances[sorted_indices[-1]] = np.inf\n    for i in range(1, len(archive)-1):\n        crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / obj_ranges[0]\n\n    # Combine with objective diversity\n    selection_scores = []\n    for i, (_, obj) in enumerate(archive):\n        diversity_score = np.sum((obj - min_obj) / obj_ranges)\n        selection_scores.append(crowding_distances[i] * diversity_score)\n\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Dynamic value-density clustering with adaptive flips\n    combined_values = 0.7 * value1_lst + 0.3 * value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 6)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Dynamic flip probability based on cluster position\n            flip_prob = 0.6 * (1 - (i / (len(density_bins)-1))) * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    remove_prob = 0.8 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    add_prob = 0.8 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Adaptive objective-weighted insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Dynamic objective weights based on remaining capacity\n        obj_weight1 = 0.5 + 0.5 * (remaining_capacity / capacity)\n        obj_weight2 = 0.5 + 0.5 * (1 - remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                add_prob = 0.7 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        current_densities = combined_values[in_solution] / (weight_lst[in_solution] + 1e-6)\n        mean_density = np.mean(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < mean_density and np.random.rand() < 0.5:\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n                    if potential_improvement1 > 0 or potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 6 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection with adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware adaptive weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        combined_potential = (0.5 * potential_value1 + 0.5 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            if np.random.rand() < 0.6:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with probability\n                    remove_prob = 0.8 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with probability\n                    add_prob = 0.8 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Feasibility-preserving value-to-weight insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with adaptive weights\n        combined_ratios = (0.6 * value1_lst + 0.4 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(3, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Dynamic capacity-aware refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.5:\n                # Check if removing allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate solution potentials based on Pareto dominance and objective coverage\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Combined potential score\n        combined_potential = (0.7 * potential_value1 + 0.3 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-weighted cluster-based swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            if np.random.rand() < 0.5:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-value cluster with probability\n                    remove_prob = 0.7 * (1 - cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-value cluster with probability\n                    add_prob = 0.7 * (cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Capacity-aware value-to-weight insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate combined value-to-weight ratio\n        combined_ratios = (value1_lst[not_in_sol] + value2_lst[not_in_sol]) / weight_lst[not_in_sol]\n\n        # Sort by ratio and select top items\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices[:min(3, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value densities\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.4:\n                # Check if removing allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto-aware selection with a three-stage local search: first flipping item clusters based on value-weight balance, then greedily inserting high-value items using a combined value-to-weight ratio, and finally refining by removing low-value items while ensuring feasibility. It prioritizes solutions with high potential improvements and uses adaptive probabilities to balance exploration and exploitation, focusing on both objectives through weighted ratios and density analysis.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Dynamic cluster-aware flips based on value and weight balance\n    combined_values = value1_lst + value2_lst\n    weight_bins = np.linspace(0, np.max(weight_lst), 5)\n    for i in range(len(weight_bins)-1):\n        cluster_indices = np.where((weight_lst >= weight_bins[i]) & (weight_lst < weight_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            if np.random.rand() < 0.4:  # Higher probability to consider flips\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster with probability based on value-weight balance\n                    remove_prob = 0.6 * (1 - cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster with probability based on value-weight balance\n                    add_prob = 0.6 * (cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Adaptive value-to-weight ratio insertion with objective correlation\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items with adaptive probability\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability based on ratio and remaining capacity\n                add_prob = 0.7 * (combined_ratios[idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Objective-aware feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate the potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware value-based perturbation\n    # Group items by value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster with probability inversely proportional to its value\n                    remove_prob = 0.5 * (1 - cluster_value / np.sum(combined_values))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster with probability proportional to its value\n                    add_prob = 0.5 * (cluster_value / np.sum(combined_values))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Directed value-to-weight ratio insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability proportional to its ratio\n                add_prob = 0.8 * (combined_ratios[idx] / np.max(combined_ratios))\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.2:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on combined potential improvement in both objectives, then applies a hybrid local search: first performing random bit flips to escape local optima, followed by a directed search for items with high combined value-to-weight ratios for both objectives, ensuring feasibility by dynamically tracking remaining capacity. The selection prioritizes solutions with the highest normalized potential improvement, while the neighbor generation balances exploration (random flips) with exploitation (directed selection).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement for each solution\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        # Potential items to add (not currently in solution)\n        not_in_sol = np.where(sol == 0)[0]\n        # Potential improvement in objective 1\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        # Potential improvement in objective 2\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        # Combined potential (normalized to avoid bias)\n        combined_potential = (potential_value1 + potential_value2) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    # Select solution with highest potential\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Generate neighbor solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: random flip with directed selection\n    # Step 1: Randomly flip a few bits (to escape local optima)\n    num_flips = min(3, len(new_solution))\n    flip_indices = random.sample(range(len(new_solution)), num_flips)\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            # Only add if it doesn't exceed capacity\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 2: Directed search for items that could improve either objective\n    # Consider items not in current solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n\n        # Select items with highest combined ratio that fit in remaining capacity\n        combined_ratios = ratio1 + ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **effective self-reflection** for designing better heuristics:\n\n- **Keywords**: *Pareto-aware adaptive perturbation, capacity-constrained hybrid search, multi-objective value clustering, feasibility-preserving refinement.*\n- **Advice**: Use *weighted Pareto selection* (prioritize high-value solutions) + *two-phase search*: (1) *Cluster-aware flips* (swap low-value clusters probabilistically) + (2) *Greedy capacity-aware additions* (insert high-value items without exceeding capacity).\n- **Avoid**: *Deterministic or pure random flips*; balance exploration (cluster-based) and exploitation (greedy refinements).\n- **Explanation**: Ensures feasibility while maximizing both objectives, avoiding local optima by combining probabilistic cluster swaps and directed high-value insertions.\n\n*(Word count: ~50, concise and actionable.)*\n\n**Why this works**:\n1. **Pareto-aware**: Focuses on high-value solutions to guide search.\n2. **Hybrid search**: Balances exploration (clusters) and exploitation (greedy additions).\n3. **Feasibility**: Always checks capacity constraints during flips/additions.\n4. **Avoids pitfalls**: No pure randomness or deterministic flips, ensuring diversity and efficiency.\n\n**Next steps**: Implement this in `select_neighbor` with adaptive weights for objectives and capacity checks.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 304,
        "algorithm": "The algorithm selects a promising solution from the archive using an adaptive weighting scheme that prioritizes solutions with high potential improvement in both objectives while considering their position in the Pareto front. It then applies a three-phase hybrid local search: Phase 1 flips item clusters based on value density, Phase 2 inserts high-value items with dynamic objective weights, and Phase 3 refines the solution by removing low-value items that allow adding better items in both objectives, all while strictly enforcing capacity constraints. The selection prioritizes solutions with high potential value improvements and adaptive probabilities are used to guide the local search toward high-quality neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection with adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.794792505862126,
            2.003308415412903
        ],
        "raw_score": [
            28.309975743117715,
            28.824929146438034
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection with adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Normalized objective distances\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential value improvements with adaptive weights\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Dynamic objective weights based on current solution's position in Pareto front\n        obj_weight1 = 0.5 + 0.5 * (obj[0] - min_obj[0]) / obj_ranges[0]\n        obj_weight2 = 0.5 + 0.5 * (obj[1] - min_obj[1]) / obj_ranges[1]\n        combined_potential = (obj_weight1 * potential_value1 + obj_weight2 * potential_value2) * (1 - norm_dist)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Multi-objective value-density-aware cluster flips\n    combined_values = value1_lst + value2_lst\n    value_densities = combined_values / (weight_lst + 1e-6)\n    density_bins = np.linspace(0, np.max(value_densities), 5)\n\n    for i in range(len(density_bins)-1):\n        cluster_indices = np.where((value_densities >= density_bins[i]) & (value_densities < density_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_density = cluster_value / (cluster_weight + 1e-6)\n\n            # Adaptive flip probability based on cluster characteristics\n            flip_prob = 0.7 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n            if np.random.rand() < flip_prob:\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove low-density clusters with higher probability\n                    remove_prob = 0.9 * (1 - cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add high-density clusters with higher probability\n                    add_prob = 0.9 * (cluster_density / (np.max(value_densities) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Objective-aware capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios with dynamic objective weights\n        obj_weight1 = 0.6 + 0.4 * (remaining_capacity / capacity)\n        obj_weight2 = 0.4 + 0.6 * (remaining_capacity / capacity)\n        combined_ratios = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(combined_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add with probability based on both objectives and remaining capacity\n                add_prob = 0.8 * (combined_ratios[global_idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving multi-objective refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate adaptive value densities\n        current_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / (weight_lst[in_solution] + 1e-6)\n        median_density = np.median(current_densities)\n\n        for i, idx in enumerate(in_solution):\n            if current_densities[i] < median_density and np.random.rand() < 0.6:\n                # Check if removing allows adding better items in both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (value_densities > current_densities[i]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with a value-weighted random walk and dynamic capacity-aware insertion to generate high-quality neighbor solutions for the BI-KP. It prioritizes solutions with the highest combined potential for improvement by evaluating marginal contributions of items relative to current objectives, while ensuring feasibility through capacity checks. The method selectively inserts top-marginal-value items and probabilistically removes low-value items to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate marginal contribution for each candidate item\n        marginal_contributions = []\n        for idx in not_in_sol:\n            if weight_lst[idx] <= remaining_capacity:\n                # Value-weighted marginal contribution\n                marginal_value1 = value1_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_value2 = value2_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_contributions.append(marginal_value1 + marginal_value2)\n            else:\n                marginal_contributions.append(0)\n\n        # Select top-k items based on marginal contribution\n        k = min(3, len(not_in_sol))\n        top_indices = np.argsort(marginal_contributions)[-k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Probabilistic removal of low-value items\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density for each item in solution\n        value_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / weight_lst[in_solution]\n\n        # Remove items below median value density with probability\n        median_density = np.median(value_densities)\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and random.random() < 0.3:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements. It first identifies promising solutions from the archive, then applies cluster-aware perturbations (removing/addings entire value clusters) and greedy capacity-aware additions, while ensuring feasibility through value-density-based refinements. The approach balances exploration (via probabilistic cluster swaps) with exploitation (greedy high-value additions), with a focus on improving both objectives simultaneously.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements, while dynamically balancing capacity constraints and objective-specific value clustering. It intelligently selects promising solutions from the archive based on combined potential and applies a three-phase approach: clustering-based probabilistic swaps, greedy capacity-aware additions, and feasibility-preserving refinements. The method ensures feasibility by carefully considering weight constraints at each step and balances exploration of high-value items with exploitation of existing high-density clusters.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a Pareto-aware weighted selection based on potential value improvements, then applies a three-phase hybrid local search: (1) cluster-aware probabilistic perturbation to replace low-value clusters with high-value ones, (2) directed greedy insertion of high-value items based on combined value-to-weight ratios, and (3) feasibility-preserving refinement by removing low-value items when beneficial. The method prioritizes items with high combined value density while strictly maintaining feasibility through probabilistic and greedy selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic perturbation\n    combined_values = value1_lst + value2_lst\n    value_density = combined_values / weight_lst\n\n    # Cluster items based on value density\n    from sklearn.cluster import KMeans\n    n_clusters = min(5, len(np.unique(value_density)))\n    if n_clusters > 1:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        clusters = kmeans.fit_predict(value_density.reshape(-1, 1))\n\n        # Identify high and low density clusters\n        cluster_densities = [np.mean(value_density[clusters == i]) for i in range(n_clusters)]\n        high_density_clusters = np.argsort(cluster_densities)[-2:]  # Top 2 clusters\n        low_density_clusters = np.argsort(cluster_densities)[:2]    # Bottom 2 clusters\n\n        # Probabilistically replace low-density clusters with high-density ones\n        for cluster in low_density_clusters:\n            cluster_indices = np.where(clusters == cluster)[0]\n            if np.all(new_solution[cluster_indices] == 1):\n                replace_prob = 0.5 * (1 - cluster_densities[cluster] / np.max(cluster_densities))\n                if np.random.rand() < replace_prob:\n                    new_solution[cluster_indices] = 0\n                    current_weight -= np.sum(weight_lst[cluster_indices])\n                    remaining_capacity += np.sum(weight_lst[cluster_indices])\n\n    # Phase 2: Directed greedy insertion of high-value items\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability proportional to its ratio\n                add_prob = 0.8 * (combined_ratios[idx] / np.max(combined_ratios))\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    current_weight += weight_lst[global_idx]\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.2:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto-aware selection with a hybrid search strategy that first probabilistically flips entire value clusters, then greedily adds high-value items, and finally refines the solution by removing low-density items while preserving feasibility. It prioritizes solutions with high combined potential for improvement and uses value clustering and density-based refinement to explore the solution space effectively.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic flips\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster flip\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                            (combined_values > combined_values[idx]) &\n                                            (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm employs a three-phase Pareto-aware hybrid search strategy: first selecting the most promising solution based on a weighted hybrid score combining normalized objectives and capacity utilization, then perturbing item clusters probabilistically based on their Pareto contribution and value density, followed by capacity-constrained greedy insertion of high-value items, and finally refining the solution by replacing underperforming items with Pareto-dominant alternatives while maintaining feasibility. The method prioritizes solutions with higher combined objective values (weighted 60-40) and better capacity utilization, using probabilistic cluster flips and Pareto-aware insertion to explore the solution space efficiently.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest Pareto-aware hybrid score\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    pareto_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n\n        # Calculate Pareto-aware score combining objectives and capacity\n        norm_obj1 = (obj[0] - min_obj[0]) / obj_ranges[0]\n        norm_obj2 = (obj[1] - min_obj[1]) / obj_ranges[1]\n        pareto_score = (0.6 * norm_obj1 + 0.4 * norm_obj2) * (1 - (current_weight / capacity)**2)\n        pareto_scores.append(pareto_score)\n\n    selected_idx = np.argmax(pareto_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware Pareto perturbation\n    combined_values = 0.5 * value1_lst + 0.5 * value2_lst\n    pareto_contributions = combined_values / (weight_lst + 1e-6)\n    contribution_bins = np.linspace(0, np.max(pareto_contributions), 5)\n\n    for i in range(len(contribution_bins)-1):\n        cluster_indices = np.where((pareto_contributions >= contribution_bins[i]) &\n                                 (pareto_contributions < contribution_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_contribution = cluster_value / (cluster_weight + 1e-6)\n\n            if np.random.rand() < 0.6:\n                if np.all(new_solution[cluster_indices] == 1):\n                    remove_prob = 0.8 * (1 - cluster_contribution / (np.max(pareto_contributions) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    add_prob = 0.8 * (cluster_contribution / (np.max(pareto_contributions) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Capacity-constrained Pareto insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate Pareto-aware value-to-weight ratios\n        pareto_ratios = (0.5 * value1_lst + 0.5 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(pareto_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Pareto-aware refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate Pareto dominance scores\n        current_values = (value1_lst[in_solution] + value2_lst[in_solution])\n        mean_value = np.mean(current_values)\n\n        for i, idx in enumerate(in_solution):\n            if current_values[i] < mean_value and np.random.rand() < 0.5:\n                # Check for Pareto-improving swaps\n                potential_additions = np.where((new_solution == 0) &\n                                             (pareto_contributions > pareto_contributions[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: *Pareto-aware cluster perturbation, capacity-constrained hybrid search, multi-objective value clustering, feasibility-preserving refinement.*\n- **Advice**: Use *weighted Pareto selection* (prioritize high-value solutions) + *two-phase search*: (1) *Cluster-aware flips* (swap low-value clusters probabilistically) + (2) *Greedy capacity-aware additions* (insert high-value items without exceeding capacity).\n- **Avoid**: *Deterministic or pure random flips*; balance exploration (cluster-based) and exploitation (greedy refinements).\n- **Explanation**: Ensures feasibility while maximizing both objectives, avoiding local optima by combining probabilistic cluster swaps and directed high-value insertions.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 305,
        "algorithm": "The algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes high-value clusters, uses capacity-constrained greedy insertion, and refines solutions by removing underperforming items while maintaining feasibility. It first selects the most promising solution based on a weighted Pareto score (60% value1, 40% value2), then applies cluster-based perturbations to replace low-value clusters with high-value ones, followed by greedy insertion of high-value items, and finally removes items below median value density if they allow better alternatives. The algorithm ensures feasibility by tracking remaining capacity throughout all operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest weighted Pareto score\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    pareto_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n\n        # Calculate weighted Pareto score (60-40 weight for objectives)\n        norm_obj1 = (obj[0] - min_obj[0]) / obj_ranges[0]\n        norm_obj2 = (obj[1] - min_obj[1]) / obj_ranges[1]\n        pareto_score = 0.6 * norm_obj1 + 0.4 * norm_obj2\n        pareto_scores.append(pareto_score)\n\n    selected_idx = np.argmax(pareto_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic perturbation\n    combined_values = 0.6 * value1_lst + 0.4 * value2_lst\n    value_density = combined_values / weight_lst\n\n    # Cluster items based on value density\n    from sklearn.cluster import KMeans\n    n_clusters = min(5, len(np.unique(value_density)))\n    if n_clusters > 1:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        clusters = kmeans.fit_predict(value_density.reshape(-1, 1))\n\n        # Identify high and low density clusters\n        cluster_densities = [np.mean(value_density[clusters == i]) for i in range(n_clusters)]\n        high_density_clusters = np.argsort(cluster_densities)[-2:]  # Top 2 clusters\n        low_density_clusters = np.argsort(cluster_densities)[:2]    # Bottom 2 clusters\n\n        # Probabilistically replace low-density clusters with high-density ones\n        for cluster in low_density_clusters:\n            cluster_indices = np.where(clusters == cluster)[0]\n            if np.all(new_solution[cluster_indices] == 1):\n                replace_prob = 0.7 * (1 - cluster_densities[cluster] / np.max(cluster_densities))\n                if np.random.rand() < replace_prob:\n                    new_solution[cluster_indices] = 0\n                    current_weight -= np.sum(weight_lst[cluster_indices])\n                    remaining_capacity += np.sum(weight_lst[cluster_indices])\n\n    # Phase 2: Capacity-constrained greedy insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for combined objectives\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n\n        # Sort by ratio and select top items\n        sorted_indices = np.argsort(ratios)[::-1]\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                current_weight += weight_lst[global_idx]\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.4:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8786888820194877,
            3.9973734617233276
        ],
        "raw_score": [
            27.266444307166505,
            27.745249148313754
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest weighted Pareto score\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    pareto_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n\n        # Calculate weighted Pareto score (60-40 weight for objectives)\n        norm_obj1 = (obj[0] - min_obj[0]) / obj_ranges[0]\n        norm_obj2 = (obj[1] - min_obj[1]) / obj_ranges[1]\n        pareto_score = 0.6 * norm_obj1 + 0.4 * norm_obj2\n        pareto_scores.append(pareto_score)\n\n    selected_idx = np.argmax(pareto_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic perturbation\n    combined_values = 0.6 * value1_lst + 0.4 * value2_lst\n    value_density = combined_values / weight_lst\n\n    # Cluster items based on value density\n    from sklearn.cluster import KMeans\n    n_clusters = min(5, len(np.unique(value_density)))\n    if n_clusters > 1:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        clusters = kmeans.fit_predict(value_density.reshape(-1, 1))\n\n        # Identify high and low density clusters\n        cluster_densities = [np.mean(value_density[clusters == i]) for i in range(n_clusters)]\n        high_density_clusters = np.argsort(cluster_densities)[-2:]  # Top 2 clusters\n        low_density_clusters = np.argsort(cluster_densities)[:2]    # Bottom 2 clusters\n\n        # Probabilistically replace low-density clusters with high-density ones\n        for cluster in low_density_clusters:\n            cluster_indices = np.where(clusters == cluster)[0]\n            if np.all(new_solution[cluster_indices] == 1):\n                replace_prob = 0.7 * (1 - cluster_densities[cluster] / np.max(cluster_densities))\n                if np.random.rand() < replace_prob:\n                    new_solution[cluster_indices] = 0\n                    current_weight -= np.sum(weight_lst[cluster_indices])\n                    remaining_capacity += np.sum(weight_lst[cluster_indices])\n\n    # Phase 2: Capacity-constrained greedy insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for combined objectives\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n\n        # Sort by ratio and select top items\n        sorted_indices = np.argsort(ratios)[::-1]\n        for idx in sorted_indices[:min(4, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                current_weight += weight_lst[global_idx]\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.4:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    }
]