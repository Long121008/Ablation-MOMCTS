[
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and capacity utilization\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Find best swap candidates\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        if (value1_lst[out_idx] + value2_lst[out_idx]) > (value1_lst[in_idx] + value2_lst[in_idx]):\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items\n            if random.random() < 0.15 * (1 - (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))):\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items\n            if weight_lst[i] <= remaining_capacity:\n                if random.random() < 0.15 * (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst)):\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.971754987227483,
            3.0680152773857117
        ],
        "raw_score": [
            27.532413913689474,
            28.212249960363902
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and normalized Pareto dominance\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize objectives\n    norm_value1 = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    norm_value2 = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score (Pareto-guided)\n    scores = 0.5 * norm_value1 + 0.5 * norm_value2\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-aware cluster flips\n    # Group items by value ranges and flip clusters probabilistically\n    value_bins = np.linspace(0, np.max(value1_lst + value2_lst), 5)\n    value_centers = (value_bins[:-1] + value_bins[1:]) / 2\n    item_values = value1_lst + value2_lst\n\n    for i in range(len(value_centers)):\n        cluster_indices = np.where((item_values >= value_bins[i]) & (item_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(item_values[cluster_indices])\n            flip_prob = 0.3 if np.random.rand() < 0.5 else 0.1  # Higher probability for some clusters\n\n            if np.random.rand() < flip_prob:\n                # Flip entire cluster if it fits\n                if np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    new_solution[cluster_indices] = 1\n                    remaining_capacity -= cluster_weight\n                elif np.all(new_solution[cluster_indices] == 1):\n                    new_solution[cluster_indices] = 0\n                    remaining_capacity += cluster_weight\n\n    # Phase 2: Capacity-aware sequential additions\n    # Add items in order of decreasing value-to-weight ratio, with probabilistic skip\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)[::-1]\n\n    for i in sorted_indices:\n        if new_solution[i] == 0 and weight_lst[i] <= remaining_capacity:\n            # Add with probability based on value\n            add_prob = 0.8 * (item_values[i] / np.max(item_values)) ** 2\n            if np.random.rand() < add_prob:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Phase 3: Feasibility-preserving swaps\n    # Swap low-value items with high-value items when beneficial\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        low_value_included = included[np.argsort(item_values[included])[:len(included)//2]]\n        high_value_excluded = excluded[np.argsort(item_values[excluded])[::-1][:len(excluded)//2]]\n\n        for in_idx in low_value_included:\n            for out_idx in high_value_excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity and item_values[out_idx] > item_values[in_idx]:\n                        new_solution[in_idx] = 0\n                        new_solution[out_idx] = 1\n                        current_weight = new_weight\n                        remaining_capacity = capacity - current_weight\n                        break\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9225156853153487,
            0.8376076817512512
        ],
        "raw_score": [
            27.39273147521852,
            28.34011453450967
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive by prioritizing under-explored regions in the objective space, then generates a neighbor through adaptive bit flips (with higher probability for high-value items) and objective-aware ratio-based additions, while dynamically balancing capacity constraints. Objective 1 is weighted more heavily (0.7) than Objective 2 (0.3) in both selection and neighbor generation. The method combines exploration (underexplored regions), exploitation (high-value items), and diversity (remaining capacity) to guide the local search.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate exploration potential based on objective space coverage\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6  # Avoid division by zero\n\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate normalized objective distances from boundaries\n        norm_dist = np.sum((obj - min_obj) / obj_ranges)\n\n        # Calculate potential improvements with objective-specific ratios\n        ratio1 = value1_lst[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        ratio2 = value2_lst[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        combined_ratios = 0.6 * ratio1 + 0.4 * ratio2  # Objective 1 weighted more\n\n        potential_value = np.sum(combined_ratios * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Combined score with adaptive weights\n        alpha = 0.6  # Weight for exploration\n        beta = 0.3  # Weight for potential\n        gamma = 0.1 # Weight for diversity\n        combined_score = (alpha * (1 - norm_dist) + beta * potential_value + gamma * len(not_in_sol)) / (1 + obj[0] + obj[1])\n        potentials.append(combined_score)\n\n    # Select solution with highest potential\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Generate neighbor solution\n    new_solution = base_solution.copy()\n\n    # Adaptive flip with objective-aware probabilities\n    flip_prob = 0.4  # Higher base probability\n    for i in range(len(new_solution)):\n        if random.random() < flip_prob:\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n            else:\n                # Objective-specific flip probability\n                obj_prob = 0.7 if (value1_lst[i] > np.mean(value1_lst) and value2_lst[i] > np.mean(value2_lst)) else 0.3\n                if random.random() < obj_prob and weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    # Directed search with objective-aware ratio\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        ratio1 = value1_lst[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        ratio2 = value2_lst[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        combined_ratios = 0.7 * ratio1 + 0.3 * ratio2  # Objective 1 weighted more\n\n        candidate_indices = np.argsort(combined_ratios)[-min(3, len(not_in_sol)):][::-1]\n        for idx in candidate_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9425404995471139,
            2.1861519515514374
        ],
        "raw_score": [
            27.656865748208762,
            28.14967955224577
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto dominance filtering with a value-weighted random walk and dynamic capacity-aware insertion to generate high-quality neighbor solutions for the BI-KP. It prioritizes solutions with the highest combined potential for improvement by evaluating marginal contributions of items relative to current objectives, while ensuring feasibility through capacity checks. The method selectively inserts top-marginal-value items and probabilistically removes low-value items to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate marginal contribution for each candidate item\n        marginal_contributions = []\n        for idx in not_in_sol:\n            if weight_lst[idx] <= remaining_capacity:\n                # Value-weighted marginal contribution\n                marginal_value1 = value1_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_value2 = value2_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_contributions.append(marginal_value1 + marginal_value2)\n            else:\n                marginal_contributions.append(0)\n\n        # Select top-k items based on marginal contribution\n        k = min(3, len(not_in_sol))\n        top_indices = np.argsort(marginal_contributions)[-k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Probabilistic removal of low-value items\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density for each item in solution\n        value_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / weight_lst[in_solution]\n\n        # Remove items below median value density with probability\n        median_density = np.median(value_densities)\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and random.random() < 0.3:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7288380710470508,
            0.43793824315071106
        ],
        "raw_score": [
            28.497859376231716,
            29.141616079332508
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements. It first identifies promising solutions from the archive, then applies cluster-aware perturbations (removing/addings entire value clusters) and greedy capacity-aware additions, while ensuring feasibility through value-density-based refinements. The approach balances exploration (via probabilistic cluster swaps) with exploitation (greedy high-value additions), with a focus on improving both objectives simultaneously.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9076216564059063,
            0.64542555809021
        ],
        "raw_score": [
            28.013844779263998,
            28.45696990371601
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements, while dynamically balancing capacity constraints and objective-specific value clustering. It intelligently selects promising solutions from the archive based on combined potential and applies a three-phase approach: clustering-based probabilistic swaps, greedy capacity-aware additions, and feasibility-preserving refinements. The method ensures feasibility by carefully considering weight constraints at each step and balances exploration of high-value items with exploitation of existing high-density clusters.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7578796122809431,
            0.6056255400180817
        ],
        "raw_score": [
            27.79683777677714,
            28.152085089337195
        ]
    },
    {
        "algorithm": "The algorithm combines probabilistic selection of promising solutions from the archive (based on normalized objective scores) with a two-phase local search: first applying objective-aware perturbations (flipping items with low marginal contribution or high potential) and then performing feasibility-aware greedy refinements (adding high-value items without exceeding capacity), while occasionally introducing random flips for diversification. It prioritizes items with high combined value-to-weight ratios while ensuring feasibility throughout the process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Probabilistic selection based on normalized objective scores\n    objectives = np.array([obj for _, obj in archive])\n    normalized_scores = objectives / (np.max(objectives, axis=0) + 1e-6)\n    combined_scores = np.sum(normalized_scores, axis=1)\n    selection_probs = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Two-phase local search\n    # Part A: Objective-aware perturbations\n    marginal_contrib = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    flip_prob = 0.2 * (1 - new_solution) + 0.1 * new_solution  # Higher for excluded items\n\n    for i in range(len(new_solution)):\n        if np.random.rand() < flip_prob[i]:\n            if new_solution[i] == 1:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n                    remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n                    remaining_capacity -= weight_lst[i]\n\n    # Part B: Feasibility-aware greedy refinements\n    if remaining_capacity > 0:\n        # Prioritize items with highest value-to-weight ratio\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(value_ratios)[::-1]\n\n        for i in sorted_indices:\n            if new_solution[i] == 0 and weight_lst[i] <= remaining_capacity:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n                if remaining_capacity <= 0:\n                    break\n\n    # Additional diversification\n    if np.random.rand() < 0.2:  # 20% chance to perform a random flip\n        flip_idx = np.random.choice(len(new_solution))\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n        else:\n            if weight_lst[flip_idx] <= capacity - np.sum(weight_lst * new_solution):\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.40520448053214925,
            0.556917279958725
        ],
        "raw_score": [
            52.31080247199637,
            51.89192637766801
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid scoring mechanism that prioritizes high-value items, capacity utilization, and diversity, then applies a multi-phase local search: Phase 1 adds high-value items, Phase 2 performs capacity-aware swaps with a value improvement threshold, and Phase 3 uses adaptive probabilistic diversification to balance exploration and exploitation. The hybrid scoring gives higher weight to value objectives when solutions are above median quality, while diversity is always considered.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate hybrid selection scores combining objectives, capacity, and diversity\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Calculate diversity scores (distance to other solutions)\n    diversity_scores = []\n    for i, (sol, _) in enumerate(archive):\n        distances = [np.sum(sol != s[0]) for j, (s, _) in enumerate(archive) if j != i]\n        diversity_scores.append(np.mean(distances) if distances else 0)\n    diversity_scores = np.array(diversity_scores)\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n    diversity_scores = diversity_scores / (np.max(diversity_scores) + 1e-8)\n\n    # Combined selection score with adaptive weights\n    total_score = total_value1 + total_value2\n    avg_score = np.mean(total_score)\n    weight1 = 0.4 if avg_score > np.median(total_score) else 0.3\n    weight2 = 0.3 if avg_score > np.median(total_score) else 0.4\n    scores = weight_scores * 0.3 + value1_scores * weight1 + value2_scores * weight2 + diversity_scores * 0.2\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios with adaptive importance\n        importance = (value1_lst + value2_lst) / (np.max(value1_lst) + np.max(value2_lst))\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) * importance[excluded] / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps with value improvement threshold\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate average value per weight for current solution\n        avg_value_per_weight = (total_value1[selected_idx] + total_value2[selected_idx]) / total_weights[selected_idx]\n\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        value_improvement = (value1_lst[out_idx] + value2_lst[out_idx]) - (value1_lst[in_idx] + value2_lst[in_idx])\n                        if value_improvement > 0.1 * avg_value_per_weight:  # Only accept if significant improvement\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Adaptive probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items with adaptive threshold\n            value_ratio = (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))\n            prob_threshold = 0.2 * (1 - value_ratio) * (1 - current_weight/capacity)\n            if random.random() < prob_threshold:\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items with capacity consideration\n            if weight_lst[i] <= remaining_capacity:\n                value_ratio = (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))\n                prob_threshold = 0.2 * value_ratio * (remaining_capacity/capacity)\n                if random.random() < prob_threshold:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9383033652959242,
            9.323322862386703
        ],
        "raw_score": [
            27.29910286413015,
            27.850987440226127
        ]
    },
    {
        "algorithm": "The algorithm selects high-value solutions from the archive using Pareto dominance filtering and value-weighted potential scoring, then applies a hybrid search strategy with cluster-aware probabilistic flips, greedy capacity-aware additions, and feasibility-preserving refinements to generate improved neighbor solutions while ensuring weight constraints are met. It prioritizes value density and combined objective improvements, with probabilistic choices to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic flips\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster flip\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7025376968566284,
            0.6306066811084747
        ],
        "raw_score": [
            28.7091377291053,
            29.538415942261363
        ]
    },
    {
        "algorithm": "The algorithm dynamically selects a base solution from the archive by balancing Pareto dominance (60% weight) and capacity utilization (40% weight), then applies a hybrid local search combining value-weighted swaps and adaptive flipping, ensuring feasibility by prioritizing items with higher value-to-weight ratios and adjusting probabilities based on remaining capacity. The selection favors non-dominated solutions with better capacity use, while the local search intelligently explores neighbors by probabilistically swapping and flipping items, with higher probabilities for items offering better value density and tighter capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic solution selection based on Pareto dominance and capacity\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Calculate dominance scores (1 if non-dominated, 0 otherwise)\n    dominance = np.ones(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (total_value1[i] <= total_value1[j] and total_value2[i] <= total_value2[j] and\n                    (total_value1[i] < total_value1[j] or total_value2[i] < total_value2[j])):\n                    dominance[i] = 0\n                    break\n\n    # Calculate capacity utilization scores\n    capacity_scores = (capacity - total_weights) / capacity\n\n    # Combine dominance and capacity scores for selection\n    combined_scores = dominance * 0.6 + capacity_scores * 0.4\n    selected_idx = np.random.choice(len(archive), p=combined_scores/combined_scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive probabilities\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Step 1: Value-weighted probabilistic swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value densities\n        included_densities = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        excluded_densities = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n\n        # Select items to swap with probability proportional to value density\n        swap_in = np.random.choice(included, p=included_densities/included_densities.sum())\n        swap_out = np.random.choice(excluded, p=excluded_densities/excluded_densities.sum())\n\n        # Check feasibility\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        new_weight = current_weight - weight_lst[swap_in] + weight_lst[swap_out]\n\n        if new_weight <= capacity:\n            new_solution[swap_in] = 0\n            new_solution[swap_out] = 1\n\n    # Step 2: Adaptive flipping based on value and capacity\n    for i in range(len(new_solution)):\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        remaining_capacity = capacity - current_weight\n\n        if new_solution[i] == 1:\n            # Probability to remove based on value density and remaining capacity\n            flip_prob = 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (1 - remaining_capacity/capacity)\n            if random.random() < flip_prob:\n                new_solution[i] = 0\n        else:\n            # Probability to add based on value density and capacity\n            if weight_lst[i] <= remaining_capacity:\n                flip_prob = 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (remaining_capacity/capacity)\n                if random.random() < flip_prob:\n                    new_solution[i] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9309139264047235,
            3.033954620361328
        ],
        "raw_score": [
            27.608625485677514,
            28.167711895727706
        ]
    }
]