[
    {
        "algorithm": "The algorithm selects a promising solution from the archive based on combined potential improvement in both objectives, then applies a hybrid local search: first performing random bit flips to escape local optima, followed by a directed search for items with high combined value-to-weight ratios for both objectives, ensuring feasibility by dynamically tracking remaining capacity. The selection prioritizes solutions with the highest normalized potential improvement, while the neighbor generation balances exploration (random flips) with exploitation (directed selection).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement for each solution\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        # Potential items to add (not currently in solution)\n        not_in_sol = np.where(sol == 0)[0]\n        # Potential improvement in objective 1\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        # Potential improvement in objective 2\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        # Combined potential (normalized to avoid bias)\n        combined_potential = (potential_value1 + potential_value2) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    # Select solution with highest potential\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Generate neighbor solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: random flip with directed selection\n    # Step 1: Randomly flip a few bits (to escape local optima)\n    num_flips = min(3, len(new_solution))\n    flip_indices = random.sample(range(len(new_solution)), num_flips)\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            # Only add if it doesn't exceed capacity\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 2: Directed search for items that could improve either objective\n    # Consider items not in current solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n\n        # Select items with highest combined ratio that fit in remaining capacity\n        combined_ratios = ratio1 + ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9079633024147904,
            1.0491748750209808
        ],
        "raw_score": [
            27.36814387336429,
            27.95142773130081
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and capacity utilization\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Find best swap candidates\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        if (value1_lst[out_idx] + value2_lst[out_idx]) > (value1_lst[in_idx] + value2_lst[in_idx]):\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items\n            if random.random() < 0.15 * (1 - (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))):\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items\n            if weight_lst[i] <= remaining_capacity:\n                if random.random() < 0.15 * (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst)):\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.971754987227483,
            3.0680152773857117
        ],
        "raw_score": [
            27.532413913689474,
            28.212249960363902
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement scores\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score with emphasis on both objectives and capacity\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.random.choice(len(archive), p=scores/scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Novel hybrid local search: Value-aware cluster swapping with probabilistic refinement\n    # Step 1: Identify value clusters and swap between them\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 1 and len(excluded) > 1:\n        # Create value clusters based on combined value-to-weight ratios\n        included_ratios = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        excluded_ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n\n        # Sort items in each cluster by their value-to-weight ratio\n        high_included = included[np.argsort(included_ratios)[-min(3, len(included)):]]\n        low_excluded = excluded[np.argsort(excluded_ratios)[:min(3, len(excluded))]]\n\n        # Attempt swaps between high-value included and low-value excluded items\n        for hi in high_included:\n            for lo in low_excluded:\n                new_weight = current_weight - weight_lst[hi] + weight_lst[lo]\n                if new_weight <= capacity:\n                    new_solution[hi] = 0\n                    new_solution[lo] = 1\n                    current_weight = new_weight\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Step 2: Probabilistic refinement with value-aware flips and capacity-aware additions\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove based on value density and capacity pressure\n            remove_prob = 0.1 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (1 - remaining_capacity/capacity)\n            if random.random() < remove_prob:\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n        else:\n            # Probability to add based on value density and capacity availability\n            add_prob = 0.1 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (remaining_capacity/weight_lst[i])\n            if (remaining_capacity >= weight_lst[i]) and (random.random() < add_prob):\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Step 3: Greedy capacity-aware refinement\n    while True:\n        # Calculate value-to-weight ratios for items not in solution\n        not_in_sol = np.where(new_solution == 0)[0]\n        if len(not_in_sol) == 0:\n            break\n\n        # Filter items that fit in remaining capacity\n        feasible_items = not_in_sol[weight_lst[not_in_sol] <= remaining_capacity]\n        if len(feasible_items) == 0:\n            break\n\n        # Select item with highest combined value-to-weight ratio\n        combined_ratios = (value1_lst[feasible_items] + value2_lst[feasible_items]) / weight_lst[feasible_items]\n        best_item = feasible_items[np.argmax(combined_ratios)]\n\n        # Add the best item\n        new_solution[best_item] = 1\n        remaining_capacity -= weight_lst[best_item]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.43483846301394125,
            0.9316346347332001
        ],
        "raw_score": [
            43.37854733532269,
            43.24897827956235
        ]
    },
    {
        "algorithm": "The algorithm dynamically selects a base solution from the archive by balancing Pareto dominance (60% weight) and capacity utilization (40% weight), then applies a hybrid local search combining value-weighted swaps and adaptive flipping, ensuring feasibility by prioritizing items with higher value-to-weight ratios and adjusting probabilities based on remaining capacity. The selection favors non-dominated solutions with better capacity use, while the local search intelligently explores neighbors by probabilistically swapping and flipping items, with higher probabilities for items offering better value density and tighter capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic solution selection based on Pareto dominance and capacity\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Calculate dominance scores (1 if non-dominated, 0 otherwise)\n    dominance = np.ones(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (total_value1[i] <= total_value1[j] and total_value2[i] <= total_value2[j] and\n                    (total_value1[i] < total_value1[j] or total_value2[i] < total_value2[j])):\n                    dominance[i] = 0\n                    break\n\n    # Calculate capacity utilization scores\n    capacity_scores = (capacity - total_weights) / capacity\n\n    # Combine dominance and capacity scores for selection\n    combined_scores = dominance * 0.6 + capacity_scores * 0.4\n    selected_idx = np.random.choice(len(archive), p=combined_scores/combined_scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive probabilities\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Step 1: Value-weighted probabilistic swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value densities\n        included_densities = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        excluded_densities = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n\n        # Select items to swap with probability proportional to value density\n        swap_in = np.random.choice(included, p=included_densities/included_densities.sum())\n        swap_out = np.random.choice(excluded, p=excluded_densities/excluded_densities.sum())\n\n        # Check feasibility\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        new_weight = current_weight - weight_lst[swap_in] + weight_lst[swap_out]\n\n        if new_weight <= capacity:\n            new_solution[swap_in] = 0\n            new_solution[swap_out] = 1\n\n    # Step 2: Adaptive flipping based on value and capacity\n    for i in range(len(new_solution)):\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        remaining_capacity = capacity - current_weight\n\n        if new_solution[i] == 1:\n            # Probability to remove based on value density and remaining capacity\n            flip_prob = 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (1 - remaining_capacity/capacity)\n            if random.random() < flip_prob:\n                new_solution[i] = 0\n        else:\n            # Probability to add based on value density and capacity\n            if weight_lst[i] <= remaining_capacity:\n                flip_prob = 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (remaining_capacity/capacity)\n                if random.random() < flip_prob:\n                    new_solution[i] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9309139264047235,
            3.033954620361328
        ],
        "raw_score": [
            27.608625485677514,
            28.167711895727706
        ]
    },
    {
        "algorithm": "The algorithm combines probabilistic selection of promising solutions from the archive (based on normalized objective scores) with a two-phase local search: first applying objective-aware perturbations (flipping items with low marginal contribution or high potential) and then performing feasibility-aware greedy refinements (adding high-value items without exceeding capacity), while occasionally introducing random flips for diversification. It prioritizes items with high combined value-to-weight ratios while ensuring feasibility throughout the process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Probabilistic selection based on normalized objective scores\n    objectives = np.array([obj for _, obj in archive])\n    normalized_scores = objectives / (np.max(objectives, axis=0) + 1e-6)\n    combined_scores = np.sum(normalized_scores, axis=1)\n    selection_probs = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Two-phase local search\n    # Part A: Objective-aware perturbations\n    marginal_contrib = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    flip_prob = 0.2 * (1 - new_solution) + 0.1 * new_solution  # Higher for excluded items\n\n    for i in range(len(new_solution)):\n        if np.random.rand() < flip_prob[i]:\n            if new_solution[i] == 1:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n                    remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n                    remaining_capacity -= weight_lst[i]\n\n    # Part B: Feasibility-aware greedy refinements\n    if remaining_capacity > 0:\n        # Prioritize items with highest value-to-weight ratio\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(value_ratios)[::-1]\n\n        for i in sorted_indices:\n            if new_solution[i] == 0 and weight_lst[i] <= remaining_capacity:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n                if remaining_capacity <= 0:\n                    break\n\n    # Additional diversification\n    if np.random.rand() < 0.2:  # 20% chance to perform a random flip\n        flip_idx = np.random.choice(len(new_solution))\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n        else:\n            if weight_lst[flip_idx] <= capacity - np.sum(weight_lst * new_solution):\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.40520448053214925,
            0.556917279958725
        ],
        "raw_score": [
            52.31080247199637,
            51.89192637766801
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive by balancing potential objective improvements and diversity, then generates a neighbor through a hybrid local search combining probabilistic bit flips and value-to-weight ratio-based directed additions, ensuring feasibility by dynamically tracking remaining capacity. It prioritizes solutions with higher combined potential scores (70% improvement, 30% diversity) and refines the solution with adaptive flips and capacity-aware high-ratio item additions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement and diversity for each solution\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Calculate diversity (number of unique items not in solution)\n        diversity = len(not_in_sol)\n\n        # Combined score with adaptive weighting\n        alpha = 0.7  # Weight for improvement\n        beta = 0.3   # Weight for diversity\n        combined_score = (alpha * (potential_value1 + potential_value2) + beta * diversity) / (1 + obj[0] + obj[1])\n        potentials.append(combined_score)\n\n    # Select solution with highest potential\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Generate neighbor solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: probabilistic flip with adaptive intensity\n    flip_prob = 0.3  # Base flip probability\n    for i in range(len(new_solution)):\n        if random.random() < flip_prob:\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    # Directed search for items with high combined value-to-weight ratio\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate normalized value-to-weight ratios\n        ratio1 = value1_lst[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        ratio2 = value2_lst[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        combined_ratios = ratio1 + ratio2\n\n        # Select top candidates and filter by capacity\n        candidate_indices = np.argsort(combined_ratios)[-min(5, len(not_in_sol)):][::-1]\n        for idx in candidate_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8240901047809942,
            1.0695575177669525
        ],
        "raw_score": [
            27.25824576339125,
            27.480145508309896
        ]
    },
    {
        "algorithm": "The algorithm combines a weighted selection strategy (prioritizing solutions with high potential improvement based on normalized value and remaining capacity scores) with a hybrid local search that first probabilistically perturbs items based on their combined value-to-weight ratios and then greedily refines the solution by iteratively adding the most promising items until no further feasible improvements can be made. The selection emphasizes both objectives and capacity awareness, while the local search balances exploration (via probabilistic flips) with exploitation (via directed capacity-aware additions).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement scores\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.4 + value1_scores * 0.3 + value2_scores * 0.3\n    selected_idx = np.random.choice(len(archive), p=scores/scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Value-aware perturbation phase\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove based on value density\n            if random.random() < 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8):\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n        else:\n            # Probability to add based on value density\n            if (remaining_capacity >= weight_lst[i]) and \\\n               (random.random() < 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8)):\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Capacity-aware refinement phase\n    added_items = []\n    while True:\n        # Calculate value-to-weight ratios for items not in solution\n        not_in_sol = np.where(new_solution == 0)[0]\n        if len(not_in_sol) == 0:\n            break\n\n        # Filter items that fit in remaining capacity\n        feasible_items = not_in_sol[weight_lst[not_in_sol] <= remaining_capacity]\n        if len(feasible_items) == 0:\n            break\n\n        # Select item with highest combined value-to-weight ratio\n        combined_ratios = (value1_lst[feasible_items] + value2_lst[feasible_items]) / weight_lst[feasible_items]\n        best_item = feasible_items[np.argmax(combined_ratios)]\n\n        # Add the best item\n        new_solution[best_item] = 1\n        remaining_capacity -= weight_lst[best_item]\n        added_items.append(best_item)\n\n        # Check if adding more items is beneficial\n        if len(added_items) > 3:  # Limit to prevent excessive computation\n            break\n\n    return new_solution\n\n",
        "metric_score": [
            -0.3888242975453579,
            0.6292133629322052
        ],
        "raw_score": [
            36.7724156200035,
            37.5580101790531
        ]
    },
    {
        "algorithm": "The algorithm combines probabilistic selection of solutions from the archive (prioritizing those with higher combined normalized objective scores) with a two-phase local search: Phase 1 probabilistically flips items based on their combined marginal contribution to both objectives, favoring inclusion of items with high value-to-weight ratios, while Phase 2 greedily adds high-value items that fit within remaining capacity. The method balances exploration (via probabilistic perturbations) with exploitation (via greedy refinements) while ensuring feasibility through dynamic capacity tracking.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Probabilistic selection based on normalized objective scores\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    normalized_scores = objectives / (max_obj + 1e-6)  # Avoid division by zero\n    combined_scores = np.sum(normalized_scores, axis=1)\n    selection_probs = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Objective-aware probabilistic perturbations\n    marginal_contrib1 = value1_lst / (weight_lst + 1e-6)\n    marginal_contrib2 = value2_lst / (weight_lst + 1e-6)\n    combined_contrib = marginal_contrib1 + marginal_contrib2\n    flip_probs = 0.5 * (1 - base_solution) * combined_contrib + 0.1 * base_solution  # Higher prob for 0s\n    flip_probs = flip_probs / (np.max(flip_probs) + 1e-6)  # Normalize to [0,1]\n\n    for i in range(len(new_solution)):\n        if np.random.rand() < flip_probs[i]:\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    # Phase 2: Greedy refinements for high-value items\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Sort by combined value-to-weight ratio\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.38505190229076525,
            0.566772073507309
        ],
        "raw_score": [
            53.38986924694199,
            53.050468075553894
        ]
    },
    {
        "algorithm": "The algorithm intelligently selects a base solution from the archive using a weighted probability that prioritizes solutions with higher potential improvement (valuing both objective values and remaining capacity), then applies a hybrid local search combining item swaps and value-density-based probabilistic flips to generate a feasible neighbor solution. The selection emphasizes solutions with better normalized objective scores, while the local search balances exploration (random swaps) and exploitation (value-driven flips) to maintain feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a base solution with probability proportional to its potential improvement\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Calculate potential improvement scores (normalized)\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    scores = weight_scores * 0.4 + value1_scores * 0.3 + value2_scores * 0.3\n    selected_idx = np.random.choice(len(archive), p=scores/scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n\n    # Create a copy of the base solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search operator\n    # Step 1: Item swapping (swap items between included and excluded)\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Randomly select items to swap\n        swap_in = random.choice(included)\n        swap_out = random.choice(excluded)\n\n        # Check feasibility of swap\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        new_weight = current_weight - weight_lst[swap_in] + weight_lst[swap_out]\n\n        if new_weight <= capacity:\n            new_solution[swap_in] = 0\n            new_solution[swap_out] = 1\n\n    # Step 2: Probabilistic flipping (flip items with probability based on value density)\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove based on value density\n            if random.random() < 0.1 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8):\n                new_solution[i] = 0\n        else:\n            # Probability to add based on value density\n            current_weight = np.sum(weight_lst[new_solution == 1])\n            if (current_weight + weight_lst[i] <= capacity) and \\\n               (random.random() < 0.1 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8)):\n                new_solution[i] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.5807389084963747,
            1.9469241797924042
        ],
        "raw_score": [
            37.42782793116257,
            37.34662950259658
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive by balancing diversity and potential improvement, then applies a hybrid local search that combines probabilistic bit flips (with higher probability for items with lower value-to-weight ratios) and a greedy selection of items with the highest normalized combined value-to-weight ratios, ensuring feasibility through dynamic capacity tracking. The selection prioritizes solutions with higher diversity and potential improvement, while the local search strategically explores the solution space by probabilistically flipping bits and greedily adding high-value items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate diversity and potential improvement for each solution\n    diversity_scores = []\n    potential_scores = []\n    for sol, obj in archive:\n        # Diversity: count of items not in other solutions\n        diversity = sum(1 for s, _ in archive if not np.array_equal(s, sol))\n        diversity_scores.append(diversity)\n\n        # Potential improvement\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_scores.append((potential_value1 + potential_value2) / (1 + np.sum(obj)))\n\n    # Combine diversity and potential scores (weighted)\n    combined_scores = [0.7 * d + 0.3 * p for d, p in zip(diversity_scores, potential_scores)]\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Hybrid local search\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic bit flips (higher probability for worse items)\n    for i in range(len(new_solution)):\n        if random.random() < 0.3:  # Base probability\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    # Higher probability to add items with lower combined ratio\n                    ratio1 = value1_lst[i] / weight_lst[i]\n                    ratio2 = value2_lst[i] / weight_lst[i]\n                    combined_ratio = ratio1 + ratio2\n                    if random.random() < 1 / (1 + combined_ratio):  # Inverse probability\n                        new_solution[i] = 1\n                        remaining_capacity -= weight_lst[i]\n\n    # Step 2: Greedy selection of items with highest normalized combined ratio\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        normalized_ratio1 = (value1_lst[not_in_sol] / weight_lst[not_in_sol]) / (1 + np.sum(value1_lst))\n        normalized_ratio2 = (value2_lst[not_in_sol] / weight_lst[not_in_sol]) / (1 + np.sum(value2_lst))\n        combined_ratios = normalized_ratio1 + normalized_ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.3426667545408827,
            0.7310943007469177
        ],
        "raw_score": [
            52.0280329321086,
            52.01414403529617
        ]
    }
]