[
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for both objectives, then applies a novel hybrid local search strategy that combines probabilistic objective-weighted item selection, adaptive value-weighted swapping, and diversity-aware bit flipping to generate a high-quality neighbor solution while ensuring feasibility through a multi-phase approach that balances exploration and exploitation.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-weighted item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective weights based on current solution's balance\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        obj_weight1 = current_val1 / (current_val1 + current_val2 + 1e-6)\n        obj_weight2 = current_val2 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate selection probabilities for excluded items\n        selection_probs = (obj_weight1 * value1_lst[excluded_items] + obj_weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        selection_probs = selection_probs / np.sum(selection_probs)\n\n        # Select an item to add\n        add_idx = np.random.choice(excluded_items, p=selection_probs)\n\n        # Select an item to remove based on its contribution to both objectives\n        removal_scores = (obj_weight1 * value1_lst[included_items] + obj_weight2 * value2_lst[included_items]) / weight_lst[included_items]\n        remove_idx = included_items[np.argmin(removal_scores)]\n\n        # Perform swap if feasible\n        if current_weight - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n            current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Step 2: Adaptive value-weighted swapping\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value-weighted probabilities for included items\n        swap_probs = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        # Select an item to potentially swap\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n\n        # Find a suitable item to swap with from excluded items\n        potential_swaps = excluded_items[weight_lst[excluded_items] <= weight_lst[swap_idx]]\n        if len(potential_swaps) > 0:\n            # Select the best swap candidate based on combined value\n            swap_candidate = potential_swaps[np.argmax(value1_lst[potential_swaps] + value2_lst[potential_swaps])]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[swap_idx] + weight_lst[swap_candidate] <= capacity:\n                new_solution[swap_idx] = 0\n                new_solution[swap_candidate] = 1\n\n    # Step 3: Diversity-aware bit flipping\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_val1 = len(np.unique(value1_lst[included_items]))\n        unique_val2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_val1, unique_val2) / max(unique_val1, unique_val2) if max(unique_val1, unique_val2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(2 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.937480718347008,
            -18.696077869537106
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for multi-objective improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility, using a novel approach that balances exploration of the objective space with exploitation of high-potential regions.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.193564737015997,
            -18.5791289379654
        ]
    },
    {
        "algorithm": "{This algorithm enhances solution quality by combining objective-specific scoring with adaptive item selection and probabilistic swaps, using a novel hybrid of value-to-weight ratios and diversity-aware exploration to generate high-quality neighbor solutions while maintaining feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Novel objective-aware scoring and selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate novel combined scores\n        combined_scores = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace using novel scoring\n            worst_scores = (value1_lst[included_items] * 0.6 + value2_lst[included_items] * 0.4) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Enhanced adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity using novel metric\n        obj1_diversity = len(np.unique(value1_lst[included_items])) / len(included_items) if len(included_items) > 0 else 0.5\n        obj2_diversity = len(np.unique(value2_lst[included_items])) / len(included_items) if len(included_items) > 0 else 0.5\n        diversity = (obj1_diversity + obj2_diversity) / 2\n\n        # Determine exploration intensity with novel calculation\n        exploration_intensity = max(1, int(4 * (1 - diversity) * random.uniform(0.5, 1.5)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Novel objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios with novel weights\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 * 0.7 + current_val2 * 0.3 + 1e-6)\n\n        # Select items based on novel objective balance\n        if ratio > 0.6:\n            # Focus on value2 improvement with novel scoring\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] * 0.4 + 1e-6)\n        else:\n            # Focus on value1 improvement with novel scoring\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] * 0.6 + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace with novel scoring\n            worst_scores = (value1_lst[included_items] * 0.5 + value2_lst[included_items] * 0.5) / (weight_lst[included_items] * 0.8 + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.721816977107125,
            -18.82205720281772
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its dominance rank and diversity, then applies a hybrid local search strategy that combines probabilistic item swapping, value-weighted replacement, and adaptive objective-balancing exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high dominance rank and diversity\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Objective-aware item selection (select items based on their contribution to both objectives)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate normalized objective contributions\n        norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n        norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n        combined_value = norm_value1 + norm_value2\n\n        # Select items to remove based on their combined value\n        removal_probs = combined_value[included_items] / np.sum(combined_value[included_items])\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n\n        # Select items to add based on their combined value and weight\n        insertion_probs = (combined_value[excluded_items] / weight_lst[excluded_items]) / np.sum(combined_value[excluded_items] / weight_lst[excluded_items])\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n        # Perform swap if feasible\n        if current_weight - weight_lst[remove_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[remove_idx] + weight_lst[insert_idx]\n\n    # Step 2: Objective-balanced item replacement (replace items considering both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate current objective balance\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        balance_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate replacement potential for excluded items\n        replacement_potential = (value1_lst[excluded_items] * (1 - balance_ratio) + value2_lst[excluded_items] * balance_ratio) / weight_lst[excluded_items]\n        insert_idx = np.random.choice(excluded_items, p=replacement_potential / np.sum(replacement_potential))\n\n        # Find the worst included item to replace\n        worst_potential = (value1_lst[included_items] * (1 - balance_ratio) + value2_lst[included_items] * balance_ratio) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(worst_potential)]\n\n        # Perform replacement if feasible\n        if current_weight - weight_lst[worst_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[worst_idx] = 0\n            new_solution[insert_idx] = 1\n\n    # Step 3: Adaptive objective exploration (flip bits based on objective trade-offs)\n    if len(included_items) > 0:\n        # Calculate objective trade-off\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        tradeoff_ratio = current_val1 / (current_val2 + 1e-6)\n\n        # Flip bits based on trade-off (favor objective with lower current value)\n        flip_idx = random.randint(0, len(new_solution) - 1)\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n        else:\n            if current_weight + weight_lst[flip_idx] <= capacity and random.random() < (1 / (1 + tradeoff_ratio)):\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.153230877017076,
            -18.519716801527775
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for both objectives, then applies a novel hybrid local search strategy that combines probabilistic objective-weighted item selection, adaptive value-weighted swapping, and diversity-aware bit flipping to generate a high-quality neighbor solution while ensuring feasibility through a multi-phase approach that balances exploration and exploitation.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-weighted item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective weights based on current solution's balance\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        obj_weight1 = current_val1 / (current_val1 + current_val2 + 1e-6)\n        obj_weight2 = current_val2 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate selection probabilities for excluded items\n        selection_probs = (obj_weight1 * value1_lst[excluded_items] + obj_weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        selection_probs = selection_probs / np.sum(selection_probs)\n\n        # Select an item to add\n        add_idx = np.random.choice(excluded_items, p=selection_probs)\n\n        # Select an item to remove based on its contribution to both objectives\n        removal_scores = (obj_weight1 * value1_lst[included_items] + obj_weight2 * value2_lst[included_items]) / weight_lst[included_items]\n        remove_idx = included_items[np.argmin(removal_scores)]\n\n        # Perform swap if feasible\n        if current_weight - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n            current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Step 2: Adaptive value-weighted swapping\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value-weighted probabilities for included items\n        swap_probs = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        # Select an item to potentially swap\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n\n        # Find a suitable item to swap with from excluded items\n        potential_swaps = excluded_items[weight_lst[excluded_items] <= weight_lst[swap_idx]]\n        if len(potential_swaps) > 0:\n            # Select the best swap candidate based on combined value\n            swap_candidate = potential_swaps[np.argmax(value1_lst[potential_swaps] + value2_lst[potential_swaps])]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[swap_idx] + weight_lst[swap_candidate] <= capacity:\n                new_solution[swap_idx] = 0\n                new_solution[swap_candidate] = 1\n\n    # Step 3: Diversity-aware bit flipping\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_val1 = len(np.unique(value1_lst[included_items]))\n        unique_val2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_val1, unique_val2) / max(unique_val1, unique_val2) if max(unique_val1, unique_val2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(2 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.937480718347008,
            -18.696077869537106
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for multi-objective improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility, using a novel approach that balances exploration of the objective space with exploitation of high-potential regions.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.193564737015997,
            -18.5791289379654
        ]
    },
    {
        "algorithm": "{This algorithm enhances solution quality by combining objective-specific scoring with adaptive item selection and probabilistic swaps, using a novel hybrid of value-to-weight ratios and diversity-aware exploration to generate high-quality neighbor solutions while maintaining feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Novel objective-aware scoring and selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate novel combined scores\n        combined_scores = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace using novel scoring\n            worst_scores = (value1_lst[included_items] * 0.6 + value2_lst[included_items] * 0.4) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Enhanced adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity using novel metric\n        obj1_diversity = len(np.unique(value1_lst[included_items])) / len(included_items) if len(included_items) > 0 else 0.5\n        obj2_diversity = len(np.unique(value2_lst[included_items])) / len(included_items) if len(included_items) > 0 else 0.5\n        diversity = (obj1_diversity + obj2_diversity) / 2\n\n        # Determine exploration intensity with novel calculation\n        exploration_intensity = max(1, int(4 * (1 - diversity) * random.uniform(0.5, 1.5)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Novel objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios with novel weights\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 * 0.7 + current_val2 * 0.3 + 1e-6)\n\n        # Select items based on novel objective balance\n        if ratio > 0.6:\n            # Focus on value2 improvement with novel scoring\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] * 0.4 + 1e-6)\n        else:\n            # Focus on value1 improvement with novel scoring\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] * 0.6 + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace with novel scoring\n            worst_scores = (value1_lst[included_items] * 0.5 + value2_lst[included_items] * 0.5) / (weight_lst[included_items] * 0.8 + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.721816977107125,
            -18.82205720281772
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for both objectives, then applies a novel hybrid local search strategy that combines probabilistic objective-weighted item selection, adaptive value-weighted swapping, and diversity-aware bit flipping to generate a high-quality neighbor solution while ensuring feasibility through a multi-phase approach that balances exploration and exploitation.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-weighted item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective weights based on current solution's balance\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        obj_weight1 = current_val1 / (current_val1 + current_val2 + 1e-6)\n        obj_weight2 = current_val2 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate selection probabilities for excluded items\n        selection_probs = (obj_weight1 * value1_lst[excluded_items] + obj_weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        selection_probs = selection_probs / np.sum(selection_probs)\n\n        # Select an item to add\n        add_idx = np.random.choice(excluded_items, p=selection_probs)\n\n        # Select an item to remove based on its contribution to both objectives\n        removal_scores = (obj_weight1 * value1_lst[included_items] + obj_weight2 * value2_lst[included_items]) / weight_lst[included_items]\n        remove_idx = included_items[np.argmin(removal_scores)]\n\n        # Perform swap if feasible\n        if current_weight - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n            current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Step 2: Adaptive value-weighted swapping\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value-weighted probabilities for included items\n        swap_probs = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        # Select an item to potentially swap\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n\n        # Find a suitable item to swap with from excluded items\n        potential_swaps = excluded_items[weight_lst[excluded_items] <= weight_lst[swap_idx]]\n        if len(potential_swaps) > 0:\n            # Select the best swap candidate based on combined value\n            swap_candidate = potential_swaps[np.argmax(value1_lst[potential_swaps] + value2_lst[potential_swaps])]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[swap_idx] + weight_lst[swap_candidate] <= capacity:\n                new_solution[swap_idx] = 0\n                new_solution[swap_candidate] = 1\n\n    # Step 3: Diversity-aware bit flipping\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_val1 = len(np.unique(value1_lst[included_items]))\n        unique_val2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_val1, unique_val2) / max(unique_val1, unique_val2) if max(unique_val1, unique_val2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(2 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.937480718347008,
            -18.696077869537106
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for multi-objective improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility, using a novel approach that balances exploration of the objective space with exploitation of high-potential regions.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.193564737015997,
            -18.5791289379654
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm enhances solution quality by combining objective-specific scoring with adaptive item selection and probabilistic swaps, using a novel hybrid of value-to-weight ratios and diversity-aware exploration to generate high-quality neighbor solutions while maintaining feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Novel objective-aware scoring and selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate novel combined scores\n        combined_scores = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace using novel scoring\n            worst_scores = (value1_lst[included_items] * 0.6 + value2_lst[included_items] * 0.4) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Enhanced adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity using novel metric\n        obj1_diversity = len(np.unique(value1_lst[included_items])) / len(included_items) if len(included_items) > 0 else 0.5\n        obj2_diversity = len(np.unique(value2_lst[included_items])) / len(included_items) if len(included_items) > 0 else 0.5\n        diversity = (obj1_diversity + obj2_diversity) / 2\n\n        # Determine exploration intensity with novel calculation\n        exploration_intensity = max(1, int(4 * (1 - diversity) * random.uniform(0.5, 1.5)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Novel objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios with novel weights\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 * 0.7 + current_val2 * 0.3 + 1e-6)\n\n        # Select items based on novel objective balance\n        if ratio > 0.6:\n            # Focus on value2 improvement with novel scoring\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] * 0.4 + 1e-6)\n        else:\n            # Focus on value1 improvement with novel scoring\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] * 0.6 + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace with novel scoring\n            worst_scores = (value1_lst[included_items] * 0.5 + value2_lst[included_items] * 0.5) / (weight_lst[included_items] * 0.8 + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.721816977107125,
            -18.82205720281772
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for multi-objective improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility, using a novel approach that balances exploration of the objective space with exploitation of high-potential regions.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.193564737015997,
            -18.5791289379654
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for both objectives, then applies a novel hybrid local search strategy that combines probabilistic objective-weighted item selection, adaptive value-weighted swapping, and diversity-aware bit flipping to generate a high-quality neighbor solution while ensuring feasibility through a multi-phase approach that balances exploration and exploitation.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-weighted item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective weights based on current solution's balance\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        obj_weight1 = current_val1 / (current_val1 + current_val2 + 1e-6)\n        obj_weight2 = current_val2 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate selection probabilities for excluded items\n        selection_probs = (obj_weight1 * value1_lst[excluded_items] + obj_weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        selection_probs = selection_probs / np.sum(selection_probs)\n\n        # Select an item to add\n        add_idx = np.random.choice(excluded_items, p=selection_probs)\n\n        # Select an item to remove based on its contribution to both objectives\n        removal_scores = (obj_weight1 * value1_lst[included_items] + obj_weight2 * value2_lst[included_items]) / weight_lst[included_items]\n        remove_idx = included_items[np.argmin(removal_scores)]\n\n        # Perform swap if feasible\n        if current_weight - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n            current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Step 2: Adaptive value-weighted swapping\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value-weighted probabilities for included items\n        swap_probs = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        # Select an item to potentially swap\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n\n        # Find a suitable item to swap with from excluded items\n        potential_swaps = excluded_items[weight_lst[excluded_items] <= weight_lst[swap_idx]]\n        if len(potential_swaps) > 0:\n            # Select the best swap candidate based on combined value\n            swap_candidate = potential_swaps[np.argmax(value1_lst[potential_swaps] + value2_lst[potential_swaps])]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[swap_idx] + weight_lst[swap_candidate] <= capacity:\n                new_solution[swap_idx] = 0\n                new_solution[swap_candidate] = 1\n\n    # Step 3: Diversity-aware bit flipping\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_val1 = len(np.unique(value1_lst[included_items]))\n        unique_val2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_val1, unique_val2) / max(unique_val1, unique_val2) if max(unique_val1, unique_val2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(2 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.937480718347008,
            -18.696077869537106
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for multi-objective improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility, using a novel approach that balances exploration of the objective space with exploitation of high-potential regions.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.193564737015997,
            -18.5791289379654
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive, applies a hybrid local search that combines probabilistic item swapping based on marginal contribution, value-driven insertion with an adaptive crowding-aware bit flip strategy that prioritizes items with high marginal contribution in both objectives, while dynamically adjusting the exploration-exploitation balance based on the archive's diversity.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Dynamic probabilistic item swapping (swap items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal contribution for each item\n        marginal_value1 = value1_lst[included_items] / weight_lst[included_items]\n        marginal_value2 = value2_lst[included_items] / weight_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Swap items with low marginal contribution with higher probability\n        swap_probs = 1 / (1 + marginal_contribution)\n        swap_probs = swap_probs / np.sum(swap_probs)\n\n        swap_idx = np.random.choice(included_items, p=swap_probs)\n        new_solution[swap_idx] = 0\n\n        # Select an excluded item to swap with based on normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n        insert_idx = np.random.choice(excluded_items, p=norm_value_ratio)\n\n        if current_weight - weight_lst[swap_idx] + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight = current_weight - weight_lst[swap_idx] + weight_lst[insert_idx]\n\n    # Step 2: Adaptive value-driven insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized value-to-weight ratio\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        norm_value_ratio = value_ratio / np.sum(value_ratio)\n\n        # Adjust insertion probability based on archive diversity\n        diversity_factor = 1.0 - (len(archive) / (len(weight_lst) * 2))\n        insertion_probs = norm_value_ratio * diversity_factor\n\n        if np.sum(insertion_probs) > 0:\n            insertion_probs = insertion_probs / np.sum(insertion_probs)\n            insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n            if current_weight + weight_lst[insert_idx] <= capacity:\n                new_solution[insert_idx] = 1\n                current_weight += weight_lst[insert_idx]\n\n    # Step 3: Crowding-aware bit flip with dynamic exploration\n    if len(archive) > 1:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            # Calculate crowding distances for each objective\n            val1_values = np.array([obj[0] for _, obj in archive])\n            val2_values = np.array([obj[1] for _, obj in archive])\n\n            crowding_dist1 = np.sort(val1_values)[-1] - np.sort(val1_values)[-2] if len(val1_values) > 1 else 0\n            crowding_dist2 = np.sort(val2_values)[-1] - np.sort(val2_values)[-2] if len(val2_values) > 1 else 0\n\n            # Flip bits based on crowding distances and marginal contribution\n            marginal_value = (value1_lst + value2_lst) / weight_lst\n            flip_probs = marginal_value * (crowding_dist1 + crowding_dist2)\n\n            if np.sum(flip_probs) > 0:\n                flip_probs = flip_probs / np.sum(flip_probs)\n                flip_idx = np.random.choice(len(new_solution), p=flip_probs)\n\n                if new_solution[flip_idx] == 1:\n                    if current_weight - weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 0\n                else:\n                    if current_weight + weight_lst[flip_idx] <= capacity:\n                        new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.81821790627548,
            -18.73388005295616
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution with high potential for improvement in both objectives by combining probabilistic item selection with objective-aware swaps and adaptive neighborhood exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic objective-aware selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined objective scores\n        combined_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Select top items to consider\n        k = min(4, len(excluded_items))\n        top_indices = np.argpartition(combined_scores[excluded_items], -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            selected_idx = np.random.choice(top_indices)\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[selected_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[selected_idx] = 1\n                current_weight = current_weight - weight_lst[worst_idx] + weight_lst[selected_idx]\n\n    # Step 2: Adaptive neighborhood exploration\n    if len(included_items) > 0:\n        # Calculate solution diversity\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine exploration intensity\n        exploration_intensity = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(exploration_intensity):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight -= weight_lst[flip_idx]\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight += weight_lst[flip_idx]\n\n    # Step 3: Objective-specific fine-tuning\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific ratios\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Select items based on objective balance\n        if ratio > 0.5:\n            # Focus on value2 improvement\n            improvement_scores = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        else:\n            # Focus on value1 improvement\n            improvement_scores = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top item for potential addition\n        if len(improvement_scores) > 0:\n            best_idx = np.argmax(improvement_scores)\n            best_idx = excluded_items[best_idx]\n\n            # Find worst item to replace\n            worst_scores = (value1_lst[included_items] * ratio + value2_lst[included_items] * (1 - ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[best_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[best_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.125194556064613,
            -18.64008991764989
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / (weight_lst[included_items] + 1e-6)\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (weight_lst[excluded_items] + 1e-6)\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6))\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(4 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.21401900835604,
            -18.48074810770611
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for objective-specific improvement, then applies a hybrid local search strategy that combines probabilistic item prioritization, objective-aware replacement, and adaptive neighborhood diversification to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for objective-specific improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item prioritization (select items based on objective-specific priorities)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate objective-specific priorities for included items\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        priority_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        # Calculate prioritization scores for excluded items\n        priority_scores = (value1_lst[excluded_items] * priority_ratio + value2_lst[excluded_items] * (1 - priority_ratio)) / weight_lst[excluded_items]\n\n        # Select top k items to consider for prioritization\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(priority_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            prioritize_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective priorities\n            worst_scores = (value1_lst[included_items] * priority_ratio + value2_lst[included_items] * (1 - priority_ratio)) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform prioritization if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[prioritize_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[prioritize_idx] = 1\n\n    # Step 2: Objective-aware replacement (replace items based on their potential to improve both objectives)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential improvement scores for excluded items\n        improvement_scores = (value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for improvement\n        k = min(5, len(excluded_items))\n        top_indices = np.argpartition(improvement_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            improve_idx = np.random.choice(top_indices)\n\n            # Find the least valuable item to replace based on combined objective values\n            least_val_idx = np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])\n            least_val_idx = included_items[least_val_idx]\n\n            # Perform improvement if feasible\n            if current_weight - weight_lst[least_val_idx] + weight_lst[improve_idx] <= capacity:\n                new_solution[least_val_idx] = 0\n                new_solution[improve_idx] = 1\n\n    # Step 3: Adaptive neighborhood diversification (flip bits based on solution diversity)\n    if len(included_items) > 0:\n        # Calculate solution diversity as the ratio of unique objective contributions\n        unique_obj1 = len(np.unique(value1_lst[included_items]))\n        unique_obj2 = len(np.unique(value2_lst[included_items]))\n        diversity = min(unique_obj1, unique_obj2) / max(unique_obj1, unique_obj2) if max(unique_obj1, unique_obj2) > 0 else 0.5\n\n        # Determine number of bits to flip based on diversity (less diverse solutions get more changes)\n        num_flips = max(1, int(3 * (1 - diversity)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.096530030707598,
            -18.662091361335012
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its dominance rank and diversity, then applies a novel hybrid local search strategy that combines probabilistic item grouping, objective-aware clustering, and adaptive weight redistribution to generate high-quality neighbor solutions while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high dominance rank and diversity\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 1: Probabilistic item grouping (cluster items by objective dominance)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        # Calculate objective dominance for included items\n        obj_dominance = value1_lst[included_items] / (value1_lst[included_items] + value2_lst[included_items] + 1e-6)\n        group_probs = obj_dominance / np.sum(obj_dominance)\n\n        # Select a group to modify\n        group_idx = np.random.choice(included_items, p=group_probs)\n\n        # Step 2: Objective-aware clustering (replace or remove based on objective balance)\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        balance_ratio = current_val1 / (current_val1 + current_val2 + 1e-6)\n\n        if random.random() < 0.7:  # Higher probability to remove\n            if current_weight - weight_lst[group_idx] <= capacity:\n                new_solution[group_idx] = 0\n        else:  # Insert from excluded items\n            if len(excluded_items) > 0:\n                insertion_value = (value1_lst[excluded_items] * (1 - balance_ratio) + value2_lst[excluded_items] * balance_ratio) / weight_lst[excluded_items]\n                insertion_probs = insertion_value / np.sum(insertion_value)\n                insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n\n                if current_weight - weight_lst[group_idx] + weight_lst[insert_idx] <= capacity:\n                    new_solution[group_idx] = 0\n                    new_solution[insert_idx] = 1\n\n    # Step 3: Adaptive weight redistribution (flip bits based on weight balance)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        total_weight = np.sum(weight_lst)\n        weight_balance = current_weight / total_weight\n\n        flip_idx = random.randint(0, len(new_solution) - 1)\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity and random.random() < weight_balance:\n                new_solution[flip_idx] = 0\n        else:\n            if current_weight + weight_lst[flip_idx] <= capacity and random.random() > weight_balance:\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -17.810234601881376,
            -18.920031096646298
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for multi-objective improvement, then applies a hybrid local search strategy that combines probabilistic item clustering, objective-weighted swapping, and adaptive neighborhood expansion to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for multi-objective improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item clustering (group items with similar characteristics)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Cluster items based on value correlation\n        value_correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n        cluster_threshold = 0.5 + 0.5 * random.random()  # Random threshold between 0.5 and 1.0\n\n        # Identify items that are strongly correlated in values\n        correlated_items = np.where(np.abs(np.corrcoef(value1_lst[included_items], value2_lst[included_items])[0, 1]) > cluster_threshold)[0]\n        correlated_items = included_items[correlated_items]\n\n        if len(correlated_items) > 0:\n            # Select a random item from the correlated cluster\n            swap_out_idx = np.random.choice(correlated_items)\n\n            # Find the best item to swap in from excluded items\n            swap_in_idx = np.argmax((value1_lst[excluded_items] * value2_lst[excluded_items]) / weight_lst[excluded_items])\n            swap_in_idx = excluded_items[swap_in_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[swap_out_idx] + weight_lst[swap_in_idx] <= capacity:\n                new_solution[swap_out_idx] = 0\n                new_solution[swap_in_idx] = 1\n                current_weight = current_weight - weight_lst[swap_out_idx] + weight_lst[swap_in_idx]\n\n    # Step 2: Objective-weighted swapping (swap items based on their relative importance in each objective)\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        total_value = current_val1 + current_val2\n\n        # Calculate objective weights\n        obj1_weight = current_val1 / total_value if total_value > 0 else 0.5\n        obj2_weight = current_val2 / total_value if total_value > 0 else 0.5\n\n        # Calculate swap scores for excluded items\n        swap_scores = (obj1_weight * value1_lst[excluded_items] + obj2_weight * value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Select top k items to consider for swapping\n        k = min(3, len(excluded_items))\n        top_indices = np.argpartition(swap_scores, -k)[-k:]\n        top_indices = excluded_items[top_indices]\n\n        if len(top_indices) > 0:\n            swap_in_idx = np.random.choice(top_indices)\n\n            # Find the worst item to replace based on current objective weights\n            worst_scores = (obj1_weight * value1_lst[included_items] + obj2_weight * value2_lst[included_items]) / weight_lst[included_items]\n            worst_idx = np.argmin(worst_scores)\n            worst_idx = included_items[worst_idx]\n\n            # Perform swap if feasible\n            if current_weight - weight_lst[worst_idx] + weight_lst[swap_in_idx] <= capacity:\n                new_solution[worst_idx] = 0\n                new_solution[swap_in_idx] = 1\n\n    # Step 3: Adaptive neighborhood expansion (flip multiple bits based on solution quality)\n    if len(included_items) > 0:\n        # Calculate solution quality as the product of normalized objective values\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        max_val1 = np.sum(value1_lst)\n        max_val2 = np.sum(value2_lst)\n        quality = (current_val1 / max_val1) * (current_val2 / max_val2)\n\n        # Determine number of bits to flip based on quality (poorer solutions get more aggressive changes)\n        num_flips = max(1, int(5 * (1 - quality)))\n\n        for _ in range(num_flips):\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.96323194131221,
            -18.69300438716042
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a novel local search strategy that combines probabilistic item removal, value-weighted insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-weighted insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -19.289591987269574,
            -17.927642823018033
        ]
    },
    {
        "algorithm": "{This algorithm selects a solution from the archive based on its potential for improvement, then applies a hybrid local search strategy that combines probabilistic item removal, value-to-weight ratio insertion, and adaptive crowding-aware exploration to generate a high-quality neighbor solution while ensuring feasibility.}",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (prioritize those with lower crowding distance if available)\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic item removal (remove items with low marginal contribution)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate marginal contribution for each included item\n        marginal_value1 = value1_lst[included_items]\n        marginal_value2 = value2_lst[included_items]\n        marginal_contribution = marginal_value1 + marginal_value2\n\n        # Remove items with low marginal contribution with higher probability\n        removal_probs = 1 / (1 + marginal_contribution)\n        removal_probs = removal_probs / np.sum(removal_probs)\n\n        remove_idx = np.random.choice(included_items, p=removal_probs)\n        if current_weight - weight_lst[remove_idx] <= capacity:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    # Step 2: Value-to-weight ratio insertion (insert items with high value-to-weight ratio)\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate value-to-weight ratio for each excluded item\n        value_ratio1 = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio2 = value2_lst[excluded_items] / weight_lst[excluded_items]\n        value_ratio = value_ratio1 + value_ratio2\n\n        # Insert items with high value-to-weight ratio with higher probability\n        insertion_probs = value_ratio / np.sum(value_ratio)\n\n        insert_idx = np.random.choice(excluded_items, p=insertion_probs)\n        if current_weight + weight_lst[insert_idx] <= capacity:\n            new_solution[insert_idx] = 1\n            current_weight += weight_lst[insert_idx]\n\n    # Step 3: Adaptive crowding-aware exploration (flip a random bit if solution is not on the Pareto front)\n    if len(archive) > 1:\n        # Check if the current solution is dominated by any other solution in the archive\n        current_val1 = np.sum(value1_lst * new_solution)\n        current_val2 = np.sum(value2_lst * new_solution)\n        is_dominated = False\n\n        for sol, (val1, val2) in archive:\n            if (val1 >= current_val1 and val2 > current_val2) or (val1 > current_val1 and val2 >= current_val2):\n                is_dominated = True\n                break\n\n        if is_dominated:\n            flip_idx = random.randint(0, len(new_solution) - 1)\n            if new_solution[flip_idx] == 1:\n                if current_weight - weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 0\n            else:\n                if current_weight + weight_lst[flip_idx] <= capacity:\n                    new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -18.553997020849362,
            -18.850015607233225
        ]
    }
]