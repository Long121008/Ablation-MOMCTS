[
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm selects a base solution from the archive by identifying the solution with the highest potential for improvement in both objectives while considering the solution's position in the Pareto front, then applies a novel hybrid local search operator that dynamically combines a multi-objective value-weighted random walk with a capacity-aware item clustering strategy. This operator first groups items into clusters based on their value ratios and weights, then probabilistically selects clusters to flip based on their combined marginal utility for both objectives, with the selection probability biased towards clusters that offer the highest improvements in either objective while maintaining feasibility through a capacity-aware filtering mechanism. The operator also incorporates a dynamic perturbation mechanism that randomly swaps entire clusters between objectives with a small probability to further explore the solution space, while ensuring the solution remains within the capacity constraint through a greedy repair mechanism that prioritizes the removal of items with the lowest marginal utility when necessary. The algorithm also employs a novel \"cluster dominance\" criterion that evaluates the impact of flipping each cluster on the Pareto front, favoring flips that are either non-dominated or have the highest combined marginal utility for both objectives, with the selection process guided by a utility function that weighs both objective improvements and solution diversity.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives to calculate potential improvement\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n    potentials = (1 - norm_obj1) + (1 - norm_obj2)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Cluster items based on value ratios and weights\n    value_ratios = value1_lst / (value2_lst + 1e-10)\n    cluster_labels = np.digitize(value_ratios, bins=np.percentile(value_ratios, [33, 66]))\n    unique_clusters = np.unique(cluster_labels)\n\n    # Step 3: Calculate cluster properties\n    cluster_properties = []\n    for cluster in unique_clusters:\n        cluster_items = np.where(cluster_labels == cluster)[0]\n        cluster_weight = np.sum(weight_lst[cluster_items])\n        cluster_value1 = np.sum(value1_lst[cluster_items])\n        cluster_value2 = np.sum(value2_lst[cluster_items])\n        cluster_properties.append((cluster, cluster_items, cluster_weight, cluster_value1, cluster_value2))\n\n    # Step 4: Dynamic cluster selection for flipping\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal utility for each cluster\n    cluster_utilities = []\n    for cluster, items, c_weight, c_value1, c_value2 in cluster_properties:\n        # Calculate marginal gain if cluster is flipped\n        cluster_in_solution = np.any(new_solution[items] == 1)\n        if cluster_in_solution:\n            marginal_gain = -c_value1 - c_value2\n        else:\n            marginal_gain = c_value1 + c_value2\n        cluster_utilities.append((cluster, marginal_gain, c_weight, items))\n\n    # Sort clusters by utility (descending)\n    cluster_utilities.sort(key=lambda x: -x[1])\n\n    # Step 5: Apply hybrid local search operator\n    for cluster, marginal_gain, c_weight, items in cluster_utilities:\n        cluster_in_solution = np.any(new_solution[items] == 1)\n\n        # Probabilistic flip based on marginal utility\n        flip_prob = 1 / (1 + np.exp(-0.1 * marginal_gain))\n        if np.random.rand() < flip_prob:\n            if cluster_in_solution:\n                # Remove cluster if it's in solution\n                new_solution[items] = 0\n                remaining_capacity += c_weight\n            else:\n                # Add cluster if feasible\n                if c_weight <= remaining_capacity:\n                    new_solution[items] = 1\n                    remaining_capacity -= c_weight\n\n    # Step 6: Dynamic perturbation (random cluster swap)\n    if np.random.rand() < 0.1 and len(unique_clusters) > 1:\n        cluster1, cluster2 = np.random.choice(unique_clusters, size=2, replace=False)\n        items1 = np.where(cluster_labels == cluster1)[0]\n        items2 = np.where(cluster_labels == cluster2)[0]\n\n        # Check if swap is feasible\n        weight1 = np.sum(weight_lst[items1]) if np.any(new_solution[items1] == 1) else 0\n        weight2 = np.sum(weight_lst[items2]) if np.any(new_solution[items2] == 1) else 0\n\n        if (weight1 == 0 and weight2 <= remaining_capacity + weight1) or \\\n           (weight2 == 0 and weight1 <= remaining_capacity + weight2):\n            # Perform cluster swap\n            if weight1 > 0:\n                new_solution[items1] = 0\n                new_solution[items2] = 1\n                remaining_capacity += weight1 - weight2\n            else:\n                new_solution[items1] = 1\n                new_solution[items2] = 0\n                remaining_capacity += weight2 - weight1\n\n    # Step 7: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Find items with lowest marginal utility\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The proposed algorithm selects a solution from the archive by identifying the least explored region of the Pareto front based on the local density of solutions, then applies a novel local search operator that performs a series of intelligent, multi-objective flips by considering the Pareto front contributions of items. The operator dynamically adjusts the selection based on a hybrid of lexicographic ordering and dominance-based selection, prioritizing solutions that are either non-dominated or have the highest combined marginal utility for both objectives, while also incorporating a probabilistic acceptance criterion that favors flips leading to non-dominated solutions or significant trade-offs. The selection process is guided by a utility function that weighs both objective improvements and solution diversity, ensuring a robust exploration of the search space, and includes an additional perturbation mechanism that randomly swaps items between objectives with a small probability to further explore the solution space while maintaining feasibility. Additionally, the algorithm employs a novel \"value-weighted random walk\" mechanism that probabilistically selects items to flip based on their normalized values, biased towards items that offer the highest marginal improvements in either objective, with a dynamic adjustment of the selection probability based on the current solution's objective values. This approach ensures a balance between exploitation of high-value items and exploration of the solution space, leading to more diverse and high-quality solutions.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify the least explored region of the Pareto front\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate local density around each solution\n        distances = np.linalg.norm(objectives[:, None] - objectives, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        min_distances = np.min(distances, axis=1)\n        selected_idx = np.argmin(min_distances)\n        base_solution = archive[selected_idx][0].copy()\n\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate normalized marginal contributions\n    total_value1 = np.sum(value1_lst * base_solution)\n    total_value2 = np.sum(value2_lst * base_solution)\n\n    # Normalize values to [0, 1] range\n    max_value1 = np.max(value1_lst) if np.max(value1_lst) > 0 else 1\n    max_value2 = np.max(value2_lst) if np.max(value2_lst) > 0 else 1\n    norm_value1 = value1_lst / max_value1\n    norm_value2 = value2_lst / max_value2\n\n    # Step 3: Value-weighted random walk for item selection\n    candidates = []\n    for i in range(len(base_solution)):\n        if base_solution[i] == 1:\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                # Probability based on normalized values and current solution's objectives\n                prob = (norm_value1[i] * (1 - total_value1 / (total_value1 + total_value2 + 1e-10)) +\n                        norm_value2[i] * (total_value1 / (total_value1 + total_value2 + 1e-10)))\n                candidates.append((i, -1, prob))\n        else:\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                prob = (norm_value1[i] * (1 - total_value1 / (total_value1 + total_value2 + 1e-10)) +\n                        norm_value2[i] * (total_value1 / (total_value1 + total_value2 + 1e-10)))\n                candidates.append((i, 1, prob))\n\n    if not candidates:\n        # No feasible flips, perform a random feasible flip\n        feasible_indices = [i for i in range(len(base_solution))\n                          if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n        return new_solution\n\n    # Step 4: Select items to flip based on probability distribution\n    items, flips, probs = zip(*candidates)\n    probs = np.array(probs) / np.sum(probs)  # Normalize probabilities\n    selected_idx = np.random.choice(len(items), p=probs)\n    flip_idx = items[selected_idx]\n    flip = flips[selected_idx]\n\n    new_solution[flip_idx] = 1 if flip == 1 else 0\n\n    # Step 5: Additional perturbation - swap items if beneficial\n    if random.random() < 0.2:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) >= 2:\n            i, j = random.sample(list(included_items), 2)\n            if (current_weight - weight_lst[i] - weight_lst[j] + weight_lst[i] + weight_lst[j]) <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 301,
          "algorithm": "{The new algorithm selects a base solution from the archive by identifying the solution with the highest potential for improvement in both objectives while considering the solution's position in the Pareto front, then applies a novel hybrid local search operator that dynamically combines a multi-objective value-weighted random walk with a capacity-aware item clustering strategy. This operator first groups items into clusters based on their value ratios and weights, then probabilistically selects clusters to flip based on their combined marginal utility for both objectives, with the selection probability biased towards clusters that offer the highest improvements in either objective while maintaining feasibility through a capacity-aware filtering mechanism. The operator also incorporates a dynamic perturbation mechanism that randomly swaps entire clusters between objectives with a small probability to further explore the solution space, while ensuring the solution remains within the capacity constraint through a greedy repair mechanism that prioritizes the removal of items with the lowest marginal utility when necessary. The algorithm also employs a novel \"cluster dominance\" criterion that evaluates the impact of flipping each cluster on the Pareto front, favoring flips that are either non-dominated or have the highest combined marginal utility for both objectives, with the selection process guided by a utility function that weighs both objective improvements and solution diversity.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives to calculate potential improvement\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n    potentials = (1 - norm_obj1) + (1 - norm_obj2)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate item utilities\n    item_utilities = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            marginal_gain = -value1_lst[i] - value2_lst[i]\n            item_utilities.append((i, marginal_gain, weight_lst[i]))\n        else:\n            if weight_lst[i] <= remaining_capacity:\n                marginal_gain = value1_lst[i] + value2_lst[i]\n                item_utilities.append((i, marginal_gain, weight_lst[i]))\n\n    # Step 4: Sort items by utility (descending)\n    item_utilities.sort(key=lambda x: -x[1])\n\n    # Step 5: Apply value-weighted random walk\n    if item_utilities:\n        # Calculate probabilities based on normalized utilities\n        utilities = np.array([x[1] for x in item_utilities])\n        if np.sum(utilities) > 0:\n            probs = utilities / np.sum(utilities)\n        else:\n            probs = np.ones(len(utilities)) / len(utilities)\n\n        selected_idx = np.random.choice(len(item_utilities), p=probs)\n        item_idx, marginal_gain, item_weight = item_utilities[selected_idx]\n\n        # Flip the selected item\n        new_solution[item_idx] = 1 - new_solution[item_idx]\n        if new_solution[item_idx] == 1:\n            remaining_capacity -= item_weight\n        else:\n            remaining_capacity += item_weight\n\n    # Step 6: Dynamic perturbation (random item swap)\n    if np.random.rand() < 0.1:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) >= 2:\n            i, j = np.random.choice(included_items, size=2, replace=False)\n            if (current_weight - weight_lst[i] - weight_lst[j] + weight_lst[i] + weight_lst[j]) <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Step 7: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
          "score": [
               -0.8270968517177525,
               2.287377566099167
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives to calculate potential improvement\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n    potentials = (1 - norm_obj1) + (1 - norm_obj2)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate item utilities\n    item_utilities = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            marginal_gain = -value1_lst[i] - value2_lst[i]\n            item_utilities.append((i, marginal_gain, weight_lst[i]))\n        else:\n            if weight_lst[i] <= remaining_capacity:\n                marginal_gain = value1_lst[i] + value2_lst[i]\n                item_utilities.append((i, marginal_gain, weight_lst[i]))\n\n    # Step 4: Sort items by utility (descending)\n    item_utilities.sort(key=lambda x: -x[1])\n\n    # Step 5: Apply value-weighted random walk\n    if item_utilities:\n        # Calculate probabilities based on normalized utilities\n        utilities = np.array([x[1] for x in item_utilities])\n        if np.sum(utilities) > 0:\n            probs = utilities / np.sum(utilities)\n        else:\n            probs = np.ones(len(utilities)) / len(utilities)\n\n        selected_idx = np.random.choice(len(item_utilities), p=probs)\n        item_idx, marginal_gain, item_weight = item_utilities[selected_idx]\n\n        # Flip the selected item\n        new_solution[item_idx] = 1 - new_solution[item_idx]\n        if new_solution[item_idx] == 1:\n            remaining_capacity -= item_weight\n        else:\n            remaining_capacity += item_weight\n\n    # Step 6: Dynamic perturbation (random item swap)\n    if np.random.rand() < 0.1:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) >= 2:\n            i, j = np.random.choice(included_items, size=2, replace=False)\n            if (current_weight - weight_lst[i] - weight_lst[j] + weight_lst[i] + weight_lst[j]) <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Step 7: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm selects a base solution from the archive by identifying the solution with the highest potential for improvement in both objectives while considering the solution's position in the Pareto front, then applies a novel hybrid local search operator that dynamically combines a multi-objective value-weighted random walk with a capacity-aware item clustering strategy. This operator first groups items into clusters based on their value ratios and weights, then probabilistically selects clusters to flip based on their combined marginal utility for both objectives, with the selection probability biased towards clusters that offer the highest improvements in either objective while maintaining feasibility through a capacity-aware filtering mechanism. The operator also incorporates a dynamic perturbation mechanism that randomly swaps entire clusters between objectives with a small probability to further explore the solution space, while ensuring the solution remains within the capacity constraint through a greedy repair mechanism that prioritizes the removal of items with the lowest marginal utility when necessary. The algorithm also employs a novel \"cluster dominance\" criterion that evaluates the impact of flipping each cluster on the Pareto front, favoring flips that are either non-dominated or have the highest combined marginal utility for both objectives, with the selection process guided by a utility function that weighs both objective improvements and solution diversity.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives to calculate potential improvement\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n    potentials = (1 - norm_obj1) + (1 - norm_obj2)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Cluster items based on value ratios and weights\n    value_ratios = value1_lst / (value2_lst + 1e-10)\n    cluster_labels = np.digitize(value_ratios, bins=np.percentile(value_ratios, [33, 66]))\n    unique_clusters = np.unique(cluster_labels)\n\n    # Step 3: Calculate cluster properties\n    cluster_properties = []\n    for cluster in unique_clusters:\n        cluster_items = np.where(cluster_labels == cluster)[0]\n        cluster_weight = np.sum(weight_lst[cluster_items])\n        cluster_value1 = np.sum(value1_lst[cluster_items])\n        cluster_value2 = np.sum(value2_lst[cluster_items])\n        cluster_properties.append((cluster, cluster_items, cluster_weight, cluster_value1, cluster_value2))\n\n    # Step 4: Dynamic cluster selection for flipping\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal utility for each cluster\n    cluster_utilities = []\n    for cluster, items, c_weight, c_value1, c_value2 in cluster_properties:\n        # Calculate marginal gain if cluster is flipped\n        cluster_in_solution = np.any(new_solution[items] == 1)\n        if cluster_in_solution:\n            marginal_gain = -c_value1 - c_value2\n        else:\n            marginal_gain = c_value1 + c_value2\n        cluster_utilities.append((cluster, marginal_gain, c_weight, items))\n\n    # Sort clusters by utility (descending)\n    cluster_utilities.sort(key=lambda x: -x[1])\n\n    # Step 5: Apply hybrid local search operator\n    for cluster, marginal_gain, c_weight, items in cluster_utilities:\n        cluster_in_solution = np.any(new_solution[items] == 1)\n\n        # Probabilistic flip based on marginal utility\n        flip_prob = 1 / (1 + np.exp(-0.1 * marginal_gain))\n        if np.random.rand() < flip_prob:\n            if cluster_in_solution:\n                # Remove cluster if it's in solution\n                new_solution[items] = 0\n                remaining_capacity += c_weight\n            else:\n                # Add cluster if feasible\n                if c_weight <= remaining_capacity:\n                    new_solution[items] = 1\n                    remaining_capacity -= c_weight\n\n    # Step 6: Dynamic perturbation (random cluster swap)\n    if np.random.rand() < 0.1 and len(unique_clusters) > 1:\n        cluster1, cluster2 = np.random.choice(unique_clusters, size=2, replace=False)\n        items1 = np.where(cluster_labels == cluster1)[0]\n        items2 = np.where(cluster_labels == cluster2)[0]\n\n        # Check if swap is feasible\n        weight1 = np.sum(weight_lst[items1]) if np.any(new_solution[items1] == 1) else 0\n        weight2 = np.sum(weight_lst[items2]) if np.any(new_solution[items2] == 1) else 0\n\n        if (weight1 == 0 and weight2 <= remaining_capacity + weight1) or \\\n           (weight2 == 0 and weight1 <= remaining_capacity + weight2):\n            # Perform cluster swap\n            if weight1 > 0:\n                new_solution[items1] = 0\n                new_solution[items2] = 1\n                remaining_capacity += weight1 - weight2\n            else:\n                new_solution[items1] = 1\n                new_solution[items2] = 0\n                remaining_capacity += weight2 - weight1\n\n    # Step 7: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Find items with lowest marginal utility\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution with high potential for improvement based on its objective values and weight utilization\n    selected_idx = np.random.choice(len(archive))\n    base_solution, (base_val1, base_val2) = archive[selected_idx]\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate the normalized objective values to determine the focus of improvement\n    max_val1 = np.max([obj[0] for _, obj in archive])\n    max_val2 = np.max([obj[1] for _, obj in archive])\n    norm_val1 = base_val1 / max_val1 if max_val1 > 0 else 0\n    norm_val2 = base_val2 / max_val2 if max_val2 > 0 else 0\n\n    # Step 3: Determine the focus of improvement (the objective that is relatively weaker)\n    focus_obj = 1 if norm_val1 < norm_val2 else 2\n\n    # Step 4: Generate candidate items for flipping based on their potential to improve the focused objective\n    candidates = []\n    for i in range(len(base_solution)):\n        if base_solution[i] == 1:\n            # Consider removing item i if it doesn't significantly reduce the focused objective\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, -1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, -1, value2_lst[i], value1_lst[i]))\n        else:\n            # Consider adding item i if it improves the focused objective\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, 1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, 1, value2_lst[i], value1_lst[i]))\n\n    if not candidates:\n        # Step 5: No feasible flips, perform a random feasible flip\n        feasible_indices = [i for i in range(len(base_solution))\n                          if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n        return new_solution\n\n    # Step 6: Select the best candidate based on the focused objective\n    best_candidate = None\n    best_score = -np.inf\n    for candidate in candidates:\n        idx, flip, val_focus, val_other = candidate\n        if flip == 1:\n            score = val_focus  # Adding item: maximize the focused objective\n        else:\n            score = -val_focus  # Removing item: minimize the focused objective\n        if score > best_score:\n            best_score = score\n            best_candidate = candidate\n\n    if best_candidate is not None:\n        idx, flip, _, _ = best_candidate\n        new_solution[idx] = 1 if flip == 1 else 0\n\n    # Step 7: Additional perturbation: perform a series of intelligent swaps between objectives\n    if random.random() < 0.3 and len(new_solution) > 1:\n        # Select a subset of items to consider for swapping\n        swap_candidates = np.where(new_solution == 1)[0]\n        if len(swap_candidates) > 1:\n            np.random.shuffle(swap_candidates)\n            for i in range(min(3, len(swap_candidates)-1)):\n                j = i + 1\n                if j >= len(swap_candidates):\n                    break\n                # Swap items if it maintains feasibility\n                if (current_weight - weight_lst[swap_candidates[i]] - weight_lst[swap_candidates[j]] +\n                    weight_lst[swap_candidates[i]] + weight_lst[swap_candidates[j]]) <= capacity:\n                    new_solution[swap_candidates[i]], new_solution[swap_candidates[j]] = \\\n                    new_solution[swap_candidates[j]], new_solution[swap_candidates[i]]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 302,
          "algorithm": "{The new algorithm identifies promising solutions in the archive by analyzing both objective values and solution diversity, then applies a novel \"objective-balanced value-driven flip\" strategy that probabilistically flips items based on their marginal utility for both objectives, while dynamically adjusting the selection probability to balance improvements in either objective. This approach combines a value-weighted probabilistic flip mechanism with a diversity-aware solution selection process, incorporating a novel \"objective-balance score\" that considers both the current solution's objective values and their potential for improvement across both dimensions, with the selection probability biased towards solutions that show the highest combined potential for improvement in either objective while maintaining feasibility through a capacity-constrained item selection process. The algorithm also employs a dynamic \"flip intensity\" mechanism that adjusts the probability of flipping items based on their relative contribution to each objective, with higher-intensity flips more likely when the current solution shows significant imbalance between the two objectives, while ensuring the solution remains feasible through a capacity-aware greedy repair mechanism that prioritizes the removal of items with the lowest combined marginal utility when necessary.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Calculate objective balance score (1 - balance, where balance is min(obj1/max1, obj2/max2))\n    balance_scores = []\n    for obj1, obj2 in objectives:\n        norm1 = obj1 / max_obj1 if max_obj1 > 0 else 0\n        norm2 = obj2 / max_obj2 if max_obj2 > 0 else 0\n        balance = min(norm1, norm2)\n        balance_scores.append(1 - balance)\n\n    # Select solution with highest balance score (most imbalanced)\n    selected_idx = np.argmax(balance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current solution properties\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate item utilities for both objectives\n    item_utilities = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Calculate marginal utility if removed\n            marginal_val1 = -value1_lst[i]\n            marginal_val2 = -value2_lst[i]\n            item_utilities.append((i, marginal_val1, marginal_val2, -weight_lst[i]))\n        else:\n            # Calculate marginal utility if added\n            if weight_lst[i] <= remaining_capacity:\n                marginal_val1 = value1_lst[i]\n                marginal_val2 = value2_lst[i]\n                item_utilities.append((i, marginal_val1, marginal_val2, weight_lst[i]))\n\n    # Step 4: Dynamic flip intensity based on objective balance\n    base_obj1, base_obj2 = archive[selected_idx][1]\n    norm1 = base_obj1 / max_obj1 if max_obj1 > 0 else 0\n    norm2 = base_obj2 / max_obj2 if max_obj2 > 0 else 0\n    flip_intensity = 1 - min(norm1, norm2)  # Higher when objectives are more imbalanced\n\n    # Step 5: Probabilistic flip based on combined utility\n    for i, marginal_val1, marginal_val2, delta_weight in item_utilities:\n        # Calculate combined utility score\n        utility_score = (marginal_val1 + marginal_val2) * flip_intensity\n\n        # Probabilistic flip based on utility score\n        flip_prob = 1 / (1 + np.exp(-0.1 * utility_score))\n        if np.random.rand() < flip_prob:\n            if new_solution[i] == 1:\n                # Remove item\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n            else:\n                # Add item if feasible\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    # Step 6: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Find items with lowest combined marginal utility\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_utilities = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_utilities)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
          "score": [
               -0.9556300488422049,
               2.7361185550689697
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Calculate objective balance score (1 - balance, where balance is min(obj1/max1, obj2/max2))\n    balance_scores = []\n    for obj1, obj2 in objectives:\n        norm1 = obj1 / max_obj1 if max_obj1 > 0 else 0\n        norm2 = obj2 / max_obj2 if max_obj2 > 0 else 0\n        balance = min(norm1, norm2)\n        balance_scores.append(1 - balance)\n\n    # Select solution with highest balance score (most imbalanced)\n    selected_idx = np.argmax(balance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current solution properties\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate item utilities for both objectives\n    item_utilities = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Calculate marginal utility if removed\n            marginal_val1 = -value1_lst[i]\n            marginal_val2 = -value2_lst[i]\n            item_utilities.append((i, marginal_val1, marginal_val2, -weight_lst[i]))\n        else:\n            # Calculate marginal utility if added\n            if weight_lst[i] <= remaining_capacity:\n                marginal_val1 = value1_lst[i]\n                marginal_val2 = value2_lst[i]\n                item_utilities.append((i, marginal_val1, marginal_val2, weight_lst[i]))\n\n    # Step 4: Dynamic flip intensity based on objective balance\n    base_obj1, base_obj2 = archive[selected_idx][1]\n    norm1 = base_obj1 / max_obj1 if max_obj1 > 0 else 0\n    norm2 = base_obj2 / max_obj2 if max_obj2 > 0 else 0\n    flip_intensity = 1 - min(norm1, norm2)  # Higher when objectives are more imbalanced\n\n    # Step 5: Probabilistic flip based on combined utility\n    for i, marginal_val1, marginal_val2, delta_weight in item_utilities:\n        # Calculate combined utility score\n        utility_score = (marginal_val1 + marginal_val2) * flip_intensity\n\n        # Probabilistic flip based on utility score\n        flip_prob = 1 / (1 + np.exp(-0.1 * utility_score))\n        if np.random.rand() < flip_prob:\n            if new_solution[i] == 1:\n                # Remove item\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n            else:\n                # Add item if feasible\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    # Step 6: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Find items with lowest combined marginal utility\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_utilities = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_utilities)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel algorithm first identifies the solution in the archive with the highest combined marginal utility for both objectives while considering the solution's position in the Pareto front, then applies a dynamic multi-phase local search operator that sequentially performs item additions, removals, and swaps based on an adaptive priority score that combines objective improvements, solution diversity, and capacity utilization, with each phase's intensity and selection criteria dynamically adjusted based on the current solution's performance and the archive's distribution, while maintaining feasibility through a capacity-aware filtering mechanism that prioritizes items with the highest marginal gains for both objectives and ensures the solution remains within the capacity constraint through a greedy repair mechanism when necessary.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest combined marginal utility\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n\n    marginal_utility = []\n    for sol, obj in archive:\n        utility = (max_obj1 - obj[0]) + (max_obj2 - obj[1])\n        marginal_utility.append(utility)\n\n    selected_idx = np.argmax(marginal_utility)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate adaptive priority scores for items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Priority for inclusion: combined marginal gain normalized by weight\n    if len(excluded_items) > 0:\n        marginal_gain_in = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        priority_in = marginal_gain_in / (np.max(marginal_gain_in) + 1e-10)\n    else:\n        priority_in = np.array([])\n\n    # Priority for exclusion: combined marginal gain normalized by weight\n    if len(included_items) > 0:\n        marginal_gain_out = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        priority_out = marginal_gain_out / (np.max(marginal_gain_out) + 1e-10)\n    else:\n        priority_out = np.array([])\n\n    # Step 4: Dynamic multi-phase local search\n    phases = ['add', 'remove', 'swap']\n    phase_order = random.sample(phases, len(phases))\n\n    for phase in phase_order:\n        if phase == 'add' and len(excluded_items) > 0:\n            # Add items with highest priority\n            sorted_in = excluded_items[np.argsort(-priority_in)]\n            for idx in sorted_in:\n                if weight_lst[idx] <= remaining_capacity:\n                    new_solution[idx] = 1\n                    remaining_capacity -= weight_lst[idx]\n                    break  # Only add one item per phase\n\n        elif phase == 'remove' and len(included_items) > 0:\n            # Remove items with lowest priority\n            sorted_out = included_items[np.argsort(priority_out)]\n            for idx in sorted_out:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n                break  # Only remove one item per phase\n\n        elif phase == 'swap' and len(included_items) > 0 and len(excluded_items) > 0:\n            # Find best pair to swap\n            best_in = excluded_items[np.argmax(priority_in)]\n            best_out = included_items[np.argmax(priority_out)]\n\n            # Check if swap is feasible\n            if weight_lst[best_in] <= remaining_capacity + weight_lst[best_out]:\n                new_solution[best_in] = 1\n                new_solution[best_out] = 0\n                remaining_capacity += weight_lst[best_out] - weight_lst[best_in]\n\n    # Step 5: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove the item with the lowest marginal utility\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_values = (np.max(obj1_vals), np.max(obj2_vals))\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = [(max_values[0] - obj[0])/max_values[0] + (max_values[1] - obj[1])/max_values[1] for _, obj in archive]\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Dynamic operator selection based on solution's position\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dominance score (number of solutions dominated by this solution)\n    dominance_scores = []\n    for sol, obj in archive:\n        dominates = (obj[0] >= archive[selected_idx][1][0] and obj[1] >= archive[selected_idx][1][1]) and \\\n                    (obj[0] > archive[selected_idx][1][0] or obj[1] > archive[selected_idx][1][1])\n        dominance_scores.append(dominates)\n    dominance_score = sum(dominance_scores)\n\n    # Operator selection probabilities\n    if dominance_score > len(archive) * 0.6:  # Solution is on Pareto front\n        operator_probs = [0.3, 0.4, 0.3]  # More exploration (insert/remove)\n    else:\n        operator_probs = [0.6, 0.2, 0.2]  # More exploitation (swap)\n\n    operator = np.random.choice(['swap', 'insert', 'remove'], p=operator_probs)\n\n    # Step 4: Apply selected operator\n    if operator == 'swap' and len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal gains using a weighted combination of objectives\n        weight_obj1 = 0.6 if np.random.rand() > 0.5 else 0.4\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain_in = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        marginal_gain_out = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n\n        best_in_idx = excluded_items[np.argmax(marginal_gain_in)]\n        best_out_idx = included_items[np.argmax(marginal_gain_out)]\n\n        # Check if swap is feasible\n        if weight_lst[best_in_idx] <= remaining_capacity + weight_lst[best_out_idx]:\n            new_solution[best_in_idx] = 1\n            new_solution[best_out_idx] = 0\n\n    elif operator == 'insert' and len(excluded_items) > 0:\n        # Insert items with highest combined marginal gain with adaptive weights\n        weight_obj1 = 0.7 if np.random.rand() > 0.5 else 0.3\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        sorted_indices = excluded_items[np.argsort(-marginal_gain)]\n\n        # Adaptive insertion intensity based on remaining capacity and randomness\n        insertion_intensity = min(0.4, remaining_capacity / capacity) * (0.8 + 0.4 * np.random.rand())\n        num_to_insert = max(1, int(len(sorted_indices) * insertion_intensity))\n\n        for idx in sorted_indices[:num_to_insert]:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    elif operator == 'remove' and len(included_items) > 0:\n        # Remove items with lowest combined marginal gain with adaptive weights\n        weight_obj1 = 0.5 if np.random.rand() > 0.5 else 0.5\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(marginal_gain)]\n\n        # Adaptive removal intensity based on solution's position and randomness\n        removal_intensity = (0.3 if dominance_score > len(archive) * 0.6 else 0.1) * (0.8 + 0.4 * np.random.rand())\n        num_to_remove = max(1, int(len(sorted_indices) * removal_intensity))\n\n        for idx in sorted_indices[:num_to_remove]:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        heaviest_idx = included_items[np.argmax(weight_lst[included_items])]\n        new_solution[heaviest_idx] = 0\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 303,
          "algorithm": "{The novel algorithm first identifies the solution in the archive with the highest potential for improvement by considering both objective values and their relative positions in the Pareto front, then applies a hybrid local search operator that combines adaptive item selection based on marginal gains, capacity-aware swapping, and dynamic exploration/exploitation phases, where each phase's intensity and selection criteria are dynamically adjusted based on the solution's current performance and the archive's diversity, while maintaining feasibility through a multi-stage filtering mechanism that prioritizes items with the highest combined marginal gains for both objectives and ensures the solution remains within capacity constraints by progressively removing items with the lowest marginal utility when necessary.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1_vals), np.max(obj2_vals)\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = []\n    for sol, obj in archive:\n        potential = (max_obj1 - obj[0]) / (max_obj1 + 1e-10) + (max_obj2 - obj[1]) / (max_obj2 + 1e-10)\n        potentials.append(potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate adaptive priority scores for items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Priority for inclusion: combined marginal gain normalized by weight\n    if len(excluded_items) > 0:\n        marginal_gain_in = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        priority_in = marginal_gain_in / (np.max(marginal_gain_in) + 1e-10)\n    else:\n        priority_in = np.array([])\n\n    # Priority for exclusion: combined marginal gain normalized by weight\n    if len(included_items) > 0:\n        marginal_gain_out = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        priority_out = marginal_gain_out / (np.max(marginal_gain_out) + 1e-10)\n    else:\n        priority_out = np.array([])\n\n    # Step 4: Hybrid local search operator\n    # Phase 1: Adaptive item selection based on marginal gains\n    if len(excluded_items) > 0 and np.random.rand() < 0.6:\n        sorted_in = excluded_items[np.argsort(-priority_in)]\n        for idx in sorted_in:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n                break\n\n    # Phase 2: Capacity-aware swapping\n    if len(included_items) > 0 and len(excluded_items) > 0 and np.random.rand() < 0.4:\n        best_in = excluded_items[np.argmax(priority_in)]\n        best_out = included_items[np.argmax(priority_out)]\n\n        if weight_lst[best_in] <= remaining_capacity + weight_lst[best_out]:\n            new_solution[best_in] = 1\n            new_solution[best_out] = 0\n            remaining_capacity += weight_lst[best_out] - weight_lst[best_in]\n\n    # Phase 3: Dynamic exploration/exploitation\n    if np.random.rand() < 0.3:\n        if np.random.rand() < 0.5 and len(excluded_items) > 0:\n            # Explore: add random item\n            random_idx = np.random.choice(excluded_items)\n            if weight_lst[random_idx] <= remaining_capacity:\n                new_solution[random_idx] = 1\n        elif len(included_items) > 0:\n            # Exploit: remove worst item\n            worst_idx = included_items[np.argmin(priority_out)]\n            new_solution[worst_idx] = 0\n\n    # Step 5: Ensure feasibility through multi-stage repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove the item with the lowest marginal utility\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
          "score": [
               -0.7736042568673399,
               1.6840257346630096
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1_vals), np.max(obj2_vals)\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = []\n    for sol, obj in archive:\n        potential = (max_obj1 - obj[0]) / (max_obj1 + 1e-10) + (max_obj2 - obj[1]) / (max_obj2 + 1e-10)\n        potentials.append(potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate adaptive priority scores for items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Priority for inclusion: combined marginal gain normalized by weight\n    if len(excluded_items) > 0:\n        marginal_gain_in = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        priority_in = marginal_gain_in / (np.max(marginal_gain_in) + 1e-10)\n    else:\n        priority_in = np.array([])\n\n    # Priority for exclusion: combined marginal gain normalized by weight\n    if len(included_items) > 0:\n        marginal_gain_out = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        priority_out = marginal_gain_out / (np.max(marginal_gain_out) + 1e-10)\n    else:\n        priority_out = np.array([])\n\n    # Step 4: Hybrid local search operator\n    # Phase 1: Adaptive item selection based on marginal gains\n    if len(excluded_items) > 0 and np.random.rand() < 0.6:\n        sorted_in = excluded_items[np.argsort(-priority_in)]\n        for idx in sorted_in:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n                break\n\n    # Phase 2: Capacity-aware swapping\n    if len(included_items) > 0 and len(excluded_items) > 0 and np.random.rand() < 0.4:\n        best_in = excluded_items[np.argmax(priority_in)]\n        best_out = included_items[np.argmax(priority_out)]\n\n        if weight_lst[best_in] <= remaining_capacity + weight_lst[best_out]:\n            new_solution[best_in] = 1\n            new_solution[best_out] = 0\n            remaining_capacity += weight_lst[best_out] - weight_lst[best_in]\n\n    # Phase 3: Dynamic exploration/exploitation\n    if np.random.rand() < 0.3:\n        if np.random.rand() < 0.5 and len(excluded_items) > 0:\n            # Explore: add random item\n            random_idx = np.random.choice(excluded_items)\n            if weight_lst[random_idx] <= remaining_capacity:\n                new_solution[random_idx] = 1\n        elif len(included_items) > 0:\n            # Exploit: remove worst item\n            worst_idx = included_items[np.argmin(priority_out)]\n            new_solution[worst_idx] = 0\n\n    # Step 5: Ensure feasibility through multi-stage repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove the item with the lowest marginal utility\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution with high potential for improvement based on its objective values and weight utilization\n    selected_idx = np.random.choice(len(archive))\n    base_solution, (base_val1, base_val2) = archive[selected_idx]\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate the normalized objective values to determine the focus of improvement\n    max_val1 = np.max([obj[0] for _, obj in archive])\n    max_val2 = np.max([obj[1] for _, obj in archive])\n    norm_val1 = base_val1 / max_val1 if max_val1 > 0 else 0\n    norm_val2 = base_val2 / max_val2 if max_val2 > 0 else 0\n\n    # Step 3: Determine the focus of improvement (the objective that is relatively weaker)\n    focus_obj = 1 if norm_val1 < norm_val2 else 2\n\n    # Step 4: Generate candidate items for flipping based on their potential to improve the focused objective\n    candidates = []\n    for i in range(len(base_solution)):\n        if base_solution[i] == 1:\n            # Consider removing item i if it doesn't significantly reduce the focused objective\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, -1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, -1, value2_lst[i], value1_lst[i]))\n        else:\n            # Consider adding item i if it improves the focused objective\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, 1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, 1, value2_lst[i], value1_lst[i]))\n\n    if not candidates:\n        # Step 5: No feasible flips, perform a random feasible flip\n        feasible_indices = [i for i in range(len(base_solution))\n                          if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n        return new_solution\n\n    # Step 6: Select the best candidate based on the focused objective\n    best_candidate = None\n    best_score = -np.inf\n    for candidate in candidates:\n        idx, flip, val_focus, val_other = candidate\n        if flip == 1:\n            score = val_focus  # Adding item: maximize the focused objective\n        else:\n            score = -val_focus  # Removing item: minimize the focused objective\n        if score > best_score:\n            best_score = score\n            best_candidate = candidate\n\n    if best_candidate is not None:\n        idx, flip, _, _ = best_candidate\n        new_solution[idx] = 1 if flip == 1 else 0\n\n    # Step 7: Additional perturbation: perform a series of intelligent swaps between objectives\n    if random.random() < 0.3 and len(new_solution) > 1:\n        # Select a subset of items to consider for swapping\n        swap_candidates = np.where(new_solution == 1)[0]\n        if len(swap_candidates) > 1:\n            np.random.shuffle(swap_candidates)\n            for i in range(min(3, len(swap_candidates)-1)):\n                j = i + 1\n                if j >= len(swap_candidates):\n                    break\n                # Swap items if it maintains feasibility\n                if (current_weight - weight_lst[swap_candidates[i]] - weight_lst[swap_candidates[j]] +\n                    weight_lst[swap_candidates[i]] + weight_lst[swap_candidates[j]]) <= capacity:\n                    new_solution[swap_candidates[i]], new_solution[swap_candidates[j]] = \\\n                    new_solution[swap_candidates[j]], new_solution[swap_candidates[i]]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm selects a base solution from the archive by identifying the solution with the highest potential for improvement in both objectives while considering the solution's position in the Pareto front, then applies a novel hybrid local search operator that dynamically combines a multi-objective value-weighted random walk with a capacity-aware item clustering strategy. This operator first groups items into clusters based on their value ratios and weights, then probabilistically selects clusters to flip based on their combined marginal utility for both objectives, with the selection probability biased towards clusters that offer the highest improvements in either objective while maintaining feasibility through a capacity-aware filtering mechanism. The operator also incorporates a dynamic perturbation mechanism that randomly swaps entire clusters between objectives with a small probability to further explore the solution space, while ensuring the solution remains within the capacity constraint through a greedy repair mechanism that prioritizes the removal of items with the lowest marginal utility when necessary. The algorithm also employs a novel \"cluster dominance\" criterion that evaluates the impact of flipping each cluster on the Pareto front, favoring flips that are either non-dominated or have the highest combined marginal utility for both objectives, with the selection process guided by a utility function that weighs both objective improvements and solution diversity.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives to calculate potential improvement\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n    potentials = (1 - norm_obj1) + (1 - norm_obj2)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Cluster items based on value ratios and weights\n    value_ratios = value1_lst / (value2_lst + 1e-10)\n    cluster_labels = np.digitize(value_ratios, bins=np.percentile(value_ratios, [33, 66]))\n    unique_clusters = np.unique(cluster_labels)\n\n    # Step 3: Calculate cluster properties\n    cluster_properties = []\n    for cluster in unique_clusters:\n        cluster_items = np.where(cluster_labels == cluster)[0]\n        cluster_weight = np.sum(weight_lst[cluster_items])\n        cluster_value1 = np.sum(value1_lst[cluster_items])\n        cluster_value2 = np.sum(value2_lst[cluster_items])\n        cluster_properties.append((cluster, cluster_items, cluster_weight, cluster_value1, cluster_value2))\n\n    # Step 4: Dynamic cluster selection for flipping\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal utility for each cluster\n    cluster_utilities = []\n    for cluster, items, c_weight, c_value1, c_value2 in cluster_properties:\n        # Calculate marginal gain if cluster is flipped\n        cluster_in_solution = np.any(new_solution[items] == 1)\n        if cluster_in_solution:\n            marginal_gain = -c_value1 - c_value2\n        else:\n            marginal_gain = c_value1 + c_value2\n        cluster_utilities.append((cluster, marginal_gain, c_weight, items))\n\n    # Sort clusters by utility (descending)\n    cluster_utilities.sort(key=lambda x: -x[1])\n\n    # Step 5: Apply hybrid local search operator\n    for cluster, marginal_gain, c_weight, items in cluster_utilities:\n        cluster_in_solution = np.any(new_solution[items] == 1)\n\n        # Probabilistic flip based on marginal utility\n        flip_prob = 1 / (1 + np.exp(-0.1 * marginal_gain))\n        if np.random.rand() < flip_prob:\n            if cluster_in_solution:\n                # Remove cluster if it's in solution\n                new_solution[items] = 0\n                remaining_capacity += c_weight\n            else:\n                # Add cluster if feasible\n                if c_weight <= remaining_capacity:\n                    new_solution[items] = 1\n                    remaining_capacity -= c_weight\n\n    # Step 6: Dynamic perturbation (random cluster swap)\n    if np.random.rand() < 0.1 and len(unique_clusters) > 1:\n        cluster1, cluster2 = np.random.choice(unique_clusters, size=2, replace=False)\n        items1 = np.where(cluster_labels == cluster1)[0]\n        items2 = np.where(cluster_labels == cluster2)[0]\n\n        # Check if swap is feasible\n        weight1 = np.sum(weight_lst[items1]) if np.any(new_solution[items1] == 1) else 0\n        weight2 = np.sum(weight_lst[items2]) if np.any(new_solution[items2] == 1) else 0\n\n        if (weight1 == 0 and weight2 <= remaining_capacity + weight1) or \\\n           (weight2 == 0 and weight1 <= remaining_capacity + weight2):\n            # Perform cluster swap\n            if weight1 > 0:\n                new_solution[items1] = 0\n                new_solution[items2] = 1\n                remaining_capacity += weight1 - weight2\n            else:\n                new_solution[items1] = 1\n                new_solution[items2] = 0\n                remaining_capacity += weight2 - weight1\n\n    # Step 7: Ensure feasibility through greedy repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Find items with lowest marginal utility\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 304,
          "algorithm": "{The new algorithm first analyzes the archive to identify solutions with high potential for improvement by evaluating their objective values and weight utilization, then selects a base solution with a hybrid approach that combines both objective improvements and solution diversity. It then applies a novel \"value-aware item substitution\" operator that dynamically replaces items in the solution with others from the archive, prioritizing substitutions that offer the highest combined marginal utility for both objectives while maintaining feasibility through a capacity-aware filtering mechanism. The operator also incorporates a probabilistic \"objective-balancing\" mechanism that randomly swaps entire subsets of items between objectives with a small probability to further explore the solution space, with the selection process guided by a utility function that weighs both objective improvements and solution diversity. Additionally, the algorithm employs a \"Pareto-front aware\" repair mechanism that ensures the solution remains within the capacity constraint by prioritizing the removal of items that are least Pareto-efficient, favoring those with the lowest marginal utility for both objectives.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with hybrid approach\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n\n    # Calculate selection probability\n    selection_probs = (1 - norm_obj1) + (1 - norm_obj2)\n    selection_probs = selection_probs / np.sum(selection_probs)\n\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Value-aware item substitution\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate substitution utilities\n    substitution_utilities = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Calculate marginal gain if removed\n            marginal_gain = -(value1_lst[i] + value2_lst[i])\n            substitution_utilities.append((i, marginal_gain, -1))  # -1 for removal\n        else:\n            # Calculate marginal gain if added (if feasible)\n            if weight_lst[i] <= remaining_capacity:\n                marginal_gain = value1_lst[i] + value2_lst[i]\n                substitution_utilities.append((i, marginal_gain, 1))  # 1 for addition\n\n    # Sort by utility (descending)\n    substitution_utilities.sort(key=lambda x: -x[1])\n\n    # Apply substitutions\n    for item_idx, marginal_gain, action in substitution_utilities:\n        if action == 1:  # Addition\n            if weight_lst[item_idx] <= remaining_capacity:\n                new_solution[item_idx] = 1\n                remaining_capacity -= weight_lst[item_idx]\n        else:  # Removal\n            new_solution[item_idx] = 0\n            remaining_capacity += weight_lst[item_idx]\n\n    # Step 3: Objective-balancing perturbation\n    if np.random.rand() < 0.2:\n        # Select a subset of items to swap objectives\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 1:\n            np.random.shuffle(included_items)\n            swap_size = min(3, len(included_items) // 2)\n            for i in range(swap_size):\n                # Find an item to swap with\n                for j in range(len(included_items)):\n                    if i == j:\n                        continue\n                    # Check if swap maintains feasibility\n                    if (weight_lst[included_items[i]] <= remaining_capacity + weight_lst[included_items[j]] and\n                        weight_lst[included_items[j]] <= remaining_capacity + weight_lst[included_items[i]]):\n                        # Perform swap\n                        new_solution[included_items[i]], new_solution[included_items[j]] = (\n                            new_solution[included_items[j]], new_solution[included_items[i]])\n                        remaining_capacity += weight_lst[included_items[j]] - weight_lst[included_items[i]]\n                        break\n\n    # Step 4: Pareto-front aware repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate Pareto efficiency score (lower is better)\n        pareto_scores = []\n        for idx in included_items:\n            # Score based on both objectives (normalized)\n            score = (value1_lst[idx] / (max_obj1 + 1e-10)) + (value2_lst[idx] / (max_obj2 + 1e-10))\n            pareto_scores.append((idx, score))\n\n        # Remove least efficient item\n        worst_idx = min(pareto_scores, key=lambda x: x[1])[0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
          "score": [
               -0.9761672771221968,
               2.3054015934467316
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with hybrid approach\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n\n    # Calculate selection probability\n    selection_probs = (1 - norm_obj1) + (1 - norm_obj2)\n    selection_probs = selection_probs / np.sum(selection_probs)\n\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Value-aware item substitution\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate substitution utilities\n    substitution_utilities = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Calculate marginal gain if removed\n            marginal_gain = -(value1_lst[i] + value2_lst[i])\n            substitution_utilities.append((i, marginal_gain, -1))  # -1 for removal\n        else:\n            # Calculate marginal gain if added (if feasible)\n            if weight_lst[i] <= remaining_capacity:\n                marginal_gain = value1_lst[i] + value2_lst[i]\n                substitution_utilities.append((i, marginal_gain, 1))  # 1 for addition\n\n    # Sort by utility (descending)\n    substitution_utilities.sort(key=lambda x: -x[1])\n\n    # Apply substitutions\n    for item_idx, marginal_gain, action in substitution_utilities:\n        if action == 1:  # Addition\n            if weight_lst[item_idx] <= remaining_capacity:\n                new_solution[item_idx] = 1\n                remaining_capacity -= weight_lst[item_idx]\n        else:  # Removal\n            new_solution[item_idx] = 0\n            remaining_capacity += weight_lst[item_idx]\n\n    # Step 3: Objective-balancing perturbation\n    if np.random.rand() < 0.2:\n        # Select a subset of items to swap objectives\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 1:\n            np.random.shuffle(included_items)\n            swap_size = min(3, len(included_items) // 2)\n            for i in range(swap_size):\n                # Find an item to swap with\n                for j in range(len(included_items)):\n                    if i == j:\n                        continue\n                    # Check if swap maintains feasibility\n                    if (weight_lst[included_items[i]] <= remaining_capacity + weight_lst[included_items[j]] and\n                        weight_lst[included_items[j]] <= remaining_capacity + weight_lst[included_items[i]]):\n                        # Perform swap\n                        new_solution[included_items[i]], new_solution[included_items[j]] = (\n                            new_solution[included_items[j]], new_solution[included_items[i]])\n                        remaining_capacity += weight_lst[included_items[j]] - weight_lst[included_items[i]]\n                        break\n\n    # Step 4: Pareto-front aware repair\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate Pareto efficiency score (lower is better)\n        pareto_scores = []\n        for idx in included_items:\n            # Score based on both objectives (normalized)\n            score = (value1_lst[idx] / (max_obj1 + 1e-10)) + (value2_lst[idx] / (max_obj2 + 1e-10))\n            pareto_scores.append((idx, score))\n\n        # Remove least efficient item\n        worst_idx = min(pareto_scores, key=lambda x: x[1])[0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The proposed algorithm first identifies the most promising solution in the archive by evaluating its potential for improvement through a novel hybrid local search strategy that combines a multi-objective greedy insertion strategy with a probabilistic multi-item perturbation, while ensuring feasibility by using a capacity-aware knapsack construction heuristic. It then applies a creative local search operator that dynamically adapts between item swaps, insertions, and removals based on the current solution's objective values and the archive's distribution, prioritizing items that offer the highest marginal gain in either objective while allowing controlled exploration of the solution space to balance exploitation and exploration. The operator explores neighboring solutions by flipping bits of items with high marginal contributions to both objectives, while also considering the impact on the solution's position in the Pareto front, and the selection of the base solution is biased towards those with the highest potential for improvement as estimated by the ratio of the current objective values to the maximum possible values in the archive, with the perturbation intensity adaptively adjusted based on the solution's proximity to the Pareto front.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with high potential for improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_values = (np.max(obj1_vals), np.max(obj2_vals))\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = [(max_values[0] - obj[0])/max_values[0] + (max_values[1] - obj[1])/max_values[1] for _, obj in archive]\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Novel hybrid local search strategy\n    # Calculate dominance score (number of solutions dominated by this solution)\n    dominance_scores = []\n    for sol, obj in archive:\n        dominates = (obj[0] >= archive[selected_idx][1][0] and obj[1] >= archive[selected_idx][1][1]) and \\\n                   (obj[0] > archive[selected_idx][1][0] or obj[1] > archive[selected_idx][1][1])\n        dominance_scores.append(dominates)\n    dominance_score = sum(dominance_scores)\n\n    # Step 4: Dynamic operator selection with novel hybrid approach\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate objective ratios for adaptive weighting\n    obj_ratio = archive[selected_idx][1][0] / archive[selected_idx][1][1] if archive[selected_idx][1][1] > 0 else 1.0\n\n    # Novel hybrid operator selection\n    if dominance_score > len(archive) * 0.5:  # Solution is on Pareto front\n        # More exploration: favor insertions and removals\n        operator_probs = [0.2, 0.5, 0.3]  # swap, insert, remove\n    else:\n        # More exploitation: favor swaps\n        operator_probs = [0.7, 0.15, 0.15]  # swap, insert, remove\n\n    operator = np.random.choice(['swap', 'insert', 'remove'], p=operator_probs)\n\n    # Step 5: Apply selected operator with novel hybrid approach\n    if operator == 'swap' and len(included_items) > 0 and len(excluded_items) > 0:\n        # Novel hybrid swap strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.5 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        # Calculate marginal gains for both objectives\n        marginal_gain_in = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        marginal_gain_out = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n\n        # Select best items to swap\n        best_in_idx = excluded_items[np.argmax(marginal_gain_in)]\n        best_out_idx = included_items[np.argmax(marginal_gain_out)]\n\n        # Check if swap is feasible\n        if weight_lst[best_in_idx] <= remaining_capacity + weight_lst[best_out_idx]:\n            new_solution[best_in_idx] = 1\n            new_solution[best_out_idx] = 0\n\n    elif operator == 'insert' and len(excluded_items) > 0:\n        # Novel hybrid insertion strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.6 + 0.2 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        sorted_indices = excluded_items[np.argsort(-marginal_gain)]\n\n        # Adaptive insertion intensity based on remaining capacity and randomness\n        insertion_intensity = min(0.5, remaining_capacity / capacity) * (0.7 + 0.3 * np.random.rand())\n        num_to_insert = max(1, int(len(sorted_indices) * insertion_intensity))\n\n        for idx in sorted_indices[:num_to_insert]:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    elif operator == 'remove' and len(included_items) > 0:\n        # Novel hybrid removal strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.4 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(marginal_gain)]\n\n        # Adaptive removal intensity based on solution's position and randomness\n        removal_intensity = (0.2 if dominance_score > len(archive) * 0.5 else 0.1) * (0.7 + 0.3 * np.random.rand())\n        num_to_remove = max(1, int(len(sorted_indices) * removal_intensity))\n\n        for idx in sorted_indices[:num_to_remove]:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n\n    # Step 6: Ensure feasibility with novel hybrid approach\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        # Remove items with lowest combined marginal gain\n        weight_obj1 = 0.5 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        heaviest_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[heaviest_idx] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The proposed algorithm first identifies the most promising solution in the archive by evaluating its potential for improvement through a novel hybrid local search strategy that combines a multi-objective greedy insertion strategy with a probabilistic multi-item perturbation, while ensuring feasibility by using a capacity-aware knapsack construction heuristic. It then applies a creative local search operator that dynamically adapts between item swaps, insertions, and removals based on the current solution's objective values and the archive's distribution, prioritizing items that offer the highest marginal gain in either objective while allowing controlled exploration of the solution space to balance exploitation and exploration. The operator explores neighboring solutions by flipping bits of items with high marginal contributions to both objectives, while also considering the impact on the solution's position in the Pareto front, and the selection of the base solution is biased towards those with the highest potential for improvement as estimated by the ratio of the current objective values to the maximum possible values in the archive, with the perturbation intensity adaptively adjusted based on the solution's proximity to the Pareto front.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_values = (np.max(obj1_vals), np.max(obj2_vals))\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = [(max_values[0] - obj[0])/max_values[0] + (max_values[1] - obj[1])/max_values[1] for _, obj in archive]\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Dynamic operator selection based on solution's position\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dominance score (number of solutions dominated by this solution)\n    dominance_scores = []\n    for sol, obj in archive:\n        dominates = (obj[0] >= archive[selected_idx][1][0] and obj[1] >= archive[selected_idx][1][1]) and \\\n                    (obj[0] > archive[selected_idx][1][0] or obj[1] > archive[selected_idx][1][1])\n        dominance_scores.append(dominates)\n    dominance_score = sum(dominance_scores)\n\n    # Operator selection probabilities\n    if dominance_score > len(archive) * 0.6:  # Solution is on Pareto front\n        operator_probs = [0.3, 0.4, 0.3]  # More exploration (insert/remove)\n    else:\n        operator_probs = [0.6, 0.2, 0.2]  # More exploitation (swap)\n\n    operator = np.random.choice(['swap', 'insert', 'remove'], p=operator_probs)\n\n    # Step 4: Apply selected operator\n    if operator == 'swap' and len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal gains using a weighted combination of objectives\n        weight_obj1 = 0.6 if np.random.rand() > 0.5 else 0.4\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain_in = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        marginal_gain_out = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n\n        best_in_idx = excluded_items[np.argmax(marginal_gain_in)]\n        best_out_idx = included_items[np.argmax(marginal_gain_out)]\n\n        # Check if swap is feasible\n        if weight_lst[best_in_idx] <= remaining_capacity + weight_lst[best_out_idx]:\n            new_solution[best_in_idx] = 1\n            new_solution[best_out_idx] = 0\n\n    elif operator == 'insert' and len(excluded_items) > 0:\n        # Insert items with highest combined marginal gain with adaptive weights\n        weight_obj1 = 0.7 if np.random.rand() > 0.5 else 0.3\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        sorted_indices = excluded_items[np.argsort(-marginal_gain)]\n\n        # Adaptive insertion intensity based on remaining capacity and randomness\n        insertion_intensity = min(0.4, remaining_capacity / capacity) * (0.8 + 0.4 * np.random.rand())\n        num_to_insert = max(1, int(len(sorted_indices) * insertion_intensity))\n\n        for idx in sorted_indices[:num_to_insert]:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    elif operator == 'remove' and len(included_items) > 0:\n        # Remove items with lowest combined marginal gain with adaptive weights\n        weight_obj1 = 0.5 if np.random.rand() > 0.5 else 0.5\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(marginal_gain)]\n\n        # Adaptive removal intensity based on solution's position and randomness\n        removal_intensity = (0.3 if dominance_score > len(archive) * 0.6 else 0.1) * (0.8 + 0.4 * np.random.rand())\n        num_to_remove = max(1, int(len(sorted_indices) * removal_intensity))\n\n        for idx in sorted_indices[:num_to_remove]:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        heaviest_idx = included_items[np.argmax(weight_lst[included_items])]\n        new_solution[heaviest_idx] = 0\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 305,
          "algorithm": "{The proposed algorithm first identifies the most promising solution in the archive by evaluating its potential for improvement through a novel hybrid local search strategy that combines a multi-objective greedy insertion strategy with a probabilistic multi-item perturbation, while ensuring feasibility by using a capacity-aware knapsack construction heuristic. It then applies a creative local search operator that dynamically adapts between item swaps, insertions, and removals based on the current solution's objective values and the archive's distribution, prioritizing items that offer the highest marginal gain in either objective while allowing controlled exploration of the solution space to balance exploitation and exploration. The operator explores neighboring solutions by flipping bits of items with high marginal contributions to both objectives, while also considering the impact on the solution's position in the Pareto front, and the selection of the base solution is biased towards those with the highest potential for improvement as estimated by the ratio of the current objective values to the maximum possible values in the archive, with the perturbation intensity adaptively adjusted based on the solution's proximity to the Pareto front.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_values = (np.max(obj1_vals), np.max(obj2_vals))\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = [(max_values[0] - obj[0])/max_values[0] + (max_values[1] - obj[1])/max_values[1] for _, obj in archive]\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Novel hybrid local search strategy\n    # Calculate dominance score (number of solutions dominated by this solution)\n    dominance_scores = []\n    for sol, obj in archive:\n        dominates = (obj[0] >= archive[selected_idx][1][0] and obj[1] >= archive[selected_idx][1][1]) and \\\n                   (obj[0] > archive[selected_idx][1][0] or obj[1] > archive[selected_idx][1][1])\n        dominance_scores.append(dominates)\n    dominance_score = sum(dominance_scores)\n\n    # Step 4: Dynamic operator selection with novel hybrid approach\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate objective ratios for adaptive weighting\n    obj_ratio = archive[selected_idx][1][0] / archive[selected_idx][1][1] if archive[selected_idx][1][1] > 0 else 1.0\n\n    # Novel hybrid operator selection\n    if dominance_score > len(archive) * 0.5:  # Solution is on Pareto front\n        # More exploration: favor insertions and removals\n        operator_probs = [0.2, 0.5, 0.3]  # swap, insert, remove\n    else:\n        # More exploitation: favor swaps\n        operator_probs = [0.7, 0.15, 0.15]  # swap, insert, remove\n\n    operator = np.random.choice(['swap', 'insert', 'remove'], p=operator_probs)\n\n    # Step 5: Apply selected operator with novel hybrid approach\n    if operator == 'swap' and len(included_items) > 0 and len(excluded_items) > 0:\n        # Novel hybrid swap strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.5 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        # Calculate marginal gains for both objectives\n        marginal_gain_in = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        marginal_gain_out = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n\n        # Select best items to swap\n        best_in_idx = excluded_items[np.argmax(marginal_gain_in)]\n        best_out_idx = included_items[np.argmax(marginal_gain_out)]\n\n        # Check if swap is feasible\n        if weight_lst[best_in_idx] <= remaining_capacity + weight_lst[best_out_idx]:\n            new_solution[best_in_idx] = 1\n            new_solution[best_out_idx] = 0\n\n    elif operator == 'insert' and len(excluded_items) > 0:\n        # Novel hybrid insertion strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.6 + 0.2 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        sorted_indices = excluded_items[np.argsort(-marginal_gain)]\n\n        # Adaptive insertion intensity based on remaining capacity and randomness\n        insertion_intensity = min(0.5, remaining_capacity / capacity) * (0.7 + 0.3 * np.random.rand())\n        num_to_insert = max(1, int(len(sorted_indices) * insertion_intensity))\n\n        for idx in sorted_indices[:num_to_insert]:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    elif operator == 'remove' and len(included_items) > 0:\n        # Novel hybrid removal strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.4 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(marginal_gain)]\n\n        # Adaptive removal intensity based on solution's position and randomness\n        removal_intensity = (0.2 if dominance_score > len(archive) * 0.5 else 0.1) * (0.7 + 0.3 * np.random.rand())\n        num_to_remove = max(1, int(len(sorted_indices) * removal_intensity))\n\n        for idx in sorted_indices[:num_to_remove]:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n\n    # Step 6: Ensure feasibility with novel hybrid approach\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        # Remove items with lowest combined marginal gain\n        weight_obj1 = 0.5 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        heaviest_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[heaviest_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -0.7502853454242171,
               1.6071198284626007
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution with highest potential improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_values = (np.max(obj1_vals), np.max(obj2_vals))\n\n    # Calculate potential for each solution (normalized difference)\n    potentials = [(max_values[0] - obj[0])/max_values[0] + (max_values[1] - obj[1])/max_values[1] for _, obj in archive]\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Novel hybrid local search strategy\n    # Calculate dominance score (number of solutions dominated by this solution)\n    dominance_scores = []\n    for sol, obj in archive:\n        dominates = (obj[0] >= archive[selected_idx][1][0] and obj[1] >= archive[selected_idx][1][1]) and \\\n                   (obj[0] > archive[selected_idx][1][0] or obj[1] > archive[selected_idx][1][1])\n        dominance_scores.append(dominates)\n    dominance_score = sum(dominance_scores)\n\n    # Step 4: Dynamic operator selection with novel hybrid approach\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate objective ratios for adaptive weighting\n    obj_ratio = archive[selected_idx][1][0] / archive[selected_idx][1][1] if archive[selected_idx][1][1] > 0 else 1.0\n\n    # Novel hybrid operator selection\n    if dominance_score > len(archive) * 0.5:  # Solution is on Pareto front\n        # More exploration: favor insertions and removals\n        operator_probs = [0.2, 0.5, 0.3]  # swap, insert, remove\n    else:\n        # More exploitation: favor swaps\n        operator_probs = [0.7, 0.15, 0.15]  # swap, insert, remove\n\n    operator = np.random.choice(['swap', 'insert', 'remove'], p=operator_probs)\n\n    # Step 5: Apply selected operator with novel hybrid approach\n    if operator == 'swap' and len(included_items) > 0 and len(excluded_items) > 0:\n        # Novel hybrid swap strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.5 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        # Calculate marginal gains for both objectives\n        marginal_gain_in = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        marginal_gain_out = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n\n        # Select best items to swap\n        best_in_idx = excluded_items[np.argmax(marginal_gain_in)]\n        best_out_idx = included_items[np.argmax(marginal_gain_out)]\n\n        # Check if swap is feasible\n        if weight_lst[best_in_idx] <= remaining_capacity + weight_lst[best_out_idx]:\n            new_solution[best_in_idx] = 1\n            new_solution[best_out_idx] = 0\n\n    elif operator == 'insert' and len(excluded_items) > 0:\n        # Novel hybrid insertion strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.6 + 0.2 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[excluded_items] + weight_obj2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        sorted_indices = excluded_items[np.argsort(-marginal_gain)]\n\n        # Adaptive insertion intensity based on remaining capacity and randomness\n        insertion_intensity = min(0.5, remaining_capacity / capacity) * (0.7 + 0.3 * np.random.rand())\n        num_to_insert = max(1, int(len(sorted_indices) * insertion_intensity))\n\n        for idx in sorted_indices[:num_to_insert]:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    elif operator == 'remove' and len(included_items) > 0:\n        # Novel hybrid removal strategy: consider both objectives with adaptive weights\n        weight_obj1 = 0.4 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(marginal_gain)]\n\n        # Adaptive removal intensity based on solution's position and randomness\n        removal_intensity = (0.2 if dominance_score > len(archive) * 0.5 else 0.1) * (0.7 + 0.3 * np.random.rand())\n        num_to_remove = max(1, int(len(sorted_indices) * removal_intensity))\n\n        for idx in sorted_indices[:num_to_remove]:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n\n    # Step 6: Ensure feasibility with novel hybrid approach\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        # Remove items with lowest combined marginal gain\n        weight_obj1 = 0.5 + 0.3 * obj_ratio * (0.8 + 0.4 * np.random.rand())\n        weight_obj2 = 1 - weight_obj1\n\n        marginal_gain = (weight_obj1 * value1_lst[included_items] + weight_obj2 * value2_lst[included_items]) / weight_lst[included_items]\n        heaviest_idx = included_items[np.argmin(marginal_gain)]\n        new_solution[heaviest_idx] = 0\n\n    return new_solution\n\n"
     }
]