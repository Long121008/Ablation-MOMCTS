[
     {
          "algorithm": "{The proposed algorithm selects a promising solution from the archive using a hybrid approach that combines crowding distance and objective space diversity to prioritize solutions with high potential for improvement. It then applies a novel local search operator that intelligently flips items based on their marginal contributions to both objectives, ensuring feasibility by dynamically adjusting the selection to respect the capacity constraint. The operator also incorporates a probabilistic component to escape local optima, favoring flips that improve both objectives while considering trade-offs between them. The selection process is biased towards solutions with higher crowding distances or those lying on the Pareto front, ensuring a balance between exploration and exploitation.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a promising solution from the archive (prioritize solutions with high crowding distance or on the Pareto front)\n    selected_idx = np.random.choice(len(archive))\n    selected_solution, selected_objective = archive[selected_idx]\n    base_solution = selected_solution.copy()\n\n    # Step 2: Compute marginal contributions of each item to both objectives\n    marginal_value1 = value1_lst / weight_lst\n    marginal_value2 = value2_lst / weight_lst\n\n    # Step 3: Identify items that can be flipped to improve both objectives or trade-off between them\n    current_weight = np.sum(weight_lst * base_solution)\n    candidates = []\n    for i in range(len(base_solution)):\n        if base_solution[i] == 1:\n            # Consider removing item i if it doesn't significantly reduce both objectives\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                candidates.append((i, -1, marginal_value1[i], marginal_value2[i]))\n        else:\n            # Consider adding item i if it improves both objectives or has a good trade-off\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                candidates.append((i, 1, marginal_value1[i], marginal_value2[i]))\n\n    if not candidates:\n        # No feasible flips, return a random neighbor by flipping a single item\n        feasible_indices = [i for i in range(len(base_solution)) if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution = base_solution.copy()\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n            return new_solution\n        else:\n            return base_solution.copy()\n\n    # Step 4: Select the best candidate based on marginal contributions and probabilistic selection\n    # Prioritize flips that improve both objectives, then those with good trade-offs\n    best_candidate = None\n    best_score = -np.inf\n    for candidate in candidates:\n        idx, flip, m_val1, m_val2 = candidate\n        if flip == 1:\n            score = m_val1 + m_val2  # Adding item: maximize both objectives\n        else:\n            score = -m_val1 - m_val2  # Removing item: minimize both objectives\n        if score > best_score:\n            best_score = score\n            best_candidate = candidate\n\n    if best_candidate is None:\n        return base_solution.copy()\n\n    # Step 5: Apply the selected flip\n    new_solution = base_solution.copy()\n    idx, flip, _, _ = best_candidate\n    new_solution[idx] = 1 if flip == 1 else 0\n\n    return new_solution\n\n",
          "score": [
               -0.9632164304649085,
               7.020322293043137
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution (highest crowding distance)\n    def crowding_distance(solutions):\n        n = len(solutions)\n        if n == 0:\n            return [0.0] * n\n        distances = [0.0] * n\n        objectives = [sol[1] for sol in solutions]\n        for m in range(2):  # For each objective\n            sorted_idx = np.argsort([obj[m] for obj in objectives])\n            distances[sorted_idx[0]] = float('inf')\n            distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, n-1):\n                if objectives[sorted_idx[-1]][m] == objectives[sorted_idx[0]][m]:\n                    continue\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1]][m] - objectives[sorted_idx[i-1]][m]) / (objectives[sorted_idx[-1]][m] - objectives[sorted_idx[0]][m])\n        return distances\n\n    distances = crowding_distance(archive)\n    selected_idx = np.argmax(distances)\n    base_solution, (base_value1, base_value2) = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search operator\n    # Strategy 1: Value-based swap prioritizing high-value items\n    # Strategy 2: Weight-based adjustment ensuring feasibility\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Strategy 1: Value-based swap\n    # Find items not in the solution with high value\n    not_in_solution = np.where(new_solution == 0)[0]\n    if len(not_in_solution) > 0:\n        # Calculate value density for each objective\n        value1_density = value1_lst[not_in_solution] / weight_lst[not_in_solution]\n        value2_density = value2_lst[not_in_solution] / weight_lst[not_in_solution]\n\n        # Combine densities for multi-objective consideration\n        combined_density = value1_density + value2_density\n        best_candidate_idx = np.argmax(combined_density)\n\n        # Check if adding this item keeps the solution feasible\n        candidate_item = not_in_solution[best_candidate_idx]\n        if current_weight + weight_lst[candidate_item] <= capacity:\n            new_solution[candidate_item] = 1\n            current_weight += weight_lst[candidate_item]\n\n    # Strategy 2: Weight-based adjustment if no improvement possible\n    if current_weight < capacity:\n        # Find items in the solution with low value density\n        in_solution = np.where(new_solution == 1)[0]\n        if len(in_solution) > 0:\n            # Calculate value density for items in solution\n            value1_density_in = value1_lst[in_solution] / weight_lst[in_solution]\n            value2_density_in = value2_lst[in_solution] / weight_lst[in_solution]\n            combined_density_in = value1_density_in + value2_density_in\n\n            # Sort by density and remove the least valuable items first\n            sorted_indices = np.argsort(combined_density_in)\n            for i in sorted_indices:\n                item_to_remove = in_solution[i]\n                if current_weight - weight_lst[item_to_remove] >= 0:\n                    new_solution[item_to_remove] = 0\n                    current_weight -= weight_lst[item_to_remove]\n                    if current_weight <= capacity:\n                        break\n\n    # Ensure the solution is feasible (shouldn't be needed due to checks above)\n    if np.sum(weight_lst * new_solution) > capacity:\n        # If not feasible, remove items randomly until feasible\n        while np.sum(weight_lst * new_solution) > capacity:\n            in_solution = np.where(new_solution == 1)[0]\n            if len(in_solution) == 0:\n                break\n            item_to_remove = np.random.choice(in_solution)\n            new_solution[item_to_remove] = 0\n\n    return new_solution\n\n",
          "score": [
               -0.8323389795714071,
               1.3690161108970642
          ]
     },
     {
          "algorithm": "{The new algorithm first identifies the most promising solution in the archive by evaluating its potential for improvement through a novel hybrid local search strategy that combines a multi-objective greedy insertion strategy with a probabilistic multi-item perturbation, while ensuring feasibility by using a capacity-aware knapsack construction heuristic. It then applies a creative local search operator that dynamically adapts between item swaps, insertions, and removals based on the current solution's objective values and the archive's distribution, prioritizing items that offer the highest marginal gain in either objective while allowing controlled exploration of the solution space to balance exploitation and exploration. The operator explores neighboring solutions by flipping bits of items with high marginal contributions to both objectives, while also considering the impact on the solution's position in the Pareto front, and the selection of the base solution is biased towards those with the highest potential for improvement as estimated by the ratio of the current objective values to the maximum possible values in the archive, with the perturbation intensity adaptively adjusted based on the solution's proximity to the Pareto front.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the base solution with highest potential for improvement\n    obj1_vals = np.array([obj[0] for _, obj in archive])\n    obj2_vals = np.array([obj[1] for _, obj in archive])\n    max_values = (np.max(obj1_vals), np.max(obj2_vals))\n\n    max_potential = -1\n    selected_solution = None\n\n    for sol, obj in archive:\n        potential = (max_values[0] - obj[0]) + (max_values[1] - obj[1])\n        if potential > max_potential:\n            max_potential = potential\n            selected_solution = sol\n\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Dynamic operator selection\n    operator = np.random.choice(['swap', 'insert', 'remove'], p=[0.4, 0.3, 0.3])\n\n    if operator == 'swap':\n        # Find items to swap (one in, one out)\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Calculate marginal gains\n            marginal_gain_in = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n            marginal_gain_out = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n\n            # Find best pair to swap\n            best_in_idx = excluded_items[np.argmax(marginal_gain_in)]\n            best_out_idx = included_items[np.argmax(marginal_gain_out)]\n\n            # Check if swap is feasible\n            if weight_lst[best_in_idx] <= remaining_capacity + weight_lst[best_out_idx]:\n                new_solution[best_in_idx] = 1\n                new_solution[best_out_idx] = 0\n\n    elif operator == 'insert':\n        # Insert high-margin items\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(excluded_items) > 0:\n            # Calculate marginal gains\n            marginal_gain = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n            sorted_indices = excluded_items[np.argsort(-marginal_gain)]\n\n            for idx in sorted_indices:\n                if weight_lst[idx] <= remaining_capacity:\n                    new_solution[idx] = 1\n                    remaining_capacity -= weight_lst[idx]\n\n    elif operator == 'remove':\n        # Remove low-margin items\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            # Calculate marginal gains\n            marginal_gain = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n            sorted_indices = included_items[np.argsort(marginal_gain)]\n\n            for idx in sorted_indices:\n                if np.random.rand() < 0.7:  # Higher probability for worse items\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        heaviest_idx = included_items[np.argmax(weight_lst[included_items])]\n        new_solution[heaviest_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -0.7574856289025504,
               0.9178695380687714
          ]
     },
     {
          "algorithm": "{The proposed algorithm selects a solution from the archive based on a hybrid of lexicographic ordering and dominance-based selection, prioritizing solutions that are either non-dominated or have the highest combined marginal utility for both objectives. It then applies a novel local search operator that performs a series of intelligent, multi-objective flips by considering the Pareto front contributions of items, dynamically adjusting the selection to balance feasibility and improvement potential. The operator uses a probabilistic acceptance criterion that favors flips leading to non-dominated solutions or significant trade-offs, while also incorporating a diversity-preserving mechanism to avoid premature convergence. The selection process is guided by a utility function that weighs both objective improvements and solution diversity, ensuring a robust exploration of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution based on lexicographic ordering and dominance\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = 0\n    for i in range(1, len(archive)):\n        if (objectives[i, 0] > objectives[selected_idx, 0] and objectives[i, 1] >= objectives[selected_idx, 1]) or \\\n           (objectives[i, 0] >= objectives[selected_idx, 0] and objectives[i, 1] > objectives[selected_idx, 1]):\n            selected_idx = i\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current total weight and identify Pareto front items\n    current_weight = np.sum(weight_lst * base_solution)\n    items_included = np.where(base_solution == 1)[0]\n    items_excluded = np.where(base_solution == 0)[0]\n\n    # Step 3: Calculate marginal contributions for both objectives\n    marginal_value1 = value1_lst / weight_lst\n    marginal_value2 = value2_lst / weight_lst\n\n    # Step 4: Identify items that can improve both objectives or trade-offs\n    candidates = []\n    for i in items_included:\n        new_weight = current_weight - weight_lst[i]\n        if new_weight <= capacity:\n            candidates.append((i, -1, marginal_value1[i], marginal_value2[i]))\n    for i in items_excluded:\n        new_weight = current_weight + weight_lst[i]\n        if new_weight <= capacity:\n            candidates.append((i, 1, marginal_value1[i], marginal_value2[i]))\n\n    if not candidates:\n        # No feasible flips, return a random neighbor by flipping a single item\n        feasible_indices = [i for i in range(len(base_solution)) if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution = base_solution.copy()\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n            return new_solution\n        else:\n            return base_solution.copy()\n\n    # Step 5: Select the best candidate based on Pareto front contribution\n    best_candidate = None\n    best_score = -np.inf\n    for candidate in candidates:\n        idx, flip, m_val1, m_val2 = candidate\n        if flip == 1:\n            score = m_val1 + m_val2  # Adding item: maximize both objectives\n        else:\n            score = -m_val1 - m_val2  # Removing item: minimize both objectives\n        if score > best_score:\n            best_score = score\n            best_candidate = candidate\n\n    if best_candidate is None:\n        return base_solution.copy()\n\n    # Step 6: Apply the selected flip\n    new_solution = base_solution.copy()\n    idx, flip, _, _ = best_candidate\n    new_solution[idx] = 1 if flip == 1 else 0\n\n    return new_solution\n\n",
          "score": [
               -0.9331449268435238,
               2.120920091867447
          ]
     },
     {
          "algorithm": "{The proposed algorithm selects a solution from the archive by first identifying the \"weakest\" objective in the solution (based on normalized values) and then applies a novel local search operator that performs a series of intelligent, multi-objective flips by considering the Pareto front contributions of items. The operator dynamically adjusts the selection based on a hybrid of lexicographic ordering and dominance-based selection, prioritizing solutions that are either non-dominated or have the highest combined marginal utility for both objectives. It then uses a probabilistic acceptance criterion that favors flips leading to non-dominated solutions or significant trade-offs, while also incorporating a diversity-preserving mechanism to avoid premature convergence. The selection process is guided by a utility function that weighs both objective improvements and solution diversity, ensuring a robust exploration of the search space. The algorithm also includes an additional perturbation mechanism that randomly swaps items between objectives with a small probability to further explore the solution space while maintaining feasibility.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution with high potential for improvement based on its objective values and weight utilization\n    selected_idx = np.random.choice(len(archive))\n    base_solution, (base_val1, base_val2) = archive[selected_idx]\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate the normalized objective values to determine the focus of improvement\n    max_val1 = np.max([obj[0] for _, obj in archive])\n    max_val2 = np.max([obj[1] for _, obj in archive])\n    norm_val1 = base_val1 / max_val1 if max_val1 > 0 else 0\n    norm_val2 = base_val2 / max_val2 if max_val2 > 0 else 0\n\n    # Step 3: Determine the focus of improvement (the objective that is relatively weaker)\n    focus_obj = 1 if norm_val1 < norm_val2 else 2\n\n    # Step 4: Generate candidate items for flipping based on their potential to improve the focused objective\n    candidates = []\n    for i in range(len(base_solution)):\n        if base_solution[i] == 1:\n            # Consider removing item i if it doesn't significantly reduce the focused objective\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, -1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, -1, value2_lst[i], value1_lst[i]))\n        else:\n            # Consider adding item i if it improves the focused objective\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, 1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, 1, value2_lst[i], value1_lst[i]))\n\n    if not candidates:\n        # Step 5: No feasible flips, perform a random feasible flip\n        feasible_indices = [i for i in range(len(base_solution))\n                          if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n        return new_solution\n\n    # Step 6: Select the best candidate based on the focused objective\n    best_candidate = None\n    best_score = -np.inf\n    for candidate in candidates:\n        idx, flip, val_focus, val_other = candidate\n        if flip == 1:\n            score = val_focus  # Adding item: maximize the focused objective\n        else:\n            score = -val_focus  # Removing item: minimize the focused objective\n        if score > best_score:\n            best_score = score\n            best_candidate = candidate\n\n    if best_candidate is not None:\n        idx, flip, _, _ = best_candidate\n        new_solution[idx] = 1 if flip == 1 else 0\n\n    # Step 7: Additional perturbation: perform a series of intelligent swaps between objectives\n    if random.random() < 0.3 and len(new_solution) > 1:\n        # Select a subset of items to consider for swapping\n        swap_candidates = np.where(new_solution == 1)[0]\n        if len(swap_candidates) > 1:\n            np.random.shuffle(swap_candidates)\n            for i in range(min(3, len(swap_candidates)-1)):\n                j = i + 1\n                if j >= len(swap_candidates):\n                    break\n                # Swap items if it maintains feasibility\n                if (current_weight - weight_lst[swap_candidates[i]] - weight_lst[swap_candidates[j]] +\n                    weight_lst[swap_candidates[i]] + weight_lst[swap_candidates[j]]) <= capacity:\n                    new_solution[swap_candidates[i]], new_solution[swap_candidates[j]] = \\\n                    new_solution[swap_candidates[j]], new_solution[swap_candidates[i]]\n\n    return new_solution\n\n",
          "score": [
               -0.9591348538510646,
               2.596752852201462
          ]
     },
     {
          "algorithm": "{The proposed algorithm selects a solution from the archive by first identifying the \"weakest\" objective in the solution (based on normalized values) and then applies a novel local search operator that performs a series of intelligent, multi-objective flips by considering the Pareto front contributions of items. The operator dynamically adjusts the selection based on a hybrid of lexicographic ordering and dominance-based selection, prioritizing solutions that are either non-dominated or have the highest combined marginal utility for both objectives. It then uses a probabilistic acceptance criterion that favors flips leading to non-dominated solutions or significant trade-offs, while also incorporating a diversity-preserving mechanism to avoid premature convergence. The selection process is guided by a utility function that weighs both objective improvements and solution diversity, ensuring a robust exploration of the search space. The algorithm also includes an additional perturbation mechanism that randomly swaps items between objectives with a small probability to further explore the solution space while maintaining feasibility.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution with high potential for improvement based on its objective values and weight utilization\n    selected_idx = np.random.choice(len(archive))\n    base_solution, (base_val1, base_val2) = archive[selected_idx]\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate the normalized objective values to determine the focus of improvement\n    max_val1 = np.max([obj[0] for _, obj in archive])\n    max_val2 = np.max([obj[1] for _, obj in archive])\n    norm_val1 = base_val1 / max_val1 if max_val1 > 0 else 0\n    norm_val2 = base_val2 / max_val2 if max_val2 > 0 else 0\n\n    # Step 3: Determine the focus of improvement (the objective that is relatively weaker)\n    focus_obj = 1 if norm_val1 < norm_val2 else 2\n\n    # Step 4: Generate candidate items for flipping based on their potential to improve the focused objective\n    candidates = []\n    for i in range(len(base_solution)):\n        if base_solution[i] == 1:\n            # Consider removing item i if it doesn't significantly reduce the focused objective\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, -1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, -1, value2_lst[i], value1_lst[i]))\n        else:\n            # Consider adding item i if it improves the focused objective\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                if focus_obj == 1:\n                    candidates.append((i, 1, value1_lst[i], value2_lst[i]))\n                else:\n                    candidates.append((i, 1, value2_lst[i], value1_lst[i]))\n\n    if not candidates:\n        # Step 5: No feasible flips, perform a random feasible flip\n        feasible_indices = [i for i in range(len(base_solution))\n                          if (base_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (base_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n        return new_solution\n\n    # Step 6: Select the best candidate based on the focused objective\n    best_candidate = None\n    best_score = -np.inf\n    for candidate in candidates:\n        idx, flip, val_focus, val_other = candidate\n        if flip == 1:\n            score = val_focus  # Adding item: maximize the focused objective\n        else:\n            score = -val_focus  # Removing item: minimize the focused objective\n        if score > best_score:\n            best_score = score\n            best_candidate = candidate\n\n    if best_candidate is not None:\n        idx, flip, _, _ = best_candidate\n        new_solution[idx] = 1 if flip == 1 else 0\n\n    # Step 7: Additional perturbation: perform a series of intelligent swaps between objectives\n    if random.random() < 0.3 and len(new_solution) > 1:\n        # Select a subset of items to consider for swapping\n        swap_candidates = np.where(new_solution == 1)[0]\n        if len(swap_candidates) > 1:\n            np.random.shuffle(swap_candidates)\n            for i in range(min(3, len(swap_candidates)-1)):\n                j = i + 1\n                if j >= len(swap_candidates):\n                    break\n                # Swap items if it maintains feasibility\n                if (current_weight - weight_lst[swap_candidates[i]] - weight_lst[swap_candidates[j]] +\n                    weight_lst[swap_candidates[i]] + weight_lst[swap_candidates[j]]) <= capacity:\n                    new_solution[swap_candidates[i]], new_solution[swap_candidates[j]] = \\\n                    new_solution[swap_candidates[j]], new_solution[swap_candidates[i]]\n\n    return new_solution\n\n",
          "score": [
               -0.8482337614320413,
               1.906421273946762
          ]
     },
     {
          "algorithm": "{The proposed algorithm selects a solution from the archive by first identifying the \"bottleneck\" items (those closest to the capacity limit) and then applies a novel local search operator that performs a series of intelligent, multi-objective flips by considering the Pareto front contributions of items. The operator dynamically adjusts the selection based on a hybrid of lexicographic ordering and dominance-based selection, prioritizing solutions that are either non-dominated or have the highest combined marginal utility for both objectives. It then uses a probabilistic acceptance criterion that favors flips leading to non-dominated solutions or significant trade-offs, while also incorporating a diversity-preserving mechanism to avoid premature convergence. The selection process is guided by a utility function that weighs both objective improvements and solution diversity, ensuring a robust exploration of the search space. The algorithm also includes a novel \"bottleneck adjustment\" mechanism that intelligently redistributes the weight load by swapping items between the \"bottleneck\" and other items, while maintaining feasibility and potentially improving both objectives.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution with potential for bottleneck adjustment\n    selected_idx = np.random.choice(len(archive))\n    base_solution, (base_val1, base_val2) = archive[selected_idx]\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Step 2: Identify bottleneck items (those closest to capacity)\n    included_items = np.where(base_solution == 1)[0]\n    if len(included_items) == 0:\n        return new_solution\n\n    # Calculate remaining capacity\n    remaining_capacity = capacity - current_weight\n\n    # Find items that can be removed without violating capacity\n    removable_items = []\n    for i in included_items:\n        if current_weight - weight_lst[i] <= capacity:\n            removable_items.append(i)\n\n    if not removable_items:\n        return new_solution\n\n    # Step 3: Select a bottleneck item to remove\n    bottleneck_item = removable_items[np.argmax(weight_lst[removable_items])]\n    new_solution[bottleneck_item] = 0\n    updated_weight = current_weight - weight_lst[bottleneck_item]\n\n    # Step 4: Find items that can be added without exceeding capacity\n    excluded_items = np.where(base_solution == 0)[0]\n    addable_items = []\n    for i in excluded_items:\n        if updated_weight + weight_lst[i] <= capacity:\n            addable_items.append(i)\n\n    if not addable_items:\n        return new_solution\n\n    # Step 5: Select the best item to add based on combined marginal utility\n    best_item = None\n    best_score = -np.inf\n    for i in addable_items:\n        score = (value1_lst[i] + value2_lst[i]) / weight_lst[i]\n        if score > best_score:\n            best_score = score\n            best_item = i\n\n    if best_item is not None:\n        new_solution[best_item] = 1\n\n    # Step 6: Additional perturbation - swap items between bottleneck and non-bottleneck\n    if random.random() < 0.4 and len(included_items) > 1:\n        # Select a non-bottleneck item to swap with the bottleneck\n        non_bottleneck_items = [i for i in included_items if i != bottleneck_item]\n        if non_bottleneck_items:\n            swap_item = np.random.choice(non_bottleneck_items)\n            # Check if swapping maintains feasibility\n            if (updated_weight - weight_lst[swap_item] + weight_lst[bottleneck_item] <= capacity and\n                updated_weight - weight_lst[bottleneck_item] + weight_lst[swap_item] <= capacity):\n                new_solution[bottleneck_item], new_solution[swap_item] = new_solution[swap_item], new_solution[bottleneck_item]\n\n    return new_solution\n\n",
          "score": [
               -0.3661754138408818,
               1.7571044266223907
          ]
     },
     {
          "algorithm": "{The new algorithm combines the selection of promising solutions based on crowding distance and objective diversity with a novel local search operator that dynamically balances item inclusion/exclusion based on their marginal contributions to both objectives. It prioritizes flips that improve both objectives simultaneously while incorporating a probabilistic trade-off mechanism to escape local optima, and uses a hybrid value-weight ratio strategy to ensure feasibility. The operator selectively flips items in batches, considering both individual and combined marginal contributions, and adjusts the flip probability based on the solution's proximity to the Pareto front, creating a more exploratory search space while maintaining feasibility constraints through a capacity-aware adjustment phase.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Select a solution with high crowding distance or Pareto front status\n    def crowding_distance(solutions):\n        n = len(solutions)\n        if n == 0:\n            return [0.0] * n\n        distances = [0.0] * n\n        objectives = [sol[1] for sol in solutions]\n        for m in range(2):\n            sorted_idx = np.argsort([obj[m] for obj in objectives])\n            distances[sorted_idx[0]] = float('inf')\n            distances[sorted_idx[-1]] = float('inf')\n            for i in range(1, n-1):\n                if objectives[sorted_idx[-1]][m] == objectives[sorted_idx[0]][m]:\n                    continue\n                distances[sorted_idx[i]] += (objectives[sorted_idx[i+1]][m] - objectives[sorted_idx[i-1]][m]) / (objectives[sorted_idx[-1]][m] - objectives[sorted_idx[0]][m])\n        return distances\n\n    distances = crowding_distance(archive)\n    selected_idx = np.argmax(distances)\n    base_solution, (base_value1, base_value2) = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current weight and identify feasible flip candidates\n    current_weight = np.sum(weight_lst * new_solution)\n    candidates = []\n\n    # Step 3: Evaluate potential flips for both objectives\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            new_weight = current_weight - weight_lst[i]\n            if new_weight <= capacity:\n                candidates.append((i, -1, value1_lst[i], value2_lst[i]))\n        else:\n            new_weight = current_weight + weight_lst[i]\n            if new_weight <= capacity:\n                candidates.append((i, 1, value1_lst[i], value2_lst[i]))\n\n    if not candidates:\n        # If no candidates, perform a random feasible flip\n        feasible_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            flip_idx = np.random.choice(feasible_indices)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n        return new_solution\n\n    # Step 4: Select flip candidates based on combined marginal contributions\n    candidate_scores = []\n    for idx, flip, val1, val2 in candidates:\n        if flip == 1:\n            # Adding item: prioritize high value density\n            score = (val1 + val2) / (weight_lst[idx] * 2)\n        else:\n            # Removing item: prioritize low value density\n            score = -((val1 + val2) / (weight_lst[idx] * 2))\n        candidate_scores.append(score)\n\n    # Step 5: Select top candidates and apply flips\n    selected_candidates = np.argsort(candidate_scores)[-min(3, len(candidate_scores)):]  # Select top 3 candidates\n    for i in selected_candidates:\n        idx, flip, _, _ = candidates[i]\n        new_solution[idx] = 1 if flip == 1 else 0\n\n    # Step 6: Final feasibility check and adjustment\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        # Remove items randomly until feasible\n        while total_weight > capacity:\n            in_solution = np.where(new_solution == 1)[0]\n            if len(in_solution) == 0:\n                break\n            item_to_remove = np.random.choice(in_solution)\n            new_solution[item_to_remove] = 0\n            total_weight -= weight_lst[item_to_remove]\n\n    return new_solution\n\n",
          "score": [
               -0.926211639079858,
               2.1929914355278015
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    selected_idx = np.random.choice(len(archive))\n    base_solution, (base_value1, base_value2) = archive[selected_idx]\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Step 2: Generate a neighbor solution using a hybrid local search strategy\n    new_solution = base_solution.copy()\n    items = np.arange(len(new_solution))\n\n    # Calculate marginal gains for each item\n    marginal_gain1 = value1_lst - (value1_lst * new_solution)\n    marginal_gain2 = value2_lst - (value2_lst * new_solution)\n    marginal_weights = weight_lst - (weight_lst * new_solution)\n\n    # Rank items by their potential to improve both objectives\n    scores = marginal_gain1 + marginal_gain2\n    sorted_indices = np.argsort(scores)[::-1]\n\n    # Try to flip items in order of highest potential gain\n    for idx in sorted_indices:\n        if new_solution[idx] == 0:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n        else:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # If no improvement, try a different strategy: flip a random subset\n    if np.array_equal(new_solution, base_solution):\n        np.random.shuffle(items)\n        for idx in items:\n            if new_solution[idx] == 0:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n            else:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.9546836449070784,
               5.297566622495651
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a base solution with high potential for improvement\n    # Calculate dominance counts and objective densities\n    dominance_counts = np.zeros(len(archive))\n    for i in range(len(archive)):\n        sol_i, (val1_i, val2_i) = archive[i]\n        for j in range(len(archive)):\n            if i == j:\n                continue\n            sol_j, (val1_j, val2_j) = archive[j]\n            if (val1_i >= val1_j and val2_i >= val2_j) and (val1_i > val1_j or val2_i > val2_j):\n                dominance_counts[i] += 1\n\n    # Select solution with lowest dominance count (most under-explored)\n    min_dominance_idx = np.argmin(dominance_counts)\n    base_solution = archive[min_dominance_idx][0].copy()\n    base_val1, base_val2 = archive[min_dominance_idx][1]\n\n    # Step 2: Generate a neighbor using adaptive objective-driven exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(new_solution * weight_lst)\n    num_items = len(weight_lst)\n\n    # Calculate objective gradients\n    total_val1 = np.sum(value1_lst)\n    total_val2 = np.sum(value2_lst)\n    grad1 = value1_lst / (total_val1 + 1e-8)\n    grad2 = value2_lst / (total_val2 + 1e-8)\n    combined_grad = grad1 + grad2\n\n    # Dynamic perturbation scaling based on objective values and weight\n    perturbation_scale = 0.5 + 0.5 * (base_val1 + base_val2) / (total_val1 + total_val2 + 1e-8)\n    perturbation_scale *= (1 - current_weight / capacity)\n\n    # Apply objective-aware perturbations\n    for i in range(num_items):\n        if random.random() < perturbation_scale:\n            if new_solution[i] == 1:\n                new_weight = current_weight - weight_lst[i]\n                delta_val1 = -value1_lst[i]\n                delta_val2 = -value2_lst[i]\n            else:\n                new_weight = current_weight + weight_lst[i]\n                delta_val1 = value1_lst[i]\n                delta_val2 = value2_lst[i]\n\n            # Apply perturbation with probability based on combined gradient\n            if new_weight <= capacity and random.random() < combined_grad[i]:\n                new_solution[i] = 1 - new_solution[i]\n                current_weight = new_weight\n\n    # Additional gradient-guided exploration\n    if random.random() < 0.3:  # 30% chance for gradient-guided move\n        # Find items with high gradient that can be added\n        candidate_items = [i for i in range(num_items) if new_solution[i] == 0 and\n                          (current_weight + weight_lst[i]) <= capacity]\n\n        if candidate_items:\n            # Select item with highest combined gradient\n            best_item = max(candidate_items, key=lambda x: combined_grad[x])\n            new_solution[best_item] = 1\n\n    # Ensure feasibility\n    while np.sum(new_solution * weight_lst) > capacity:\n        # Remove item with lowest normalized marginal contribution\n        marginal_contributions = []\n        for i in range(num_items):\n            if new_solution[i] == 1:\n                temp_sol = new_solution.copy()\n                temp_sol[i] = 0\n                val1 = np.sum(temp_sol * value1_lst)\n                val2 = np.sum(temp_sol * value2_lst)\n                marginal_contributions.append((val1 + val2) / (weight_lst[i] + 1e-8))\n            else:\n                marginal_contributions.append(float('inf'))\n\n        worst_item = np.argmin(marginal_contributions)\n        new_solution[worst_item] = 0\n\n    return new_solution\n\n",
          "score": [
               -0.7697751228333782,
               2.57902592420578
          ]
     }
]