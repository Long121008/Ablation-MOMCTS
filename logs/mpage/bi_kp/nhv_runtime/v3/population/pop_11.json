[
     {
          "algorithm": "{The new algorithm combines a \"value-dominance clustering\" strategy with a \"multi-objective gradient ascent\" local search, where it first identifies clusters of items based on their dominance in either objective, then performs a gradient-like search in the objective space by iteratively selecting items that maximize the weighted sum of marginal gains in both objectives, where weights are dynamically adjusted based on the current solution's position in the objective space relative to the archive's distribution, while maintaining feasibility through a capacity-aware item replacement mechanism that prioritizes items with the highest marginal gain-to-weight ratios.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster items based on value dominance\n    objectives = np.array([obj for _, obj in archive])\n    obj1_mean, obj2_mean = np.mean(objectives, axis=0)\n    dominance_scores = (value1_lst > obj1_mean).astype(int) + (value2_lst > obj2_mean).astype(int)\n    cluster_labels = np.unique(dominance_scores)\n    selected_cluster = random.choice(cluster_labels)\n    cluster_indices = np.where(dominance_scores == selected_cluster)[0]\n\n    # Step 2: Calculate position in objective space\n    current_obj = objectives[0]\n    obj_space_center = np.mean(objectives, axis=0)\n    direction_vector = current_obj - obj_space_center\n    direction_norm = np.linalg.norm(direction_vector)\n    if direction_norm > 0:\n        direction_vector /= direction_norm\n\n    # Step 3: Calculate gradient scores\n    remaining_capacity = capacity - np.sum(weight_lst * archive[0][0])\n    gradient_scores = np.zeros(len(weight_lst))\n    for i in cluster_indices:\n        if weight_lst[i] <= remaining_capacity:\n            weight_factor = 1 / (1 + weight_lst[i])\n            obj1_gain = value1_lst[i] * weight_factor\n            obj2_gain = value2_lst[i] * weight_factor\n            gradient_scores[i] = (direction_vector[0] * obj1_gain + direction_vector[1] * obj2_gain)\n\n    # Step 4: Select items to add/remove\n    new_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Add most promising items\n    candidate_indices = np.argsort(gradient_scores)[::-1]\n    for i in candidate_indices:\n        if gradient_scores[i] <= 0:\n            break\n        if weight_lst[i] <= remaining_capacity and new_solution[i] == 0:\n            new_solution[i] = 1\n            remaining_capacity -= weight_lst[i]\n\n    # Remove least promising items if needed\n    if current_weight > capacity:\n        excess = current_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        remove_scores = gradient_scores[included_items]\n        sorted_indices = included_items[np.argsort(remove_scores)]\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            if weight_lst[i] <= excess:\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n\n    return new_solution\n\n",
          "score": [
               -0.9659891977418682,
               1.0242119133472443
          ]
     },
     {
          "algorithm": "{The new algorithm first identifies the solution with the highest combined objective value by normalizing both objectives and then applies a novel 'adaptive dominance-aware perturbation' strategy that combines probabilistic item selection with a guided exploration of adjacent solutions, where items are selected based on their dominance in either objective while dynamically adjusting the selection to maintain feasibility, ensuring the generated neighbor solution remains feasible and explores diverse regions of the search space by balancing exploration and exploitation through adaptive neighborhood exploration and probabilistic selection of items with high dominance in either objective, while also incorporating a dynamic capacity adjustment mechanism that prioritizes items with balanced dominance contributions to both objectives.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with the highest combined normalized objective value\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2])\n    combined_obj = np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_obj)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Adaptive dominance-aware perturbation\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Select items to add based on dominance in either objective\n    candidate_indices = np.where(new_solution == 0)[0]\n    if len(candidate_indices) > 0:\n        # Select items that are dominant in either objective\n        v1_dominant = candidate_indices[dominance_v1[candidate_indices] > dominance_v2[candidate_indices]]\n        v2_dominant = candidate_indices[dominance_v2[candidate_indices] > dominance_v1[candidate_indices]]\n\n        # Add items from both dominant sets with probability\n        for idx in v1_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n        for idx in v2_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Guided exploration of adjacent solutions\n    # Remove items based on dominance in the opposite objective\n    in_solution_indices = np.where(new_solution == 1)[0]\n    if len(in_solution_indices) > 0:\n        # Remove items that are weak in both objectives\n        weak_items = in_solution_indices[(dominance_v1[in_solution_indices] < 0.5) & (dominance_v2[in_solution_indices] < 0.5)]\n        for idx in weak_items:\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n\n    # Ensure feasibility with dynamic adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.9672504101123383,
               2.7086717784404755
          ]
     },
     {
          "algorithm": "{The new algorithm builds on the core idea of selecting promising solutions from the archive and applying a hybrid local search strategy, but instead of relying on marginal contributions or objective balancing, it employs a \"trade-off aware multi-objective flip\" approach. This method first identifies solutions with high potential for improvement by analyzing their objective trade-offs, then applies a \"value-weighted perturbation\" that selectively flips bits based on their relative contribution to both objectives, while dynamically adjusting the perturbation intensity to balance exploration and exploitation. The algorithm ensures feasibility by maintaining a running weight check and only allowing moves that don't exceed capacity, while also incorporating a novel \"trade-off sensitivity\" metric to prioritize items that could significantly improve the Pareto front by better addressing the current objective trade-offs.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the most unbalanced objectives\n    selected_idx = 0\n    max_balance_diff = -float('inf')\n    for i, (sol, obj) in enumerate(archive):\n        obj1, obj2 = obj\n        balance_diff = abs(obj1 - obj2) / max(1, obj1 + obj2)\n        if balance_diff > max_balance_diff:\n            max_balance_diff = balance_diff\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate trade-off sensitivity for each item\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off = (value1_lst / (value2_lst + 1e-6)) * (obj2_total / (obj1_total + 1e-6))\n\n    # Value-weighted perturbation\n    items = np.arange(len(base_solution))\n    np.random.shuffle(items)\n\n    for item in items:\n        if new_solution[item] == 1:\n            # Try removing item\n            new_weight = current_weight - weight_lst[item]\n            if new_weight <= capacity:\n                # Remove if it improves trade-off\n                if trade_off[item] > 1.5 or (np.random.random() < 0.3 and trade_off[item] > 0.8):\n                    new_solution[item] = 0\n                    current_weight = new_weight\n                    break\n        else:\n            # Try adding item\n            new_weight = current_weight + weight_lst[item]\n            if new_weight <= capacity:\n                # Add if it improves trade-off\n                if trade_off[item] < 0.5 or (np.random.random() < 0.3 and trade_off[item] < 1.2):\n                    new_solution[item] = 1\n                    current_weight = new_weight\n                    break\n\n    # Additional diversification step\n    if np.random.random() < 0.2:\n        # Randomly flip a small number of items to maintain diversity\n        flip_indices = np.random.choice(len(base_solution), size=min(2, len(base_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.6669301154527879,
               1.3673640489578247
          ]
     },
     {
          "algorithm": "{The new algorithm first identifies the least crowded solution in the archive to focus on under-explored regions, then applies a targeted cluster-based local search that selectively flips items within high-value clusters while maintaining feasibility, using a dynamic capacity adjustment mechanism that prioritizes items with balanced marginal contributions to both objectives. The algorithm intelligently balances exploration of clustered high-value items with exploitation of underutilized capacity regions, ensuring the neighbor solution remains feasible by dynamically adjusting the selection based on cluster-based capacity utilization and probabilistic flip mechanisms weighted by both objective contributions.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for i in range(2):\n        sorted_idx = np.argsort(objectives[:, i])\n        sorted_obj = objectives[sorted_idx, i]\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for j in range(1, len(sorted_idx) - 1):\n            crowding_distances[sorted_idx[j]] += (sorted_obj[j + 1] - sorted_obj[j - 1]) / (sorted_obj[-1] - sorted_obj[0] + 1e-10)\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Cluster-based local search\n    # Identify clusters of high-value items\n    value_combined = value1_lst + value2_lst\n    sorted_items = np.argsort(-value_combined)\n    cluster_size = max(1, len(weight_lst) // 10)  # Cluster top 10% items\n\n    # Process clusters in descending order of value\n    for i in sorted_items[:cluster_size]:\n        if new_solution[i] == 1:\n            # Remove item if it's in the cluster and has low marginal contribution\n            if np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[i] = 0\n        else:\n            # Add item if it fits and is in the cluster\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < 0.6:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Ensure feasibility with dynamic adjustment\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items from least valuable clusters until feasible\n        for i in reversed(sorted_items):\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                if current_weight <= capacity:\n                    break\n\n    return new_solution\n\n",
          "score": [
               -0.9430162127106001,
               1.6847892701625824
          ]
     },
     {
          "algorithm": "{The new algorithm builds on the core idea of leveraging solution diversity and objective trade-offs from the archive, but instead of using marginal contributions or probabilistic flips, it employs a \"trade-off driven hierarchical perturbation\" approach. This method first identifies solutions with the most extreme objective trade-offs, then applies a hierarchical perturbation strategy that systematically explores item swaps at different levels of granularity (coarse-grained for large items and fine-grained for small items), while dynamically balancing the trade-off between objectives by prioritizing items that could significantly improve the Pareto front through a novel \"trade-off sensitivity\" metric. The algorithm ensures feasibility by maintaining a running weight check and only allowing moves that don't exceed capacity, while also incorporating an adaptive neighborhood exploration mechanism that alternates between objective-specific and combined objective perturbations to maintain diversity and exploit promising regions of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the most extreme objective trade-off\n    selected_idx = 0\n    max_trade_off_ratio = -float('inf')\n    for i, (sol, obj) in enumerate(archive):\n        obj1, obj2 = obj\n        trade_off_ratio = max(obj1 / (obj2 + 1e-6), obj2 / (obj1 + 1e-6))\n        if trade_off_ratio > max_trade_off_ratio:\n            max_trade_off_ratio = trade_off_ratio\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate trade-off sensitivity for each item\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off = (value1_lst * obj2_total) / (value2_lst * obj1_total + 1e-6)\n\n    # Hierarchical perturbation based on item size and trade-off\n    items = np.arange(len(base_solution))\n\n    # Sort items by size (weight) for hierarchical processing\n    size_order = np.argsort(weight_lst)\n\n    # Alternate between coarse and fine-grained perturbations\n    for phase in range(2):\n        if phase == 0:\n            # Coarse-grained: process large items first\n            ordered_items = size_order[::-1]\n        else:\n            # Fine-grained: process small items first\n            ordered_items = size_order\n\n        for item in ordered_items:\n            if new_solution[item] == 1:\n                # Try removing item if it has low trade-off sensitivity\n                if trade_off[item] < 0.7 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n            else:\n                # Try adding item if it has high trade-off sensitivity\n                if trade_off[item] > 1.3 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n    # Adaptive neighborhood exploration\n    if np.random.random() < 0.3:\n        # Objective-specific perturbation\n        if np.random.random() < 0.5:\n            # Focus on objective 1\n            items = np.argsort(-value1_lst / (weight_lst + 1e-6))\n        else:\n            # Focus on objective 2\n            items = np.argsort(-value2_lst / (weight_lst + 1e-6))\n\n        for item in items:\n            if new_solution[item] == 1:\n                if current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n            else:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n    return new_solution\n\n",
          "score": [
               -0.8371991196122957,
               1.556203156709671
          ]
     },
     {
          "algorithm": "{The new algorithm builds upon the backbone idea of selecting promising solutions from the archive by leveraging both objective trade-offs and solution diversity, but instead of using marginal contributions or novelty scores, it employs a \"dynamic trade-off exploration\" approach that first identifies solutions with high potential for improvement by analyzing their objective trade-offs and solution diversity, then applies a \"trade-off aware perturbation\" that selectively flips bits based on their relative contribution to both objectives, while dynamically adjusting the perturbation intensity to balance exploration and exploitation. This is achieved by calculating a dynamic trade-off score for each item that combines its value contributions with the current solution's objective trade-off, and then probabilistically selecting items to flip based on this score, ensuring feasibility through a capacity-aware selection process. The algorithm also incorporates a \"diversity-aware mutation\" step that randomly flips a small number of items to maintain diversity while still favoring those with high trade-off potential.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high trade-off potential\n    selected_idx = 0\n    max_tradeoff_score = -float('inf')\n    for i, (sol, obj) in enumerate(archive):\n        obj1, obj2 = obj\n        tradeoff_score = (obj1 / (obj2 + 1e-6)) + (obj2 / (obj1 + 1e-6))\n        if tradeoff_score > max_tradeoff_score:\n            max_tradeoff_score = tradeoff_score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate dynamic trade-off scores for items\n    obj1_total, obj2_total = archive[selected_idx][1]\n    tradeoff_scores = (value1_lst / (value2_lst + 1e-6)) * (obj2_total / (obj1_total + 1e-6))\n\n    # Step 3: Trade-off aware perturbation\n    items = np.arange(len(base_solution))\n    np.random.shuffle(items)\n\n    for item in items:\n        if new_solution[item] == 1:\n            # Try removing item if it has low trade-off contribution\n            new_weight = current_weight - weight_lst[item]\n            if new_weight <= capacity and tradeoff_scores[item] < 0.7:\n                new_solution[item] = 0\n                current_weight = new_weight\n                break\n        else:\n            # Try adding item if it has high trade-off contribution\n            new_weight = current_weight + weight_lst[item]\n            if new_weight <= capacity and tradeoff_scores[item] > 1.3:\n                new_solution[item] = 1\n                current_weight = new_weight\n                break\n\n    # Step 4: Diversity-aware mutation\n    if np.random.random() < 0.3:\n        # Select items with low trade-off contribution for mutation\n        low_tradeoff_items = np.where(tradeoff_scores < 0.9)[0]\n        if len(low_tradeoff_items) > 0:\n            mutation_items = np.random.choice(low_tradeoff_items, size=min(3, len(low_tradeoff_items)), replace=False)\n            for item in mutation_items:\n                if new_solution[item] == 1:\n                    if current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                else:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -0.6441608270367991,
               1.1908688843250275
          ]
     },
     {
          "algorithm": "{The new algorithm combines the backbone ideas of diversity-aware selection with a novel \"objective-balancing flip\" strategy that dynamically adjusts item inclusion/exclusion based on their relative contributions to both objectives, while using a probabilistic \"exploration-exploitation\" mechanism to balance between intensifying search around high-performing solutions and diversifying exploration of underrepresented regions, all while maintaining feasibility through a capacity-aware item selection process that prioritizes items with high marginal gains in either objective while respecting the knapsack constraints.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with balanced objective contributions\n    objectives = np.array([obj for _, obj in archive])\n    norms = np.linalg.norm(objectives, axis=1)\n    selected_idx = np.argmin(norms) if len(archive) > 1 else 0\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Calculate objective-balancing scores\n    obj1_total, obj2_total = objectives[selected_idx]\n    balance_factor = obj1_total / (obj1_total + obj2_total) if (obj1_total + obj2_total) > 0 else 0.5\n\n    # Step 3: Probabilistic flip mechanism\n    new_solution = base_solution.copy()\n    items = np.arange(len(base_solution))\n    np.random.shuffle(items)\n\n    for item in items:\n        if base_solution[item] == 1:\n            # Probability to remove based on balanced contribution\n            contrib1 = value1_lst[item] / (obj1_total + 1e-6)\n            contrib2 = value2_lst[item] / (obj2_total + 1e-6)\n            remove_prob = (balance_factor * contrib1 + (1 - balance_factor) * contrib2) * 0.7\n\n            if random.random() < remove_prob and current_weight - weight_lst[item] <= capacity:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                remaining_capacity += weight_lst[item]\n        else:\n            # Probability to add based on potential\n            potential1 = value1_lst[item] / (obj1_total + 1e-6)\n            potential2 = value2_lst[item] / (obj2_total + 1e-6)\n            add_prob = (balance_factor * potential1 + (1 - balance_factor) * potential2) * 0.3\n\n            if random.random() < add_prob and weight_lst[item] <= remaining_capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                remaining_capacity -= weight_lst[item]\n\n    # Step 4: Capacity adjustment if needed\n    if current_weight > capacity:\n        excess = current_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(weight_lst[included_items])[::-1]]\n\n        for item in sorted_items:\n            if excess <= 0:\n                break\n            if weight_lst[item] <= excess:\n                new_solution[item] = 0\n                excess -= weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -0.9571448747106439,
               5.100673079490662
          ]
     },
     {
          "algorithm": "{The new algorithm uniquely combines a 'correlation-aware diversification' strategy with an 'objective-space partitioning' heuristic. It first analyzes the correlation between items' values and weights across the archive to identify clusters of highly correlated items, then probabilistically selects a cluster to focus on for diversification. Within this cluster, it partitions the objective space into regions based on the current archive's distribution and selects items that would create solutions in underrepresented regions. The local search operator then constructs a new solution by greedily adding items that maximize the product of their marginal gains and their distance to the nearest existing solution in the archive's objective space, while maintaining feasibility through a capacity-aware selection process that dynamically adjusts its acceptance threshold based on the cluster's density.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate correlation between value1 and value2 for each item\n    item_correlations = np.zeros(len(weight_lst))\n    for sol, _ in archive:\n        item_correlations += sol * (value1_lst + value2_lst)\n\n    # Step 2: Cluster items based on correlation and select a cluster to focus on\n    cluster_centers = np.unique(np.round(item_correlations, 2))\n    selected_cluster = random.choice(cluster_centers)\n    cluster_indices = np.where(np.isclose(item_correlations, selected_cluster, atol=0.01))[0]\n\n    # Step 3: Partition objective space and identify underrepresented regions\n    objectives = np.array([obj for _, obj in archive])\n    min_vals = np.min(objectives, axis=0)\n    max_vals = np.max(objectives, axis=0)\n    range_vals = max_vals - min_vals\n\n    # Create 4 quadrants in objective space\n    q1 = objectives[(objectives[:,0] >= min_vals[0] + range_vals[0]*0.5) &\n                   (objectives[:,1] >= min_vals[1] + range_vals[1]*0.5)]\n    q2 = objectives[(objectives[:,0] >= min_vals[0] + range_vals[0]*0.5) &\n                   (objectives[:,1] < min_vals[1] + range_vals[1]*0.5)]\n    q3 = objectives[(objectives[:,0] < min_vals[0] + range_vals[0]*0.5) &\n                   (objectives[:,1] >= min_vals[1] + range_vals[1]*0.5)]\n    q4 = objectives[(objectives[:,0] < min_vals[0] + range_vals[0]*0.5) &\n                   (objectives[:,1] < min_vals[1] + range_vals[1]*0.5)]\n\n    # Select quadrant with fewest solutions\n    quadrant_counts = [len(q1), len(q2), len(q3), len(q4)]\n    selected_quadrant = np.argmin(quadrant_counts)\n\n    # Step 4: Select items that can potentially fill the selected quadrant\n    if selected_quadrant == 0:\n        target_v1 = max_vals[0]\n        target_v2 = max_vals[1]\n    elif selected_quadrant == 1:\n        target_v1 = max_vals[0]\n        target_v2 = min_vals[1]\n    elif selected_quadrant == 2:\n        target_v1 = min_vals[0]\n        target_v2 = max_vals[1]\n    else:\n        target_v1 = min_vals[0]\n        target_v2 = min_vals[1]\n\n    # Calculate distance to target in objective space\n    distance_scores = np.abs(value1_lst - target_v1) + np.abs(value2_lst - target_v2)\n\n    # Step 5: Construct new solution with items from selected cluster and quadrant-aware selection\n    new_solution = np.zeros(len(weight_lst), dtype=int)\n    remaining_capacity = capacity\n    candidate_indices = cluster_indices[np.argsort(distance_scores[cluster_indices])]\n\n    for idx in candidate_indices:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            remaining_capacity -= weight_lst[idx]\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.953878554892426,
               5.012989789247513
          ]
     },
     {
          "algorithm": "{The new algorithm builds upon the common backbone of selecting promising solutions from the archive by prioritizing high-objective-value or diverse candidates, but instead of using marginal contribution or random swaps, it employs a novelty-based search that dynamically evaluates items based on their potential to create novel combinations, balancing exploitation of known good regions with exploration of less-explored item interactions. This is achieved by calculating a novelty score for each item based on its historical inclusion patterns in the archive, then probabilistically selecting items with low novelty to introduce diversity while still favoring those with high marginal gains. The local search operator then constructs a new solution by iteratively adding items in descending order of their combined novelty and marginal gains, ensuring feasibility through a capacity-aware greedy selection process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate novelty scores for each item\n    item_inclusion_counts = np.zeros(len(weight_lst))\n    for sol, _ in archive:\n        item_inclusion_counts += sol\n\n    novelty_scores = 1.0 / (1.0 + item_inclusion_counts)  # Lower count = higher novelty\n\n    # Step 2: Select a base solution (prioritize high-objective-value solutions)\n    objectives = np.array([obj for _, obj in archive])\n    max_values = np.max(objectives, axis=0)\n    threshold1 = max_values[0] * 0.8\n    threshold2 = max_values[1] * 0.8\n\n    promising_indices = [\n        i for i, (obj1, obj2) in enumerate(objectives)\n        if obj1 >= threshold1 or obj2 >= threshold2\n    ]\n\n    if not promising_indices:\n        promising_indices = list(range(len(archive)))\n\n    selected_idx = random.choice(promising_indices)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 3: Generate neighbor solution using novelty-aware greedy construction\n    new_solution = np.zeros(len(weight_lst), dtype=int)\n    remaining_capacity = capacity\n    candidate_indices = np.where(base_solution == 0)[0]\n\n    # Calculate combined scores (novelty + marginal gain)\n    combined_scores = novelty_scores * (value1_lst + value2_lst)\n\n    # Sort candidates by combined score (descending)\n    sorted_indices = candidate_indices[np.argsort(-combined_scores[candidate_indices])]\n\n    for idx in sorted_indices:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            remaining_capacity -= weight_lst[idx]\n\n    # Ensure feasibility by removing heaviest items if needed\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.4108628732063897,
               1.336624413728714
          ]
     },
     {
          "algorithm": "{The new algorithm combines the selection of promising solutions based on crowding distance and dominance count with a novel 'objective-balanced flip' operator that intelligently flips items by considering their contributions to both objectives while maintaining feasibility. It first identifies the most diverse solution in the archive using crowding distance and dominance count, then applies a hybrid local search that probabilistically flips items based on their marginal contributions to both objectives, weighted by their balance between the two objectives. The operator also incorporates a targeted random walk to explore adjacent solutions, ensuring the neighbor solution remains feasible by dynamically adjusting the selection based on remaining capacity and objective trade-offs.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution (high crowding distance or low dominance count)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for i in range(2):  # For each objective\n        sorted_idx = np.argsort(objectives[:, i])\n        sorted_obj = objectives[sorted_idx, i]\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for j in range(1, len(sorted_idx) - 1):\n            crowding_distances[sorted_idx[j]] += (sorted_obj[j + 1] - sorted_obj[j - 1]) / (sorted_obj[-1] - sorted_obj[0] + 1e-10)\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Calculate current total weight and remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate objective-balanced marginal contributions\n    marginal_value1 = value1_lst / (weight_lst + 1e-10)\n    marginal_value2 = value2_lst / (weight_lst + 1e-10)\n    marginal_balance = (marginal_value1 + marginal_value2) / (np.abs(marginal_value1 - marginal_value2) + 1e-10)\n\n    # Step 4: Apply hybrid local search with probabilistic flips\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            if np.random.rand() < 0.3 * (1 - marginal_balance[i] / (np.max(marginal_balance) + 1e-10)):\n                new_solution[i] = 0\n        else:\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < 0.7 * (marginal_balance[i] / (np.max(marginal_balance) + 1e-10)):\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Step 5: Ensure feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        while current_weight > capacity:\n            item_to_remove = np.argmax(new_solution * marginal_balance)\n            if new_solution[item_to_remove] == 1:\n                new_solution[item_to_remove] = 0\n                current_weight -= weight_lst[item_to_remove]\n\n    return new_solution\n\n",
          "score": [
               -0.9560950330652951,
               9.071940302848816
          ]
     }
]