[
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm combines the strengths of the existing approaches by first selecting a solution from the archive based on a novel \"objective-space entropy\" metric that measures the diversity and potential for improvement in both objectives, then applies a hybrid local search operator that alternates between \"value-driven\" and \"weight-aware\" perturbations, where value-driven perturbations prioritize items with high marginal gains in either objective while maintaining feasibility, and weight-aware perturbations systematically remove or add items based on their weight efficiency and current capacity utilization, dynamically adjusting the search strategy to balance exploitation of high-value items and exploration of underutilized capacity regions, with a special focus on maintaining a Pareto front-aware balance between the two objectives through a calculated \"trade-off sensitivity\" metric that guides the selection of items to replace or add.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest objective-space entropy\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.sum(np.log(objectives + 1) * (1 - objectives/np.max(objectives + 1e-6, axis=0)), axis=1))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate trade-off sensitivity metric\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off_sensitivity = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Hybrid local search with alternating perturbations\n    for _ in range(2):\n        if np.random.random() < 0.5:\n            # Value-driven perturbation (prioritize high-value items)\n            if np.random.random() < 0.5:\n                # Focus on objective 1\n                items = np.argsort(-value1_lst / (weight_lst + 1e-6))\n            else:\n                # Focus on objective 2\n                items = np.argsort(-value2_lst / (weight_lst + 1e-6))\n\n            for item in items:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n                elif new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n        else:\n            # Weight-aware perturbation (balance capacity utilization)\n            if current_weight < capacity * 0.7:\n                # Add items to utilize capacity\n                items = np.argsort(-trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove items to balance capacity\n                items = np.argsort(trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm first identifies the solution with the highest combined objective value by normalizing both objectives and then applies a novel 'adaptive dominance-aware perturbation' strategy that combines probabilistic item selection with a guided exploration of adjacent solutions, where items are selected based on their dominance in either objective while dynamically adjusting the selection to maintain feasibility, ensuring the generated neighbor solution remains feasible and explores diverse regions of the search space by balancing exploration and exploitation through adaptive neighborhood exploration and probabilistic selection of items with high dominance in either objective, while also incorporating a dynamic capacity adjustment mechanism that prioritizes items with balanced dominance contributions to both objectives.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with the highest combined normalized objective value\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2])\n    combined_obj = np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_obj)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Adaptive dominance-aware perturbation\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Select items to add based on dominance in either objective\n    candidate_indices = np.where(new_solution == 0)[0]\n    if len(candidate_indices) > 0:\n        # Select items that are dominant in either objective\n        v1_dominant = candidate_indices[dominance_v1[candidate_indices] > dominance_v2[candidate_indices]]\n        v2_dominant = candidate_indices[dominance_v2[candidate_indices] > dominance_v1[candidate_indices]]\n\n        # Add items from both dominant sets with probability\n        for idx in v1_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n        for idx in v2_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Guided exploration of adjacent solutions\n    # Remove items based on dominance in the opposite objective\n    in_solution_indices = np.where(new_solution == 1)[0]\n    if len(in_solution_indices) > 0:\n        # Remove items that are weak in both objectives\n        weak_items = in_solution_indices[(dominance_v1[in_solution_indices] < 0.5) & (dominance_v2[in_solution_indices] < 0.5)]\n        for idx in weak_items:\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n\n    # Ensure feasibility with dynamic adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 301,
          "algorithm": "{The new algorithm leverages a \"multi-objective fitness landscape analysis\" approach to identify solutions with underutilized capacity and high potential for improvement, combining probabilistic item selection with a \"value-weight divergence\" metric that prioritizes items with significant differences in their contributions to the two objectives, while dynamically adjusting the search strategy to explore both \"high-value\" and \"capacity-balancing\" neighborhoods through alternating phases of \"objective-aware\" and \"capacity-aware\" perturbations, ensuring feasibility by maintaining a strict capacity constraint and incorporating a \"trade-off sensitivity\" metric to guide the selection of items to replace or add, ultimately generating high-quality neighbor solutions that balance exploitation of high-value items with exploration of underutilized capacity regions.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest value-weight divergence\n    objectives = np.array([obj for _, obj in archive])\n    value_weight_divergence = np.abs((value1_lst - value2_lst) / (weight_lst + 1e-6))\n    selected_idx = np.argmax(np.sum(value_weight_divergence[archive[selected_idx][0] == 1]) for selected_idx in range(len(archive)))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate trade-off sensitivity metric\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off_sensitivity = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Multi-phase perturbation strategy\n    for phase in range(3):\n        if phase == 0 or (phase == 1 and current_weight < 0.8 * capacity):\n            # Objective-aware perturbation\n            if np.random.random() < 0.5:\n                # Add high-divergence items\n                items = np.argsort(-value_weight_divergence)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove low-divergence items\n                items = np.argsort(value_weight_divergence)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n        else:\n            # Capacity-aware perturbation\n            if current_weight < 0.7 * capacity:\n                # Add items with balanced trade-off\n                items = np.argsort(trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove items to balance capacity\n                items = np.argsort(-trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.9357596916518947,
               2.47769558429718
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest value-weight divergence\n    objectives = np.array([obj for _, obj in archive])\n    value_weight_divergence = np.abs((value1_lst - value2_lst) / (weight_lst + 1e-6))\n    selected_idx = np.argmax(np.sum(value_weight_divergence[archive[selected_idx][0] == 1]) for selected_idx in range(len(archive)))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate trade-off sensitivity metric\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off_sensitivity = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Multi-phase perturbation strategy\n    for phase in range(3):\n        if phase == 0 or (phase == 1 and current_weight < 0.8 * capacity):\n            # Objective-aware perturbation\n            if np.random.random() < 0.5:\n                # Add high-divergence items\n                items = np.argsort(-value_weight_divergence)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove low-divergence items\n                items = np.argsort(value_weight_divergence)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n        else:\n            # Capacity-aware perturbation\n            if current_weight < 0.7 * capacity:\n                # Add items with balanced trade-off\n                items = np.argsort(trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove items to balance capacity\n                items = np.argsort(-trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm combines the strengths of the existing approaches by first selecting a solution from the archive based on a novel \"objective-space entropy\" metric that measures the diversity and potential for improvement in both objectives, then applies a hybrid local search operator that alternates between \"value-driven\" and \"weight-aware\" perturbations, where value-driven perturbations prioritize items with high marginal gains in either objective while maintaining feasibility, and weight-aware perturbations systematically remove or add items based on their weight efficiency and current capacity utilization, dynamically adjusting the search strategy to balance exploitation of high-value items and exploration of underutilized capacity regions, with a special focus on maintaining a Pareto front-aware balance between the two objectives through a calculated \"trade-off sensitivity\" metric that guides the selection of items to replace or add.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest objective-space entropy\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.sum(np.log(objectives + 1) * (1 - objectives/np.max(objectives + 1e-6, axis=0)), axis=1))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate trade-off sensitivity metric\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off_sensitivity = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Hybrid local search with alternating perturbations\n    for _ in range(2):\n        if np.random.random() < 0.5:\n            # Value-driven perturbation (prioritize high-value items)\n            if np.random.random() < 0.5:\n                # Focus on objective 1\n                items = np.argsort(-value1_lst / (weight_lst + 1e-6))\n            else:\n                # Focus on objective 2\n                items = np.argsort(-value2_lst / (weight_lst + 1e-6))\n\n            for item in items:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n                elif new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n        else:\n            # Weight-aware perturbation (balance capacity utilization)\n            if current_weight < capacity * 0.7:\n                # Add items to utilize capacity\n                items = np.argsort(-trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove items to balance capacity\n                items = np.argsort(trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm first identifies the solution with the highest combined objective value by normalizing both objectives and then applies a novel 'adaptive dominance-aware perturbation' strategy that combines probabilistic item selection with a guided exploration of adjacent solutions, where items are selected based on their dominance in either objective while dynamically adjusting the selection to maintain feasibility, ensuring the generated neighbor solution remains feasible and explores diverse regions of the search space by balancing exploration and exploitation through adaptive neighborhood exploration and probabilistic selection of items with high dominance in either objective, while also incorporating a dynamic capacity adjustment mechanism that prioritizes items with balanced dominance contributions to both objectives.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with the highest combined normalized objective value\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2])\n    combined_obj = np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_obj)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Adaptive dominance-aware perturbation\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Select items to add based on dominance in either objective\n    candidate_indices = np.where(new_solution == 0)[0]\n    if len(candidate_indices) > 0:\n        # Select items that are dominant in either objective\n        v1_dominant = candidate_indices[dominance_v1[candidate_indices] > dominance_v2[candidate_indices]]\n        v2_dominant = candidate_indices[dominance_v2[candidate_indices] > dominance_v1[candidate_indices]]\n\n        # Add items from both dominant sets with probability\n        for idx in v1_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n        for idx in v2_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Guided exploration of adjacent solutions\n    # Remove items based on dominance in the opposite objective\n    in_solution_indices = np.where(new_solution == 1)[0]\n    if len(in_solution_indices) > 0:\n        # Remove items that are weak in both objectives\n        weak_items = in_solution_indices[(dominance_v1[in_solution_indices] < 0.5) & (dominance_v2[in_solution_indices] < 0.5)]\n        for idx in weak_items:\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n\n    # Ensure feasibility with dynamic adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 302,
          "algorithm": "{The new algorithm first selects a solution from the archive based on a novel \"frontier dominance\" metric that evaluates how close each solution is to the Pareto front by considering both objective values and their relative positions in the objective space, then applies a \"multi-phase evolutionary perturbation\" strategy that systematically combines crossover-like operations with mutation-like adjustments, where the crossover phase exchanges subsets of items between the selected solution and a randomly chosen reference solution to explore new combinations, while the mutation phase performs targeted modifications based on a \"value-weight synergy\" score that measures how well an item's values align with its weight, dynamically adjusting the search by prioritizing items that show high synergy in either objective while maintaining feasibility, with a special focus on creating balanced trade-offs between the objectives through a calculated \"objective correlation\" metric that guides the selection of items to include or exclude, ensuring the generated neighbor solution remains feasible and explores diverse regions of the search space by balancing both objectives through a novel \"adaptive trade-off exploration\" mechanism.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest frontier dominance\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    normalized_obj = objectives / max_obj\n    frontier_dominance = np.sum(normalized_obj * (1 - normalized_obj), axis=1)\n    selected_idx = np.argmax(frontier_dominance)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Multi-phase evolutionary perturbation\n    # Crossover phase (exchange subsets with random reference solution)\n    ref_solution = archive[np.random.randint(len(archive))][0]\n    crossover_mask = np.random.rand(len(base_solution)) < 0.5\n    temp_solution = np.where(crossover_mask, ref_solution, base_solution)\n    temp_weight = np.sum(weight_lst[temp_solution == 1])\n\n    if temp_weight <= capacity:\n        new_solution = temp_solution\n        current_weight = temp_weight\n    else:\n        # If crossover exceeds capacity, perform mutation to fix\n        included_indices = np.where(temp_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if temp_weight <= capacity:\n                break\n            temp_solution[idx] = 0\n            temp_weight -= weight_lst[idx]\n        new_solution = temp_solution\n        current_weight = temp_weight\n\n    # Mutation phase (targeted modifications based on value-weight synergy)\n    value_weight_synergy = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-value_weight_synergy)\n\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n        elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Step 3: Adaptive trade-off exploration\n    obj1_total, obj2_total = archive[selected_idx][1]\n    objective_correlation = value1_lst / (value2_lst + 1e-6)\n\n    if obj1_total > obj2_total:\n        # Prioritize objective 1 when it's currently dominant\n        items_to_add = np.argsort(-value1_lst / (weight_lst + 1e-6))\n        items_to_remove = np.argsort(objective_correlation)\n    else:\n        # Prioritize objective 2 when it's currently dominant\n        items_to_add = np.argsort(-value2_lst / (weight_lst + 1e-6))\n        items_to_remove = np.argsort(-objective_correlation)\n\n    # Add items with high priority\n    for idx in items_to_add:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Remove items with low priority\n    for idx in items_to_remove:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.9058295012261257,
               6.547152817249298
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest frontier dominance\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    normalized_obj = objectives / max_obj\n    frontier_dominance = np.sum(normalized_obj * (1 - normalized_obj), axis=1)\n    selected_idx = np.argmax(frontier_dominance)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Multi-phase evolutionary perturbation\n    # Crossover phase (exchange subsets with random reference solution)\n    ref_solution = archive[np.random.randint(len(archive))][0]\n    crossover_mask = np.random.rand(len(base_solution)) < 0.5\n    temp_solution = np.where(crossover_mask, ref_solution, base_solution)\n    temp_weight = np.sum(weight_lst[temp_solution == 1])\n\n    if temp_weight <= capacity:\n        new_solution = temp_solution\n        current_weight = temp_weight\n    else:\n        # If crossover exceeds capacity, perform mutation to fix\n        included_indices = np.where(temp_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if temp_weight <= capacity:\n                break\n            temp_solution[idx] = 0\n            temp_weight -= weight_lst[idx]\n        new_solution = temp_solution\n        current_weight = temp_weight\n\n    # Mutation phase (targeted modifications based on value-weight synergy)\n    value_weight_synergy = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-value_weight_synergy)\n\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n        elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Step 3: Adaptive trade-off exploration\n    obj1_total, obj2_total = archive[selected_idx][1]\n    objective_correlation = value1_lst / (value2_lst + 1e-6)\n\n    if obj1_total > obj2_total:\n        # Prioritize objective 1 when it's currently dominant\n        items_to_add = np.argsort(-value1_lst / (weight_lst + 1e-6))\n        items_to_remove = np.argsort(objective_correlation)\n    else:\n        # Prioritize objective 2 when it's currently dominant\n        items_to_add = np.argsort(-value2_lst / (weight_lst + 1e-6))\n        items_to_remove = np.argsort(-objective_correlation)\n\n    # Add items with high priority\n    for idx in items_to_add:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Remove items with low priority\n    for idx in items_to_remove:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 1 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm leverages a \"multi-objective adaptive Pareto frontier expansion\" strategy that first identifies solutions with the highest combined objective diversity and then applies a novel \"dominance-aware item clustering\" approach that groups items based on their Pareto dominance relationships, followed by a multi-stage local search that systematically explores the trade-off space by considering both absolute and relative item contributions to each objective while maintaining feasibility through a \"dynamic capacity-aware item substitution\" mechanism that prioritizes items based on their potential to expand the Pareto frontier through a calculated \"objective dominance score,\" and incorporates an adaptive perturbation strategy that alternates between objective-specific and combined objective transformations based on the current solution's position relative to the archive's Pareto front, ensuring both exploration of under-explored regions and exploitation of promising areas through a \"guided neighborhood diversification\" mechanism that dynamically adjusts the search radius based on the current solution's quality and diversity.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest combined objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.std(objectives, axis=0).sum(axis=0))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Cluster items based on Pareto dominance\n    included_indices = np.where(new_solution == 1)[0]\n    excluded_indices = np.where(new_solution == 0)[0]\n\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify Pareto clusters\n    if len(included_indices) > 0:\n        # Cluster items based on dominance in both objectives\n        cluster1 = included_indices[(dominance_v1[included_indices] > dominance_v2[included_indices])]\n        cluster2 = included_indices[(dominance_v2[included_indices] > dominance_v1[included_indices])]\n        balanced_cluster = included_indices[(dominance_v1[included_indices] == dominance_v2[included_indices])]\n\n        # Remove weak items from balanced cluster\n        for idx in balanced_cluster:\n            if (dominance_v1[idx] < 0.5) and (dominance_v2[idx] < 0.5):\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 3: Multi-stage local search\n    # Stage 1: Expand in objective 1\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        best_item = candidate_items[np.argmax(dominance_v1[candidate_items])]\n        new_solution[best_item] = 1\n        current_weight += weight_lst[best_item]\n\n    # Stage 2: Expand in objective 2\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        best_item = candidate_items[np.argmax(dominance_v2[candidate_items])]\n        new_solution[best_item] = 1\n        current_weight += weight_lst[best_item]\n\n    # Stage 3: Balance both objectives\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        balance_scores = (value1_lst[candidate_items] + value2_lst[candidate_items]) / (weight_lst[candidate_items] + 1e-6)\n        best_item = candidate_items[np.argmax(balance_scores)]\n        new_solution[best_item] = 1\n\n    # Step 4: Guided neighborhood diversification\n    if np.random.random() < 0.3:\n        # Randomly remove items from clusters with low dominance\n        for idx in np.concatenate([cluster1, cluster2]):\n            if np.random.random() < 0.2 and (dominance_v1[idx] < 0.7 or dominance_v2[idx] < 0.7):\n                new_solution[idx] = 0\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 303,
          "algorithm": "{The new algorithm employs a \"multi-objective adaptive Pareto frontier expansion\" strategy that first selects a solution based on its position relative to the archive's Pareto front, then applies a novel \"dominance-aware item clustering\" approach that groups items based on their Pareto dominance relationships, followed by a multi-stage local search that systematically explores the trade-off space by considering both absolute and relative item contributions to each objective while maintaining feasibility through a \"dynamic capacity-aware item substitution\" mechanism that prioritizes items based on their potential to expand the Pareto frontier through a calculated \"objective dominance score,\" and incorporates an adaptive perturbation strategy that alternates between objective-specific and combined objective transformations based on the current solution's position relative to the archive's Pareto front, ensuring both exploration of under-explored regions and exploitation of promising areas through a \"guided neighborhood diversification\" mechanism that dynamically adjusts the search radius based on the current solution's quality and diversity.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest combined objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.std(objectives, axis=0).sum(axis=0))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Cluster items based on Pareto dominance\n    included_indices = np.where(new_solution == 1)[0]\n    excluded_indices = np.where(new_solution == 0)[0]\n\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify Pareto clusters\n    if len(included_indices) > 0:\n        # Cluster items based on dominance in both objectives\n        cluster1 = included_indices[(dominance_v1[included_indices] > dominance_v2[included_indices])]\n        cluster2 = included_indices[(dominance_v2[included_indices] > dominance_v1[included_indices])]\n        balanced_cluster = included_indices[(dominance_v1[included_indices] == dominance_v2[included_indices])]\n\n        # Remove weak items from balanced cluster\n        for idx in balanced_cluster:\n            if (dominance_v1[idx] < 0.5) and (dominance_v2[idx] < 0.5):\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 3: Multi-stage local search\n    # Stage 1: Expand in objective 1\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        best_item = candidate_items[np.argmax(dominance_v1[candidate_items])]\n        new_solution[best_item] = 1\n        current_weight += weight_lst[best_item]\n\n    # Stage 2: Expand in objective 2\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        best_item = candidate_items[np.argmax(dominance_v2[candidate_items])]\n        new_solution[best_item] = 1\n        current_weight += weight_lst[best_item]\n\n    # Stage 3: Balance both objectives\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        balance_scores = (value1_lst[candidate_items] + value2_lst[candidate_items]) / (weight_lst[candidate_items] + 1e-6)\n        best_item = candidate_items[np.argmax(balance_scores)]\n        new_solution[best_item] = 1\n\n    # Step 4: Guided neighborhood diversification\n    if np.random.random() < 0.3:\n        # Randomly remove items from clusters with low dominance\n        for idx in np.concatenate([cluster1, cluster2]):\n            if np.random.random() < 0.2 and (dominance_v1[idx] < 0.7 or dominance_v2[idx] < 0.7):\n                new_solution[idx] = 0\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.8035679346775448,
               8.77680292725563
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest combined objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.std(objectives, axis=0).sum(axis=0))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Cluster items based on Pareto dominance\n    included_indices = np.where(new_solution == 1)[0]\n    excluded_indices = np.where(new_solution == 0)[0]\n\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify Pareto clusters\n    if len(included_indices) > 0:\n        # Cluster items based on dominance in both objectives\n        cluster1 = included_indices[(dominance_v1[included_indices] > dominance_v2[included_indices])]\n        cluster2 = included_indices[(dominance_v2[included_indices] > dominance_v1[included_indices])]\n        balanced_cluster = included_indices[(dominance_v1[included_indices] == dominance_v2[included_indices])]\n\n        # Remove weak items from balanced cluster\n        for idx in balanced_cluster:\n            if (dominance_v1[idx] < 0.5) and (dominance_v2[idx] < 0.5):\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 3: Multi-stage local search\n    # Stage 1: Expand in objective 1\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        best_item = candidate_items[np.argmax(dominance_v1[candidate_items])]\n        new_solution[best_item] = 1\n        current_weight += weight_lst[best_item]\n\n    # Stage 2: Expand in objective 2\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        best_item = candidate_items[np.argmax(dominance_v2[candidate_items])]\n        new_solution[best_item] = 1\n        current_weight += weight_lst[best_item]\n\n    # Stage 3: Balance both objectives\n    candidate_items = excluded_indices[weight_lst[excluded_indices] <= capacity - current_weight]\n    if len(candidate_items) > 0:\n        balance_scores = (value1_lst[candidate_items] + value2_lst[candidate_items]) / (weight_lst[candidate_items] + 1e-6)\n        best_item = candidate_items[np.argmax(balance_scores)]\n        new_solution[best_item] = 1\n\n    # Step 4: Guided neighborhood diversification\n    if np.random.random() < 0.3:\n        # Randomly remove items from clusters with low dominance\n        for idx in np.concatenate([cluster1, cluster2]):\n            if np.random.random() < 0.2 and (dominance_v1[idx] < 0.7 or dominance_v2[idx] < 0.7):\n                new_solution[idx] = 0\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{This new algorithm combines the trade-off awareness of the first approach with the swarm intelligence of the second by first identifying solutions with high potential through a dynamic trade-off analysis, then employing a parallelized local search that explores multiple item combinations simultaneously while dynamically adjusting the search intensity based on the current solution's objective trade-off, and finally incorporating a hybrid perturbation mechanism that selectively flips items based on their relative contribution to both objectives while maintaining feasibility through a capacity-constrained search space exploration, and periodically introducing targeted perturbations to escape local optima while preserving the most promising item combinations.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest trade-off potential\n    selected_idx = 0\n    max_tradeoff = -float('inf')\n    for i, (sol, obj) in enumerate(archive):\n        obj1, obj2 = obj\n        tradeoff = (obj1 / (obj2 + 1e-6)) + (obj2 / (obj1 + 1e-6))\n        if tradeoff > max_tradeoff:\n            max_tradeoff = tradeoff\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate dynamic item importance scores\n    obj1_total, obj2_total = archive[selected_idx][1]\n    importance_scores = (value1_lst * (obj2_total / (obj1_total + 1e-6)) +\n                        value2_lst * (obj1_total / (obj2_total + 1e-6))) / weight_lst\n\n    # Step 3: Parallelized trade-off aware search\n    search_depth = min(5, len(weight_lst))\n    for _ in range(3):\n        # Select top items to consider\n        candidate_items = np.argsort(importance_scores)[-search_depth:]\n\n        # Evaluate each candidate in parallel\n        best_improvement = -np.inf\n        best_item = None\n        best_action = None\n\n        for item in candidate_items:\n            if new_solution[item] == 0:\n                # Try adding item\n                new_weight = current_weight + weight_lst[item]\n                if new_weight <= capacity:\n                    improvement = importance_scores[item]\n                    if improvement > best_improvement:\n                        best_improvement = improvement\n                        best_item = item\n                        best_action = 1\n            else:\n                # Try removing item\n                new_weight = current_weight - weight_lst[item]\n                if new_weight >= 0:\n                    improvement = -importance_scores[item]\n                    if improvement > best_improvement:\n                        best_improvement = improvement\n                        best_item = item\n                        best_action = 0\n\n        # Apply best action if found\n        if best_item is not None:\n            new_solution[best_item] = best_action\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 4: Targeted perturbation\n    if np.random.random() < 0.4:\n        # Select items with low importance for perturbation\n        low_importance_items = np.where(importance_scores < np.percentile(importance_scores, 30))[0]\n        if len(low_importance_items) > 0:\n            perturb_item = np.random.choice(low_importance_items)\n            if new_solution[perturb_item] == 0:\n                if current_weight + weight_lst[perturb_item] <= capacity:\n                    new_solution[perturb_item] = 1\n            else:\n                new_solution[perturb_item] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm employs a \"multi-objective dominance-aware perturbation\" approach that first selects the solution with the highest combined objective dominance while considering the solution's distance to the archive's Pareto front, then applies a novel \"adaptive dominance exploration\" strategy that dynamically balances between objective-specific and combined objective transformations through a probabilistic selection mechanism, where each transformation type is chosen based on the current solution's dominance in each objective space, while maintaining feasibility through a \"dominance-aware capacity adjustment\" mechanism that prioritizes items based on their potential to dominate either objective while dynamically adjusting the selection pressure based on the current solution's dominance in each objective space, ultimately generating a neighbor solution that explores both under-dominated regions and promising areas of the search space through a combination of guided local search and probabilistic exploration.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest combined dominance\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance scores\n    obj1_max, obj2_max = np.max(objectives, axis=0)\n    dominance_scores = (objectives[:, 0] / obj1_max) + (objectives[:, 1] / obj2_max)\n\n    # Calculate Pareto distance\n    pareto_front = np.max(objectives, axis=0)\n    pareto_dist = np.sum(np.abs(pareto_front - objectives), axis=1)\n\n    # Select solution with highest combined score\n    selection_scores = dominance_scores * pareto_dist\n    selected_idx = np.argmax(selection_scores)\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive dominance exploration\n    obj1_total, obj2_total = objectives[selected_idx]\n\n    # Calculate selection probabilities based on current solution's dominance\n    if obj1_total > obj2_total * 1.2:  # Strong in objective 1\n        prob_obj1 = 0.7\n        prob_obj2 = 0.3\n    elif obj2_total > obj1_total * 1.2:  # Strong in objective 2\n        prob_obj1 = 0.3\n        prob_obj2 = 0.7\n    else:  # Balanced\n        prob_obj1 = 0.5\n        prob_obj2 = 0.5\n\n    # Step 3: Dominance-aware capacity adjustment\n    if np.random.rand() < prob_obj1:\n        # Objective 1 focused transformation\n        items = np.argsort(-value1_lst / (weight_lst + 1e-6))\n    else:\n        # Objective 2 focused transformation\n        items = np.argsort(-value2_lst / (weight_lst + 1e-6))\n\n    # Perform item swaps with dominance consideration\n    for item in items:\n        if new_solution[item] == 1:\n            # Try to remove if it doesn't dominate in either objective\n            if (value1_lst[item] <= obj1_total * 0.9 or value2_lst[item] <= obj2_total * 0.9) and (current_weight - weight_lst[item] <= capacity):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n        elif new_solution[item] == 0:\n            # Try to add if it improves either objective\n            if (value1_lst[item] > obj1_total * 0.9 or value2_lst[item] > obj2_total * 0.9) and (current_weight + weight_lst[item] <= capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Step 4: Probabilistic dominance-based exploration\n    if np.random.rand() < 0.4:\n        # Randomly flip items based on dominance\n        flip_indices = np.random.choice(len(new_solution), size=min(2, len(new_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if (value1_lst[idx] <= obj1_total * 0.9 or value2_lst[idx] <= obj2_total * 0.9) and (current_weight - weight_lst[idx] <= capacity):\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if (value1_lst[idx] > obj1_total * 0.9 or value2_lst[idx] > obj2_total * 0.9) and (current_weight + weight_lst[idx] <= capacity):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(weight_lst[included_items])]\n        for item in sorted_items:\n            if excess <= 0:\n                break\n            if value1_lst[item] <= obj1_total * 0.9 or value2_lst[item] <= obj2_total * 0.9:\n                new_solution[item] = 0\n                excess -= weight_lst[item]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 304,
          "algorithm": "{The new algorithm incorporates a \"multi-objective hypervolume-aware evolutionary search\" approach that first selects the solution with the highest hypervolume contribution to the Pareto front, then employs a \"guided evolutionary mutation\" strategy that dynamically combines both objective-specific and combined objective transformations through a probabilistic selection mechanism, where each mutation type is chosen based on the current solution's hypervolume contribution and its proximity to the Pareto front, while maintaining feasibility through a \"hypervolume-constrained capacity adjustment\" mechanism that prioritizes items based on their potential to improve the hypervolume while dynamically adjusting the selection pressure based on the current solution's hypervolume contribution, ultimately generating a neighbor solution that explores both under-dominated regions and promising areas of the search space through a combination of guided local search and probabilistic exploration, and periodically introduces targeted mutations based on the solution's hypervolume contribution to escape local optima while preserving the most promising item combinations.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest hypervolume contribution\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate hypervolume contribution (simplified)\n    ref_point = np.max(objectives, axis=0) * 1.1\n    hypervolumes = np.zeros(len(objectives))\n    for i, obj in enumerate(objectives):\n        hypervolumes[i] = (ref_point[0] - obj[0]) * (ref_point[1] - obj[1])\n\n    selected_idx = np.argmax(hypervolumes)\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate hypervolume-aware item scores\n    obj1_total, obj2_total = objectives[selected_idx]\n    hypervolume_scores = np.zeros(len(weight_lst))\n\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            # Score for removal\n            new_obj1 = obj1_total - value1_lst[i]\n            new_obj2 = obj2_total - value2_lst[i]\n            new_hv = (ref_point[0] - new_obj1) * (ref_point[1] - new_obj2)\n            hypervolume_scores[i] = hypervolumes[selected_idx] - new_hv\n        else:\n            # Score for addition\n            if current_weight + weight_lst[i] <= capacity:\n                new_obj1 = obj1_total + value1_lst[i]\n                new_obj2 = obj2_total + value2_lst[i]\n                new_hv = (ref_point[0] - new_obj1) * (ref_point[1] - new_obj2)\n                hypervolume_scores[i] = new_hv - hypervolumes[selected_idx]\n\n    # Step 3: Guided evolutionary mutation\n    mutation_type = np.random.choice(['hypervolume', 'objective1', 'objective2'], p=[0.6, 0.2, 0.2])\n\n    if mutation_type == 'hypervolume':\n        # Hypervolume-focused mutation\n        candidate_items = np.argsort(-hypervolume_scores)[:min(5, len(weight_lst))]\n    elif mutation_type == 'objective1':\n        # Objective 1-focused mutation\n        candidate_items = np.argsort(-value1_lst / (weight_lst + 1e-6))[:min(5, len(weight_lst))]\n    else:\n        # Objective 2-focused mutation\n        candidate_items = np.argsort(-value2_lst / (weight_lst + 1e-6))[:min(5, len(weight_lst))]\n\n    # Apply mutations\n    for item in candidate_items:\n        if new_solution[item] == 1:\n            # Try removal\n            if hypervolume_scores[item] > 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Try addition\n            if hypervolume_scores[item] > 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Probabilistic hypervolume-based exploration\n    if np.random.rand() < 0.3:\n        # Randomly flip items based on hypervolume contribution\n        flip_indices = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if hypervolume_scores[idx] > 0:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if hypervolume_scores[idx] > 0 and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(weight_lst[included_items])]\n        for item in sorted_items:\n            if excess <= 0:\n                break\n            if hypervolume_scores[item] > 0:\n                new_solution[item] = 0\n                excess -= weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -0.9033075253261673,
               3.3394522666931152
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest hypervolume contribution\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate hypervolume contribution (simplified)\n    ref_point = np.max(objectives, axis=0) * 1.1\n    hypervolumes = np.zeros(len(objectives))\n    for i, obj in enumerate(objectives):\n        hypervolumes[i] = (ref_point[0] - obj[0]) * (ref_point[1] - obj[1])\n\n    selected_idx = np.argmax(hypervolumes)\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate hypervolume-aware item scores\n    obj1_total, obj2_total = objectives[selected_idx]\n    hypervolume_scores = np.zeros(len(weight_lst))\n\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            # Score for removal\n            new_obj1 = obj1_total - value1_lst[i]\n            new_obj2 = obj2_total - value2_lst[i]\n            new_hv = (ref_point[0] - new_obj1) * (ref_point[1] - new_obj2)\n            hypervolume_scores[i] = hypervolumes[selected_idx] - new_hv\n        else:\n            # Score for addition\n            if current_weight + weight_lst[i] <= capacity:\n                new_obj1 = obj1_total + value1_lst[i]\n                new_obj2 = obj2_total + value2_lst[i]\n                new_hv = (ref_point[0] - new_obj1) * (ref_point[1] - new_obj2)\n                hypervolume_scores[i] = new_hv - hypervolumes[selected_idx]\n\n    # Step 3: Guided evolutionary mutation\n    mutation_type = np.random.choice(['hypervolume', 'objective1', 'objective2'], p=[0.6, 0.2, 0.2])\n\n    if mutation_type == 'hypervolume':\n        # Hypervolume-focused mutation\n        candidate_items = np.argsort(-hypervolume_scores)[:min(5, len(weight_lst))]\n    elif mutation_type == 'objective1':\n        # Objective 1-focused mutation\n        candidate_items = np.argsort(-value1_lst / (weight_lst + 1e-6))[:min(5, len(weight_lst))]\n    else:\n        # Objective 2-focused mutation\n        candidate_items = np.argsort(-value2_lst / (weight_lst + 1e-6))[:min(5, len(weight_lst))]\n\n    # Apply mutations\n    for item in candidate_items:\n        if new_solution[item] == 1:\n            # Try removal\n            if hypervolume_scores[item] > 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Try addition\n            if hypervolume_scores[item] > 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Probabilistic hypervolume-based exploration\n    if np.random.rand() < 0.3:\n        # Randomly flip items based on hypervolume contribution\n        flip_indices = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if hypervolume_scores[idx] > 0:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if hypervolume_scores[idx] > 0 and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Ensure feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(weight_lst[included_items])]\n        for item in sorted_items:\n            if excess <= 0:\n                break\n            if hypervolume_scores[item] > 0:\n                new_solution[item] = 0\n                excess -= weight_lst[item]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm combines the selection of under-explored solutions with a novel \"objective-balanced perturbation\" approach that first identifies solutions with high potential for improvement by analyzing both objective trade-offs and solution diversity, then applies a \"value-aware cluster expansion\" that selectively expands or contracts clusters of items based on their balanced contributions to both objectives, while dynamically adjusting the perturbation intensity to balance exploration and exploitation. This is achieved by calculating a dynamic balance score for each item that combines its value contributions with the current solution's objective balance, and then probabilistically selecting items to flip based on this score, ensuring feasibility through a capacity-aware selection process. The algorithm also incorporates a \"trade-off-aware diversification\" step that randomly flips a small number of items to maintain diversity while still favoring those with high balance potential.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high objective balance potential\n    selected_idx = 0\n    max_balance_score = -float('inf')\n    for i, (sol, obj) in enumerate(archive):\n        obj1, obj2 = obj\n        balance_score = abs(obj1 - obj2) / (obj1 + obj2 + 1e-6)\n        if balance_score > max_balance_score:\n            max_balance_score = balance_score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate dynamic balance scores for items\n    obj1_total, obj2_total = archive[selected_idx][1]\n    balance_scores = abs((value1_lst - value2_lst) / (value1_lst + value2_lst + 1e-6))\n\n    # Step 3: Value-aware cluster expansion\n    items = np.arange(len(base_solution))\n    np.random.shuffle(items)\n\n    for item in items:\n        if new_solution[item] == 1:\n            # Try removing item if it has low balance contribution\n            new_weight = current_weight - weight_lst[item]\n            if new_weight <= capacity and balance_scores[item] < 0.5:\n                new_solution[item] = 0\n                current_weight = new_weight\n                break\n        else:\n            # Try adding item if it has high balance contribution\n            new_weight = current_weight + weight_lst[item]\n            if new_weight <= capacity and balance_scores[item] > 0.7:\n                new_solution[item] = 1\n                current_weight = new_weight\n                break\n\n    # Step 4: Trade-off-aware diversification\n    if np.random.random() < 0.4:\n        # Select items with medium balance contribution for diversification\n        medium_balance_items = np.where((balance_scores >= 0.5) & (balance_scores <= 0.7))[0]\n        if len(medium_balance_items) > 0:\n            diversification_items = np.random.choice(medium_balance_items, size=min(2, len(medium_balance_items)), replace=False)\n            for item in diversification_items:\n                if new_solution[item] == 1:\n                    if current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                else:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm combines the strengths of the existing approaches by first selecting a solution from the archive based on a novel \"objective-space entropy\" metric that measures the diversity and potential for improvement in both objectives, then applies a hybrid local search operator that alternates between \"value-driven\" and \"weight-aware\" perturbations, where value-driven perturbations prioritize items with high marginal gains in either objective while maintaining feasibility, and weight-aware perturbations systematically remove or add items based on their weight efficiency and current capacity utilization, dynamically adjusting the search strategy to balance exploitation of high-value items and exploration of underutilized capacity regions, with a special focus on maintaining a Pareto front-aware balance between the two objectives through a calculated \"trade-off sensitivity\" metric that guides the selection of items to replace or add.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest objective-space entropy\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.sum(np.log(objectives + 1) * (1 - objectives/np.max(objectives + 1e-6, axis=0)), axis=1))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate trade-off sensitivity metric\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off_sensitivity = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Hybrid local search with alternating perturbations\n    for _ in range(2):\n        if np.random.random() < 0.5:\n            # Value-driven perturbation (prioritize high-value items)\n            if np.random.random() < 0.5:\n                # Focus on objective 1\n                items = np.argsort(-value1_lst / (weight_lst + 1e-6))\n            else:\n                # Focus on objective 2\n                items = np.argsort(-value2_lst / (weight_lst + 1e-6))\n\n            for item in items:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n                elif new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n        else:\n            # Weight-aware perturbation (balance capacity utilization)\n            if current_weight < capacity * 0.7:\n                # Add items to utilize capacity\n                items = np.argsort(-trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove items to balance capacity\n                items = np.argsort(trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 305,
          "algorithm": "{The new algorithm introduces a \"multi-objective cluster-based perturbation\" approach that first identifies solutions with high potential for improvement by analyzing both objective trade-offs and solution diversity, then applies a \"hierarchical cluster expansion\" that groups items based on their combined value contributions and spatial proximity in the objective space, followed by a \"value-weighted cluster perturbation\" that probabilistically selects and flips entire clusters of items based on their collective balance score, while dynamically adjusting the perturbation intensity to balance exploration and exploitation. This is achieved through a \"cluster trade-off sensitivity\" metric that evaluates each cluster's contribution to both objectives, and a \"capacity-aware cluster merging\" step that ensures feasibility by systematically merging or splitting clusters to maintain the total weight constraint. The algorithm also incorporates a \"diversity-preserving cluster redistribution\" step that randomly reassigns items between clusters to maintain solution diversity while still favoring those with high balance potential.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest objective diversity\n    selected_idx = np.argmax([np.std([obj1, obj2]) for _, (obj1, obj2) in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate cluster trade-off sensitivity\n    obj1_total, obj2_total = archive[selected_idx][1]\n    cluster_scores = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Hierarchical cluster expansion\n    items = np.arange(len(base_solution))\n    np.random.shuffle(items)\n\n    for item in items:\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            if cluster_scores[item] > 1.0:  # High trade-off sensitivity\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n        elif new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n            if cluster_scores[item] < 0.8:  # Low trade-off sensitivity\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Capacity-aware cluster merging\n    if current_weight > capacity * 0.9:\n        heavy_items = np.argsort(-weight_lst[new_solution == 1])\n        for idx in heavy_items:\n            if current_weight <= capacity:\n                break\n            if new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 5: Diversity-preserving cluster redistribution\n    if np.random.random() < 0.3:\n        medium_items = np.where((cluster_scores >= 0.9) & (cluster_scores <= 1.1))[0]\n        if len(medium_items) > 0:\n            for item in np.random.choice(medium_items, size=min(3, len(medium_items)), replace=False):\n                if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                elif new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -0.7761008646778298,
               9.689242452383041
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest objective diversity\n    selected_idx = np.argmax([np.std([obj1, obj2]) for _, (obj1, obj2) in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate cluster trade-off sensitivity\n    obj1_total, obj2_total = archive[selected_idx][1]\n    cluster_scores = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Hierarchical cluster expansion\n    items = np.arange(len(base_solution))\n    np.random.shuffle(items)\n\n    for item in items:\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            if cluster_scores[item] > 1.0:  # High trade-off sensitivity\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n        elif new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n            if cluster_scores[item] < 0.8:  # Low trade-off sensitivity\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Capacity-aware cluster merging\n    if current_weight > capacity * 0.9:\n        heavy_items = np.argsort(-weight_lst[new_solution == 1])\n        for idx in heavy_items:\n            if current_weight <= capacity:\n                break\n            if new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 5: Diversity-preserving cluster redistribution\n    if np.random.random() < 0.3:\n        medium_items = np.where((cluster_scores >= 0.9) & (cluster_scores <= 1.1))[0]\n        if len(medium_items) > 0:\n            for item in np.random.choice(medium_items, size=min(3, len(medium_items)), replace=False):\n                if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                elif new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm combines the strengths of the existing approaches by first selecting a solution from the archive based on a novel \"objective-space entropy\" metric that measures the diversity and potential for improvement in both objectives, then applies a hybrid local search operator that alternates between \"value-driven\" and \"weight-aware\" perturbations, where value-driven perturbations prioritize items with high marginal gains in either objective while maintaining feasibility, and weight-aware perturbations systematically remove or add items based on their weight efficiency and current capacity utilization, dynamically adjusting the search strategy to balance exploitation of high-value items and exploration of underutilized capacity regions, with a special focus on maintaining a Pareto front-aware balance between the two objectives through a calculated \"trade-off sensitivity\" metric that guides the selection of items to replace or add.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest objective-space entropy\n    objectives = np.array([obj for _, obj in archive])\n    selected_idx = np.argmax(np.sum(np.log(objectives + 1) * (1 - objectives/np.max(objectives + 1e-6, axis=0)), axis=1))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate trade-off sensitivity metric\n    obj1_total, obj2_total = archive[selected_idx][1]\n    trade_off_sensitivity = (value1_lst * (obj2_total + 1e-6)) / (value2_lst * (obj1_total + 1e-6) + 1e-6)\n\n    # Step 3: Hybrid local search with alternating perturbations\n    for _ in range(2):\n        if np.random.random() < 0.5:\n            # Value-driven perturbation (prioritize high-value items)\n            if np.random.random() < 0.5:\n                # Focus on objective 1\n                items = np.argsort(-value1_lst / (weight_lst + 1e-6))\n            else:\n                # Focus on objective 2\n                items = np.argsort(-value2_lst / (weight_lst + 1e-6))\n\n            for item in items:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n                elif new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n        else:\n            # Weight-aware perturbation (balance capacity utilization)\n            if current_weight < capacity * 0.7:\n                # Add items to utilize capacity\n                items = np.argsort(-trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n            else:\n                # Remove items to balance capacity\n                items = np.argsort(trade_off_sensitivity)\n                for item in items:\n                    if new_solution[item] == 1 and current_weight - weight_lst[item] <= capacity:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n                        break\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm first identifies the solution with the highest combined objective value by normalizing both objectives and then applies a novel 'adaptive dominance-aware perturbation' strategy that combines probabilistic item selection with a guided exploration of adjacent solutions, where items are selected based on their dominance in either objective while dynamically adjusting the selection to maintain feasibility, ensuring the generated neighbor solution remains feasible and explores diverse regions of the search space by balancing exploration and exploitation through adaptive neighborhood exploration and probabilistic selection of items with high dominance in either objective, while also incorporating a dynamic capacity adjustment mechanism that prioritizes items with balanced dominance contributions to both objectives.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with the highest combined normalized objective value\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2])\n    combined_obj = np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_obj)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Adaptive dominance-aware perturbation\n    # Calculate dominance scores for both objectives\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Select items to add based on dominance in either objective\n    candidate_indices = np.where(new_solution == 0)[0]\n    if len(candidate_indices) > 0:\n        # Select items that are dominant in either objective\n        v1_dominant = candidate_indices[dominance_v1[candidate_indices] > dominance_v2[candidate_indices]]\n        v2_dominant = candidate_indices[dominance_v2[candidate_indices] > dominance_v1[candidate_indices]]\n\n        # Add items from both dominant sets with probability\n        for idx in v1_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n        for idx in v2_dominant:\n            if weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Guided exploration of adjacent solutions\n    # Remove items based on dominance in the opposite objective\n    in_solution_indices = np.where(new_solution == 1)[0]\n    if len(in_solution_indices) > 0:\n        # Remove items that are weak in both objectives\n        weak_items = in_solution_indices[(dominance_v1[in_solution_indices] < 0.5) & (dominance_v2[in_solution_indices] < 0.5)]\n        for idx in weak_items:\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n\n    # Ensure feasibility with dynamic adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 306,
          "algorithm": "{The new algorithm first identifies the solution with the highest \"trade-off sensitivity\" by calculating a metric that balances the normalized marginal gains of both objectives, then applies a novel \"adaptive dominance-aware perturbation\" strategy that combines probabilistic item selection with a guided exploration of adjacent solutions, where items are selected based on their dominance in either objective while dynamically adjusting the selection to maintain feasibility, ensuring the generated neighbor solution remains feasible and explores diverse regions of the search space by balancing exploration and exploitation through adaptive neighborhood exploration and probabilistic selection of items with high dominance in either objective, while also incorporating a dynamic capacity adjustment mechanism that prioritizes items with balanced dominance contributions to both objectives, and finally applies a \"value-aware dominance\" refinement step that systematically removes or adds items based on their dominance in either objective while maintaining feasibility, ensuring the final solution is both high-quality and feasible.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest trade-off sensitivity\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2])\n    trade_off_sensitivity = np.abs(normalized_obj[:, 0] - normalized_obj[:, 1])\n    selected_idx = np.argmax(trade_off_sensitivity)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive dominance-aware perturbation\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Select items to add based on dominance in either objective\n    candidate_indices = np.where(new_solution == 0)[0]\n    if len(candidate_indices) > 0:\n        v1_dominant = candidate_indices[dominance_v1[candidate_indices] > dominance_v2[candidate_indices]]\n        v2_dominant = candidate_indices[dominance_v2[candidate_indices] > dominance_v1[candidate_indices]]\n\n        # Add items from both dominant sets with probability\n        for idx in v1_dominant:\n            if weight_lst[idx] <= (capacity - current_weight) and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n        for idx in v2_dominant:\n            if weight_lst[idx] <= (capacity - current_weight) and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 3: Dynamic capacity adjustment\n    if current_weight < capacity * 0.8:\n        # Add items with balanced dominance\n        balanced_items = np.where((dominance_v1 > 0.5) & (dominance_v2 > 0.5) & (new_solution == 0))[0]\n        for idx in balanced_items:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                if current_weight >= capacity * 0.8:\n                    break\n\n    # Step 4: Value-aware dominance refinement\n    in_solution_indices = np.where(new_solution == 1)[0]\n    if len(in_solution_indices) > 0:\n        # Remove items that are weak in both objectives\n        weak_items = in_solution_indices[(dominance_v1[in_solution_indices] < 0.3) & (dominance_v2[in_solution_indices] < 0.3)]\n        for idx in weak_items:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -0.839800624440297,
               1.6505511403083801
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest trade-off sensitivity\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2])\n    trade_off_sensitivity = np.abs(normalized_obj[:, 0] - normalized_obj[:, 1])\n    selected_idx = np.argmax(trade_off_sensitivity)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive dominance-aware perturbation\n    dominance_v1 = value1_lst / (weight_lst + 1e-6)\n    dominance_v2 = value2_lst / (weight_lst + 1e-6)\n\n    # Select items to add based on dominance in either objective\n    candidate_indices = np.where(new_solution == 0)[0]\n    if len(candidate_indices) > 0:\n        v1_dominant = candidate_indices[dominance_v1[candidate_indices] > dominance_v2[candidate_indices]]\n        v2_dominant = candidate_indices[dominance_v2[candidate_indices] > dominance_v1[candidate_indices]]\n\n        # Add items from both dominant sets with probability\n        for idx in v1_dominant:\n            if weight_lst[idx] <= (capacity - current_weight) and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n        for idx in v2_dominant:\n            if weight_lst[idx] <= (capacity - current_weight) and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 3: Dynamic capacity adjustment\n    if current_weight < capacity * 0.8:\n        # Add items with balanced dominance\n        balanced_items = np.where((dominance_v1 > 0.5) & (dominance_v2 > 0.5) & (new_solution == 0))[0]\n        for idx in balanced_items:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                if current_weight >= capacity * 0.8:\n                    break\n\n    # Step 4: Value-aware dominance refinement\n    in_solution_indices = np.where(new_solution == 1)[0]\n    if len(in_solution_indices) > 0:\n        # Remove items that are weak in both objectives\n        weak_items = in_solution_indices[(dominance_v1[in_solution_indices] < 0.3) & (dominance_v2[in_solution_indices] < 0.3)]\n        for idx in weak_items:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Ensure feasibility with final adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        sorted_indices = included_indices[np.argsort(weight_lst[included_indices])[::-1]]\n        for idx in sorted_indices:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n"
     }
]