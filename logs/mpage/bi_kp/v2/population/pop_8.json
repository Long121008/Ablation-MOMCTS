[
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    base_solution = max(archive, key=lambda x: (x[1][0] + x[1][1]))[0].copy()\n\n    # Calculate current total weight and values\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Create a candidate solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(base_solution), size=min(3, len(base_solution)), replace=False)\n\n    for idx in flip_indices:\n        # 2. Flip the item if it improves both objectives or weight allows\n        if new_solution[idx] == 1:\n            # Try to remove the item if it's not critical for weight\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item if it fits in capacity\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # 3. Additional improvement: swap two items if it improves both objectives\n    if len(base_solution) >= 2:\n        i, j = np.random.choice(len(base_solution), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            # Calculate potential new weight and values\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -18.586423720786442,
               -18.76328847454988
          ]
     },
     {
          "algorithm": "{The proposed algorithm, \"Objective-Driven Multi-Phase Exploration with Adaptive Constraint Handling,\" begins by identifying the least explored region of the objective space and selecting a solution from it to balance exploration and exploitation. It then employs a multi-phase approach that first performs a greedy improvement phase to maximize one objective while maintaining feasibility, followed by a constraint-adaptive phase that refines the solution by selectively flipping items based on their impact on both objectives and the remaining capacity. The algorithm dynamically adjusts its exploration strategy based on the current solution's position in the objective space, using a combination of value-to-weight ratios, marginal improvements, and capacity-aware perturbations. After each phase, it performs a feasibility check and objective evaluation, reverting any infeasible changes. The process iterates through these phases with decreasing intensity, culminating in a final refinement step that ensures the solution lies on or near the Pareto front by selectively adding items that provide the best combined improvement in both objectives without exceeding capacity.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify least explored region in objective space\n    def partition_archive(archive):\n        if len(archive) < 4:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 4 partitions\n        split1 = len(sorted_archive) // 4\n        split2 = len(sorted_archive) // 2\n        split3 = 3 * len(sorted_archive) // 4\n        return [sorted_archive[:split1], sorted_archive[split1:split2],\n                sorted_archive[split2:split3], sorted_archive[split3:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Multi-phase exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Phase 1: Greedy improvement for one objective\n    for _ in range(3):\n        # Randomly select an objective to improve\n        if random.random() < 0.5:\n            # Improve objective 1\n            marginal1 = value1_lst / (weight_lst + 1e-6)\n            candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n        else:\n            # Improve objective 2\n            marginal2 = value2_lst / (weight_lst + 1e-6)\n            candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Phase 2: Constraint-adaptive refinement\n    for _ in range(5):\n        # Calculate potential improvements for both objectives\n        potential_obj1 = np.sum(value1_lst * new_solution) + value1_lst * (1 - new_solution)\n        potential_obj2 = np.sum(value2_lst * new_solution) + value2_lst * (1 - new_solution)\n\n        # Find items that could improve both objectives\n        improvement1 = potential_obj1 - np.sum(value1_lst * new_solution)\n        improvement2 = potential_obj2 - np.sum(value2_lst * new_solution)\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    # Phase 3: Capacity-aware perturbations\n    for _ in range(3):\n        # Randomly select items to flip based on capacity\n        idx = random.randint(0, len(new_solution)-1)\n        if new_solution[idx] == 0:\n            if weight_lst[idx] <= available_capacity:\n                new_solution[idx] = 1\n                available_capacity -= weight_lst[idx]\n        else:\n            new_solution[idx] = 0\n            available_capacity += weight_lst[idx]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    for _ in range(2):\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_idx = np.argmax(potential_improvement)\n\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -16.54517292621003,
               -18.782324216935123
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    # This operator first identifies items that are critical for one objective but not the other\n    # and then performs targeted swaps to improve the solution\n\n    # Calculate objective-specific utilities\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    # This perturbation adds or removes items based on their combined utility\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        # Sort by combined utility in descending order\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        # Try to add the top item if it fits\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        # Sort by combined utility in ascending order\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        # Try to remove the worst item\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    # This step performs small random changes that maintain feasibility\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            # Remove item if it's not critical for either objective\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Add item if it fits and is not too expensive\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.00282112755363,
               -18.655065486064522
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.16981140357512,
               -18.519434126196355
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.224458802519536,
               -18.424070730144123
          ]
     },
     {
          "algorithm": "{The new algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.980831889190338,
               -18.713690870174126
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    max_v1 = max(x[1][0] for x in archive) if archive else 1.0\n    max_v2 = max(x[1][1] for x in archive) if archive else 1.0\n    selected_idx = max(range(len(archive)), key=lambda i: (archive[i][1][0]/max_v1 + archive[i][1][1]/max_v2))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic objective prioritization with adaptive flipping\n    objective_weights = [0.5, 0.5]\n    n_items = len(weight_lst)\n\n    for _ in range(10):\n        # Update objective weights based on current solution's performance\n        v1_ratio = (value1_lst * new_solution).sum() / (value1_lst.sum() + 1e-6)\n        v2_ratio = (value2_lst * new_solution).sum() / (value2_lst.sum() + 1e-6)\n        objective_weights[0] = 0.6 * v1_ratio + 0.4 * (1 - v2_ratio)\n        objective_weights[1] = 0.6 * v2_ratio + 0.4 * (1 - v1_ratio)\n\n        # Select an objective to focus on\n        if random.random() < objective_weights[0]:\n            values = value1_lst\n        else:\n            values = value2_lst\n\n        # Find items to flip based on value and current weight\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            # Try to add items with highest value-to-weight ratio first\n            ratios = values / (weight_lst + 1e-6)\n            sorted_items = sorted(candidate_items, key=lambda x: -ratios[x])\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            # Try to remove items with lowest value-to-weight ratio first\n            ratios = values / (weight_lst + 1e-6)\n            sorted_items = sorted(candidate_items, key=lambda x: ratios[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Randomized diversification with feasibility check\n    for _ in range(5):\n        item = random.randint(0, n_items - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.01087539225249,
               -18.134394186781496
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    # Calculate improvement potential based on normalized objective values\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    def improvement_potential(sol_obj):\n        v1, v2 = sol_obj\n        norm_v1 = v1 / max_v1 if max_v1 > 0 else 0\n        norm_v2 = v2 / max_v2 if max_v2 > 0 else 0\n        return (1 - norm_v1) * (1 - norm_v2)  # High potential when either objective is far from max\n\n    selected_idx = max(range(len(archive)), key=lambda i: improvement_potential(archive[i][1]))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic objective balancing and item selection\n    # Calculate utility scores for both objectives\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Alternate between objectives in each iteration\n    for _ in range(10):\n        # Randomly select an objective to focus on\n        if random.random() < 0.5:\n            # Focus on objective 1\n            utility = utility1\n            values = value1_lst\n        else:\n            # Focus on objective 2\n            utility = utility2\n            values = value2_lst\n\n        # Find items with highest utility that can be added\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by utility in descending order\n        sorted_items = sorted(candidate_items, key=lambda x: utility[x], reverse=True)\n\n        # Try to add items with highest utility first\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n        # Find items with lowest utility that can be removed\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by utility in ascending order\n        sorted_items = sorted(candidate_items, key=lambda x: utility[x])\n\n        # Try to remove items with lowest utility first\n        for item in sorted_items:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 3: Intelligent random perturbations with objective balancing\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            # Remove item if it's not critical for either objective\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Add item if it improves at least one objective significantly\n            if (current_weight + weight_lst[item] <= capacity and\n                (value1_lst[item] > np.median(value1_lst) or value2_lst[item] > np.median(value2_lst))):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.93891881164476,
               -18.19800975992053
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    max_v1 = max(x[1][0] for x in archive) if archive else 1.0\n    max_v2 = max(x[1][1] for x in archive) if archive else 1.0\n    selected_idx = max(range(len(archive)), key=lambda i: (archive[i][1][0]/max_v1 + archive[i][1][1]/max_v2))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Swap\" that considers both objectives\n    # This operator identifies items that are valuable for one objective but not the other\n    # and performs targeted swaps to improve both objectives simultaneously\n\n    # Calculate objective-specific utilities\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are valuable for objective 1 but not objective 2\n    valuable1 = (value1_lst > 0.5 * np.max(value1_lst)) & (value2_lst < 0.5 * np.max(value2_lst))\n    # Identify items that are valuable for objective 2 but not objective 1\n    valuable2 = (value2_lst > 0.5 * np.max(value2_lst)) & (value1_lst < 0.5 * np.max(value1_lst))\n\n    # Perform swaps between valuable items of different objectives\n    for i in np.where(valuable1 & (new_solution == 1))[0]:\n        for j in np.where(valuable2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Balanced Greedy Perturbation\" that considers both objectives\n    # This perturbation adds or removes items based on their combined utility and weight\n    combined_utility = (utility1 + utility2) / (weight_lst + 1e-6)\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        # Sort by combined utility in descending order\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        # Try to add the top item if it fits\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        # Sort by combined utility in ascending order\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        # Try to remove the worst item\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" with objective-aware selection\n    # This step performs small random changes that maintain feasibility and consider both objectives\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            # Remove item if it's not valuable for either objective\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) and value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Add item if it fits and is valuable for at least one objective\n            if (current_weight + weight_lst[item] <= capacity and\n                (value1_lst[item] > np.median(value1_lst) or value2_lst[item] > np.median(value2_lst))):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.93161065631989,
               -18.360538970559908
          ]
     },
     {
          "algorithm": "{The novel algorithm, \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion,\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    def improvement_potential(sol_obj):\n        v1, v2 = sol_obj\n        return v1 + v2  # Total value as improvement potential\n\n    selected_idx = max(range(len(archive)), key=lambda i: improvement_potential(archive[i][1]))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic objective balancing and item selection\n    for _ in range(10):\n        # Randomly select an objective to focus on\n        if random.random() < 0.5:\n            # Focus on objective 1\n            utility = value1_lst / (weight_lst + 1e-6)\n        else:\n            # Focus on objective 2\n            utility = value2_lst / (weight_lst + 1e-6)\n\n        # Find items with highest utility that can be added\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by utility in descending order\n        sorted_items = sorted(candidate_items, key=lambda x: utility[x], reverse=True)\n\n        # Try to add items with highest utility first\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n        # Find items with lowest utility that can be removed\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by utility in ascending order\n        sorted_items = sorted(candidate_items, key=lambda x: utility[x])\n\n        # Try to remove items with lowest utility first\n        for item in sorted_items:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 3: Intelligent random perturbations with objective balancing\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            # Remove item if it's not critical for either objective\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Add item if it improves at least one objective significantly\n            if (current_weight + weight_lst[item] <= capacity and\n                (value1_lst[item] > np.median(value1_lst) or value2_lst[item] > np.median(value2_lst))):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.908299344333273,
               -18.470111771438212
          ]
     }
]