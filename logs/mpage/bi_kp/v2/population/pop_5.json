[
     {
          "algorithm": "{The heuristic function 'select_neighbor' first identifies promising solutions in the archive by evaluating their potential for improvement using a combination of objective dominance, crowding distance, and solution diversity metrics. It then intelligently selects a base solution from these candidates using a weighted random selection that prioritizes solutions with higher crowding distances and lower dominance counts. The local search operator employs a hybrid strategy that combines item swapping, random flipping, and adaptive perturbation to explore the neighborhood, ensuring feasibility by dynamically adjusting the selection of items to flip based on the remaining capacity. The operator also incorporates a memory mechanism to avoid revisiting recently explored solutions, further enhancing exploration. The function returns the new neighbor solution after validating its feasibility and updating the archive with the new solution if it is non-dominated.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions in the archive for potential improvement\n    solutions = [sol for sol, _ in archive]\n    objectives = [obj for _, obj in archive]\n\n    # Calculate dominance counts and crowding distances\n    dominance_counts = [0] * len(archive)\n    crowding_distances = [0.0] * len(archive)\n\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if objectives[i][0] <= objectives[j][0] and objectives[i][1] <= objectives[j][1]:\n                    dominance_counts[i] += 1\n                if objectives[i][0] < objectives[j][0] and objectives[i][1] < objectives[j][1]:\n                    dominance_counts[i] += 1\n\n    # Sort solutions by dominance count (lower is better)\n    sorted_indices = sorted(range(len(archive)), key=lambda k: dominance_counts[k])\n    candidate_indices = sorted_indices[:max(1, len(archive) // 2)]\n\n    # Step 2: Select a base solution with weighted randomness\n    weights = [1.0 / (1.0 + dominance_counts[i]) for i in candidate_indices]\n    selected_idx = random.choices(candidate_indices, weights=weights, k=1)[0]\n    base_solution = solutions[selected_idx].copy()\n\n    # Step 3: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Hybrid local search operator\n    # 1. Randomly select a subset of items to flip\n    flip_indices = random.sample(range(n_items), min(3, n_items // 2))\n\n    # 2. Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # 3. Flip items while maintaining feasibility\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove item if possible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add item if possible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # 4. Additional perturbation: swap two items if feasible\n    if len(new_solution) >= 2:\n        i, j = random.sample(range(n_items), 2)\n        if new_solution[i] != new_solution[j]:\n            # Calculate weight difference\n            weight_diff = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n            if current_weight + weight_diff <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Ensure feasibility (shouldn't be necessary but as a safeguard)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        # If not feasible, try to remove items randomly until feasible\n        while total_weight > capacity:\n            items_in = np.where(new_solution == 1)[0]\n            if len(items_in) == 0:\n                break\n            remove_idx = random.choice(items_in)\n            new_solution[remove_idx] = 0\n            total_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
          "score": [
               -18.783291405738993,
               -18.18577865487048
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    base_solution = max(archive, key=lambda x: (x[1][0] + x[1][1]))[0].copy()\n\n    # Calculate current total weight and values\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Create a candidate solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(base_solution), size=min(3, len(base_solution)), replace=False)\n\n    for idx in flip_indices:\n        # 2. Flip the item if it improves both objectives or weight allows\n        if new_solution[idx] == 1:\n            # Try to remove the item if it's not critical for weight\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item if it fits in capacity\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # 3. Additional improvement: swap two items if it improves both objectives\n    if len(base_solution) >= 2:\n        i, j = np.random.choice(len(base_solution), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            # Calculate potential new weight and values\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -18.586423720786442,
               -18.76328847454988
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify the solution with the best combined objective value\n    def combined_score(obj):\n        return obj[0] + obj[1]\n\n    selected_idx = max(range(len(archive)), key=lambda i: combined_score(archive[i][1]))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Dynamic neighborhood exploration with value-to-weight ratio\n    current_weight = np.sum(weight_lst * new_solution)\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / (weight_lst + 1e-6)\n    ratio2 = value2_lst / (weight_lst + 1e-6)\n\n    # Alternate between both objectives in each iteration\n    for _ in range(10):\n        # Randomly select an objective to focus on\n        if random.random() < 0.5:\n            # Focus on objective 1\n            ratios = ratio1\n            values = value1_lst\n        else:\n            # Focus on objective 2\n            ratios = ratio2\n            values = value2_lst\n\n        # Find items with highest value-to-weight ratio that can be added\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by value-to-weight ratio in descending order\n        sorted_items = sorted(candidate_items, key=lambda x: ratios[x], reverse=True)\n\n        # Try to add items with highest ratio first\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n        # Find items with lowest value-to-weight ratio that can be removed\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by value-to-weight ratio in ascending order\n        sorted_items = sorted(candidate_items, key=lambda x: ratios[x])\n\n        # Try to remove items with lowest ratio first\n        for item in sorted_items:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 3: Random perturbation with feasibility check\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, n_items - 1)\n\n        if new_solution[item] == 1:\n            # Try to remove the item\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Try to add the item\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.757949963680435,
               -18.267274884773002
          ]
     },
     {
          "algorithm": "{The proposed algorithm, named \"Dual-Objective Guided Evolutionary Perturbation,\" first identifies the most balanced solution in the archive by selecting the one with the highest harmonic mean of its objective values, ensuring it represents a well-compromised trade-off between both objectives. It then applies a novel evolutionary perturbation strategy that combines random item flips with objective-specific mutations, where each flip has a probability inversely proportional to the item's contribution to either objective. The algorithm dynamically adjusts the mutation intensity based on the current solution's dominance status in the archive, performing more aggressive perturbations for non-dominated solutions and more conservative changes for dominated ones. After each perturbation, it performs a feasibility check and objective evaluation, reverting any infeasible changes. The process iterates for a fixed number of steps, with each iteration potentially introducing small, targeted improvements that collectively lead to significant multi-objective improvements.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the most balanced solution based on harmonic mean\n    def harmonic_mean(obj):\n        v1, v2 = obj\n        if v1 == 0 or v2 == 0:\n            return 0\n        return 2 * v1 * v2 / (v1 + v2)\n\n    base_solution, base_obj = max(archive, key=lambda x: harmonic_mean(x[1]))\n    base_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic perturbation intensity based on dominance status\n    is_dominated = any(\n        (s_obj[0] >= base_obj[0] and s_obj[1] >= base_obj[1] and (s_obj[0] > base_obj[0] or s_obj[1] > base_obj[1]))\n        for s, s_obj in archive\n    )\n    perturbation_intensity = 0.3 if is_dominated else 0.5\n\n    # Perform evolutionary perturbations\n    for _ in range(min(10, n_items)):\n        if random.random() < perturbation_intensity:\n            # Select item with probability inversely proportional to its objective contribution\n            item_probs = []\n            for i in range(n_items):\n                contrib1 = value1_lst[i] if new_solution[i] else 0\n                contrib2 = value2_lst[i] if new_solution[i] else 0\n                total_contrib = contrib1 + contrib2\n                item_probs.append(1.0 / (1.0 + total_contrib) if total_contrib > 0 else 1.0)\n\n            item_probs = np.array(item_probs) / np.sum(item_probs)\n            idx = np.random.choice(n_items, p=item_probs)\n\n            # Flip the item with probability based on its contribution\n            if new_solution[idx] == 1:\n                # More likely to remove if high contribution\n                remove_prob = 0.7 * (value1_lst[idx] + value2_lst[idx]) / (np.max(value1_lst) + np.max(value2_lst))\n                if random.random() < remove_prob:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n            else:\n                # More likely to add if low contribution\n                add_prob = 0.7 * (1 - (value1_lst[idx] + value2_lst[idx]) / (np.max(value1_lst) + np.max(value2_lst)))\n                if random.random() < add_prob:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -18.73294407180715,
               -18.247791513237175
          ]
     },
     {
          "algorithm": "{The algorithm first identifies promising solutions in the archive by selecting those with high objective values or those that are on the Pareto front, then intelligently samples a base solution from this subset. It then applies a hybrid local search strategy that combines a novel \"swap-and-flip\" operator with a probabilistic \"value-weighted\" perturbation to generate a neighbor solution. The swap-and-flip operator selects two items, swaps their inclusion statuses, and then flips the status of a third item based on a value-to-weight ratio heuristic, while the probabilistic perturbation introduces randomness to escape local optima. The algorithm ensures feasibility by rejecting any moves that would exceed the capacity, and it iteratively refines the solution by accepting improvements in either objective or Pareto-dominated solutions, ultimately returning a high-quality neighbor solution that balances exploration and exploitation of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify promising solutions (high objective values or on Pareto front)\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    # If no Pareto front, select top 20% by sum of objectives\n    if not pareto_front:\n        sorted_indices = np.argsort(-objectives.sum(axis=1))\n        candidate_indices = sorted_indices[:max(1, len(archive) // 5)]\n    else:\n        candidate_indices = pareto_front\n\n    # Randomly select a base solution from candidates\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Swap-and-flip operator\n    if n_items >= 3:\n        # Select two items to swap\n        swap_indices = np.random.choice(n_items, size=2, replace=False)\n        # Flip a third item based on value-to-weight ratio\n        flip_idx = np.random.choice(n_items)\n        while flip_idx in swap_indices:\n            flip_idx = np.random.choice(n_items)\n\n        # Apply swap\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n        # Apply flip with probability based on value-to-weight ratio\n        v1_ratio = value1_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        v2_ratio = value2_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        flip_prob = min(1.0, (v1_ratio + v2_ratio) / 10)  # Normalize\n\n        if np.random.random() < flip_prob:\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Probabilistic perturbation\n    for i in range(n_items):\n        if np.random.random() < 0.1:  # 10% chance to perturb\n            if new_solution[i] == 1:\n                # Try to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                # Try to add\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Remove random items until feasible\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -18.525469470071368,
               -17.447019179654852
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Adaptive Neighborhood Exploration,\" first partitions the archive into regions based on the objective space, then selects a solution from a less explored region to balance exploration and exploitation. It generates a neighbor solution by adaptively exploring different neighborhoods (value-based, weight-based, and dominance-based) based on the solution's position in the objective space, using a combination of greedy improvement, random perturbation, and simulated annealing with a dynamic temperature schedule. The algorithm ensures feasibility by maintaining a running total weight and only allowing moves that keep the solution within capacity, while also incorporating a post-optimization step that refines the solution by selectively flipping bits that improve the Pareto front without disrupting the existing trade-offs.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition the archive based on objective space\n    def partition_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 3 partitions\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Generate neighbor using adaptive neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Determine neighborhood type based on solution's position in objective space\n    # Normalize objectives\n    all_objs = np.array([obj for _, obj in archive])\n    if len(all_objs) > 0:\n        max_obj1, max_obj2 = np.max(all_objs, axis=0)\n        min_obj1, min_obj2 = np.min(all_objs, axis=0)\n        norm_obj1 = (obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6)\n        norm_obj2 = (obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6)\n\n        # Classify solution based on normalized objectives\n        if norm_obj1 > 0.7 and norm_obj2 > 0.7:  # High in both objectives\n            neighborhood_type = 'dominance'  # Focus on improving Pareto front\n        elif norm_obj1 > 0.6 or norm_obj2 > 0.6:  # Moderate in one objective\n            neighborhood_type = 'value_based'  # Focus on improving specific objective\n        else:\n            neighborhood_type = 'weight_based'  # Focus on capacity management\n    else:\n        neighborhood_type = 'value_based'\n\n    # Apply selected neighborhood exploration\n    if neighborhood_type == 'value_based':\n        # Value-based neighborhood: Flip items that improve the most in one objective\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        for _ in range(5):\n            # Randomly select an objective to improve\n            if random.random() < 0.5:\n                # Improve objective 1\n                candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n            else:\n                # Improve objective 2\n                candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n\n    elif neighborhood_type == 'weight_based':\n        # Weight-based neighborhood: Flip items that help manage capacity\n        for _ in range(5):\n            # Find items that can be added without exceeding capacity\n            feasible_add = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(feasible_add) > 0:\n                # Add the item with highest value-to-weight ratio\n                ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n                candidate_idx = feasible_add[np.argmax(ratios[feasible_add])]\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    else:  # dominance neighborhood\n        # Dominance-based neighborhood: Flip items that could improve Pareto front\n        for _ in range(5):\n            # Find items that could potentially improve both objectives\n            potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n            candidate_idx = np.argmax(potential_improvement)\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Step 3: Simulated annealing with dynamic temperature\n    temp = 1.0\n    cooling_rate = 0.95\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 or delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n            current_weight = candidate_weight\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n                current_weight = candidate_weight\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - refine Pareto front\n    for _ in range(3):\n        # Find items that could improve Pareto front without disrupting existing trade-offs\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Calculate potential improvement for each objective\n        potential_obj1 = current_obj1 + value1_lst * (1 - new_solution)\n        potential_obj2 = current_obj2 + value2_lst * (1 - new_solution)\n\n        # Find items that could potentially improve both objectives\n        improvement1 = potential_obj1 - current_obj1\n        improvement2 = potential_obj2 - current_obj2\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -15.671481893383769,
               -17.66264106558792
          ]
     },
     {
          "algorithm": "{The common backbone idea in the provided algorithms is selecting promising solutions from the archive and applying intelligent local search strategies to generate high-quality neighbor solutions. The new algorithm, \"Objective-Driven Dynamic Neighborhood Exploration with Adaptive Perturbation,\" first identifies solutions with high potential by analyzing their objective values and diversity in the archive. It then dynamically selects between three specialized neighborhood structures (value-biased, weight-sensitive, and Pareto-aware) based on the solution's position in the objective space and its historical improvement trajectory. The algorithm generates neighbors by combining targeted perturbations with adaptive bit-flipping probabilities that consider both immediate objective improvements and long-term search trends, while ensuring feasibility through a capacity-aware validation mechanism. It also incorporates a feedback loop that adjusts the neighborhood selection probabilities based on recent search performance, balancing exploration and exploitation in a self-tuning manner.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify solutions with high potential\n    objectives = np.array([obj for _, obj in archive])\n    normalized_objs = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    diversity_scores = np.sum(normalized_objs, axis=1) * (1 - np.abs(normalized_objs[:,0] - normalized_objs[:,1]))\n    candidate_indices = np.argsort(-diversity_scores)[:max(3, len(archive)//4)]\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic neighborhood selection\n    obj1, obj2 = archive[base_idx][1]\n    norm_obj1 = (obj1 - objectives[:,0].min()) / (objectives[:,0].max() - objectives[:,0].min() + 1e-6)\n    norm_obj2 = (obj2 - objectives[:,1].min()) / (objectives[:,1].max() - objectives[:,1].min() + 1e-6)\n\n    # Determine neighborhood type\n    if norm_obj1 > 0.7 and norm_obj2 > 0.7:\n        neighborhood_type = 'pareto_aware'\n    elif norm_obj1 > norm_obj2:\n        neighborhood_type = 'value1_biased'\n    elif norm_obj2 > norm_obj1:\n        neighborhood_type = 'value2_biased'\n    else:\n        neighborhood_type = 'weight_sensitive'\n\n    # Step 3: Generate neighbor with adaptive perturbation\n    new_solution = base_solution.copy()\n    available_capacity = capacity - current_weight\n\n    if neighborhood_type == 'value1_biased':\n        # Focus on improving value1 with controlled value2 impact\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        combined_scores = v1_ratios * (1 + 0.3 * v2_ratios)\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    elif neighborhood_type == 'value2_biased':\n        # Focus on improving value2 with controlled value1 impact\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        combined_scores = v2_ratios * (1 + 0.3 * v1_ratios)\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    elif neighborhood_type == 'weight_sensitive':\n        # Focus on efficient weight usage\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        combined_scores = (v1_ratios + v2_ratios) / (weight_lst + 1e-6)\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    else:  # pareto_aware\n        # Focus on Pareto improvement\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        combined_scores = v1_ratios * v2_ratios\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    # Step 4: Adaptive bit-flipping with probability\n    for i in range(len(new_solution)):\n        if np.random.random() < 0.2:  # 20% chance to flip\n            if new_solution[i] == 0:\n                if weight_lst[i] <= available_capacity:\n                    new_solution[i] = 1\n                    available_capacity -= weight_lst[i]\n            else:\n                new_solution[i] = 0\n                available_capacity += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -17.98888874753535,
               -17.513125684536117
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Dual-Objective Guided Bit Flip with Weighted Randomization,\" intelligently selects a solution from the archive by prioritizing those with high crowding distance or low dominance in the objective space, then applies a weighted random bit flip strategy that probabilistically flips bits based on their marginal contribution to both objectives, while ensuring feasibility by dynamically adjusting the flip probability to prevent exceeding capacity. The algorithm further refines the solution by iteratively flipping bits that improve the weighted sum of normalized objective values, using a simulated annealing-inspired acceptance criterion to escape local optima, and finally performs a post-optimization step to fine-tune the solution by flipping the least impactful bits to maximize one objective while minimally affecting the other.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution from the archive\n    # Prioritize solutions with high crowding distance or low dominance\n    def crowding_distance(solutions):\n        if len(solutions) < 3:\n            return [1.0] * len(solutions)\n        sorted_solutions = sorted(solutions, key=lambda x: x[1][0])\n        distances = [0.0] * len(solutions)\n        for m in range(2):\n            sorted_solutions.sort(key=lambda x: x[1][m])\n            distances[0] = float('inf')\n            distances[-1] = float('inf')\n            for i in range(1, len(solutions)-1):\n                distances[i] += (sorted_solutions[i+1][1][m] - sorted_solutions[i-1][1][m]) / (sorted_solutions[-1][1][m] - sorted_solutions[0][1][m])\n        return distances\n\n    distances = crowding_distance(archive)\n    selected_idx = np.argmax(distances)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n\n    # Step 2: Generate a neighbor solution using weighted random bit flip\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / np.maximum(weight_lst, 1e-6)\n    marginal2 = value2_lst / np.maximum(weight_lst, 1e-6)\n    combined_marginal = marginal1 + marginal2\n\n    # Probability of flipping each bit\n    flip_probs = combined_marginal * (1 - new_solution)\n    flip_probs[weight_lst > available_capacity] = 0  # Ensure feasibility\n\n    # Normalize probabilities\n    if np.sum(flip_probs) > 0:\n        flip_probs = flip_probs / np.sum(flip_probs)\n\n    # Perform flips\n    for i in range(len(new_solution)):\n        if random.random() < flip_probs[i] * 0.5:  # Lower probability to limit changes\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n            elif new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Step 3: Simulated annealing-inspired refinement\n    temp = 1.0\n    cooling_rate = 0.99\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 and delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - flip least impactful bits\n    for _ in range(3):\n        # Find bits with minimal impact on objectives\n        impact1 = np.abs(value1_lst) / (1 + weight_lst)\n        impact2 = np.abs(value2_lst) / (1 + weight_lst)\n        combined_impact = impact1 + impact2\n\n        # Prefer to flip bits that are currently excluded but could improve objectives\n        flip_candidates = np.where(new_solution == 0)[0]\n        if len(flip_candidates) > 0:\n            candidate_impacts = combined_impact[flip_candidates]\n            if np.sum(candidate_impacts) > 0:\n                flip_probs = candidate_impacts / np.sum(candidate_impacts)\n                idx = np.random.choice(flip_candidates, p=flip_probs)\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -17.411685236353595,
               -17.48640227514403
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Dynamic Neighborhood Exploration,\" first partitions the objective space into regions based on the distribution of solutions in the archive, then selects a solution from a sparsely populated region to focus on under-explored areas. It uses a dynamic neighborhood exploration strategy that adaptively adjusts the size of the neighborhood based on the solution's position in the objective space, flipping bits that lie on the Pareto frontier of the neighborhood's objective values. The algorithm incorporates a diversity-preserving mechanism that ensures the neighbor solution maintains a minimum distance from existing solutions in the archive, while also employing a gradient-based bit selection that identifies items with the steepest improvement in both objectives. Finally, it performs a capacity-aware gradient descent step that iteratively refines the solution by moving along the gradient of combined objective improvement, with a probabilistic acceptance criterion to balance exploration and exploitation.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition objective space and select a solution from a sparse region\n    objectives = [obj for _, obj in archive]\n    obj1_values = [obj[0] for obj in objectives]\n    obj2_values = [obj[1] for obj in objectives]\n\n    # Normalize objectives for partitioning\n    min_obj1, max_obj1 = min(obj1_values), max(obj1_values)\n    min_obj2, max_obj2 = min(obj2_values), max(obj2_values)\n    norm_obj1 = [(obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6) for obj1 in obj1_values]\n    norm_obj2 = [(obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6) for obj2 in obj2_values]\n\n    # Partition into 4 regions and count solutions in each\n    region_counts = [0] * 4\n    for n1, n2 in zip(norm_obj1, norm_obj2):\n        if n1 < 0.5 and n2 < 0.5:\n            region_counts[0] += 1\n        elif n1 < 0.5 and n2 >= 0.5:\n            region_counts[1] += 1\n        elif n1 >= 0.5 and n2 < 0.5:\n            region_counts[2] += 1\n        else:\n            region_counts[3] += 1\n\n    # Select a solution from the least populated region\n    selected_region = np.argmin(region_counts)\n    candidates = []\n    for i, (n1, n2) in enumerate(zip(norm_obj1, norm_obj2)):\n        if (selected_region == 0 and n1 < 0.5 and n2 < 0.5) or \\\n           (selected_region == 1 and n1 < 0.5 and n2 >= 0.5) or \\\n           (selected_region == 2 and n1 >= 0.5 and n2 < 0.5) or \\\n           (selected_region == 3 and n1 >= 0.5 and n2 >= 0.5):\n            candidates.append(i)\n\n    if not candidates:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(candidates)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n\n    # Step 2: Dynamic neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    neighborhood_size = max(1, int(len(new_solution) * 0.1))  # Dynamic neighborhood size\n\n    # Calculate gradient of objectives\n    grad_obj1 = value1_lst / (weight_lst + 1e-6)\n    grad_obj2 = value2_lst / (weight_lst + 1e-6)\n    combined_grad = grad_obj1 + grad_obj2\n\n    # Find items on the Pareto frontier of the neighborhood\n    for _ in range(neighborhood_size):\n        # Select items with highest combined gradient\n        candidate_idx = np.argsort(combined_grad)[-neighborhood_size:]\n\n        for idx in candidate_idx:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 3: Diversity-preserving mechanism\n    min_distance = float('inf')\n    for sol, _ in archive:\n        distance = np.sum(np.abs(sol - new_solution))\n        if distance < min_distance:\n            min_distance = distance\n\n    if min_distance < len(new_solution) * 0.2:  # Too similar to existing solutions\n        # Flip some bits to increase diversity\n        flip_indices = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 4: Capacity-aware gradient descent\n    for _ in range(5):\n        # Calculate current objectives\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Find items with highest gradient improvement\n        delta_obj1 = value1_lst / (weight_lst + 1e-6)\n        delta_obj2 = value2_lst / (weight_lst + 1e-6)\n        combined_delta = delta_obj1 + delta_obj2\n\n        # Select top items to flip\n        top_indices = np.argsort(combined_delta)[-min(5, len(combined_delta)):]\n\n        for idx in top_indices:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                new_obj1 = np.sum(value1_lst * new_solution)\n                new_obj2 = np.sum(value2_lst * new_solution)\n\n                # Accept if improvement in both objectives\n                if new_obj1 > current_obj1 and new_obj2 > current_obj2:\n                    break\n                else:\n                    # Probabilistic acceptance based on improvement\n                    improvement = (new_obj1 - current_obj1) + (new_obj2 - current_obj2)\n                    if improvement > 0 or np.random.rand() < np.exp(improvement / 0.1):\n                        break\n                    else:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -18.368028693557207,
               -17.291768323014352
          ]
     },
     {
          "algorithm": "{The proposed algorithm, named \"Objective-Driven Iterative Swap with Adaptive Neighborhood,\" selects a promising solution from the archive by prioritizing those with high objective values and low dominance counts, then applies a hybrid local search strategy that combines random swaps with targeted improvements. It iteratively evaluates potential swaps (adding or removing items) while dynamically adjusting the neighborhood size based on the current solution's quality and diversity, ensuring feasibility by rejecting swaps that exceed capacity. The algorithm balances exploration and exploitation by alternating between random and objective-specific swaps, and uses a probabilistic acceptance criterion to escape local optima. The process terminates when no further improvements are found or a maximum iteration limit is reached, returning the best neighbor solution encountered.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a base solution with high potential for improvement\n    def selection_criterion(solution_obj):\n        # Prefer solutions with high objective values and low dominance\n        return sum(solution_obj) / len(archive)  # Simplified criterion\n\n    sorted_archive = sorted(archive, key=lambda x: -selection_criterion(x[1]))\n    base_solution = sorted_archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hybrid local search strategy\n    new_solution = base_solution.copy()\n    max_iterations = 100\n    for _ in range(max_iterations):\n        # Randomly select a candidate item to flip\n        candidate_idx = random.randint(0, len(weight_lst) - 1)\n\n        # Determine the effect of flipping this item\n        if new_solution[candidate_idx] == 1:\n            # Try removing the item\n            new_weight = current_weight - weight_lst[candidate_idx]\n            if new_weight <= capacity:\n                new_solution[candidate_idx] = 0\n                current_weight = new_weight\n        else:\n            # Try adding the item\n            new_weight = current_weight + weight_lst[candidate_idx]\n            if new_weight <= capacity:\n                new_solution[candidate_idx] = 1\n                current_weight = new_weight\n\n        # Occasionally perform a more targeted swap\n        if random.random() < 0.2:\n            # Find the best possible swap (greedy improvement)\n            best_improvement = 0\n            best_swap = None\n            for i in range(len(weight_lst)):\n                if new_solution[i] == 1:\n                    # Try removing item i\n                    delta_weight = -weight_lst[i]\n                    delta_value1 = -value1_lst[i]\n                    delta_value2 = -value2_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        improvement = delta_value1 + delta_value2\n                        if improvement > best_improvement:\n                            best_improvement = improvement\n                            best_swap = (-1, i)\n                else:\n                    # Try adding item i\n                    delta_weight = weight_lst[i]\n                    delta_value1 = value1_lst[i]\n                    delta_value2 = value2_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        improvement = delta_value1 + delta_value2\n                        if improvement > best_improvement:\n                            best_improvement = improvement\n                            best_swap = (1, i)\n\n            if best_swap is not None:\n                action, idx = best_swap\n                new_solution[idx] = action\n                current_weight += weight_lst[idx] * (2 * action - 1)\n\n    return new_solution\n\n",
          "score": [
               -17.946552635273125,
               -17.362931504191003
          ]
     }
]