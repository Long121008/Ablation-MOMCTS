[
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential for improvement (highest variance in objectives)\n    solutions = [s[0] for s in archive]\n    objectives = np.array([s[1] for s in archive])\n    variances = np.var(objectives, axis=0)\n    selected_idx = np.argmax(variances)\n    base_solution = solutions[selected_idx].copy()\n\n    # Generate a neighbor by randomly swapping items\n    new_solution = base_solution.copy()\n    n_items = len(new_solution)\n    swap_count = min(3, n_items)  # Limit the number of swaps to avoid excessive perturbation\n\n    # Randomly select items to swap\n    swap_indices = np.random.choice(n_items, size=swap_count, replace=False)\n    for idx in swap_indices:\n        new_solution[idx] = 1 - new_solution[idx]  # Flip the bit\n\n    # Check feasibility and repair if necessary\n    total_weight = np.sum(new_solution * weight_lst)\n    if total_weight > capacity:\n        # Greedy repair: remove items until feasible\n        excess_weight = total_weight - capacity\n        while excess_weight > 0:\n            # Find the item with the smallest ratio of weight to value (sum of both objectives)\n            item_weights = weight_lst[new_solution == 1]\n            item_values = value1_lst[new_solution == 1] + value2_lst[new_solution == 1]\n            if len(item_weights) == 0:\n                break  # No items left to remove\n            ratios = item_values / item_weights\n            worst_item_idx = np.argmin(ratios)\n            actual_idx = np.where(new_solution == 1)[0][worst_item_idx]\n            new_solution[actual_idx] = 0\n            excess_weight -= weight_lst[actual_idx]\n\n    # Greedy improvement: add items that improve both objectives without exceeding capacity\n    remaining_weight = capacity - np.sum(new_solution * weight_lst)\n    if remaining_weight > 0:\n        # Find items not in the solution that can be added without exceeding capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        candidate_weights = weight_lst[candidate_items]\n        candidate_values = value1_lst[candidate_items] + value2_lst[candidate_items]\n        feasible_items = candidate_weights <= remaining_weight\n\n        if np.any(feasible_items):\n            best_item_idx = np.argmax(candidate_values[feasible_items])\n            actual_idx = candidate_items[feasible_items][best_item_idx]\n            new_solution[actual_idx] = 1\n\n    return new_solution\n\n",
          "score": [
               -18.909604645239646,
               -18.814088175738103
          ]
     },
     {
          "algorithm": "{The heuristic function 'select_neighbor' first identifies the most promising solution in the archive by evaluating the potential for improvement using a hybrid metric that combines the solution's current objective values with their spread across the Pareto front. It then applies a novel local search operator that intelligently combines three strategies: (1) a probabilistic swap of items based on their marginal contribution to both objectives, (2) a guided perturbation that selectively includes/excludes items near the Pareto front's knee point, and (3) a capacity-aware random walk that ensures feasibility by only considering moves that maintain or slightly reduce the total weight. The function dynamically balances exploration and exploitation by adjusting the selection probability of these strategies based on the archive's diversity, ensuring high-quality neighbors that efficiently navigate the trade-off space while always respecting the capacity constraint.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution (highest combined objective value)\n    objectives = np.array([obj for (sol, obj) in archive])\n    combined_values = objectives[:, 0] + objectives[:, 1]\n    best_idx = np.argmax(combined_values)\n    base_solution = archive[best_idx][0].copy()\n\n    # Step 2: Generate a neighbor using a hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Probabilistic swap based on marginal contribution\n    for _ in range(min(3, n_items // 2)):\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        i = np.random.choice(candidates)\n        if new_solution[i] == 1 and np.random.rand() < 0.7:  # Higher probability to remove\n            new_solution[i] = 0\n            # Try to add a new item that improves both objectives\n            remaining_items = np.where(new_solution == 0)[0]\n            if len(remaining_items) > 0:\n                potential_adds = []\n                for j in remaining_items:\n                    if (weight_lst[j] <= capacity - np.sum(weight_lst[new_solution == 1])):\n                        potential_adds.append(j)\n                if potential_adds:\n                    j = np.random.choice(potential_adds)\n                    new_solution[j] = 1\n\n    # Strategy 2: Guided perturbation near knee point (approximate)\n    if len(archive) > 1:\n        # Approximate knee point as solution with max (v1 - v2)\n        knee_idx = np.argmax(objectives[:, 0] - objectives[:, 1])\n        knee_solution = archive[knee_idx][0]\n        # Perturb around knee items\n        for i in range(n_items):\n            if knee_solution[i] != new_solution[i] and np.random.rand() < 0.3:\n                if knee_solution[i] == 1 and (weight_lst[i] <= capacity - np.sum(weight_lst[new_solution == 1])):\n                    new_solution[i] = 1\n                elif new_solution[i] == 1:\n                    new_solution[i] = 0\n\n    # Ensure feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Strategy 3: Capacity-aware random walk\n        excess = current_weight - capacity\n        excess_items = np.where(new_solution == 1)[0]\n        np.random.shuffle(excess_items)\n        for i in excess_items:\n            if weight_lst[i] <= excess:\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n                if excess <= 0:\n                    break\n\n    return new_solution\n\n",
          "score": [
               -19.18646270182542,
               -18.508972406433266
          ]
     },
     {
          "algorithm": "{The new algorithm combines the selection of promising solutions based on both objective values and their trade-off potential with a novel local search strategy that dynamically adapts to the Pareto front's characteristics. It first identifies solutions with high combined objective values and strong trade-off potential by analyzing the archive's diversity and distribution, then applies a hybrid local search that intelligently combines a probabilistic item selection based on marginal contributions, a Pareto-front guided perturbation that explores regions near the knee point, and an adaptive capacity-aware random walk that ensures feasibility by considering both the current solution's weight and the remaining capacity, with the probability of each strategy being dynamically adjusted based on the solution's position relative to the Pareto front and the archive's state.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution based on trade-off potential\n    objectives = np.array([obj for (sol, obj) in archive])\n    combined_values = objectives[:, 0] + objectives[:, 1]\n    trade_off = objectives[:, 0] / (objectives[:, 1] + 1e-6)\n    diversity_score = combined_values * trade_off\n    best_idx = np.argmax(diversity_score)\n    base_solution = archive[best_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive hybrid local search\n    # Strategy 1: Marginal contribution-based selection\n    for i in range(n_items):\n        if np.random.rand() < 0.5:  # Probabilistic selection\n            delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n            if current_weight + delta_weight <= capacity:\n                delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n                delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n                norm_improvement = (delta_value1 + delta_value2) / (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1]) + 1e-6)\n                if np.random.rand() < min(1.0, 0.3 + 0.7 * norm_improvement):\n                    new_solution[i] = 1 - new_solution[i]\n                    current_weight += delta_weight\n\n    # Strategy 2: Pareto-front guided perturbation\n    if len(archive) > 2:\n        sorted_idx = np.argsort(objectives[:, 0] + objectives[:, 1])\n        middle_solution = archive[sorted_idx[len(archive)//2]][0]\n        for i in range(n_items):\n            if middle_solution[i] != new_solution[i] and np.random.rand() < 0.2:\n                if middle_solution[i] == 1 and (current_weight + weight_lst[i] <= capacity):\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n                elif new_solution[i] == 1:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Step 3: Adaptive capacity-aware random walk\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        candidates = np.where(new_solution == 0)[0]\n        np.random.shuffle(candidates)\n        for i in candidates:\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < 0.4:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n                if remaining_capacity <= 0:\n                    break\n\n    return new_solution\n\n",
          "score": [
               -20.177957868136076,
               -16.058563476430592
          ]
     },
     {
          "algorithm": "{The new algorithm combines the selection of promising solutions based on their potential for improvement with a novel local search strategy that dynamically adapts to the Pareto front's characteristics. It first identifies solutions with high combined objective values and strong trade-off potential by analyzing the archive's diversity and distribution, then applies a hybrid local search that intelligently combines a probabilistic item selection based on marginal contributions, a Pareto-front guided perturbation that explores regions near the knee point, and an adaptive capacity-aware random walk that ensures feasibility by considering both the current solution's weight and the remaining capacity, with the probability of each strategy being dynamically adjusted based on the solution's position relative to the Pareto front and the archive's state. Additionally, it incorporates a novel \"objective-balanced\" perturbation that selectively flips items based on their contribution to both objectives, a \"frontier-aware\" swap that prioritizes items near the Pareto frontier, and a \"capacity-sensitive\" mutation that ensures feasibility by only considering moves that maintain or slightly reduce the total weight, while dynamically balancing exploration and exploitation by adjusting the selection probability of these strategies based on the archive's diversity and the current solution's position relative to the Pareto front.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution based on combined objective values and trade-off potential\n    objectives = np.array([obj for (sol, obj) in archive])\n    combined_values = objectives[:, 0] + objectives[:, 1]\n    trade_off = objectives[:, 0] / (objectives[:, 1] + 1e-6)\n    diversity_score = combined_values * trade_off\n    best_idx = np.argmax(diversity_score)\n    base_solution = archive[best_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Objective-balanced perturbation\n    for i in range(n_items):\n        if np.random.rand() < 0.5:\n            delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n            if current_weight + delta_weight <= capacity:\n                delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n                delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n                norm_improvement = (delta_value1 + delta_value2) / (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1]) + 1e-6)\n                if np.random.rand() < min(1.0, 0.4 + 0.6 * norm_improvement):\n                    new_solution[i] = 1 - new_solution[i]\n                    current_weight += delta_weight\n\n    # Step 3: Frontier-aware swap\n    if len(archive) > 2:\n        sorted_idx = np.argsort(objectives[:, 0] + objectives[:, 1])\n        middle_solution = archive[sorted_idx[len(archive)//2]][0]\n        for i in range(n_items):\n            if middle_solution[i] != new_solution[i] and np.random.rand() < 0.3:\n                if middle_solution[i] == 1 and (current_weight + weight_lst[i] <= capacity):\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n                elif new_solution[i] == 1:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Step 4: Capacity-sensitive mutation\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        candidates = np.where(new_solution == 0)[0]\n        np.random.shuffle(candidates)\n        for i in candidates:\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < 0.5:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n                if remaining_capacity <= 0:\n                    break\n\n    return new_solution\n\n",
          "score": [
               -19.947542615525425,
               -16.70845019108186
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution from the archive\n    # Prioritize solutions with high marginal gains or close to the Pareto front\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_obj = archive[selected_idx][1]\n\n    # Step 2: Apply the guided flip and swap operator\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(weight_lst), size=min(3, len(weight_lst)), replace=False)\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove the item\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # Perform targeted swap between two items\n    swap_candidates = np.where(new_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, size=2, replace=False)\n        # Check if swapping improves the solution\n        delta_weight = weight_lst[j] - weight_lst[i]\n        delta_value1 = value1_lst[j] - value1_lst[i]\n        delta_value2 = value2_lst[j] - value2_lst[i]\n\n        if (delta_weight <= 0 or current_weight + delta_weight <= capacity) and \\\n           (delta_value1 >= 0 or delta_value2 >= 0):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n            current_weight += delta_weight\n            current_value1 += delta_value1\n            current_value2 += delta_value2\n\n    return new_solution\n\n",
          "score": [
               -18.68004542521698,
               -18.028382666602297
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the most promising solution based on marginal improvement potential\n    max_potential = -1\n    selected_solution = None\n    selected_obj = None\n\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        potential = (obj[0] / (current_weight + 1e-6) + obj[1] / (current_weight + 1e-6)) / 2\n        if potential > max_potential:\n            max_potential = potential\n            selected_solution = sol\n            selected_obj = obj\n\n    if selected_solution is None:\n        selected_solution = archive[0][0]\n        selected_obj = archive[0][1]\n\n    new_solution = selected_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Guided flip operator\n    for i in range(len(new_solution)):\n        # Calculate potential improvement if we flip this item\n        delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n        new_weight = current_weight + delta_weight\n\n        if new_weight <= capacity:\n            # Calculate marginal improvement for both objectives\n            delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n            delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n\n            # Calculate normalized improvement\n            norm_improvement = (delta_value1 / (selected_obj[0] + 1e-6) + delta_value2 / (selected_obj[1] + 1e-6)) / 2\n\n            # Flip with probability based on improvement and a small random factor\n            flip_prob = min(1.0, 0.1 + 0.9 * norm_improvement + 0.1 * np.random.random())\n\n            if np.random.random() < flip_prob:\n                new_solution[i] = 1 - new_solution[i]\n                current_weight = new_weight\n\n    return new_solution\n\n",
          "score": [
               -18.46494248406106,
               -18.273839065231257
          ]
     },
     {
          "algorithm": "{The new algorithm will combine the selection of high-potential solutions from the first approach with a novel \"objective-aware\" local search that simultaneously considers both objectives in a multi-dimensional improvement space. It will use a probabilistic item selection mechanism that prioritizes items with high marginal contributions to both objectives, while also incorporating a diversity-preserving component that avoids getting stuck in local optima by occasionally considering items that might temporarily reduce one objective to improve the other. The algorithm will maintain feasibility by using a weight-balanced repair mechanism that selectively removes items based on their combined contribution to both objectives, and will include a post-optimization step that greedily adds items that provide the best combined improvement without exceeding capacity.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective variance and marginal potential\n    solutions = [s[0] for s in archive]\n    objectives = np.array([s[1] for s in archive])\n    variances = np.var(objectives, axis=0)\n    combined_variance = np.sum(variances)\n\n    # Calculate marginal potential scores\n    marginal_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        potential = (obj[0] / (current_weight + 1e-6) + obj[1] / (current_weight + 1e-6)) / 2\n        marginal_scores.append(potential)\n\n    # Combine variance and marginal potential\n    combined_scores = [combined_variance * marginal_scores[i] for i in range(len(archive))]\n    selected_idx = np.argmax(combined_scores)\n    base_solution = solutions[selected_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Objective-aware local search with probabilistic selection\n    for i in range(len(new_solution)):\n        delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n        new_weight = current_weight + delta_weight\n\n        if new_weight <= capacity:\n            delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n            delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n\n            # Calculate normalized improvement potential\n            norm_improvement1 = delta_value1 / (current_value1 + 1e-6)\n            norm_improvement2 = delta_value2 / (current_value2 + 1e-6)\n            combined_improvement = (norm_improvement1 + norm_improvement2) / 2\n\n            # Calculate diversity factor (encourage flipping items that are currently excluded)\n            diversity_factor = 1.0 if new_solution[i] == 0 else 0.3\n\n            # Calculate flip probability\n            flip_prob = min(1.0, 0.2 + 0.8 * combined_improvement * diversity_factor + 0.1 * np.random.random())\n\n            if np.random.random() < flip_prob:\n                new_solution[i] = 1 - new_solution[i]\n                current_weight = new_weight\n                current_value1 += delta_value1\n                current_value2 += delta_value2\n\n    # Weight-balanced repair if necessary\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess_weight = total_weight - capacity\n        while excess_weight > 0:\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) == 0:\n                break\n\n            # Calculate combined value-weight ratios\n            item_weights = weight_lst[included_items]\n            item_values = value1_lst[included_items] + value2_lst[included_items]\n            ratios = item_values / (item_weights + 1e-6)\n\n            # Remove item with lowest ratio\n            worst_item_idx = np.argmin(ratios)\n            actual_idx = included_items[worst_item_idx]\n            new_solution[actual_idx] = 0\n            excess_weight -= weight_lst[actual_idx]\n\n    # Post-optimization: greedily add items with best combined improvement\n    remaining_weight = capacity - np.sum(weight_lst * new_solution)\n    if remaining_weight > 0:\n        excluded_items = np.where(new_solution == 0)[0]\n        candidate_weights = weight_lst[excluded_items]\n        candidate_values = value1_lst[excluded_items] + value2_lst[excluded_items]\n\n        # Find feasible items to add\n        feasible_items = candidate_weights <= remaining_weight\n        if np.any(feasible_items):\n            best_item_idx = np.argmax(candidate_values[feasible_items])\n            actual_idx = excluded_items[feasible_items][best_item_idx]\n            new_solution[actual_idx] = 1\n\n    return new_solution\n\n",
          "score": [
               -18.80061070904028,
               -17.941981571958156
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective variance and marginal potential\n    solutions = [s[0] for s in archive]\n    objectives = np.array([s[1] for s in archive])\n    variances = np.var(objectives, axis=0)\n    combined_variance = np.sum(variances)\n\n    marginal_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        potential = (obj[0] / (current_weight + 1e-6) + obj[1] / (current_weight + 1e-6)) / 2\n        marginal_scores.append(potential)\n\n    combined_scores = [combined_variance * marginal_scores[i] for i in range(len(archive))]\n    selected_idx = np.argmax(combined_scores)\n    base_solution = solutions[selected_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Objective-aware weighted random walk\n    for i in range(len(new_solution)):\n        delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n        new_weight = current_weight + delta_weight\n\n        if new_weight <= capacity:\n            delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n            delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n\n            norm_improvement1 = delta_value1 / (current_value1 + 1e-6)\n            norm_improvement2 = delta_value2 / (current_value2 + 1e-6)\n            combined_improvement = (norm_improvement1 + norm_improvement2) / 2\n\n            diversity_factor = 1.0 if new_solution[i] == 0 else 0.3\n            flip_prob = min(1.0, 0.2 + 0.8 * combined_improvement * diversity_factor + 0.1 * np.random.random())\n\n            if np.random.random() < flip_prob:\n                new_solution[i] = 1 - new_solution[i]\n                current_weight = new_weight\n                current_value1 += delta_value1\n                current_value2 += delta_value2\n\n    # Weight-balanced repair\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess_weight = total_weight - capacity\n        while excess_weight > 0:\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) == 0:\n                break\n\n            item_weights = weight_lst[included_items]\n            item_values = value1_lst[included_items] + value2_lst[included_items]\n            ratios = item_values / (item_weights + 1e-6)\n            worst_item_idx = np.argmin(ratios)\n            actual_idx = included_items[worst_item_idx]\n            new_solution[actual_idx] = 0\n            excess_weight -= weight_lst[actual_idx]\n\n    # Post-optimization: greedy addition\n    remaining_weight = capacity - np.sum(weight_lst * new_solution)\n    if remaining_weight > 0:\n        excluded_items = np.where(new_solution == 0)[0]\n        candidate_weights = weight_lst[excluded_items]\n        candidate_values = value1_lst[excluded_items] + value2_lst[excluded_items]\n\n        feasible_items = candidate_weights <= remaining_weight\n        if np.any(feasible_items):\n            best_item_idx = np.argmax(candidate_values[feasible_items])\n            actual_idx = excluded_items[feasible_items][best_item_idx]\n            new_solution[actual_idx] = 1\n\n    return new_solution\n\n",
          "score": [
               -18.489425745853058,
               -18.09484448090687
          ]
     },
     {
          "algorithm": "{This novel algorithm employs a multi-phase optimization strategy that begins with a \"diversity-aware\" selection of solutions based on their position in the objective space, followed by a \"greedy\" phase that systematically adds items with the highest marginal utility in both objectives while maintaining feasibility, then transitions to a \"tabu search\" phase that explores the solution space by systematically flipping items while avoiding recently visited solutions, and finally applies a \"simulated annealing\" phase with a dynamic temperature schedule that balances exploration and exploitation while ensuring feasibility through a novel \"weight-balanced\" acceptance criterion that considers both objective improvements and weight utilization. The algorithm dynamically adjusts its search focus between phases based on the current solution's position relative to the Pareto front and incorporates a \"hybrid\" crossover operator that combines features from both selected solutions while preserving feasibility through a \"greedy repair\" mechanism.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Step 1: Diversity-aware selection\n    objectives = np.array([obj for (sol, obj) in archive])\n    centroid = np.mean(objectives, axis=0)\n    distances = np.linalg.norm(objectives - centroid, axis=1)\n    selected_idx = np.argmax(distances)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Step 2: Greedy phase - add items with highest marginal utility\n    remaining_items = np.where(new_solution == 0)[0]\n    for idx in remaining_items:\n        if current_weight + weight_lst[idx] <= capacity:\n            marginal1 = value1_lst[idx] / (weight_lst[idx] + 1e-6)\n            marginal2 = value2_lst[idx] / (weight_lst[idx] + 1e-6)\n            combined_marginal = marginal1 + marginal2\n\n            if combined_marginal > 0:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # Step 3: Tabu search phase\n    tabu_list = []\n    tabu_tenure = min(5, len(weight_lst) // 2)\n    best_solution = new_solution.copy()\n    best_value1 = current_value1\n    best_value2 = current_value2\n    best_weight = current_weight\n\n    for _ in range(10):\n        # Select item to flip (not in tabu list)\n        candidates = [i for i in range(len(weight_lst)) if i not in tabu_list]\n        if not candidates:\n            break\n\n        idx = np.random.choice(candidates)\n        candidate_solution = new_solution.copy()\n        candidate_solution[idx] = 1 - candidate_solution[idx]\n\n        # Check feasibility\n        candidate_weight = np.sum(weight_lst * candidate_solution)\n        if candidate_weight > capacity:\n            continue\n\n        # Evaluate candidate\n        candidate_value1 = np.sum(value1_lst * candidate_solution)\n        candidate_value2 = np.sum(value2_lst * candidate_solution)\n\n        # Accept if better in both objectives\n        if (candidate_value1 > best_value1 and candidate_value2 > best_value2) or \\\n           (np.random.rand() < 0.2):  # Small random acceptance\n            best_solution = candidate_solution\n            best_value1 = candidate_value1\n            best_value2 = candidate_value2\n            best_weight = candidate_weight\n            tabu_list.append(idx)\n            if len(tabu_list) > tabu_tenure:\n                tabu_list.pop(0)\n\n    new_solution = best_solution\n    current_weight = best_weight\n    current_value1 = best_value1\n    current_value2 = best_value2\n\n    # Step 4: Simulated annealing with dynamic temperature\n    temperature = 5.0\n    cooling_rate = 0.95\n    for _ in range(15):\n        # Create candidate by flipping multiple bits\n        candidate_solution = new_solution.copy()\n        flip_indices = np.random.choice(len(weight_lst), size=min(2, len(weight_lst)), replace=False)\n        for idx in flip_indices:\n            candidate_solution[idx] = 1 - candidate_solution[idx]\n\n        # Check feasibility\n        candidate_weight = np.sum(weight_lst * candidate_solution)\n        if candidate_weight > capacity:\n            continue\n\n        # Evaluate candidate\n        candidate_value1 = np.sum(value1_lst * candidate_solution)\n        candidate_value2 = np.sum(value2_lst * candidate_solution)\n\n        # Acceptance criterion (weight-balanced)\n        delta1 = candidate_value1 - current_value1\n        delta2 = candidate_value2 - current_value2\n        weight_ratio = (candidate_weight / capacity) - (current_weight / capacity)\n        combined_delta = delta1 + delta2 - 0.5 * abs(weight_ratio)\n\n        if combined_delta > 0 or \\\n           (np.random.rand() < np.exp(combined_delta / temperature)):\n            new_solution = candidate_solution\n            current_weight = candidate_weight\n            current_value1 = candidate_value1\n            current_value2 = candidate_value2\n\n        temperature *= cooling_rate\n\n    # Step 5: Hybrid crossover operator\n    if len(archive) > 1 and np.random.rand() < 0.3:\n        # Select another solution for crossover\n        crossover_idx = np.random.choice([i for i in range(len(archive)) if i != selected_idx])\n        crossover_sol = archive[crossover_idx][0]\n\n        # Create candidate by combining features\n        mask = np.random.rand(len(new_solution)) < 0.5\n        candidate_solution = np.where(mask, new_solution, crossover_sol)\n\n        # Greedy repair to ensure feasibility\n        candidate_weight = np.sum(weight_lst * candidate_solution)\n        if candidate_weight > capacity:\n            excess_items = np.where(candidate_solution == 1)[0]\n            np.random.shuffle(excess_items)\n            for idx in excess_items:\n                if candidate_weight <= capacity:\n                    break\n                candidate_solution[idx] = 0\n                candidate_weight -= weight_lst[idx]\n\n        # Evaluate candidate\n        candidate_value1 = np.sum(value1_lst * candidate_solution)\n        candidate_value2 = np.sum(value2_lst * candidate_solution)\n\n        # Accept if better in both objectives\n        if candidate_value1 > current_value1 and candidate_value2 > current_value2:\n            new_solution = candidate_solution\n\n    return new_solution\n\n",
          "score": [
               -17.98099995347684,
               -18.02200719955706
          ]
     },
     {
          "algorithm": "{The novel algorithm, called \"Diversity-Enhanced Multi-Objective Local Search with Adaptive Trade-off Navigation,\" begins by selecting the solution in the archive with the highest combined objective value, then constructs a neighbor by first performing a greedy addition of items that lie on the Pareto frontier of the remaining items while prioritizing those with the best marginal trade-off between objectives, followed by a diversity-aware perturbation step that probabilistically flips items based on their distance to the centroid of all solutions in the archive, ensuring both exploitation of promising regions and exploration of underrepresented areas. The algorithm dynamically adjusts the perturbation intensity based on the solution's dominance rank and the archive's diversity, while always maintaining feasibility through adaptive capacity checks, to generate high-quality neighbors that efficiently navigate the trade-off space while respecting the capacity constraint.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the highest combined objective value\n    objectives = np.array([obj for _, obj in archive])\n    combined_obj = objectives[:, 0] + objectives[:, 1]\n    selected_idx = np.argmax(combined_obj)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Calculate centroid of all solutions in the archive\n    centroid = np.mean(objectives, axis=0)\n\n    # Greedy addition of items on Pareto frontier with best marginal trade-off\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        excluded_weights = weight_lst[excluded_items]\n        excluded_values1 = value1_lst[excluded_items]\n        excluded_values2 = value2_lst[excluded_items]\n\n        # Calculate marginal trade-off (value2/value1) for excluded items\n        marginal_tradeoff = excluded_values2 / (excluded_values1 + 1e-10)  # Avoid division by zero\n\n        # Sort items by marginal trade-off (ascending)\n        sorted_indices = np.argsort(marginal_tradeoff)\n        sorted_items = excluded_items[sorted_indices]\n\n        for item in sorted_items:\n            if remaining_capacity >= weight_lst[item]:\n                # Check if adding this item would improve both objectives\n                if (np.sum(value1_lst * new_solution) + value1_lst[item] > np.sum(value1_lst * new_solution)) and \\\n                   (np.sum(value2_lst * new_solution) + value2_lst[item] > np.sum(value2_lst * new_solution)):\n                    new_solution[item] = 1\n                    remaining_capacity -= weight_lst[item]\n\n    # Diversity-aware perturbation\n    for item in range(len(new_solution)):\n        # Calculate distance to centroid for current solution\n        current_obj = (np.sum(value1_lst * new_solution), np.sum(value2_lst * new_solution))\n        distance_to_centroid = np.linalg.norm(np.array(current_obj) - centroid)\n\n        # Higher perturbation probability for solutions farther from centroid\n        perturbation_prob = 0.1 + 0.4 * (distance_to_centroid / (np.linalg.norm(centroid) + 1e-10))\n\n        if np.random.rand() < perturbation_prob:\n            if new_solution[item] == 1:\n                # Check if removing this item would keep the solution feasible\n                if current_weight - weight_lst[item] <= capacity:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n            else:\n                # Check if adding this item would keep the solution feasible\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.336584010101532,
               -17.84717698571945
          ]
     }
]