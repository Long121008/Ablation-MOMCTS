[
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential for improvement (highest variance in objectives)\n    solutions = [s[0] for s in archive]\n    objectives = np.array([s[1] for s in archive])\n    variances = np.var(objectives, axis=0)\n    selected_idx = np.argmax(variances)\n    base_solution = solutions[selected_idx].copy()\n\n    # Generate a neighbor by randomly swapping items\n    new_solution = base_solution.copy()\n    n_items = len(new_solution)\n    swap_count = min(3, n_items)  # Limit the number of swaps to avoid excessive perturbation\n\n    # Randomly select items to swap\n    swap_indices = np.random.choice(n_items, size=swap_count, replace=False)\n    for idx in swap_indices:\n        new_solution[idx] = 1 - new_solution[idx]  # Flip the bit\n\n    # Check feasibility and repair if necessary\n    total_weight = np.sum(new_solution * weight_lst)\n    if total_weight > capacity:\n        # Greedy repair: remove items until feasible\n        excess_weight = total_weight - capacity\n        while excess_weight > 0:\n            # Find the item with the smallest ratio of weight to value (sum of both objectives)\n            item_weights = weight_lst[new_solution == 1]\n            item_values = value1_lst[new_solution == 1] + value2_lst[new_solution == 1]\n            if len(item_weights) == 0:\n                break  # No items left to remove\n            ratios = item_values / item_weights\n            worst_item_idx = np.argmin(ratios)\n            actual_idx = np.where(new_solution == 1)[0][worst_item_idx]\n            new_solution[actual_idx] = 0\n            excess_weight -= weight_lst[actual_idx]\n\n    # Greedy improvement: add items that improve both objectives without exceeding capacity\n    remaining_weight = capacity - np.sum(new_solution * weight_lst)\n    if remaining_weight > 0:\n        # Find items not in the solution that can be added without exceeding capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        candidate_weights = weight_lst[candidate_items]\n        candidate_values = value1_lst[candidate_items] + value2_lst[candidate_items]\n        feasible_items = candidate_weights <= remaining_weight\n\n        if np.any(feasible_items):\n            best_item_idx = np.argmax(candidate_values[feasible_items])\n            actual_idx = candidate_items[feasible_items][best_item_idx]\n            new_solution[actual_idx] = 1\n\n    return new_solution\n\n",
          "score": [
               -18.909604645239646,
               -18.814088175738103
          ]
     },
     {
          "algorithm": "{The heuristic function 'select_neighbor' first identifies the most promising solution in the archive by evaluating the potential for improvement using a hybrid metric that combines the solution's current objective values with their spread across the Pareto front. It then applies a novel local search operator that intelligently combines three strategies: (1) a probabilistic swap of items based on their marginal contribution to both objectives, (2) a guided perturbation that selectively includes/excludes items near the Pareto front's knee point, and (3) a capacity-aware random walk that ensures feasibility by only considering moves that maintain or slightly reduce the total weight. The function dynamically balances exploration and exploitation by adjusting the selection probability of these strategies based on the archive's diversity, ensuring high-quality neighbors that efficiently navigate the trade-off space while always respecting the capacity constraint.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution (highest combined objective value)\n    objectives = np.array([obj for (sol, obj) in archive])\n    combined_values = objectives[:, 0] + objectives[:, 1]\n    best_idx = np.argmax(combined_values)\n    base_solution = archive[best_idx][0].copy()\n\n    # Step 2: Generate a neighbor using a hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Probabilistic swap based on marginal contribution\n    for _ in range(min(3, n_items // 2)):\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        i = np.random.choice(candidates)\n        if new_solution[i] == 1 and np.random.rand() < 0.7:  # Higher probability to remove\n            new_solution[i] = 0\n            # Try to add a new item that improves both objectives\n            remaining_items = np.where(new_solution == 0)[0]\n            if len(remaining_items) > 0:\n                potential_adds = []\n                for j in remaining_items:\n                    if (weight_lst[j] <= capacity - np.sum(weight_lst[new_solution == 1])):\n                        potential_adds.append(j)\n                if potential_adds:\n                    j = np.random.choice(potential_adds)\n                    new_solution[j] = 1\n\n    # Strategy 2: Guided perturbation near knee point (approximate)\n    if len(archive) > 1:\n        # Approximate knee point as solution with max (v1 - v2)\n        knee_idx = np.argmax(objectives[:, 0] - objectives[:, 1])\n        knee_solution = archive[knee_idx][0]\n        # Perturb around knee items\n        for i in range(n_items):\n            if knee_solution[i] != new_solution[i] and np.random.rand() < 0.3:\n                if knee_solution[i] == 1 and (weight_lst[i] <= capacity - np.sum(weight_lst[new_solution == 1])):\n                    new_solution[i] = 1\n                elif new_solution[i] == 1:\n                    new_solution[i] = 0\n\n    # Ensure feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Strategy 3: Capacity-aware random walk\n        excess = current_weight - capacity\n        excess_items = np.where(new_solution == 1)[0]\n        np.random.shuffle(excess_items)\n        for i in excess_items:\n            if weight_lst[i] <= excess:\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n                if excess <= 0:\n                    break\n\n    return new_solution\n\n",
          "score": [
               -19.18646270182542,
               -18.508972406433266
          ]
     },
     {
          "algorithm": "{The new algorithm combines the selection of promising solutions based on both objective values and their trade-off potential with a novel local search strategy that dynamically adapts to the Pareto front's characteristics. It first identifies solutions with high combined objective values and strong trade-off potential by analyzing the archive's diversity and distribution, then applies a hybrid local search that intelligently combines a probabilistic item selection based on marginal contributions, a Pareto-front guided perturbation that explores regions near the knee point, and an adaptive capacity-aware random walk that ensures feasibility by considering both the current solution's weight and the remaining capacity, with the probability of each strategy being dynamically adjusted based on the solution's position relative to the Pareto front and the archive's state.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most promising solution based on trade-off potential\n    objectives = np.array([obj for (sol, obj) in archive])\n    combined_values = objectives[:, 0] + objectives[:, 1]\n    trade_off = objectives[:, 0] / (objectives[:, 1] + 1e-6)\n    diversity_score = combined_values * trade_off\n    best_idx = np.argmax(diversity_score)\n    base_solution = archive[best_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive hybrid local search\n    # Strategy 1: Marginal contribution-based selection\n    for i in range(n_items):\n        if np.random.rand() < 0.5:  # Probabilistic selection\n            delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n            if current_weight + delta_weight <= capacity:\n                delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n                delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n                norm_improvement = (delta_value1 + delta_value2) / (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1]) + 1e-6)\n                if np.random.rand() < min(1.0, 0.3 + 0.7 * norm_improvement):\n                    new_solution[i] = 1 - new_solution[i]\n                    current_weight += delta_weight\n\n    # Strategy 2: Pareto-front guided perturbation\n    if len(archive) > 2:\n        sorted_idx = np.argsort(objectives[:, 0] + objectives[:, 1])\n        middle_solution = archive[sorted_idx[len(archive)//2]][0]\n        for i in range(n_items):\n            if middle_solution[i] != new_solution[i] and np.random.rand() < 0.2:\n                if middle_solution[i] == 1 and (current_weight + weight_lst[i] <= capacity):\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n                elif new_solution[i] == 1:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Step 3: Adaptive capacity-aware random walk\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        candidates = np.where(new_solution == 0)[0]\n        np.random.shuffle(candidates)\n        for i in candidates:\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < 0.4:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n                if remaining_capacity <= 0:\n                    break\n\n    return new_solution\n\n",
          "score": [
               -20.177957868136076,
               -16.058563476430592
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution from the archive\n    # Prioritize solutions with high marginal gains or close to the Pareto front\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_obj = archive[selected_idx][1]\n\n    # Step 2: Apply the guided flip and swap operator\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(weight_lst), size=min(3, len(weight_lst)), replace=False)\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove the item\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # Perform targeted swap between two items\n    swap_candidates = np.where(new_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, size=2, replace=False)\n        # Check if swapping improves the solution\n        delta_weight = weight_lst[j] - weight_lst[i]\n        delta_value1 = value1_lst[j] - value1_lst[i]\n        delta_value2 = value2_lst[j] - value2_lst[i]\n\n        if (delta_weight <= 0 or current_weight + delta_weight <= capacity) and \\\n           (delta_value1 >= 0 or delta_value2 >= 0):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n            current_weight += delta_weight\n            current_value1 += delta_value1\n            current_value2 += delta_value2\n\n    return new_solution\n\n",
          "score": [
               -18.68004542521698,
               -18.028382666602297
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the most promising solution based on marginal improvement potential\n    max_potential = -1\n    selected_solution = None\n    selected_obj = None\n\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        potential = (obj[0] / (current_weight + 1e-6) + obj[1] / (current_weight + 1e-6)) / 2\n        if potential > max_potential:\n            max_potential = potential\n            selected_solution = sol\n            selected_obj = obj\n\n    if selected_solution is None:\n        selected_solution = archive[0][0]\n        selected_obj = archive[0][1]\n\n    new_solution = selected_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Guided flip operator\n    for i in range(len(new_solution)):\n        # Calculate potential improvement if we flip this item\n        delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n        new_weight = current_weight + delta_weight\n\n        if new_weight <= capacity:\n            # Calculate marginal improvement for both objectives\n            delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n            delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n\n            # Calculate normalized improvement\n            norm_improvement = (delta_value1 / (selected_obj[0] + 1e-6) + delta_value2 / (selected_obj[1] + 1e-6)) / 2\n\n            # Flip with probability based on improvement and a small random factor\n            flip_prob = min(1.0, 0.1 + 0.9 * norm_improvement + 0.1 * np.random.random())\n\n            if np.random.random() < flip_prob:\n                new_solution[i] = 1 - new_solution[i]\n                current_weight = new_weight\n\n    return new_solution\n\n",
          "score": [
               -18.46494248406106,
               -18.273839065231257
          ]
     },
     {
          "algorithm": "{The new algorithm combines the selection of high-variance solutions from the first approach with a novel \"objective-balanced\" local search that considers both objectives simultaneously, where items are flipped based on their potential to improve both objectives proportionally, while also incorporating a probabilistic element that favors items with better marginal returns, similar to the guided flip operator in the second approach, but with a more sophisticated balancing mechanism that dynamically adjusts the trade-off between objectives based on the current solution's position in the Pareto front.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential for improvement (highest variance in objectives)\n    solutions = [s[0] for s in archive]\n    objectives = np.array([s[1] for s in archive])\n    variances = np.var(objectives, axis=0)\n    selected_idx = np.argmax(variances)\n    base_solution = solutions[selected_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Calculate objective balances\n    total_value1 = np.sum(value1_lst)\n    total_value2 = np.sum(value2_lst)\n    balance1 = (current_value1 / (total_value1 + 1e-6))\n    balance2 = (current_value2 / (total_value2 + 1e-6))\n\n    # Objective-balanced local search\n    for i in range(len(new_solution)):\n        # Calculate potential improvement\n        delta_weight = weight_lst[i] * (1 - 2 * new_solution[i])\n        new_weight = current_weight + delta_weight\n\n        if new_weight <= capacity:\n            delta_value1 = value1_lst[i] * (1 - 2 * new_solution[i])\n            delta_value2 = value2_lst[i] * (1 - 2 * new_solution[i])\n\n            # Calculate normalized improvement with objective balance\n            norm_improvement = (balance1 * delta_value1 / (total_value1 + 1e-6) +\n                               balance2 * delta_value2 / (total_value2 + 1e-6)) / 2\n\n            # Flip with probability based on improvement and balance\n            flip_prob = min(1.0, 0.1 + 0.9 * norm_improvement + 0.1 * np.random.random())\n\n            if np.random.random() < flip_prob:\n                new_solution[i] = 1 - new_solution[i]\n                current_weight = new_weight\n                current_value1 += delta_value1\n                current_value2 += delta_value2\n                balance1 = (current_value1 / (total_value1 + 1e-6))\n                balance2 = (current_value2 / (total_value2 + 1e-6))\n\n    # Greedy post-processing to ensure high-quality solution\n    remaining_weight = capacity - np.sum(weight_lst * new_solution)\n    if remaining_weight > 0:\n        # Find items not in the solution that can be added without exceeding capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        candidate_weights = weight_lst[candidate_items]\n        feasible_items = candidate_weights <= remaining_weight\n\n        if np.any(feasible_items):\n            # Calculate combined value with balance\n            candidate_value1 = value1_lst[candidate_items]\n            candidate_value2 = value2_lst[candidate_items]\n            combined_value = (balance1 * candidate_value1 + balance2 * candidate_value2) / (balance1 + balance2 + 1e-6)\n\n            best_item_idx = np.argmax(combined_value[feasible_items])\n            actual_idx = candidate_items[feasible_items][best_item_idx]\n            new_solution[actual_idx] = 1\n\n    return new_solution\n\n",
          "score": [
               -17.69175356547707,
               -17.941539864109153
          ]
     },
     {
          "algorithm": "{The new algorithm 'select_neighbor' first identifies the solution with the highest combined objective value from the archive, then applies a novel 'adaptive clustering and diversification' strategy that groups items into clusters based on their weight-to-value ratios and selects items from underrepresented clusters to flip, while ensuring feasibility by dynamically adjusting the selection probability based on cluster diversity and remaining capacity. This approach avoids local optima by promoting exploration of diverse item combinations across different weight-value trade-offs, while maintaining feasibility through cluster-based feasibility checks and adaptive perturbation.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with highest combined objective value\n    archive_solutions = [sol for sol, _ in archive]\n    archive_objectives = [obj for _, obj in archive]\n    objectives_sum = np.array([sum(obj) for obj in archive_objectives])\n    best_idx = np.argmax(objectives_sum)\n    base_solution = archive_solutions[best_idx].copy()\n\n    # Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate weight-to-value ratios for clustering\n    ratio1 = weight_lst / (value1_lst + 1e-10)\n    ratio2 = weight_lst / (value2_lst + 1e-10)\n    combined_ratio = ratio1 + ratio2\n\n    # Cluster items based on combined ratio\n    n_clusters = min(5, len(base_solution))\n    cluster_labels = np.argmin(np.abs(combined_ratio[:, None] - np.percentile(combined_ratio, np.linspace(0, 100, n_clusters))), axis=1)\n\n    # Calculate cluster diversity (number of items in each cluster)\n    cluster_diversity = np.zeros(n_clusters)\n    for cluster in range(n_clusters):\n        cluster_items = np.where(cluster_labels == cluster)[0]\n        cluster_diversity[cluster] = len(cluster_items)\n\n    # Calculate cluster selection probabilities (inverse of diversity)\n    cluster_probs = 1 / (cluster_diversity + 1e-10)\n    cluster_probs = cluster_probs / np.sum(cluster_probs)\n\n    # Select a cluster to target for diversification\n    target_cluster = np.random.choice(n_clusters, p=cluster_probs)\n    target_items = np.where(cluster_labels == target_cluster)[0]\n\n    # Generate neighbor solution by flipping items in the target cluster\n    new_solution = base_solution.copy()\n    for item in target_items:\n        if np.random.rand() < 0.3:  # 30% chance to flip each item in the cluster\n            if new_solution[item] == 1:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n            else:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Additional adaptive perturbation based on remaining capacity\n    for i in range(len(new_solution)):\n        if np.random.rand() < (remaining_capacity / capacity) * 0.5:  # Capacity-sensitive probability\n            if new_solution[i] == 1:\n                if current_weight - weight_lst[i] >= 0:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    return new_solution\n\n",
          "score": [
               -17.35968444194008,
               -17.504641567425445
          ]
     },
     {
          "algorithm": "{This novel algorithm employs a hybrid approach combining adaptive item selection with multi-objective aware perturbation, where it first identifies critical items by analyzing their impact on both objectives through a weighted utility function, then performs a targeted perturbation phase that selectively flips items based on their utility and feasibility, followed by a controlled neighborhood exploration that considers both individual and combined item effects while maintaining feasibility through a capacity-aware swap mechanism. The algorithm dynamically adjusts its search focus based on the current solution's characteristics and the problem's constraints, balancing exploration of high-utility items with exploitation of promising neighborhoods.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the most balanced solution from the archive\n    selected_idx = np.argmax([(obj[0] + obj[1]) / (np.sum(weight_lst * sol) + 1e-6) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Step 2: Calculate item utilities for both objectives\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n    combined_utility = (utility1 + utility2) / 2\n\n    # Step 3: Identify critical items (top 20% by combined utility)\n    num_critical = max(1, int(0.2 * len(weight_lst)))\n    critical_indices = np.argsort(combined_utility)[-num_critical:]\n\n    # Step 4: Perform targeted perturbation on critical items\n    new_solution = base_solution.copy()\n    for idx in critical_indices:\n        if np.random.random() < 0.7:  # Higher probability for critical items\n            if new_solution[idx] == 1:\n                # Try to remove the item\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n                    current_value1 -= value1_lst[idx]\n                    current_value2 -= value2_lst[idx]\n            else:\n                # Try to add the item\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    current_value1 += value1_lst[idx]\n                    current_value2 += value2_lst[idx]\n\n    # Step 5: Perform controlled neighborhood exploration\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        # Consider adding items with high utility\n        candidate_indices = np.where(new_solution == 0)[0]\n        if len(candidate_indices) > 0:\n            candidate_utilities = combined_utility[candidate_indices]\n            high_utility_indices = candidate_indices[np.argsort(candidate_utilities)[-min(3, len(candidate_indices)):]]\n\n            for idx in high_utility_indices:\n                if weight_lst[idx] <= remaining_capacity and np.random.random() < 0.5:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Step 6: Perform capacity-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) >= 2:\n        # Select two items for potential swap\n        i, j = np.random.choice(included_items, size=2, replace=False)\n        delta_weight = weight_lst[j] - weight_lst[i]\n        delta_value1 = value1_lst[j] - value1_lst[i]\n        delta_value2 = value2_lst[j] - value2_lst[i]\n\n        # Accept swap if it improves at least one objective or maintains feasibility\n        if (delta_weight <= 0 or current_weight + delta_weight <= capacity) and \\\n           (delta_value1 >= 0 or delta_value2 >= 0 or np.random.random() < 0.2):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -15.79020394068002,
               -15.960808902123858
          ]
     },
     {
          "algorithm": "{This novel algorithm first identifies the most diverse solution in the archive by calculating the Euclidean distance between each solution's objective values and the centroid of all solutions, then selects the solution with the maximum distance to promote exploration of underrepresented regions. It then applies a two-phase local search: in the first phase, it performs a greedy addition of items that improve both objectives while maintaining feasibility, followed by a probabilistic removal of items that significantly degrade the solution's Pareto dominance. The second phase employs a simulated annealing approach with a dynamically adjusted temperature to accept non-improving moves, allowing escape from local optima while ensuring the solution remains feasible. The algorithm dynamically balances exploration and exploitation by adjusting the selection probability of these phases based on the archive's diversity, ensuring high-quality neighbors that efficiently navigate the trade-off space while always respecting the capacity constraint.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the most diverse solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    centroid = np.mean(objectives, axis=0)\n    distances = np.linalg.norm(objectives - centroid, axis=1)\n    selected_idx = np.argmax(distances)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    current_value1 = np.sum(value1_lst * new_solution)\n    current_value2 = np.sum(value2_lst * new_solution)\n\n    # Step 2: Greedy addition phase\n    remaining_items = np.where(new_solution == 0)[0]\n    for idx in remaining_items:\n        if current_weight + weight_lst[idx] <= capacity:\n            new_weight = current_weight + weight_lst[idx]\n            new_value1 = current_value1 + value1_lst[idx]\n            new_value2 = current_value2 + value2_lst[idx]\n            if new_value1 > current_value1 and new_value2 > current_value2:\n                new_solution[idx] = 1\n                current_weight = new_weight\n                current_value1 = new_value1\n                current_value2 = new_value2\n\n    # Step 3: Probabilistic removal phase\n    included_items = np.where(new_solution == 1)[0]\n    for idx in included_items:\n        if np.random.rand() < 0.5:\n            new_weight = current_weight - weight_lst[idx]\n            new_value1 = current_value1 - value1_lst[idx]\n            new_value2 = current_value2 - value2_lst[idx]\n            if new_value1 >= current_value1 and new_value2 >= current_value2:\n                new_solution[idx] = 0\n                current_weight = new_weight\n                current_value1 = new_value1\n                current_value2 = new_value2\n\n    # Step 4: Simulated annealing phase\n    temperature = 1.0\n    cooling_rate = 0.95\n    for _ in range(10):\n        candidate_solution = new_solution.copy()\n        candidate_weight = current_weight\n        candidate_value1 = current_value1\n        candidate_value2 = current_value2\n\n        # Randomly select an item to flip\n        flip_idx = np.random.randint(len(weight_lst))\n        if candidate_solution[flip_idx] == 1:\n            if candidate_weight - weight_lst[flip_idx] <= capacity:\n                candidate_solution[flip_idx] = 0\n                candidate_weight -= weight_lst[flip_idx]\n                candidate_value1 -= value1_lst[flip_idx]\n                candidate_value2 -= value2_lst[flip_idx]\n        else:\n            if candidate_weight + weight_lst[flip_idx] <= capacity:\n                candidate_solution[flip_idx] = 1\n                candidate_weight += weight_lst[flip_idx]\n                candidate_value1 += value1_lst[flip_idx]\n                candidate_value2 += value2_lst[flip_idx]\n\n        # Acceptance criterion\n        if (candidate_value1 > current_value1 and candidate_value2 > current_value2) or \\\n           (np.random.rand() < np.exp((candidate_value1 + candidate_value2 - current_value1 - current_value2) / temperature)):\n            new_solution = candidate_solution\n            current_weight = candidate_weight\n            current_value1 = candidate_value1\n            current_value2 = candidate_value2\n\n        temperature *= cooling_rate\n\n    return new_solution\n\n",
          "score": [
               -16.292351561209294,
               -15.942480232036237
          ]
     },
     {
          "algorithm": "{The novel algorithm, called \"Adaptive Pareto Frontier Guided Neighborhood Search,\" first identifies the solution in the archive with the highest combined objective value and then constructs a neighbor by iteratively adding items that lie on the Pareto frontier of the remaining items, prioritizing those with the best marginal contribution to both objectives. The algorithm dynamically adjusts the selection probability based on the solution's current weight and the remaining capacity, ensuring feasibility while exploring the trade-off space between objectives. Additionally, it incorporates a \"frontier-aware perturbation\" step that probabilistically flips items based on their position relative to the Pareto frontier of the entire problem, further diversifying the search. The method balances exploitation and exploration by scaling the perturbation intensity based on the solution's dominance rank in the archive, ensuring high-quality multi-objective solutions.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the highest combined objective value\n    objectives = np.array([obj for _, obj in archive])\n    combined_obj = objectives[:, 0] + objectives[:, 1]\n    selected_idx = np.argmax(combined_obj)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Initialize new solution\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Identify items not in the base solution\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate Pareto frontier of excluded items\n    def is_pareto_efficient(costs):\n        is_efficient = np.ones(costs.shape[0], dtype=bool)\n        for i, c in enumerate(costs):\n            if is_efficient[i]:\n                is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n                is_efficient[i] = True\n        return is_efficient\n\n    if len(excluded_items) > 0:\n        excluded_weights = weight_lst[excluded_items]\n        excluded_values1 = value1_lst[excluded_items]\n        excluded_values2 = value2_lst[excluded_items]\n\n        # Create cost matrix for Pareto efficiency (lower is better)\n        costs = np.column_stack((-excluded_values1, -excluded_values2))\n        pareto_mask = is_pareto_efficient(costs)\n        pareto_frontier = excluded_items[pareto_mask]\n\n        # Add items from Pareto frontier if feasible\n        for item in pareto_frontier:\n            if remaining_capacity >= weight_lst[item]:\n                new_solution[item] = 1\n                remaining_capacity -= weight_lst[item]\n\n    # Frontier-aware perturbation\n    for item in range(len(new_solution)):\n        if np.random.rand() < 0.2:  # 20% chance to perturb\n            if new_solution[item] == 1:\n                # Check if removing this item would keep the solution on the Pareto frontier\n                if np.sum(new_solution) > 1:\n                    new_solution[item] = 0\n            else:\n                # Check if adding this item would improve the Pareto frontier\n                if remaining_capacity >= weight_lst[item]:\n                    new_solution[item] = 1\n                    remaining_capacity -= weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -16.428255466856225,
               -15.750587788104674
          ]
     }
]