[
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective Balancing and Capacity-Sensitive Evolutionary Perturbations\" algorithm begins by selecting a solution from the archive using a dynamic objective balancing mechanism that combines both objective values with adaptively adjusted weights based on their current relative importance, then applies a multi-phase evolutionary perturbation process that alternates between capacity-sensitive item flips, objective-specific marginal contribution swaps, and adaptive neighborhood exploration, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective balancing selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance with adaptive cooling\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.6\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.6\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.2 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases  # Decreasing intensity\n\n        # Phase 1: Capacity-sensitive item flips\n        if phase % 2 == 0 and len(new_solution) > 1:\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                item = np.random.choice(candidate_items)\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                item = np.random.choice(candidate_items)\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n        # Phase 2: Objective-specific marginal contribution swaps\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Adaptive neighborhood exploration\n        if random.random() < phase_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.6:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-preserving item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Exploration\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Hybrid selection with dynamic weights\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.7\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.7\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-stage exploration with dynamic intensity\n    stages = 5\n    for stage in range(stages):\n        stage_intensity = (stages - stage) / stages\n\n        if stage % 2 == 0:\n            # Stage 1: Capacity-aware item swaps with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n        else:\n            # Stage 2: Objective-specific marginal contribution flips with cooling\n            if random.random() < stage_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Stage 3: Adaptive random perturbations with capacity check\n        if random.random() < stage_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Tiered refinement with objective fusion\n    for tier in range(3):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.7:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        elif tier == 1:\n            # Tier 2: Capacity-preserving item replacements with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        else:\n            # Tier 3: Objective-specific swaps with adaptive cooling\n            if len(new_solution) >= 2 and random.random() < 0.3:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 301,
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Exploration\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Hybrid selection with dynamic weights\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.7\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.7\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-stage exploration with dynamic intensity\n    stages = 5\n    for stage in range(stages):\n        stage_intensity = (stages - stage) / stages\n\n        if stage % 2 == 0:\n            # Stage 1: Capacity-aware item swaps with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n        else:\n            # Stage 2: Objective-specific marginal contribution flips with cooling\n            if random.random() < stage_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Stage 3: Adaptive random perturbations with capacity check\n        if random.random() < stage_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Tiered refinement with objective fusion\n    for tier in range(3):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.7:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        elif tier == 1:\n            # Tier 2: Capacity-preserving item replacements with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        else:\n            # Tier 3: Objective-specific swaps with adaptive cooling\n            if len(new_solution) >= 2 and random.random() < 0.3:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.931369245152098,
               -18.7884327940105
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Hybrid selection with dynamic weights\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.7\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.7\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-stage exploration with dynamic intensity\n    stages = 5\n    for stage in range(stages):\n        stage_intensity = (stages - stage) / stages\n\n        if stage % 2 == 0:\n            # Stage 1: Capacity-aware item swaps with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n        else:\n            # Stage 2: Objective-specific marginal contribution flips with cooling\n            if random.random() < stage_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Stage 3: Adaptive random perturbations with capacity check\n        if random.random() < stage_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Tiered refinement with objective fusion\n    for tier in range(3):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.7:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        elif tier == 1:\n            # Tier 2: Capacity-preserving item replacements with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        else:\n            # Tier 3: Objective-specific swaps with adaptive cooling\n            if len(new_solution) >= 2 and random.random() < 0.3:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Multi-Objective Knapsack Exploration with Adaptive Objective Fusion and Dynamic Capacity-Aware Diversification\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply capacity-aware diversification with objective fusion\n    for _ in range(5):\n        # Select items to flip based on combined marginal contribution\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = obj1_weight * marginal1 + obj2_weight * marginal2\n\n        # Add items\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        # Remove items\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 4: Apply dynamic solution fusion with random swaps\n    for _ in range(3):\n        i, j = np.random.choice(len(weight_lst), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n\n    # Step 5: Final refinement with objective-aware perturbations\n    for _ in range(2):\n        if np.random.random() < obj1_weight:\n            # Focus on objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x] + 1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Focus on objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x] + 1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 302,
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Solution Fusion\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -19.261472406462794,
               -18.542352365087652
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Diversification\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = obj1_weight * utility1 + obj2_weight * utility2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = np.random.randint(0, len(weight_lst))\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 5: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if np.random.random() < obj1_weight:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective Fusion and Capacity-Adaptive Evolutionary Perturbations\" algorithm begins by selecting a solution from the archive using a dynamic objective fusion mechanism that combines both objective values with adaptively weighted contributions, then applies a multi-phase evolutionary perturbation process that alternates between capacity-adaptive item swaps, objective-specific marginal contribution flips, and dynamic neighborhood exploration, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-aware item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective fusion selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current solution's position\n        weight1 = 1.0 if norm_v1 > 0.7 else 0.4\n        weight2 = 1.0 if norm_v2 > 0.7 else 0.4\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.3 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-adaptive item swaps\n        if phase % 2 == 0 and len(new_solution) > 1:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution flips\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.7:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.5:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-aware item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 303,
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Diversification\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = obj1_weight * utility1 + obj2_weight * utility2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = np.random.randint(0, len(weight_lst))\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 5: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if np.random.random() < obj1_weight:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -19.800290219071577,
               -17.717490564298107
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = obj1_weight * utility1 + obj2_weight * utility2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = np.random.randint(0, len(weight_lst))\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 5: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if np.random.random() < obj1_weight:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Adaptive Objective-Space Clustering with Multi-Phase Greedy Diversification and Dynamic Capacity-Adaptive Perturbations\" algorithm begins by clustering the archive solutions into regions based on their objective-space positions and selecting a solution from the least explored cluster to balance exploration and exploitation, then applies a multi-phase diversification process that alternates between capacity-aware greedy additions of items with high combined marginal contributions to both objectives, dynamic capacity-adaptive removals of items with low combined marginal contributions, and adaptive random perturbations that are guided by the current solution's position relative to the Pareto front, with each phase dynamically adjusting its intensity based on the solution's proximity to the front, culminating in a final refinement step that combines greedy value maximization in both objectives with capacity-preserving item replacements, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Cluster solutions and select from least explored cluster\n    def cluster_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by both objectives\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n        # Split into 3 clusters\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    clusters = cluster_archive(archive)\n    selected_cluster = min(clusters, key=lambda x: len(x))\n    if not selected_cluster:\n        selected_cluster = archive\n    selected_idx = np.random.randint(0, len(selected_cluster))\n    base_solution = selected_cluster[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Multi-phase diversification with dynamic intensity\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Greedy addition of high-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-adaptive removal of low-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n                for item in sorted_items:\n                    if available_capacity + weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        available_capacity += weight_lst[item]\n                        break\n\n        # Phase 3: Dynamic random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Greedy maximization of both objectives\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        potential_improvement = (value1_lst * (1 - new_solution) + value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective Balancing and Capacity-Sensitive Evolutionary Perturbations\" algorithm begins by selecting a solution from the archive using a dynamic objective balancing mechanism that combines both objective values with adaptively adjusted weights based on their current relative importance, then applies a multi-phase evolutionary perturbation process that alternates between capacity-sensitive item flips, objective-specific marginal contribution swaps, and adaptive neighborhood exploration, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective balancing selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance with adaptive cooling\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.6\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.6\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.2 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases  # Decreasing intensity\n\n        # Phase 1: Capacity-sensitive item flips\n        if phase % 2 == 0 and len(new_solution) > 1:\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                item = np.random.choice(candidate_items)\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                item = np.random.choice(candidate_items)\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n        # Phase 2: Objective-specific marginal contribution swaps\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Adaptive neighborhood exploration\n        if random.random() < phase_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.6:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-preserving item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 304,
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective-Space Partitioning, Hierarchical Neighborhood Exploration, and Capacity-Aware Greedy Diversification\" algorithm begins by partitioning the objective space into adaptive regions based on the current distribution of archive solutions and selecting a solution from the least explored region to balance exploration and exploitation, then applies a hierarchical neighborhood exploration process that combines capacity-aware greedy additions of items with high marginal contributions to both objectives, dynamic objective-specific swaps of items with complementary marginal contributions, and adaptive neighborhood perturbations that are guided by the current solution's position relative to the Pareto front, with each hierarchical level dynamically adjusting its intensity based on the solution's proximity to the front, followed by a multi-phase diversification phase that alternates between capacity-preserving item replacements, objective-specific marginal contribution maximization, and adaptive random perturbations, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive objective-space partitioning and selection\n    def partition_archive(archive):\n        if len(archive) < 4:\n            return [archive]\n        # Sort by both objectives\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n        # Split into 4 regions\n        split1 = len(sorted_archive) // 4\n        split2 = 2 * len(sorted_archive) // 4\n        split3 = 3 * len(sorted_archive) // 4\n        return [sorted_archive[:split1], sorted_archive[split1:split2],\n                sorted_archive[split2:split3], sorted_archive[split3:]]\n\n    regions = partition_archive(archive)\n    selected_region = min(regions, key=lambda x: len(x))\n    if not selected_region:\n        selected_region = archive\n    selected_idx = np.random.randint(0, len(selected_region))\n    base_solution = selected_region[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Hierarchical neighborhood exploration\n    levels = 3\n    for level in range(levels):\n        level_intensity = (levels - level) / levels\n\n        # Level 1: Capacity-aware greedy additions\n        if random.random() < level_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items[:2]:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n\n        # Level 2: Dynamic objective-specific swaps\n        if random.random() < level_intensity * 0.7:\n            if random.random() < 0.5:\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                for item in sorted_items[:1]:\n                    if available_capacity + weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        available_capacity += weight_lst[item]\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                for item in sorted_items[:1]:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n\n        # Level 3: Adaptive neighborhood perturbations\n        if random.random() < level_intensity * 0.5:\n            for _ in range(3):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Step 3: Multi-phase diversification\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-preserving replacements\n        if phase % 2 == 0 and len(new_solution) >= 2:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if available_capacity + weight_lst[i] - weight_lst[j] >= 0:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    available_capacity = available_capacity + weight_lst[i] - weight_lst[j]\n\n        # Phase 2: Objective-specific marginal maximization\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if weight_lst[item] <= available_capacity:\n                            new_solution[item] = 1\n                            available_capacity -= weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if available_capacity + weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            available_capacity += weight_lst[item]\n\n        # Phase 3: Adaptive random perturbations\n        if random.random() < phase_intensity * 0.3:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -16.93247863883345,
               -19.73889057466439
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive objective-space partitioning and selection\n    def partition_archive(archive):\n        if len(archive) < 4:\n            return [archive]\n        # Sort by both objectives\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n        # Split into 4 regions\n        split1 = len(sorted_archive) // 4\n        split2 = 2 * len(sorted_archive) // 4\n        split3 = 3 * len(sorted_archive) // 4\n        return [sorted_archive[:split1], sorted_archive[split1:split2],\n                sorted_archive[split2:split3], sorted_archive[split3:]]\n\n    regions = partition_archive(archive)\n    selected_region = min(regions, key=lambda x: len(x))\n    if not selected_region:\n        selected_region = archive\n    selected_idx = np.random.randint(0, len(selected_region))\n    base_solution = selected_region[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Hierarchical neighborhood exploration\n    levels = 3\n    for level in range(levels):\n        level_intensity = (levels - level) / levels\n\n        # Level 1: Capacity-aware greedy additions\n        if random.random() < level_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items[:2]:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n\n        # Level 2: Dynamic objective-specific swaps\n        if random.random() < level_intensity * 0.7:\n            if random.random() < 0.5:\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                for item in sorted_items[:1]:\n                    if available_capacity + weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        available_capacity += weight_lst[item]\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                for item in sorted_items[:1]:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n\n        # Level 3: Adaptive neighborhood perturbations\n        if random.random() < level_intensity * 0.5:\n            for _ in range(3):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Step 3: Multi-phase diversification\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-preserving replacements\n        if phase % 2 == 0 and len(new_solution) >= 2:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if available_capacity + weight_lst[i] - weight_lst[j] >= 0:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    available_capacity = available_capacity + weight_lst[i] - weight_lst[j]\n\n        # Phase 2: Objective-specific marginal maximization\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if weight_lst[item] <= available_capacity:\n                            new_solution[item] = 1\n                            available_capacity -= weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if available_capacity + weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            available_capacity += weight_lst[item]\n\n        # Phase 3: Adaptive random perturbations\n        if random.random() < phase_intensity * 0.3:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective Fusion and Capacity-Adaptive Evolutionary Perturbations\" algorithm begins by selecting a solution from the archive using a dynamic objective fusion mechanism that combines both objective values with adaptively weighted contributions, then applies a multi-phase evolutionary perturbation process that alternates between capacity-adaptive item swaps, objective-specific marginal contribution flips, and dynamic neighborhood exploration, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-aware item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective fusion selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current solution's position\n        weight1 = 1.0 if norm_v1 > 0.7 else 0.4\n        weight2 = 1.0 if norm_v2 > 0.7 else 0.4\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.3 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-adaptive item swaps\n        if phase % 2 == 0 and len(new_solution) > 1:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution flips\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.7:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.5:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-aware item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Objective-Space Clustering with Multi-Phase Greedy Diversification and Dynamic Capacity-Adaptive Perturbations\" algorithm begins by clustering the archive solutions into regions based on their objective-space positions and selecting a solution from the least explored cluster to balance exploration and exploitation, then applies a multi-phase diversification process that alternates between capacity-aware greedy additions of items with high combined marginal contributions to both objectives, dynamic capacity-adaptive removals of items with low combined marginal contributions, and adaptive random perturbations that are guided by the current solution's position relative to the Pareto front, with each phase dynamically adjusting its intensity based on the solution's proximity to the front, culminating in a final refinement step that combines greedy value maximization in both objectives with capacity-preserving item replacements, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Cluster solutions and select from least explored cluster\n    def cluster_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by both objectives\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n        # Split into 3 clusters\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    clusters = cluster_archive(archive)\n    selected_cluster = min(clusters, key=lambda x: len(x))\n    if not selected_cluster:\n        selected_cluster = archive\n    selected_idx = np.random.randint(0, len(selected_cluster))\n    base_solution = selected_cluster[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Multi-phase diversification with dynamic intensity\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Greedy addition of high-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-adaptive removal of low-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n                for item in sorted_items:\n                    if available_capacity + weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        available_capacity += weight_lst[item]\n                        break\n\n        # Phase 3: Dynamic random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Greedy maximization of both objectives\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        potential_improvement = (value1_lst * (1 - new_solution) + value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 305,
          "algorithm": "{The novel \"Objective-Space Guided Multi-Phase Evolutionary Perturbation with Adaptive Trade-off Exploration and Hierarchical Capacity-Aware Refinement\" algorithm begins by selecting a solution from the archive using a dynamic trade-off exploration mechanism that alternates between focusing on individual objectives and exploring balanced trade-offs, then applies a multi-phase evolutionary perturbation process that combines capacity-adaptive item substitutions, objective-specific marginal contribution analysis, and dynamic neighborhood exploration with adaptive intensity based on the solution's position relative to the Pareto front, followed by a hierarchical refinement phase that integrates both objective maximization with capacity-preserving item replacements, while incorporating a novel adaptive cooling mechanism that gradually shifts the search focus from exploration to exploitation as the solution approaches the Pareto front, thereby ensuring a balanced and focused search that efficiently navigates the objective space and maintains feasibility throughout the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic trade-off exploration selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    tradeoff_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        tradeoff_score = abs(norm_v1 - norm_v2) * (norm_v1 + norm_v2) + 0.2 * (norm_v1 + norm_v2)\n        tradeoff_scores.append((tradeoff_score, sol))\n\n    selected_sol = max(tradeoff_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-adaptive item substitutions\n        if phase % 2 == 0 and len(new_solution) > 1:\n            in_items = np.where(new_solution == 1)[0]\n            out_items = np.where(new_solution == 0)[0]\n\n            if len(in_items) > 0 and len(out_items) > 0:\n                i = random.choice(in_items)\n                j = random.choice(out_items)\n\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution analysis\n        else:\n            if random.random() < phase_intensity:\n                if phase % 3 == 0:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                elif phase % 3 == 1:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.8:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    if random.random() < 0.3:  # Higher chance to remove items\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Both objective maximization\n            potential_improvement = (value1_lst + value2_lst) * (1 - new_solution)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n                for item in sorted_items[:2]:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-preserving item replacements\n            if len(new_solution) >= 2:\n                in_items = np.where(new_solution == 1)[0]\n                out_items = np.where(new_solution == 0)[0]\n\n                if len(in_items) > 0 and len(out_items) > 0:\n                    i = random.choice(in_items)\n                    j = random.choice(out_items)\n\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution) + value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.190286773796757,
               -17.911856039739675
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic trade-off exploration selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    tradeoff_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        tradeoff_score = abs(norm_v1 - norm_v2) * (norm_v1 + norm_v2) + 0.2 * (norm_v1 + norm_v2)\n        tradeoff_scores.append((tradeoff_score, sol))\n\n    selected_sol = max(tradeoff_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-adaptive item substitutions\n        if phase % 2 == 0 and len(new_solution) > 1:\n            in_items = np.where(new_solution == 1)[0]\n            out_items = np.where(new_solution == 0)[0]\n\n            if len(in_items) > 0 and len(out_items) > 0:\n                i = random.choice(in_items)\n                j = random.choice(out_items)\n\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution analysis\n        else:\n            if random.random() < phase_intensity:\n                if phase % 3 == 0:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                elif phase % 3 == 1:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.8:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    if random.random() < 0.3:  # Higher chance to remove items\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Both objective maximization\n            potential_improvement = (value1_lst + value2_lst) * (1 - new_solution)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n                for item in sorted_items[:2]:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-preserving item replacements\n            if len(new_solution) >= 2:\n                in_items = np.where(new_solution == 1)[0]\n                out_items = np.where(new_solution == 0)[0]\n\n                if len(in_items) > 0 and len(out_items) > 0:\n                    i = random.choice(in_items)\n                    j = random.choice(out_items)\n\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution) + value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective Fusion and Capacity-Adaptive Evolutionary Perturbations\" algorithm begins by selecting a solution from the archive using a dynamic objective fusion mechanism that combines both objective values with adaptively weighted contributions, then applies a multi-phase evolutionary perturbation process that alternates between capacity-adaptive item swaps, objective-specific marginal contribution flips, and dynamic neighborhood exploration, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-aware item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective fusion selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current solution's position\n        weight1 = 1.0 if norm_v1 > 0.7 else 0.4\n        weight2 = 1.0 if norm_v2 > 0.7 else 0.4\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.3 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-adaptive item swaps\n        if phase % 2 == 0 and len(new_solution) > 1:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution flips\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.7:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.5:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-aware item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 306,
          "algorithm": "{The novel \"Pareto Frontier Guided Multi-Objective Knapsack Exploration with Adaptive Objective Balancing and Dynamic Item Substitution\" algorithm begins by selecting a solution from the archive using a dynamic objective balancing mechanism that combines both objective values with adaptively weighted contributions based on their current dominance relationships, then applies a multi-phase perturbation process that alternates between capacity-aware item substitutions, objective-specific marginal contribution flips, and dynamic neighborhood exploration with adaptive intensity, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-aware item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective balancing selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current solution's position\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.6\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.6\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.4 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase perturbation with adaptive intensity\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-aware item substitutions\n        if phase % 2 == 0 and len(new_solution) > 1:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution flips\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement with adaptive cooling\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.5:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-aware item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    return new_solution\n\n",
          "score": [
               -19.078837227646027,
               -18.573067965736456
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective balancing selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current solution's position\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.6\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.6\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.4 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase perturbation with adaptive intensity\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-aware item substitutions\n        if phase % 2 == 0 and len(new_solution) > 1:\n            i, j = np.random.choice(len(new_solution), size=2, replace=False)\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        # Phase 2: Objective-specific marginal contribution flips\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Dynamic neighborhood exploration\n        if random.random() < phase_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement with adaptive cooling\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.5:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-aware item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The novel \"Adaptive Objective-Space Clustering with Multi-Phase Greedy Diversification and Dynamic Capacity-Adaptive Perturbations\" algorithm begins by clustering the archive solutions into regions based on their objective-space positions and selecting a solution from the least explored cluster to balance exploration and exploitation, then applies a multi-phase diversification process that alternates between capacity-aware greedy additions of items with high combined marginal contributions to both objectives, dynamic capacity-adaptive removals of items with low combined marginal contributions, and adaptive random perturbations that are guided by the current solution's position relative to the Pareto front, with each phase dynamically adjusting its intensity based on the solution's proximity to the front, culminating in a final refinement step that combines greedy value maximization in both objectives with capacity-preserving item replacements, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Cluster solutions and select from least explored cluster\n    def cluster_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by both objectives\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n        # Split into 3 clusters\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    clusters = cluster_archive(archive)\n    selected_cluster = min(clusters, key=lambda x: len(x))\n    if not selected_cluster:\n        selected_cluster = archive\n    selected_idx = np.random.randint(0, len(selected_cluster))\n    base_solution = selected_cluster[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Multi-phase diversification with dynamic intensity\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Greedy addition of high-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-adaptive removal of low-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n                for item in sorted_items:\n                    if available_capacity + weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        available_capacity += weight_lst[item]\n                        break\n\n        # Phase 3: Dynamic random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Greedy maximization of both objectives\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        potential_improvement = (value1_lst * (1 - new_solution) + value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 307,
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if available_capacity + weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if available_capacity + weight_lst[item] >= 0:\n                new_solution[item] = 0\n                available_capacity += weight_lst[item]\n        else:\n            if weight_lst[item] <= available_capacity:\n                new_solution[item] = 1\n                available_capacity -= weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -18.939092187151576,
               -18.809626169595372
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if available_capacity + weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if available_capacity + weight_lst[item] >= 0:\n                new_solution[item] = 0\n                available_capacity += weight_lst[item]\n        else:\n            if weight_lst[item] <= available_capacity:\n                new_solution[item] = 1\n                available_capacity -= weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n    return new_solution\n\n"
     }
]