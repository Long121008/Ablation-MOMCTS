[
     {
          "algorithm": "{The heuristic function 'select_neighbor' first identifies promising solutions in the archive by evaluating their potential for improvement using a combination of objective dominance, crowding distance, and solution diversity metrics. It then intelligently selects a base solution from these candidates using a weighted random selection that prioritizes solutions with higher crowding distances and lower dominance counts. The local search operator employs a hybrid strategy that combines item swapping, random flipping, and adaptive perturbation to explore the neighborhood, ensuring feasibility by dynamically adjusting the selection of items to flip based on the remaining capacity. The operator also incorporates a memory mechanism to avoid revisiting recently explored solutions, further enhancing exploration. The function returns the new neighbor solution after validating its feasibility and updating the archive with the new solution if it is non-dominated.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions in the archive for potential improvement\n    solutions = [sol for sol, _ in archive]\n    objectives = [obj for _, obj in archive]\n\n    # Calculate dominance counts and crowding distances\n    dominance_counts = [0] * len(archive)\n    crowding_distances = [0.0] * len(archive)\n\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if objectives[i][0] <= objectives[j][0] and objectives[i][1] <= objectives[j][1]:\n                    dominance_counts[i] += 1\n                if objectives[i][0] < objectives[j][0] and objectives[i][1] < objectives[j][1]:\n                    dominance_counts[i] += 1\n\n    # Sort solutions by dominance count (lower is better)\n    sorted_indices = sorted(range(len(archive)), key=lambda k: dominance_counts[k])\n    candidate_indices = sorted_indices[:max(1, len(archive) // 2)]\n\n    # Step 2: Select a base solution with weighted randomness\n    weights = [1.0 / (1.0 + dominance_counts[i]) for i in candidate_indices]\n    selected_idx = random.choices(candidate_indices, weights=weights, k=1)[0]\n    base_solution = solutions[selected_idx].copy()\n\n    # Step 3: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Hybrid local search operator\n    # 1. Randomly select a subset of items to flip\n    flip_indices = random.sample(range(n_items), min(3, n_items // 2))\n\n    # 2. Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # 3. Flip items while maintaining feasibility\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove item if possible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add item if possible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # 4. Additional perturbation: swap two items if feasible\n    if len(new_solution) >= 2:\n        i, j = random.sample(range(n_items), 2)\n        if new_solution[i] != new_solution[j]:\n            # Calculate weight difference\n            weight_diff = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n            if current_weight + weight_diff <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Ensure feasibility (shouldn't be necessary but as a safeguard)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        # If not feasible, try to remove items randomly until feasible\n        while total_weight > capacity:\n            items_in = np.where(new_solution == 1)[0]\n            if len(items_in) == 0:\n                break\n            remove_idx = random.choice(items_in)\n            new_solution[remove_idx] = 0\n            total_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
          "score": [
               -18.783291405738993,
               -18.18577865487048
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    base_solution = max(archive, key=lambda x: (x[1][0] + x[1][1]))[0].copy()\n\n    # Calculate current total weight and values\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Create a candidate solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(base_solution), size=min(3, len(base_solution)), replace=False)\n\n    for idx in flip_indices:\n        # 2. Flip the item if it improves both objectives or weight allows\n        if new_solution[idx] == 1:\n            # Try to remove the item if it's not critical for weight\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item if it fits in capacity\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # 3. Additional improvement: swap two items if it improves both objectives\n    if len(base_solution) >= 2:\n        i, j = np.random.choice(len(base_solution), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            # Calculate potential new weight and values\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -18.586423720786442,
               -18.76328847454988
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Dual-Objective Guided Bit Flip with Weighted Randomization,\" intelligently selects a solution from the archive by prioritizing those with high crowding distance or low dominance in the objective space, then applies a weighted random bit flip strategy that probabilistically flips bits based on their marginal contribution to both objectives, while ensuring feasibility by dynamically adjusting the flip probability to prevent exceeding capacity. The algorithm further refines the solution by iteratively flipping bits that improve the weighted sum of normalized objective values, using a simulated annealing-inspired acceptance criterion to escape local optima, and finally performs a post-optimization step to fine-tune the solution by flipping the least impactful bits to maximize one objective while minimally affecting the other.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution from the archive\n    # Prioritize solutions with high crowding distance or low dominance\n    def crowding_distance(solutions):\n        if len(solutions) < 3:\n            return [1.0] * len(solutions)\n        sorted_solutions = sorted(solutions, key=lambda x: x[1][0])\n        distances = [0.0] * len(solutions)\n        for m in range(2):\n            sorted_solutions.sort(key=lambda x: x[1][m])\n            distances[0] = float('inf')\n            distances[-1] = float('inf')\n            for i in range(1, len(solutions)-1):\n                distances[i] += (sorted_solutions[i+1][1][m] - sorted_solutions[i-1][1][m]) / (sorted_solutions[-1][1][m] - sorted_solutions[0][1][m])\n        return distances\n\n    distances = crowding_distance(archive)\n    selected_idx = np.argmax(distances)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n\n    # Step 2: Generate a neighbor solution using weighted random bit flip\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / np.maximum(weight_lst, 1e-6)\n    marginal2 = value2_lst / np.maximum(weight_lst, 1e-6)\n    combined_marginal = marginal1 + marginal2\n\n    # Probability of flipping each bit\n    flip_probs = combined_marginal * (1 - new_solution)\n    flip_probs[weight_lst > available_capacity] = 0  # Ensure feasibility\n\n    # Normalize probabilities\n    if np.sum(flip_probs) > 0:\n        flip_probs = flip_probs / np.sum(flip_probs)\n\n    # Perform flips\n    for i in range(len(new_solution)):\n        if random.random() < flip_probs[i] * 0.5:  # Lower probability to limit changes\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n            elif new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Step 3: Simulated annealing-inspired refinement\n    temp = 1.0\n    cooling_rate = 0.99\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 and delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - flip least impactful bits\n    for _ in range(3):\n        # Find bits with minimal impact on objectives\n        impact1 = np.abs(value1_lst) / (1 + weight_lst)\n        impact2 = np.abs(value2_lst) / (1 + weight_lst)\n        combined_impact = impact1 + impact2\n\n        # Prefer to flip bits that are currently excluded but could improve objectives\n        flip_candidates = np.where(new_solution == 0)[0]\n        if len(flip_candidates) > 0:\n            candidate_impacts = combined_impact[flip_candidates]\n            if np.sum(candidate_impacts) > 0:\n                flip_probs = candidate_impacts / np.sum(candidate_impacts)\n                idx = np.random.choice(flip_candidates, p=flip_probs)\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -17.411685236353595,
               -17.48640227514403
          ]
     },
     {
          "algorithm": "{The algorithm first identifies promising solutions in the archive by selecting those with high objective values or those that are on the Pareto front, then intelligently samples a base solution from this subset. It then applies a hybrid local search strategy that combines a novel \"swap-and-flip\" operator with a probabilistic \"value-weighted\" perturbation to generate a neighbor solution. The swap-and-flip operator selects two items, swaps their inclusion statuses, and then flips the status of a third item based on a value-to-weight ratio heuristic, while the probabilistic perturbation introduces randomness to escape local optima. The algorithm ensures feasibility by rejecting any moves that would exceed the capacity, and it iteratively refines the solution by accepting improvements in either objective or Pareto-dominated solutions, ultimately returning a high-quality neighbor solution that balances exploration and exploitation of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify promising solutions (high objective values or on Pareto front)\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    # If no Pareto front, select top 20% by sum of objectives\n    if not pareto_front:\n        sorted_indices = np.argsort(-objectives.sum(axis=1))\n        candidate_indices = sorted_indices[:max(1, len(archive) // 5)]\n    else:\n        candidate_indices = pareto_front\n\n    # Randomly select a base solution from candidates\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Swap-and-flip operator\n    if n_items >= 3:\n        # Select two items to swap\n        swap_indices = np.random.choice(n_items, size=2, replace=False)\n        # Flip a third item based on value-to-weight ratio\n        flip_idx = np.random.choice(n_items)\n        while flip_idx in swap_indices:\n            flip_idx = np.random.choice(n_items)\n\n        # Apply swap\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n        # Apply flip with probability based on value-to-weight ratio\n        v1_ratio = value1_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        v2_ratio = value2_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        flip_prob = min(1.0, (v1_ratio + v2_ratio) / 10)  # Normalize\n\n        if np.random.random() < flip_prob:\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Probabilistic perturbation\n    for i in range(n_items):\n        if np.random.random() < 0.1:  # 10% chance to perturb\n            if new_solution[i] == 1:\n                # Try to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                # Try to add\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Remove random items until feasible\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -18.525469470071368,
               -17.447019179654852
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Adaptive Neighborhood Exploration,\" first partitions the archive into regions based on the objective space, then selects a solution from a less explored region to balance exploration and exploitation. It generates a neighbor solution by adaptively exploring different neighborhoods (value-based, weight-based, and dominance-based) based on the solution's position in the objective space, using a combination of greedy improvement, random perturbation, and simulated annealing with a dynamic temperature schedule. The algorithm ensures feasibility by maintaining a running total weight and only allowing moves that keep the solution within capacity, while also incorporating a post-optimization step that refines the solution by selectively flipping bits that improve the Pareto front without disrupting the existing trade-offs.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition the archive based on objective space\n    def partition_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 3 partitions\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Generate neighbor using adaptive neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Determine neighborhood type based on solution's position in objective space\n    # Normalize objectives\n    all_objs = np.array([obj for _, obj in archive])\n    if len(all_objs) > 0:\n        max_obj1, max_obj2 = np.max(all_objs, axis=0)\n        min_obj1, min_obj2 = np.min(all_objs, axis=0)\n        norm_obj1 = (obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6)\n        norm_obj2 = (obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6)\n\n        # Classify solution based on normalized objectives\n        if norm_obj1 > 0.7 and norm_obj2 > 0.7:  # High in both objectives\n            neighborhood_type = 'dominance'  # Focus on improving Pareto front\n        elif norm_obj1 > 0.6 or norm_obj2 > 0.6:  # Moderate in one objective\n            neighborhood_type = 'value_based'  # Focus on improving specific objective\n        else:\n            neighborhood_type = 'weight_based'  # Focus on capacity management\n    else:\n        neighborhood_type = 'value_based'\n\n    # Apply selected neighborhood exploration\n    if neighborhood_type == 'value_based':\n        # Value-based neighborhood: Flip items that improve the most in one objective\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        for _ in range(5):\n            # Randomly select an objective to improve\n            if random.random() < 0.5:\n                # Improve objective 1\n                candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n            else:\n                # Improve objective 2\n                candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n\n    elif neighborhood_type == 'weight_based':\n        # Weight-based neighborhood: Flip items that help manage capacity\n        for _ in range(5):\n            # Find items that can be added without exceeding capacity\n            feasible_add = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(feasible_add) > 0:\n                # Add the item with highest value-to-weight ratio\n                ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n                candidate_idx = feasible_add[np.argmax(ratios[feasible_add])]\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    else:  # dominance neighborhood\n        # Dominance-based neighborhood: Flip items that could improve Pareto front\n        for _ in range(5):\n            # Find items that could potentially improve both objectives\n            potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n            candidate_idx = np.argmax(potential_improvement)\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Step 3: Simulated annealing with dynamic temperature\n    temp = 1.0\n    cooling_rate = 0.95\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 or delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n            current_weight = candidate_weight\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n                current_weight = candidate_weight\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - refine Pareto front\n    for _ in range(3):\n        # Find items that could improve Pareto front without disrupting existing trade-offs\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Calculate potential improvement for each objective\n        potential_obj1 = current_obj1 + value1_lst * (1 - new_solution)\n        potential_obj2 = current_obj2 + value2_lst * (1 - new_solution)\n\n        # Find items that could potentially improve both objectives\n        improvement1 = potential_obj1 - current_obj1\n        improvement2 = potential_obj2 - current_obj2\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -15.671481893383769,
               -17.66264106558792
          ]
     },
     {
          "algorithm": "{The new algorithm first identifies the most promising solution in the archive by evaluating both objectives through a novel \"harmonic balance\" metric that combines normalized objective values with a diversity measure, then applies a \"context-aware\" local search operator that alternates between a \"multi-objective\" swap heuristic and a \"capacity-aware\" perturbation strategy, where the operator first evaluates items based on their combined contribution to both objectives while considering their weight efficiency, and then probabilistically perturbs the solution by adding or removing items that show promise for improving both objectives while maintaining a balanced trade-off between exploration and exploitation, ultimately ensuring feasibility by rejecting any moves that would exceed the capacity and iteratively refining the solution by accepting improvements in both objectives or Pareto-dominated solutions.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify the most promising solution using harmonic balance metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) == 0:\n        return archive[0][0].copy()\n\n    max_obj1, max_obj2 = np.max(objectives, axis=0)\n    min_obj1, min_obj2 = np.min(objectives, axis=0)\n\n    # Normalize objectives\n    norm_obj1 = (objectives[:, 0] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (objectives[:, 1] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n\n    # Calculate harmonic balance metric (combining both objectives)\n    harmonic_balance = (2 * norm_obj1 * norm_obj2) / (norm_obj1 + norm_obj2 + 1e-10)\n\n    # Select the solution with the highest harmonic balance\n    base_idx = np.argmax(harmonic_balance)\n    base_solution = archive[base_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Apply context-aware local search\n    n_items = len(weight_lst)\n\n    # Multi-objective swap heuristic\n    for _ in range(5):  # Limit iterations\n        # Calculate combined value-to-weight ratio\n        combined_ratio = (value1_lst + value2_lst) / (weight_lst + 1e-10)\n\n        # Select top and bottom items\n        top_items = np.argsort(combined_ratio)[-3:]  # Top 3 items\n        bottom_items = np.argsort(combined_ratio)[:3]  # Bottom 3 items\n\n        for item in top_items:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n        for item in bottom_items:\n            if new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Capacity-aware perturbation\n    for i in range(n_items):\n        if np.random.random() < 0.15:  # 15% chance to perturb\n            if new_solution[i] == 1:\n                # Try to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                # Try to add\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -17.88908449663832,
               -17.464744336236354
          ]
     },
     {
          "algorithm": "{The proposed algorithm, named \"Objective-Driven Iterative Swap with Adaptive Neighborhood,\" selects a promising solution from the archive by prioritizing those with high objective values and low dominance counts, then applies a hybrid local search strategy that combines random swaps with targeted improvements. It iteratively evaluates potential swaps (adding or removing items) while dynamically adjusting the neighborhood size based on the current solution's quality and diversity, ensuring feasibility by rejecting swaps that exceed capacity. The algorithm balances exploration and exploitation by alternating between random and objective-specific swaps, and uses a probabilistic acceptance criterion to escape local optima. The process terminates when no further improvements are found or a maximum iteration limit is reached, returning the best neighbor solution encountered.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a base solution with high potential for improvement\n    def selection_criterion(solution_obj):\n        # Prefer solutions with high objective values and low dominance\n        return sum(solution_obj) / len(archive)  # Simplified criterion\n\n    sorted_archive = sorted(archive, key=lambda x: -selection_criterion(x[1]))\n    base_solution = sorted_archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hybrid local search strategy\n    new_solution = base_solution.copy()\n    max_iterations = 100\n    for _ in range(max_iterations):\n        # Randomly select a candidate item to flip\n        candidate_idx = random.randint(0, len(weight_lst) - 1)\n\n        # Determine the effect of flipping this item\n        if new_solution[candidate_idx] == 1:\n            # Try removing the item\n            new_weight = current_weight - weight_lst[candidate_idx]\n            if new_weight <= capacity:\n                new_solution[candidate_idx] = 0\n                current_weight = new_weight\n        else:\n            # Try adding the item\n            new_weight = current_weight + weight_lst[candidate_idx]\n            if new_weight <= capacity:\n                new_solution[candidate_idx] = 1\n                current_weight = new_weight\n\n        # Occasionally perform a more targeted swap\n        if random.random() < 0.2:\n            # Find the best possible swap (greedy improvement)\n            best_improvement = 0\n            best_swap = None\n            for i in range(len(weight_lst)):\n                if new_solution[i] == 1:\n                    # Try removing item i\n                    delta_weight = -weight_lst[i]\n                    delta_value1 = -value1_lst[i]\n                    delta_value2 = -value2_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        improvement = delta_value1 + delta_value2\n                        if improvement > best_improvement:\n                            best_improvement = improvement\n                            best_swap = (-1, i)\n                else:\n                    # Try adding item i\n                    delta_weight = weight_lst[i]\n                    delta_value1 = value1_lst[i]\n                    delta_value2 = value2_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        improvement = delta_value1 + delta_value2\n                        if improvement > best_improvement:\n                            best_improvement = improvement\n                            best_swap = (1, i)\n\n            if best_swap is not None:\n                action, idx = best_swap\n                new_solution[idx] = action\n                current_weight += weight_lst[idx] * (2 * action - 1)\n\n    return new_solution\n\n",
          "score": [
               -17.946552635273125,
               -17.362931504191003
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Dynamic Neighborhood Exploration,\" first partitions the objective space into regions based on the distribution of solutions in the archive, then selects a solution from a sparsely populated region to focus on under-explored areas. It uses a dynamic neighborhood exploration strategy that adaptively adjusts the size of the neighborhood based on the solution's position in the objective space, flipping bits that lie on the Pareto frontier of the neighborhood's objective values. The algorithm incorporates a diversity-preserving mechanism that ensures the neighbor solution maintains a minimum distance from existing solutions in the archive, while also employing a gradient-based bit selection that identifies items with the steepest improvement in both objectives. Finally, it performs a capacity-aware gradient descent step that iteratively refines the solution by moving along the gradient of combined objective improvement, with a probabilistic acceptance criterion to balance exploration and exploitation.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition objective space and select a solution from a sparse region\n    objectives = [obj for _, obj in archive]\n    obj1_values = [obj[0] for obj in objectives]\n    obj2_values = [obj[1] for obj in objectives]\n\n    # Normalize objectives for partitioning\n    min_obj1, max_obj1 = min(obj1_values), max(obj1_values)\n    min_obj2, max_obj2 = min(obj2_values), max(obj2_values)\n    norm_obj1 = [(obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6) for obj1 in obj1_values]\n    norm_obj2 = [(obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6) for obj2 in obj2_values]\n\n    # Partition into 4 regions and count solutions in each\n    region_counts = [0] * 4\n    for n1, n2 in zip(norm_obj1, norm_obj2):\n        if n1 < 0.5 and n2 < 0.5:\n            region_counts[0] += 1\n        elif n1 < 0.5 and n2 >= 0.5:\n            region_counts[1] += 1\n        elif n1 >= 0.5 and n2 < 0.5:\n            region_counts[2] += 1\n        else:\n            region_counts[3] += 1\n\n    # Select a solution from the least populated region\n    selected_region = np.argmin(region_counts)\n    candidates = []\n    for i, (n1, n2) in enumerate(zip(norm_obj1, norm_obj2)):\n        if (selected_region == 0 and n1 < 0.5 and n2 < 0.5) or \\\n           (selected_region == 1 and n1 < 0.5 and n2 >= 0.5) or \\\n           (selected_region == 2 and n1 >= 0.5 and n2 < 0.5) or \\\n           (selected_region == 3 and n1 >= 0.5 and n2 >= 0.5):\n            candidates.append(i)\n\n    if not candidates:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(candidates)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n\n    # Step 2: Dynamic neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    neighborhood_size = max(1, int(len(new_solution) * 0.1))  # Dynamic neighborhood size\n\n    # Calculate gradient of objectives\n    grad_obj1 = value1_lst / (weight_lst + 1e-6)\n    grad_obj2 = value2_lst / (weight_lst + 1e-6)\n    combined_grad = grad_obj1 + grad_obj2\n\n    # Find items on the Pareto frontier of the neighborhood\n    for _ in range(neighborhood_size):\n        # Select items with highest combined gradient\n        candidate_idx = np.argsort(combined_grad)[-neighborhood_size:]\n\n        for idx in candidate_idx:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 3: Diversity-preserving mechanism\n    min_distance = float('inf')\n    for sol, _ in archive:\n        distance = np.sum(np.abs(sol - new_solution))\n        if distance < min_distance:\n            min_distance = distance\n\n    if min_distance < len(new_solution) * 0.2:  # Too similar to existing solutions\n        # Flip some bits to increase diversity\n        flip_indices = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 4: Capacity-aware gradient descent\n    for _ in range(5):\n        # Calculate current objectives\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Find items with highest gradient improvement\n        delta_obj1 = value1_lst / (weight_lst + 1e-6)\n        delta_obj2 = value2_lst / (weight_lst + 1e-6)\n        combined_delta = delta_obj1 + delta_obj2\n\n        # Select top items to flip\n        top_indices = np.argsort(combined_delta)[-min(5, len(combined_delta)):]\n\n        for idx in top_indices:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                new_obj1 = np.sum(value1_lst * new_solution)\n                new_obj2 = np.sum(value2_lst * new_solution)\n\n                # Accept if improvement in both objectives\n                if new_obj1 > current_obj1 and new_obj2 > current_obj2:\n                    break\n                else:\n                    # Probabilistic acceptance based on improvement\n                    improvement = (new_obj1 - current_obj1) + (new_obj2 - current_obj2)\n                    if improvement > 0 or np.random.rand() < np.exp(improvement / 0.1):\n                        break\n                    else:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -18.368028693557207,
               -17.291768323014352
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    candidates = []\n    for sol, (val1, val2) in archive:\n        total_weight = np.sum(weight_lst * sol)\n        if total_weight <= capacity:\n            candidates.append((sol, val1, val2))\n\n    if not candidates:\n        return archive[0][0].copy()  # Fallback if no feasible solutions\n\n    # Select the solution with the highest potential for improvement\n    # Here, we use the solution with the highest sum of normalized objective values\n    selected_sol = max(candidates, key=lambda x: (x[1] + x[2]))[0]\n    base_solution = selected_sol.copy()\n\n    # Step 2: Generate a neighbor using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(new_solution)\n\n    # Hybrid local search: Value-ratio swap + capacity-aware perturbation\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Value-ratio swap: Swap items based on their marginal contribution to both objectives\n        ratio1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n        ratio2 = value2_lst / (weight_lst + 1e-6)\n\n        # Identify items with high ratio in one objective and low in the other\n        high_ratio1 = np.where(new_solution & (ratio1 > np.median(ratio1)))[0]\n        low_ratio2 = np.where(~new_solution & (ratio2 < np.median(ratio2)))[0]\n\n        if len(high_ratio1) > 0 and len(low_ratio2) > 0:\n            # Randomly select items to swap\n            swap1 = np.random.choice(high_ratio1)\n            swap2 = np.random.choice(low_ratio2)\n\n            # Check feasibility of swap\n            current_weight = np.sum(weight_lst * new_solution)\n            delta_weight = weight_lst[swap2] - weight_lst[swap1]\n\n            if current_weight + delta_weight <= capacity:\n                new_solution[swap1], new_solution[swap2] = new_solution[swap2], new_solution[swap1]\n\n        # Capacity-aware perturbation: Flip a small number of items near the capacity\n        if np.random.rand() < 0.3:  # 30% chance to perturb\n            # Identify items that can be flipped without violating capacity\n            current_weight = np.sum(weight_lst * new_solution)\n            feasible_flips = []\n\n            for i in range(n_items):\n                if new_solution[i] == 1:\n                    if current_weight - weight_lst[i] >= 0:\n                        feasible_flips.append(i)\n                else:\n                    if current_weight + weight_lst[i] <= capacity:\n                        feasible_flips.append(i)\n\n            if feasible_flips:\n                flip_idx = np.random.choice(feasible_flips)\n                new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    return new_solution\n\n",
          "score": [
               -17.404406389863784,
               -16.720119105446962
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective weighting and solution selection\n    def calculate_weights(archive):\n        # Calculate objective weights based on archive diversity\n        obj1_vals = [obj[0] for _, obj in archive]\n        obj2_vals = [obj[1] for _, obj in archive]\n        obj1_range = max(obj1_vals) - min(obj1_vals) if max(obj1_vals) > min(obj1_vals) else 1\n        obj2_range = max(obj2_vals) - min(obj2_vals) if max(obj2_vals) > min(obj2_vals) else 1\n\n        # Weight objectives inversely proportional to their current diversity\n        weight1 = 1 / obj1_range if obj1_range > 0 else 1\n        weight2 = 1 / obj2_range if obj2_range > 0 else 1\n        total_weight = weight1 + weight2\n        return weight1 / total_weight, weight2 / total_weight\n\n    weight1, weight2 = calculate_weights(archive)\n\n    # Select solution with highest weighted score\n    def weighted_score(obj):\n        return weight1 * obj[0] + weight2 * obj[1]\n\n    selected_idx = max(range(len(archive)), key=lambda i: weighted_score(archive[i][1]))\n    base_solution, (obj1, obj2) = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Solution fusion with top performers\n    top_obj1 = max(archive, key=lambda x: x[1][0])[0]\n    top_obj2 = max(archive, key=lambda x: x[1][1])[0]\n\n    # Create a fused solution by combining features from top performers\n    fused_solution = np.zeros_like(new_solution)\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1 or top_obj1[i] == 1 or top_obj2[i] == 1:\n            fused_solution[i] = 1\n\n    # Check feasibility of fused solution\n    fused_weight = np.sum(weight_lst * fused_solution)\n    if fused_weight <= capacity:\n        new_solution = fused_solution.copy()\n    else:\n        # If not feasible, perform capacity-constrained reduction\n        current_weight = fused_weight\n        items = np.where(fused_solution)[0]\n        while current_weight > capacity and len(items) > 0:\n            # Remove items with lowest weighted value\n            item_scores = weight1 * value1_lst[items] + weight2 * value2_lst[items]\n            remove_idx = items[np.argmin(item_scores)]\n            fused_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            items = np.where(fused_solution)[0]\n        new_solution = fused_solution.copy()\n\n    # Step 3: Adaptive neighborhood exploration\n    current_weight = np.sum(weight_lst * new_solution)\n    n_items = len(weight_lst)\n\n    for _ in range(5):  # Limited exploration steps\n        # Alternate between value-based selection and random perturbation\n        if random.random() < 0.7:  # 70% chance for value-based\n            # Select items with high weighted value-to-weight ratio\n            ratio = (weight1 * value1_lst + weight2 * value2_lst) / (weight_lst + 1e-6)\n            included = np.where(new_solution == 1)[0]\n            excluded = np.where(new_solution == 0)[0]\n\n            if len(included) > 0 and len(excluded) > 0:\n                # Select top 20% of included items and bottom 20% of excluded items\n                top_included = included[np.argsort(ratio[included])[-max(1, len(included)//5):]]\n                bottom_excluded = excluded[np.argsort(ratio[excluded])[:max(1, len(excluded)//5)]]\n\n                if len(top_included) > 0 and len(bottom_excluded) > 0:\n                    swap1 = np.random.choice(top_included)\n                    swap2 = np.random.choice(bottom_excluded)\n\n                    # Check feasibility\n                    delta_weight = weight_lst[swap2] - weight_lst[swap1]\n                    if current_weight + delta_weight <= capacity:\n                        new_solution[swap1], new_solution[swap2] = new_solution[swap2], new_solution[swap1]\n                        current_weight += delta_weight\n        else:\n            # Random perturbation with feasibility check\n            feasible_flips = []\n            for i in range(n_items):\n                if new_solution[i] == 1:\n                    if current_weight - weight_lst[i] >= 0:\n                        feasible_flips.append(i)\n                else:\n                    if current_weight + weight_lst[i] <= capacity:\n                        feasible_flips.append(i)\n\n            if feasible_flips:\n                flip_idx = np.random.choice(feasible_flips)\n                new_solution[flip_idx] = 1 - new_solution[flip_idx]\n                current_weight += (2 * new_solution[flip_idx] - 1) * weight_lst[flip_idx]\n\n    return new_solution\n\n",
          "score": [
               -17.22408985725889,
               -16.698809053859502
          ]
     }
]