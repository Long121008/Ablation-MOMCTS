[
     {
          "algorithm": "{The proposed algorithm, \"Objective-Driven Multi-Phase Exploration with Adaptive Constraint Handling,\" begins by identifying the least explored region of the objective space and selecting a solution from it to balance exploration and exploitation. It then employs a multi-phase approach that first performs a greedy improvement phase to maximize one objective while maintaining feasibility, followed by a constraint-adaptive phase that refines the solution by selectively flipping items based on their impact on both objectives and the remaining capacity. The algorithm dynamically adjusts its exploration strategy based on the current solution's position in the objective space, using a combination of value-to-weight ratios, marginal improvements, and capacity-aware perturbations. After each phase, it performs a feasibility check and objective evaluation, reverting any infeasible changes. The process iterates through these phases with decreasing intensity, culminating in a final refinement step that ensures the solution lies on or near the Pareto front by selectively adding items that provide the best combined improvement in both objectives without exceeding capacity.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify least explored region in objective space\n    def partition_archive(archive):\n        if len(archive) < 4:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 4 partitions\n        split1 = len(sorted_archive) // 4\n        split2 = len(sorted_archive) // 2\n        split3 = 3 * len(sorted_archive) // 4\n        return [sorted_archive[:split1], sorted_archive[split1:split2],\n                sorted_archive[split2:split3], sorted_archive[split3:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Multi-phase exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Phase 1: Greedy improvement for one objective\n    for _ in range(3):\n        # Randomly select an objective to improve\n        if random.random() < 0.5:\n            # Improve objective 1\n            marginal1 = value1_lst / (weight_lst + 1e-6)\n            candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n        else:\n            # Improve objective 2\n            marginal2 = value2_lst / (weight_lst + 1e-6)\n            candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Phase 2: Constraint-adaptive refinement\n    for _ in range(5):\n        # Calculate potential improvements for both objectives\n        potential_obj1 = np.sum(value1_lst * new_solution) + value1_lst * (1 - new_solution)\n        potential_obj2 = np.sum(value2_lst * new_solution) + value2_lst * (1 - new_solution)\n\n        # Find items that could improve both objectives\n        improvement1 = potential_obj1 - np.sum(value1_lst * new_solution)\n        improvement2 = potential_obj2 - np.sum(value2_lst * new_solution)\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    # Phase 3: Capacity-aware perturbations\n    for _ in range(3):\n        # Randomly select items to flip based on capacity\n        idx = random.randint(0, len(new_solution)-1)\n        if new_solution[idx] == 0:\n            if weight_lst[idx] <= available_capacity:\n                new_solution[idx] = 1\n                available_capacity -= weight_lst[idx]\n        else:\n            new_solution[idx] = 0\n            available_capacity += weight_lst[idx]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    for _ in range(2):\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_idx = np.argmax(potential_improvement)\n\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -16.54517292621003,
               -18.782324216935123
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    base_solution = max(archive, key=lambda x: (x[1][0] + x[1][1]))[0].copy()\n\n    # Calculate current total weight and values\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Create a candidate solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(base_solution), size=min(3, len(base_solution)), replace=False)\n\n    for idx in flip_indices:\n        # 2. Flip the item if it improves both objectives or weight allows\n        if new_solution[idx] == 1:\n            # Try to remove the item if it's not critical for weight\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item if it fits in capacity\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # 3. Additional improvement: swap two items if it improves both objectives\n    if len(base_solution) >= 2:\n        i, j = np.random.choice(len(base_solution), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            # Calculate potential new weight and values\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -18.586423720786442,
               -18.76328847454988
          ]
     },
     {
          "algorithm": "{The new algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.980831889190338,
               -18.713690870174126
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.138367575930815,
               -18.57116772233012
          ]
     },
     {
          "algorithm": "{This novel algorithm, \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion,\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.190860660864438,
               -18.478275658404645
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.16981140357512,
               -18.519434126196355
          ]
     },
     {
          "algorithm": "{The new algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objective values\n    normalized_values = []\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        normalized_values.append((norm_v1 + norm_v2, sol))\n\n    selected_sol = max(normalized_values, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel local search operator called \"Objective-Specific Greedy Swap\"\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Identify items that are critical for objective 1 but not objective 2\n    critical1 = (value1_lst > 0.75 * np.max(value1_lst)) & (value2_lst < 0.25 * np.max(value2_lst))\n    # Identify items that are critical for objective 2 but not objective 1\n    critical2 = (value2_lst > 0.75 * np.max(value2_lst)) & (value1_lst < 0.25 * np.max(value1_lst))\n\n    # Perform swaps between critical items of different objectives\n    for i in np.where(critical1 & (new_solution == 1))[0]:\n        for j in np.where(critical2 & (new_solution == 0))[0]:\n            if (current_weight - weight_lst[i] + weight_lst[j] <= capacity and\n                np.sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j] > np.sum(value1_lst * new_solution) and\n                np.sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j] > np.sum(value2_lst * new_solution)):\n                new_solution[i] = 0\n                new_solution[j] = 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Step 3: Apply a \"Dual-Objective Guided Perturbation\" that considers both objectives\n    combined_utility = (utility1 + utility2) / 2\n\n    # Find items to potentially add\n    candidate_add = np.where(new_solution == 0)[0]\n    if len(candidate_add) > 0:\n        sorted_add = sorted(candidate_add, key=lambda x: combined_utility[x], reverse=True)\n        for item in sorted_add:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Find items to potentially remove\n    candidate_remove = np.where(new_solution == 1)[0]\n    if len(candidate_remove) > 0:\n        sorted_remove = sorted(candidate_remove, key=lambda x: combined_utility[x])\n        for item in sorted_remove:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 4: Apply a \"Feasibility-Preserving Random Walk\" that ensures the solution remains feasible\n    for _ in range(5):\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if (current_weight + weight_lst[item] <= capacity and\n                weight_lst[item] < 0.5 * capacity):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.159521414364384,
               -18.522447961026298
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply capacity-aware perturbation mechanism\n    for _ in range(3):\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = marginal1 + marginal2\n\n        # Select items to flip based on capacity and marginal contribution\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            # Sort by combined marginal contribution (descending)\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            # Sort by combined marginal contribution (ascending)\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Multi-phase refinement with dynamic intensity\n    phases = 5\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases  # Decreasing intensity\n\n        # Phase 1: Greedy value maximization in most promising objective\n        if random.random() < phase_intensity:\n            if random.random() < 0.5:\n                # Maximize objective 1\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                # Maximize objective 2\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-preserving random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n    return new_solution\n\n",
          "score": [
               -19.115691115587616,
               -18.692488494440948
          ]
     },
     {
          "algorithm": "{The novel \"Objective-Driven Adaptive Perturbation with Dynamic Objective Fusion and Capacity-Aware Refinement\" algorithm begins by selecting a solution from the archive using a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a multi-tiered perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a tiered refinement process that alternates between greedy value maximization in the most promising objective, capacity-preserving random perturbations, and objective-specific swaps, with each tier dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-tiered perturbation mechanism\n    for tier in range(3):\n        # Tier 1: Capacity-aware marginal contribution flips\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = marginal1 + marginal2\n\n        # Select items to flip based on capacity and marginal contribution\n        if tier == 0:\n            # Add items with highest combined marginal\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items[:2]:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n        elif tier == 1:\n            # Remove items with lowest combined marginal\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n                for item in sorted_items[:2]:\n                    if current_weight - weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        current_weight -= weight_lst[item]\n        else:\n            # Random swaps between items\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n    # Step 3: Tiered refinement process\n    for tier in range(3):\n        if tier == 0:\n            # Greedy value maximization in most promising objective\n            if random.random() < 0.7:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n        elif tier == 1:\n            # Capacity-preserving random perturbations\n            if random.random() < 0.5:\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n        else:\n            # Objective-specific swaps\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -19.16331519580946,
               -18.520615658923113
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply capacity-aware perturbation mechanism\n    for _ in range(3):\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = marginal1 + marginal2\n\n        # Select items to flip based on capacity and marginal contribution\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            # Sort by combined marginal contribution (descending)\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            # Sort by combined marginal contribution (ascending)\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Multi-phase refinement with dynamic intensity\n    phases = 5\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases  # Decreasing intensity\n\n        # Phase 1: Greedy value maximization in most promising objective\n        if random.random() < phase_intensity:\n            if random.random() < 0.5:\n                # Maximize objective 1\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                # Maximize objective 2\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-preserving random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n    return new_solution\n\n",
          "score": [
               -19.346472092697173,
               -18.459658126339118
          ]
     }
]