[
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    base_solution = max(archive, key=lambda x: (x[1][0] + x[1][1]))[0].copy()\n\n    # Calculate current total weight and values\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Create a candidate solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(base_solution), size=min(3, len(base_solution)), replace=False)\n\n    for idx in flip_indices:\n        # 2. Flip the item if it improves both objectives or weight allows\n        if new_solution[idx] == 1:\n            # Try to remove the item if it's not critical for weight\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item if it fits in capacity\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # 3. Additional improvement: swap two items if it improves both objectives\n    if len(base_solution) >= 2:\n        i, j = np.random.choice(len(base_solution), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            # Calculate potential new weight and values\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -18.586423720786442,
               -18.76328847454988
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify the solution with the best combined objective value\n    def combined_score(obj):\n        return obj[0] + obj[1]\n\n    selected_idx = max(range(len(archive)), key=lambda i: combined_score(archive[i][1]))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Dynamic neighborhood exploration with value-to-weight ratio\n    current_weight = np.sum(weight_lst * new_solution)\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / (weight_lst + 1e-6)\n    ratio2 = value2_lst / (weight_lst + 1e-6)\n\n    # Alternate between both objectives in each iteration\n    for _ in range(10):\n        # Randomly select an objective to focus on\n        if random.random() < 0.5:\n            # Focus on objective 1\n            ratios = ratio1\n            values = value1_lst\n        else:\n            # Focus on objective 2\n            ratios = ratio2\n            values = value2_lst\n\n        # Find items with highest value-to-weight ratio that can be added\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by value-to-weight ratio in descending order\n        sorted_items = sorted(candidate_items, key=lambda x: ratios[x], reverse=True)\n\n        # Try to add items with highest ratio first\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n        # Find items with lowest value-to-weight ratio that can be removed\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by value-to-weight ratio in ascending order\n        sorted_items = sorted(candidate_items, key=lambda x: ratios[x])\n\n        # Try to remove items with lowest ratio first\n        for item in sorted_items:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 3: Random perturbation with feasibility check\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, n_items - 1)\n\n        if new_solution[item] == 1:\n            # Try to remove the item\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Try to add the item\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.757949963680435,
               -18.267274884773002
          ]
     },
     {
          "algorithm": "{The proposed algorithm, \"Objective-Driven Multi-Phase Exploration with Adaptive Constraint Handling,\" begins by identifying the least explored region of the objective space and selecting a solution from it to balance exploration and exploitation. It then employs a multi-phase approach that first performs a greedy improvement phase to maximize one objective while maintaining feasibility, followed by a constraint-adaptive phase that refines the solution by selectively flipping items based on their impact on both objectives and the remaining capacity. The algorithm dynamically adjusts its exploration strategy based on the current solution's position in the objective space, using a combination of value-to-weight ratios, marginal improvements, and capacity-aware perturbations. After each phase, it performs a feasibility check and objective evaluation, reverting any infeasible changes. The process iterates through these phases with decreasing intensity, culminating in a final refinement step that ensures the solution lies on or near the Pareto front by selectively adding items that provide the best combined improvement in both objectives without exceeding capacity.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify least explored region in objective space\n    def partition_archive(archive):\n        if len(archive) < 4:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 4 partitions\n        split1 = len(sorted_archive) // 4\n        split2 = len(sorted_archive) // 2\n        split3 = 3 * len(sorted_archive) // 4\n        return [sorted_archive[:split1], sorted_archive[split1:split2],\n                sorted_archive[split2:split3], sorted_archive[split3:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Multi-phase exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Phase 1: Greedy improvement for one objective\n    for _ in range(3):\n        # Randomly select an objective to improve\n        if random.random() < 0.5:\n            # Improve objective 1\n            marginal1 = value1_lst / (weight_lst + 1e-6)\n            candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n        else:\n            # Improve objective 2\n            marginal2 = value2_lst / (weight_lst + 1e-6)\n            candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Phase 2: Constraint-adaptive refinement\n    for _ in range(5):\n        # Calculate potential improvements for both objectives\n        potential_obj1 = np.sum(value1_lst * new_solution) + value1_lst * (1 - new_solution)\n        potential_obj2 = np.sum(value2_lst * new_solution) + value2_lst * (1 - new_solution)\n\n        # Find items that could improve both objectives\n        improvement1 = potential_obj1 - np.sum(value1_lst * new_solution)\n        improvement2 = potential_obj2 - np.sum(value2_lst * new_solution)\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    # Phase 3: Capacity-aware perturbations\n    for _ in range(3):\n        # Randomly select items to flip based on capacity\n        idx = random.randint(0, len(new_solution)-1)\n        if new_solution[idx] == 0:\n            if weight_lst[idx] <= available_capacity:\n                new_solution[idx] = 1\n                available_capacity -= weight_lst[idx]\n        else:\n            new_solution[idx] = 0\n            available_capacity += weight_lst[idx]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    for _ in range(2):\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_idx = np.argmax(potential_improvement)\n\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -16.54517292621003,
               -18.782324216935123
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    # Calculate improvement potential based on normalized objective values\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    def improvement_potential(sol_obj):\n        v1, v2 = sol_obj\n        norm_v1 = v1 / max_v1 if max_v1 > 0 else 0\n        norm_v2 = v2 / max_v2 if max_v2 > 0 else 0\n        return (1 - norm_v1) * (1 - norm_v2)  # High potential when either objective is far from max\n\n    selected_idx = max(range(len(archive)), key=lambda i: improvement_potential(archive[i][1]))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic objective balancing and item selection\n    # Calculate utility scores for both objectives\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n\n    # Alternate between objectives in each iteration\n    for _ in range(10):\n        # Randomly select an objective to focus on\n        if random.random() < 0.5:\n            # Focus on objective 1\n            utility = utility1\n            values = value1_lst\n        else:\n            # Focus on objective 2\n            utility = utility2\n            values = value2_lst\n\n        # Find items with highest utility that can be added\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by utility in descending order\n        sorted_items = sorted(candidate_items, key=lambda x: utility[x], reverse=True)\n\n        # Try to add items with highest utility first\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n        # Find items with lowest utility that can be removed\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) == 0:\n            break\n\n        # Sort candidates by utility in ascending order\n        sorted_items = sorted(candidate_items, key=lambda x: utility[x])\n\n        # Try to remove items with lowest utility first\n        for item in sorted_items:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Step 3: Intelligent random perturbations with objective balancing\n    for _ in range(5):\n        # Select a random item to flip\n        item = random.randint(0, len(weight_lst) - 1)\n\n        if new_solution[item] == 1:\n            # Remove item if it's not critical for either objective\n            if (current_weight - weight_lst[item] >= 0 and\n                (value1_lst[item] < np.median(value1_lst) or value2_lst[item] < np.median(value2_lst))):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            # Add item if it improves at least one objective significantly\n            if (current_weight + weight_lst[item] <= capacity and\n                (value1_lst[item] > np.median(value1_lst) or value2_lst[item] > np.median(value2_lst))):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.93891881164476,
               -18.19800975992053
          ]
     },
     {
          "algorithm": "{The proposed algorithm, named \"Dual-Objective Guided Evolutionary Perturbation,\" first identifies the most balanced solution in the archive by selecting the one with the highest harmonic mean of its objective values, ensuring it represents a well-compromised trade-off between both objectives. It then applies a novel evolutionary perturbation strategy that combines random item flips with objective-specific mutations, where each flip has a probability inversely proportional to the item's contribution to either objective. The algorithm dynamically adjusts the mutation intensity based on the current solution's dominance status in the archive, performing more aggressive perturbations for non-dominated solutions and more conservative changes for dominated ones. After each perturbation, it performs a feasibility check and objective evaluation, reverting any infeasible changes. The process iterates for a fixed number of steps, with each iteration potentially introducing small, targeted improvements that collectively lead to significant multi-objective improvements.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the most balanced solution based on harmonic mean\n    def harmonic_mean(obj):\n        v1, v2 = obj\n        if v1 == 0 or v2 == 0:\n            return 0\n        return 2 * v1 * v2 / (v1 + v2)\n\n    base_solution, base_obj = max(archive, key=lambda x: harmonic_mean(x[1]))\n    base_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic perturbation intensity based on dominance status\n    is_dominated = any(\n        (s_obj[0] >= base_obj[0] and s_obj[1] >= base_obj[1] and (s_obj[0] > base_obj[0] or s_obj[1] > base_obj[1]))\n        for s, s_obj in archive\n    )\n    perturbation_intensity = 0.3 if is_dominated else 0.5\n\n    # Perform evolutionary perturbations\n    for _ in range(min(10, n_items)):\n        if random.random() < perturbation_intensity:\n            # Select item with probability inversely proportional to its objective contribution\n            item_probs = []\n            for i in range(n_items):\n                contrib1 = value1_lst[i] if new_solution[i] else 0\n                contrib2 = value2_lst[i] if new_solution[i] else 0\n                total_contrib = contrib1 + contrib2\n                item_probs.append(1.0 / (1.0 + total_contrib) if total_contrib > 0 else 1.0)\n\n            item_probs = np.array(item_probs) / np.sum(item_probs)\n            idx = np.random.choice(n_items, p=item_probs)\n\n            # Flip the item with probability based on its contribution\n            if new_solution[idx] == 1:\n                # More likely to remove if high contribution\n                remove_prob = 0.7 * (value1_lst[idx] + value2_lst[idx]) / (np.max(value1_lst) + np.max(value2_lst))\n                if random.random() < remove_prob:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n            else:\n                # More likely to add if low contribution\n                add_prob = 0.7 * (1 - (value1_lst[idx] + value2_lst[idx]) / (np.max(value1_lst) + np.max(value2_lst)))\n                if random.random() < add_prob:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -18.73294407180715,
               -18.247791513237175
          ]
     },
     {
          "algorithm": "{The heuristic function 'select_neighbor' first identifies promising solutions in the archive by evaluating their potential for improvement using a combination of objective dominance, crowding distance, and solution diversity metrics. It then intelligently selects a base solution from these candidates using a weighted random selection that prioritizes solutions with higher crowding distances and lower dominance counts. The local search operator employs a hybrid strategy that combines item swapping, random flipping, and adaptive perturbation to explore the neighborhood, ensuring feasibility by dynamically adjusting the selection of items to flip based on the remaining capacity. The operator also incorporates a memory mechanism to avoid revisiting recently explored solutions, further enhancing exploration. The function returns the new neighbor solution after validating its feasibility and updating the archive with the new solution if it is non-dominated.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions in the archive for potential improvement\n    solutions = [sol for sol, _ in archive]\n    objectives = [obj for _, obj in archive]\n\n    # Calculate dominance counts and crowding distances\n    dominance_counts = [0] * len(archive)\n    crowding_distances = [0.0] * len(archive)\n\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if objectives[i][0] <= objectives[j][0] and objectives[i][1] <= objectives[j][1]:\n                    dominance_counts[i] += 1\n                if objectives[i][0] < objectives[j][0] and objectives[i][1] < objectives[j][1]:\n                    dominance_counts[i] += 1\n\n    # Sort solutions by dominance count (lower is better)\n    sorted_indices = sorted(range(len(archive)), key=lambda k: dominance_counts[k])\n    candidate_indices = sorted_indices[:max(1, len(archive) // 2)]\n\n    # Step 2: Select a base solution with weighted randomness\n    weights = [1.0 / (1.0 + dominance_counts[i]) for i in candidate_indices]\n    selected_idx = random.choices(candidate_indices, weights=weights, k=1)[0]\n    base_solution = solutions[selected_idx].copy()\n\n    # Step 3: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Hybrid local search operator\n    # 1. Randomly select a subset of items to flip\n    flip_indices = random.sample(range(n_items), min(3, n_items // 2))\n\n    # 2. Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # 3. Flip items while maintaining feasibility\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove item if possible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add item if possible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # 4. Additional perturbation: swap two items if feasible\n    if len(new_solution) >= 2:\n        i, j = random.sample(range(n_items), 2)\n        if new_solution[i] != new_solution[j]:\n            # Calculate weight difference\n            weight_diff = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n            if current_weight + weight_diff <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Ensure feasibility (shouldn't be necessary but as a safeguard)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        # If not feasible, try to remove items randomly until feasible\n        while total_weight > capacity:\n            items_in = np.where(new_solution == 1)[0]\n            if len(items_in) == 0:\n                break\n            remove_idx = random.choice(items_in)\n            new_solution[remove_idx] = 0\n            total_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
          "score": [
               -18.783291405738993,
               -18.18577865487048
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" first evaluates the archive to identify the most diverse solution using a combination of crowding distance and objective dominance, then applies a dynamic objective weighting mechanism to prioritize exploration of under-represented regions of the Pareto front, followed by a solution fusion process that combines features from top-performing solutions in both objectives, while maintaining feasibility through a capacity-constrained item selection strategy that alternates between greedy value maximization and random perturbations, and finally incorporates a self-adaptive neighborhood pruning mechanism that removes redundant items while preserving solution quality, all within a controlled exploration-exploitation balance framework.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify the solution with the highest combined normalized objective value\n    def normalized_score(obj):\n        max_v1 = max(x[1][0] for x in archive) if archive else 1.0\n        max_v2 = max(x[1][1] for x in archive) if archive else 1.0\n        return (obj[0] / max_v1 + obj[1] / max_v2) / 2\n\n    selected_idx = max(range(len(archive)), key=lambda i: normalized_score(archive[i][1]))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic objective prioritization with adaptive flipping\n    objective_weights = [0.5, 0.5]  # Start with equal weights\n    n_items = len(weight_lst)\n\n    for _ in range(15):\n        # Update objective weights based on current solution's performance\n        v1_ratio = (value1_lst * new_solution).sum() / (value1_lst.sum() + 1e-6)\n        v2_ratio = (value2_lst * new_solution).sum() / (value2_lst.sum() + 1e-6)\n        objective_weights[0] = 0.7 * v1_ratio + 0.3 * (1 - v2_ratio)\n        objective_weights[1] = 0.7 * v2_ratio + 0.3 * (1 - v1_ratio)\n\n        # Select an objective to focus on\n        if random.random() < objective_weights[0]:\n            # Focus on objective 1\n            values = value1_lst\n        else:\n            # Focus on objective 2\n            values = value2_lst\n\n        # Find items to flip based on value and current weight\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            # Try to add items with highest value first\n            sorted_items = sorted(candidate_items, key=lambda x: -values[x])\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            # Try to remove items with lowest value first\n            sorted_items = sorted(candidate_items, key=lambda x: values[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Randomized diversification with feasibility check\n    for _ in range(3):\n        item = random.randint(0, n_items - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.58522813983625,
               -17.855659505420803
          ]
     },
     {
          "algorithm": "{The algorithm first identifies promising solutions in the archive by selecting those with high objective values or those that are on the Pareto front, then intelligently samples a base solution from this subset. It then applies a hybrid local search strategy that combines a novel \"swap-and-flip\" operator with a probabilistic \"value-weighted\" perturbation to generate a neighbor solution. The swap-and-flip operator selects two items, swaps their inclusion statuses, and then flips the status of a third item based on a value-to-weight ratio heuristic, while the probabilistic perturbation introduces randomness to escape local optima. The algorithm ensures feasibility by rejecting any moves that would exceed the capacity, and it iteratively refines the solution by accepting improvements in either objective or Pareto-dominated solutions, ultimately returning a high-quality neighbor solution that balances exploration and exploitation of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify promising solutions (high objective values or on Pareto front)\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    # If no Pareto front, select top 20% by sum of objectives\n    if not pareto_front:\n        sorted_indices = np.argsort(-objectives.sum(axis=1))\n        candidate_indices = sorted_indices[:max(1, len(archive) // 5)]\n    else:\n        candidate_indices = pareto_front\n\n    # Randomly select a base solution from candidates\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Swap-and-flip operator\n    if n_items >= 3:\n        # Select two items to swap\n        swap_indices = np.random.choice(n_items, size=2, replace=False)\n        # Flip a third item based on value-to-weight ratio\n        flip_idx = np.random.choice(n_items)\n        while flip_idx in swap_indices:\n            flip_idx = np.random.choice(n_items)\n\n        # Apply swap\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n        # Apply flip with probability based on value-to-weight ratio\n        v1_ratio = value1_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        v2_ratio = value2_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        flip_prob = min(1.0, (v1_ratio + v2_ratio) / 10)  # Normalize\n\n        if np.random.random() < flip_prob:\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Probabilistic perturbation\n    for i in range(n_items):\n        if np.random.random() < 0.1:  # 10% chance to perturb\n            if new_solution[i] == 1:\n                # Try to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                # Try to add\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Remove random items until feasible\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -18.525469470071368,
               -17.447019179654852
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Adaptive Neighborhood Exploration,\" first partitions the archive into regions based on the objective space, then selects a solution from a less explored region to balance exploration and exploitation. It generates a neighbor solution by adaptively exploring different neighborhoods (value-based, weight-based, and dominance-based) based on the solution's position in the objective space, using a combination of greedy improvement, random perturbation, and simulated annealing with a dynamic temperature schedule. The algorithm ensures feasibility by maintaining a running total weight and only allowing moves that keep the solution within capacity, while also incorporating a post-optimization step that refines the solution by selectively flipping bits that improve the Pareto front without disrupting the existing trade-offs.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition the archive based on objective space\n    def partition_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 3 partitions\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Generate neighbor using adaptive neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Determine neighborhood type based on solution's position in objective space\n    # Normalize objectives\n    all_objs = np.array([obj for _, obj in archive])\n    if len(all_objs) > 0:\n        max_obj1, max_obj2 = np.max(all_objs, axis=0)\n        min_obj1, min_obj2 = np.min(all_objs, axis=0)\n        norm_obj1 = (obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6)\n        norm_obj2 = (obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6)\n\n        # Classify solution based on normalized objectives\n        if norm_obj1 > 0.7 and norm_obj2 > 0.7:  # High in both objectives\n            neighborhood_type = 'dominance'  # Focus on improving Pareto front\n        elif norm_obj1 > 0.6 or norm_obj2 > 0.6:  # Moderate in one objective\n            neighborhood_type = 'value_based'  # Focus on improving specific objective\n        else:\n            neighborhood_type = 'weight_based'  # Focus on capacity management\n    else:\n        neighborhood_type = 'value_based'\n\n    # Apply selected neighborhood exploration\n    if neighborhood_type == 'value_based':\n        # Value-based neighborhood: Flip items that improve the most in one objective\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        for _ in range(5):\n            # Randomly select an objective to improve\n            if random.random() < 0.5:\n                # Improve objective 1\n                candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n            else:\n                # Improve objective 2\n                candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n\n    elif neighborhood_type == 'weight_based':\n        # Weight-based neighborhood: Flip items that help manage capacity\n        for _ in range(5):\n            # Find items that can be added without exceeding capacity\n            feasible_add = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(feasible_add) > 0:\n                # Add the item with highest value-to-weight ratio\n                ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n                candidate_idx = feasible_add[np.argmax(ratios[feasible_add])]\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    else:  # dominance neighborhood\n        # Dominance-based neighborhood: Flip items that could improve Pareto front\n        for _ in range(5):\n            # Find items that could potentially improve both objectives\n            potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n            candidate_idx = np.argmax(potential_improvement)\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Step 3: Simulated annealing with dynamic temperature\n    temp = 1.0\n    cooling_rate = 0.95\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 or delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n            current_weight = candidate_weight\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n                current_weight = candidate_weight\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - refine Pareto front\n    for _ in range(3):\n        # Find items that could improve Pareto front without disrupting existing trade-offs\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Calculate potential improvement for each objective\n        potential_obj1 = current_obj1 + value1_lst * (1 - new_solution)\n        potential_obj2 = current_obj2 + value2_lst * (1 - new_solution)\n\n        # Find items that could potentially improve both objectives\n        improvement1 = potential_obj1 - current_obj1\n        improvement2 = potential_obj2 - current_obj2\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -15.671481893383769,
               -17.66264106558792
          ]
     },
     {
          "algorithm": "{The common backbone idea in the provided algorithms is selecting promising solutions from the archive and applying intelligent local search strategies to generate high-quality neighbor solutions. The new algorithm, \"Objective-Driven Dynamic Neighborhood Exploration with Adaptive Perturbation,\" first identifies solutions with high potential by analyzing their objective values and diversity in the archive. It then dynamically selects between three specialized neighborhood structures (value-biased, weight-sensitive, and Pareto-aware) based on the solution's position in the objective space and its historical improvement trajectory. The algorithm generates neighbors by combining targeted perturbations with adaptive bit-flipping probabilities that consider both immediate objective improvements and long-term search trends, while ensuring feasibility through a capacity-aware validation mechanism. It also incorporates a feedback loop that adjusts the neighborhood selection probabilities based on recent search performance, balancing exploration and exploitation in a self-tuning manner.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify solutions with high potential\n    objectives = np.array([obj for _, obj in archive])\n    normalized_objs = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    diversity_scores = np.sum(normalized_objs, axis=1) * (1 - np.abs(normalized_objs[:,0] - normalized_objs[:,1]))\n    candidate_indices = np.argsort(-diversity_scores)[:max(3, len(archive)//4)]\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic neighborhood selection\n    obj1, obj2 = archive[base_idx][1]\n    norm_obj1 = (obj1 - objectives[:,0].min()) / (objectives[:,0].max() - objectives[:,0].min() + 1e-6)\n    norm_obj2 = (obj2 - objectives[:,1].min()) / (objectives[:,1].max() - objectives[:,1].min() + 1e-6)\n\n    # Determine neighborhood type\n    if norm_obj1 > 0.7 and norm_obj2 > 0.7:\n        neighborhood_type = 'pareto_aware'\n    elif norm_obj1 > norm_obj2:\n        neighborhood_type = 'value1_biased'\n    elif norm_obj2 > norm_obj1:\n        neighborhood_type = 'value2_biased'\n    else:\n        neighborhood_type = 'weight_sensitive'\n\n    # Step 3: Generate neighbor with adaptive perturbation\n    new_solution = base_solution.copy()\n    available_capacity = capacity - current_weight\n\n    if neighborhood_type == 'value1_biased':\n        # Focus on improving value1 with controlled value2 impact\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        combined_scores = v1_ratios * (1 + 0.3 * v2_ratios)\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    elif neighborhood_type == 'value2_biased':\n        # Focus on improving value2 with controlled value1 impact\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        combined_scores = v2_ratios * (1 + 0.3 * v1_ratios)\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    elif neighborhood_type == 'weight_sensitive':\n        # Focus on efficient weight usage\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        combined_scores = (v1_ratios + v2_ratios) / (weight_lst + 1e-6)\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    else:  # pareto_aware\n        # Focus on Pareto improvement\n        v1_ratios = value1_lst / (weight_lst + 1e-6)\n        v2_ratios = value2_lst / (weight_lst + 1e-6)\n        combined_scores = v1_ratios * v2_ratios\n\n        for _ in range(3):\n            candidates = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(candidates) > 0:\n                best_idx = candidates[np.argmax(combined_scores[candidates])]\n                new_solution[best_idx] = 1\n                available_capacity -= weight_lst[best_idx]\n\n    # Step 4: Adaptive bit-flipping with probability\n    for i in range(len(new_solution)):\n        if np.random.random() < 0.2:  # 20% chance to flip\n            if new_solution[i] == 0:\n                if weight_lst[i] <= available_capacity:\n                    new_solution[i] = 1\n                    available_capacity -= weight_lst[i]\n            else:\n                new_solution[i] = 0\n                available_capacity += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -17.98888874753535,
               -17.513125684536117
          ]
     }
]