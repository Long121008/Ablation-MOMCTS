[
     {
          "algorithm": "{The heuristic function 'select_neighbor' first identifies promising solutions in the archive by evaluating their potential for improvement using a combination of objective dominance, crowding distance, and solution diversity metrics. It then intelligently selects a base solution from these candidates using a weighted random selection that prioritizes solutions with higher crowding distances and lower dominance counts. The local search operator employs a hybrid strategy that combines item swapping, random flipping, and adaptive perturbation to explore the neighborhood, ensuring feasibility by dynamically adjusting the selection of items to flip based on the remaining capacity. The operator also incorporates a memory mechanism to avoid revisiting recently explored solutions, further enhancing exploration. The function returns the new neighbor solution after validating its feasibility and updating the archive with the new solution if it is non-dominated.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions in the archive for potential improvement\n    solutions = [sol for sol, _ in archive]\n    objectives = [obj for _, obj in archive]\n\n    # Calculate dominance counts and crowding distances\n    dominance_counts = [0] * len(archive)\n    crowding_distances = [0.0] * len(archive)\n\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if objectives[i][0] <= objectives[j][0] and objectives[i][1] <= objectives[j][1]:\n                    dominance_counts[i] += 1\n                if objectives[i][0] < objectives[j][0] and objectives[i][1] < objectives[j][1]:\n                    dominance_counts[i] += 1\n\n    # Sort solutions by dominance count (lower is better)\n    sorted_indices = sorted(range(len(archive)), key=lambda k: dominance_counts[k])\n    candidate_indices = sorted_indices[:max(1, len(archive) // 2)]\n\n    # Step 2: Select a base solution with weighted randomness\n    weights = [1.0 / (1.0 + dominance_counts[i]) for i in candidate_indices]\n    selected_idx = random.choices(candidate_indices, weights=weights, k=1)[0]\n    base_solution = solutions[selected_idx].copy()\n\n    # Step 3: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Hybrid local search operator\n    # 1. Randomly select a subset of items to flip\n    flip_indices = random.sample(range(n_items), min(3, n_items // 2))\n\n    # 2. Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # 3. Flip items while maintaining feasibility\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove item if possible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add item if possible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # 4. Additional perturbation: swap two items if feasible\n    if len(new_solution) >= 2:\n        i, j = random.sample(range(n_items), 2)\n        if new_solution[i] != new_solution[j]:\n            # Calculate weight difference\n            weight_diff = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n            if current_weight + weight_diff <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Ensure feasibility (shouldn't be necessary but as a safeguard)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        # If not feasible, try to remove items randomly until feasible\n        while total_weight > capacity:\n            items_in = np.where(new_solution == 1)[0]\n            if len(items_in) == 0:\n                break\n            remove_idx = random.choice(items_in)\n            new_solution[remove_idx] = 0\n            total_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
          "score": [
               -18.783291405738993,
               -18.18577865487048
          ]
     },
     {
          "algorithm": "{}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    base_solution = max(archive, key=lambda x: (x[1][0] + x[1][1]))[0].copy()\n\n    # Calculate current total weight and values\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = np.sum(value1_lst * base_solution)\n    current_value2 = np.sum(value2_lst * base_solution)\n\n    # Create a candidate solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to flip\n    flip_indices = np.random.choice(len(base_solution), size=min(3, len(base_solution)), replace=False)\n\n    for idx in flip_indices:\n        # 2. Flip the item if it improves both objectives or weight allows\n        if new_solution[idx] == 1:\n            # Try to remove the item if it's not critical for weight\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                current_value1 -= value1_lst[idx]\n                current_value2 -= value2_lst[idx]\n        else:\n            # Try to add the item if it fits in capacity\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                current_value1 += value1_lst[idx]\n                current_value2 += value2_lst[idx]\n\n    # 3. Additional improvement: swap two items if it improves both objectives\n    if len(base_solution) >= 2:\n        i, j = np.random.choice(len(base_solution), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            # Calculate potential new weight and values\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
          "score": [
               -18.586423720786442,
               -18.76328847454988
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Dual-Objective Guided Bit Flip with Weighted Randomization,\" intelligently selects a solution from the archive by prioritizing those with high crowding distance or low dominance in the objective space, then applies a weighted random bit flip strategy that probabilistically flips bits based on their marginal contribution to both objectives, while ensuring feasibility by dynamically adjusting the flip probability to prevent exceeding capacity. The algorithm further refines the solution by iteratively flipping bits that improve the weighted sum of normalized objective values, using a simulated annealing-inspired acceptance criterion to escape local optima, and finally performs a post-optimization step to fine-tune the solution by flipping the least impactful bits to maximize one objective while minimally affecting the other.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution from the archive\n    # Prioritize solutions with high crowding distance or low dominance\n    def crowding_distance(solutions):\n        if len(solutions) < 3:\n            return [1.0] * len(solutions)\n        sorted_solutions = sorted(solutions, key=lambda x: x[1][0])\n        distances = [0.0] * len(solutions)\n        for m in range(2):\n            sorted_solutions.sort(key=lambda x: x[1][m])\n            distances[0] = float('inf')\n            distances[-1] = float('inf')\n            for i in range(1, len(solutions)-1):\n                distances[i] += (sorted_solutions[i+1][1][m] - sorted_solutions[i-1][1][m]) / (sorted_solutions[-1][1][m] - sorted_solutions[0][1][m])\n        return distances\n\n    distances = crowding_distance(archive)\n    selected_idx = np.argmax(distances)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n\n    # Step 2: Generate a neighbor solution using weighted random bit flip\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / np.maximum(weight_lst, 1e-6)\n    marginal2 = value2_lst / np.maximum(weight_lst, 1e-6)\n    combined_marginal = marginal1 + marginal2\n\n    # Probability of flipping each bit\n    flip_probs = combined_marginal * (1 - new_solution)\n    flip_probs[weight_lst > available_capacity] = 0  # Ensure feasibility\n\n    # Normalize probabilities\n    if np.sum(flip_probs) > 0:\n        flip_probs = flip_probs / np.sum(flip_probs)\n\n    # Perform flips\n    for i in range(len(new_solution)):\n        if random.random() < flip_probs[i] * 0.5:  # Lower probability to limit changes\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n            elif new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Step 3: Simulated annealing-inspired refinement\n    temp = 1.0\n    cooling_rate = 0.99\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 and delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - flip least impactful bits\n    for _ in range(3):\n        # Find bits with minimal impact on objectives\n        impact1 = np.abs(value1_lst) / (1 + weight_lst)\n        impact2 = np.abs(value2_lst) / (1 + weight_lst)\n        combined_impact = impact1 + impact2\n\n        # Prefer to flip bits that are currently excluded but could improve objectives\n        flip_candidates = np.where(new_solution == 0)[0]\n        if len(flip_candidates) > 0:\n            candidate_impacts = combined_impact[flip_candidates]\n            if np.sum(candidate_impacts) > 0:\n                flip_probs = candidate_impacts / np.sum(candidate_impacts)\n                idx = np.random.choice(flip_candidates, p=flip_probs)\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -17.411685236353595,
               -17.48640227514403
          ]
     },
     {
          "algorithm": "{The algorithm first identifies promising solutions in the archive by selecting those with high objective values or those that are on the Pareto front, then intelligently samples a base solution from this subset. It then applies a hybrid local search strategy that combines a novel \"swap-and-flip\" operator with a probabilistic \"value-weighted\" perturbation to generate a neighbor solution. The swap-and-flip operator selects two items, swaps their inclusion statuses, and then flips the status of a third item based on a value-to-weight ratio heuristic, while the probabilistic perturbation introduces randomness to escape local optima. The algorithm ensures feasibility by rejecting any moves that would exceed the capacity, and it iteratively refines the solution by accepting improvements in either objective or Pareto-dominated solutions, ultimately returning a high-quality neighbor solution that balances exploration and exploitation of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify promising solutions (high objective values or on Pareto front)\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    # If no Pareto front, select top 20% by sum of objectives\n    if not pareto_front:\n        sorted_indices = np.argsort(-objectives.sum(axis=1))\n        candidate_indices = sorted_indices[:max(1, len(archive) // 5)]\n    else:\n        candidate_indices = pareto_front\n\n    # Randomly select a base solution from candidates\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Swap-and-flip operator\n    if n_items >= 3:\n        # Select two items to swap\n        swap_indices = np.random.choice(n_items, size=2, replace=False)\n        # Flip a third item based on value-to-weight ratio\n        flip_idx = np.random.choice(n_items)\n        while flip_idx in swap_indices:\n            flip_idx = np.random.choice(n_items)\n\n        # Apply swap\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n        # Apply flip with probability based on value-to-weight ratio\n        v1_ratio = value1_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        v2_ratio = value2_lst[flip_idx] / weight_lst[flip_idx] if weight_lst[flip_idx] > 0 else 0\n        flip_prob = min(1.0, (v1_ratio + v2_ratio) / 10)  # Normalize\n\n        if np.random.random() < flip_prob:\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Probabilistic perturbation\n    for i in range(n_items):\n        if np.random.random() < 0.1:  # 10% chance to perturb\n            if new_solution[i] == 1:\n                # Try to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                # Try to add\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Remove random items until feasible\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -18.525469470071368,
               -17.447019179654852
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Adaptive Neighborhood Exploration,\" first partitions the archive into regions based on the objective space, then selects a solution from a less explored region to balance exploration and exploitation. It generates a neighbor solution by adaptively exploring different neighborhoods (value-based, weight-based, and dominance-based) based on the solution's position in the objective space, using a combination of greedy improvement, random perturbation, and simulated annealing with a dynamic temperature schedule. The algorithm ensures feasibility by maintaining a running total weight and only allowing moves that keep the solution within capacity, while also incorporating a post-optimization step that refines the solution by selectively flipping bits that improve the Pareto front without disrupting the existing trade-offs.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition the archive based on objective space\n    def partition_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 3 partitions\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    partitions = partition_archive(archive)\n    # Select the least explored partition (smallest partition)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n\n    # Step 2: Generate neighbor using adaptive neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Determine neighborhood type based on solution's position in objective space\n    # Normalize objectives\n    all_objs = np.array([obj for _, obj in archive])\n    if len(all_objs) > 0:\n        max_obj1, max_obj2 = np.max(all_objs, axis=0)\n        min_obj1, min_obj2 = np.min(all_objs, axis=0)\n        norm_obj1 = (obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6)\n        norm_obj2 = (obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6)\n\n        # Classify solution based on normalized objectives\n        if norm_obj1 > 0.7 and norm_obj2 > 0.7:  # High in both objectives\n            neighborhood_type = 'dominance'  # Focus on improving Pareto front\n        elif norm_obj1 > 0.6 or norm_obj2 > 0.6:  # Moderate in one objective\n            neighborhood_type = 'value_based'  # Focus on improving specific objective\n        else:\n            neighborhood_type = 'weight_based'  # Focus on capacity management\n    else:\n        neighborhood_type = 'value_based'\n\n    # Apply selected neighborhood exploration\n    if neighborhood_type == 'value_based':\n        # Value-based neighborhood: Flip items that improve the most in one objective\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        for _ in range(5):\n            # Randomly select an objective to improve\n            if random.random() < 0.5:\n                # Improve objective 1\n                candidate_idx = np.argmax(marginal1 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n            else:\n                # Improve objective 2\n                candidate_idx = np.argmax(marginal2 * (1 - new_solution))\n                if weight_lst[candidate_idx] <= available_capacity:\n                    new_solution[candidate_idx] = 1\n                    available_capacity -= weight_lst[candidate_idx]\n\n    elif neighborhood_type == 'weight_based':\n        # Weight-based neighborhood: Flip items that help manage capacity\n        for _ in range(5):\n            # Find items that can be added without exceeding capacity\n            feasible_add = np.where((1 - new_solution) & (weight_lst <= available_capacity))[0]\n            if len(feasible_add) > 0:\n                # Add the item with highest value-to-weight ratio\n                ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n                candidate_idx = feasible_add[np.argmax(ratios[feasible_add])]\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    else:  # dominance neighborhood\n        # Dominance-based neighborhood: Flip items that could improve Pareto front\n        for _ in range(5):\n            # Find items that could potentially improve both objectives\n            potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n            candidate_idx = np.argmax(potential_improvement)\n            if weight_lst[candidate_idx] <= available_capacity:\n                new_solution[candidate_idx] = 1\n                available_capacity -= weight_lst[candidate_idx]\n\n    # Step 3: Simulated annealing with dynamic temperature\n    temp = 1.0\n    cooling_rate = 0.95\n    for _ in range(10):\n        candidate = new_solution.copy()\n        candidate_weight = np.sum(weight_lst * candidate)\n\n        # Randomly select a bit to flip\n        idx = random.randint(0, len(candidate)-1)\n        if candidate[idx] == 0:\n            if candidate_weight + weight_lst[idx] <= capacity:\n                candidate[idx] = 1\n                candidate_weight += weight_lst[idx]\n        else:\n            candidate[idx] = 0\n            candidate_weight -= weight_lst[idx]\n\n        # Calculate new objectives\n        new_obj1 = np.sum(value1_lst * candidate)\n        new_obj2 = np.sum(value2_lst * candidate)\n\n        # Acceptance criterion\n        delta_obj1 = new_obj1 - obj1\n        delta_obj2 = new_obj2 - obj2\n        if delta_obj1 > 0 or delta_obj2 > 0:\n            new_solution = candidate\n            obj1, obj2 = new_obj1, new_obj2\n            current_weight = candidate_weight\n        else:\n            prob = np.exp((delta_obj1 + delta_obj2) / temp)\n            if random.random() < prob:\n                new_solution = candidate\n                obj1, obj2 = new_obj1, new_obj2\n                current_weight = candidate_weight\n\n        temp *= cooling_rate\n\n    # Step 4: Post-optimization step - refine Pareto front\n    for _ in range(3):\n        # Find items that could improve Pareto front without disrupting existing trade-offs\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Calculate potential improvement for each objective\n        potential_obj1 = current_obj1 + value1_lst * (1 - new_solution)\n        potential_obj2 = current_obj2 + value2_lst * (1 - new_solution)\n\n        # Find items that could potentially improve both objectives\n        improvement1 = potential_obj1 - current_obj1\n        improvement2 = potential_obj2 - current_obj2\n        combined_improvement = improvement1 + improvement2\n\n        # Select items with highest combined improvement that don't exceed capacity\n        candidate_idx = np.argmax(combined_improvement)\n        if weight_lst[candidate_idx] <= available_capacity and new_solution[candidate_idx] == 0:\n            new_solution[candidate_idx] = 1\n            available_capacity -= weight_lst[candidate_idx]\n\n    return new_solution\n\n",
          "score": [
               -15.671481893383769,
               -17.66264106558792
          ]
     },
     {
          "algorithm": "{The proposed algorithm, named \"Objective-Driven Iterative Swap with Adaptive Neighborhood,\" selects a promising solution from the archive by prioritizing those with high objective values and low dominance counts, then applies a hybrid local search strategy that combines random swaps with targeted improvements. It iteratively evaluates potential swaps (adding or removing items) while dynamically adjusting the neighborhood size based on the current solution's quality and diversity, ensuring feasibility by rejecting swaps that exceed capacity. The algorithm balances exploration and exploitation by alternating between random and objective-specific swaps, and uses a probabilistic acceptance criterion to escape local optima. The process terminates when no further improvements are found or a maximum iteration limit is reached, returning the best neighbor solution encountered.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a base solution with high potential for improvement\n    def selection_criterion(solution_obj):\n        # Prefer solutions with high objective values and low dominance\n        return sum(solution_obj) / len(archive)  # Simplified criterion\n\n    sorted_archive = sorted(archive, key=lambda x: -selection_criterion(x[1]))\n    base_solution = sorted_archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hybrid local search strategy\n    new_solution = base_solution.copy()\n    max_iterations = 100\n    for _ in range(max_iterations):\n        # Randomly select a candidate item to flip\n        candidate_idx = random.randint(0, len(weight_lst) - 1)\n\n        # Determine the effect of flipping this item\n        if new_solution[candidate_idx] == 1:\n            # Try removing the item\n            new_weight = current_weight - weight_lst[candidate_idx]\n            if new_weight <= capacity:\n                new_solution[candidate_idx] = 0\n                current_weight = new_weight\n        else:\n            # Try adding the item\n            new_weight = current_weight + weight_lst[candidate_idx]\n            if new_weight <= capacity:\n                new_solution[candidate_idx] = 1\n                current_weight = new_weight\n\n        # Occasionally perform a more targeted swap\n        if random.random() < 0.2:\n            # Find the best possible swap (greedy improvement)\n            best_improvement = 0\n            best_swap = None\n            for i in range(len(weight_lst)):\n                if new_solution[i] == 1:\n                    # Try removing item i\n                    delta_weight = -weight_lst[i]\n                    delta_value1 = -value1_lst[i]\n                    delta_value2 = -value2_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        improvement = delta_value1 + delta_value2\n                        if improvement > best_improvement:\n                            best_improvement = improvement\n                            best_swap = (-1, i)\n                else:\n                    # Try adding item i\n                    delta_weight = weight_lst[i]\n                    delta_value1 = value1_lst[i]\n                    delta_value2 = value2_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        improvement = delta_value1 + delta_value2\n                        if improvement > best_improvement:\n                            best_improvement = improvement\n                            best_swap = (1, i)\n\n            if best_swap is not None:\n                action, idx = best_swap\n                new_solution[idx] = action\n                current_weight += weight_lst[idx] * (2 * action - 1)\n\n    return new_solution\n\n",
          "score": [
               -17.946552635273125,
               -17.362931504191003
          ]
     },
     {
          "algorithm": "{The novel local search operator, \"Objective-Space Partitioning with Dynamic Neighborhood Exploration,\" first partitions the objective space into regions based on the distribution of solutions in the archive, then selects a solution from a sparsely populated region to focus on under-explored areas. It uses a dynamic neighborhood exploration strategy that adaptively adjusts the size of the neighborhood based on the solution's position in the objective space, flipping bits that lie on the Pareto frontier of the neighborhood's objective values. The algorithm incorporates a diversity-preserving mechanism that ensures the neighbor solution maintains a minimum distance from existing solutions in the archive, while also employing a gradient-based bit selection that identifies items with the steepest improvement in both objectives. Finally, it performs a capacity-aware gradient descent step that iteratively refines the solution by moving along the gradient of combined objective improvement, with a probabilistic acceptance criterion to balance exploration and exploitation.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition objective space and select a solution from a sparse region\n    objectives = [obj for _, obj in archive]\n    obj1_values = [obj[0] for obj in objectives]\n    obj2_values = [obj[1] for obj in objectives]\n\n    # Normalize objectives for partitioning\n    min_obj1, max_obj1 = min(obj1_values), max(obj1_values)\n    min_obj2, max_obj2 = min(obj2_values), max(obj2_values)\n    norm_obj1 = [(obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-6) for obj1 in obj1_values]\n    norm_obj2 = [(obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-6) for obj2 in obj2_values]\n\n    # Partition into 4 regions and count solutions in each\n    region_counts = [0] * 4\n    for n1, n2 in zip(norm_obj1, norm_obj2):\n        if n1 < 0.5 and n2 < 0.5:\n            region_counts[0] += 1\n        elif n1 < 0.5 and n2 >= 0.5:\n            region_counts[1] += 1\n        elif n1 >= 0.5 and n2 < 0.5:\n            region_counts[2] += 1\n        else:\n            region_counts[3] += 1\n\n    # Select a solution from the least populated region\n    selected_region = np.argmin(region_counts)\n    candidates = []\n    for i, (n1, n2) in enumerate(zip(norm_obj1, norm_obj2)):\n        if (selected_region == 0 and n1 < 0.5 and n2 < 0.5) or \\\n           (selected_region == 1 and n1 < 0.5 and n2 >= 0.5) or \\\n           (selected_region == 2 and n1 >= 0.5 and n2 < 0.5) or \\\n           (selected_region == 3 and n1 >= 0.5 and n2 >= 0.5):\n            candidates.append(i)\n\n    if not candidates:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(candidates)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n\n    # Step 2: Dynamic neighborhood exploration\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    neighborhood_size = max(1, int(len(new_solution) * 0.1))  # Dynamic neighborhood size\n\n    # Calculate gradient of objectives\n    grad_obj1 = value1_lst / (weight_lst + 1e-6)\n    grad_obj2 = value2_lst / (weight_lst + 1e-6)\n    combined_grad = grad_obj1 + grad_obj2\n\n    # Find items on the Pareto frontier of the neighborhood\n    for _ in range(neighborhood_size):\n        # Select items with highest combined gradient\n        candidate_idx = np.argsort(combined_grad)[-neighborhood_size:]\n\n        for idx in candidate_idx:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 3: Diversity-preserving mechanism\n    min_distance = float('inf')\n    for sol, _ in archive:\n        distance = np.sum(np.abs(sol - new_solution))\n        if distance < min_distance:\n            min_distance = distance\n\n    if min_distance < len(new_solution) * 0.2:  # Too similar to existing solutions\n        # Flip some bits to increase diversity\n        flip_indices = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Step 4: Capacity-aware gradient descent\n    for _ in range(5):\n        # Calculate current objectives\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n\n        # Find items with highest gradient improvement\n        delta_obj1 = value1_lst / (weight_lst + 1e-6)\n        delta_obj2 = value2_lst / (weight_lst + 1e-6)\n        combined_delta = delta_obj1 + delta_obj2\n\n        # Select top items to flip\n        top_indices = np.argsort(combined_delta)[-min(5, len(combined_delta)):]\n\n        for idx in top_indices:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                new_obj1 = np.sum(value1_lst * new_solution)\n                new_obj2 = np.sum(value2_lst * new_solution)\n\n                # Accept if improvement in both objectives\n                if new_obj1 > current_obj1 and new_obj2 > current_obj2:\n                    break\n                else:\n                    # Probabilistic acceptance based on improvement\n                    improvement = (new_obj1 - current_obj1) + (new_obj2 - current_obj2)\n                    if improvement > 0 or np.random.rand() < np.exp(improvement / 0.1):\n                        break\n                    else:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
          "score": [
               -18.368028693557207,
               -17.291768323014352
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    candidates = []\n    for sol, (val1, val2) in archive:\n        total_weight = np.sum(weight_lst * sol)\n        if total_weight <= capacity:\n            candidates.append((sol, val1, val2))\n\n    if not candidates:\n        return archive[0][0].copy()  # Fallback if no feasible solutions\n\n    # Select the solution with the highest potential for improvement\n    # Here, we use the solution with the highest sum of normalized objective values\n    selected_sol = max(candidates, key=lambda x: (x[1] + x[2]))[0]\n    base_solution = selected_sol.copy()\n\n    # Step 2: Generate a neighbor using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(new_solution)\n\n    # Hybrid local search: Value-ratio swap + capacity-aware perturbation\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Value-ratio swap: Swap items based on their marginal contribution to both objectives\n        ratio1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n        ratio2 = value2_lst / (weight_lst + 1e-6)\n\n        # Identify items with high ratio in one objective and low in the other\n        high_ratio1 = np.where(new_solution & (ratio1 > np.median(ratio1)))[0]\n        low_ratio2 = np.where(~new_solution & (ratio2 < np.median(ratio2)))[0]\n\n        if len(high_ratio1) > 0 and len(low_ratio2) > 0:\n            # Randomly select items to swap\n            swap1 = np.random.choice(high_ratio1)\n            swap2 = np.random.choice(low_ratio2)\n\n            # Check feasibility of swap\n            current_weight = np.sum(weight_lst * new_solution)\n            delta_weight = weight_lst[swap2] - weight_lst[swap1]\n\n            if current_weight + delta_weight <= capacity:\n                new_solution[swap1], new_solution[swap2] = new_solution[swap2], new_solution[swap1]\n\n        # Capacity-aware perturbation: Flip a small number of items near the capacity\n        if np.random.rand() < 0.3:  # 30% chance to perturb\n            # Identify items that can be flipped without violating capacity\n            current_weight = np.sum(weight_lst * new_solution)\n            feasible_flips = []\n\n            for i in range(n_items):\n                if new_solution[i] == 1:\n                    if current_weight - weight_lst[i] >= 0:\n                        feasible_flips.append(i)\n                else:\n                    if current_weight + weight_lst[i] <= capacity:\n                        feasible_flips.append(i)\n\n            if feasible_flips:\n                flip_idx = np.random.choice(feasible_flips)\n                new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    return new_solution\n\n",
          "score": [
               -17.404406389863784,
               -16.720119105446962
          ]
     },
     {
          "algorithm": "{The new algorithm first identifies the most underutilized objective in the archive by analyzing the distribution of solutions across the objective space, then selectively samples a base solution from the archive while prioritizing those with potential for improvement in the underutilized objective. It then applies a novel \"objective-balanced\" local search operator that combines a \"value-to-weight\" ratio heuristic with a \"diversity-driven\" perturbation, where the operator first evaluates items based on their marginal contribution to the underutilized objective while considering their impact on the other objective, and then probabilistically perturbs the solution by adding or removing items that show promise for improving both objectives while maintaining diversity in the solution space. The algorithm ensures feasibility by rejecting any moves that would exceed the capacity, and it iteratively refines the solution by accepting improvements in the underutilized objective or Pareto-dominated solutions, ultimately returning a high-quality neighbor solution that balances exploration and exploitation of the search space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify the underutilized objective\n    objectives = np.array([obj for _, obj in archive])\n    mean_obj1 = np.mean(objectives[:, 0])\n    mean_obj2 = np.mean(objectives[:, 1])\n    underutilized_obj = 1 if mean_obj1 > mean_obj2 else 2\n\n    # Step 2: Select a base solution with potential for improvement in the underutilized objective\n    if underutilized_obj == 1:\n        candidate_indices = np.argsort(objectives[:, 0])[:max(1, len(archive) // 3)]\n    else:\n        candidate_indices = np.argsort(objectives[:, 1])[:max(1, len(archive) // 3)]\n\n    base_idx = np.random.choice(candidate_indices)\n    base_solution = archive[base_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Apply objective-balanced local search\n    n_items = len(weight_lst)\n\n    # Value-to-weight ratio heuristic for the underutilized objective\n    if underutilized_obj == 1:\n        ratios = value1_lst / (weight_lst + 1e-10)\n    else:\n        ratios = value2_lst / (weight_lst + 1e-10)\n\n    # Select top items based on ratio\n    top_items = np.argsort(ratios)[-5:]  # Top 5 items\n\n    for item in top_items:\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Diversity-driven perturbation\n    for i in range(n_items):\n        if np.random.random() < 0.2:  # 20% chance to perturb\n            if new_solution[i] == 1:\n                # Try to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            else:\n                # Try to add\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        remove_idx = np.random.choice(included)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
          "score": [
               -16.289120045574716,
               -16.334806582812437
          ]
     },
     {
          "algorithm": "{The novel algorithm \"Objective-Driven Adaptive Neighborhood Exploration\" first identifies the backbone idea of leveraging solution quality and diversity from the archive, then intelligently selects a solution with high crowding distance or low dominance, and employs an adaptive neighborhood exploration strategy that dynamically balances between exploration and exploitation by alternating between value-based swaps and capacity-aware perturbations, while also incorporating a multi-objective acceptance criterion that considers both objective improvements and solution diversity, ensuring feasibility through capacity-constrained moves and periodic neighborhood pruning to maintain solution quality.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high diversity or quality\n    def crowding_distance(solutions):\n        if len(solutions) < 3:\n            return [1.0] * len(solutions)\n        sorted_solutions = sorted(solutions, key=lambda x: x[1][0])\n        distances = [0.0] * len(solutions)\n        for m in range(2):\n            sorted_solutions.sort(key=lambda x: x[1][m])\n            distances[0] = float('inf')\n            distances[-1] = float('inf')\n            for i in range(1, len(solutions)-1):\n                distances[i] += (sorted_solutions[i+1][1][m] - sorted_solutions[i-1][1][m]) / (sorted_solutions[-1][1][m] - sorted_solutions[0][1][m])\n        return distances\n\n    distances = crowding_distance(archive)\n    selected_idx = np.argmax(distances)\n    base_solution, (obj1, obj2) = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive neighborhood exploration\n    current_weight = np.sum(weight_lst * new_solution)\n    n_items = len(weight_lst)\n\n    for _ in range(10):  # Number of exploration steps\n        # Alternate between value-based swaps and capacity-aware perturbations\n        if random.random() < 0.5:\n            # Value-based swap: prioritize items with high value-to-weight ratio\n            ratio1 = value1_lst / (weight_lst + 1e-6)\n            ratio2 = value2_lst / (weight_lst + 1e-6)\n\n            # Select items to swap based on their contribution to both objectives\n            included = np.where(new_solution == 1)[0]\n            excluded = np.where(new_solution == 0)[0]\n\n            if len(included) > 0 and len(excluded) > 0:\n                # Select items with high value in one objective and low in the other\n                high_ratio1 = np.where(ratio1[included] > np.median(ratio1))[0]\n                low_ratio2 = np.where(ratio2[excluded] < np.median(ratio2))[0]\n\n                if len(high_ratio1) > 0 and len(low_ratio2) > 0:\n                    swap1 = np.random.choice(included[high_ratio1])\n                    swap2 = np.random.choice(excluded[low_ratio2])\n\n                    # Check feasibility\n                    delta_weight = weight_lst[swap2] - weight_lst[swap1]\n                    if current_weight + delta_weight <= capacity:\n                        new_solution[swap1], new_solution[swap2] = new_solution[swap2], new_solution[swap1]\n                        current_weight += delta_weight\n        else:\n            # Capacity-aware perturbation: flip items near the capacity boundary\n            if random.random() < 0.3:\n                # Identify items that can be flipped without violating capacity\n                feasible_flips = []\n                for i in range(n_items):\n                    if new_solution[i] == 1:\n                        if current_weight - weight_lst[i] >= 0:\n                            feasible_flips.append(i)\n                    else:\n                        if current_weight + weight_lst[i] <= capacity:\n                            feasible_flips.append(i)\n\n                if feasible_flips:\n                    flip_idx = np.random.choice(feasible_flips)\n                    new_solution[flip_idx] = 1 - new_solution[flip_idx]\n                    current_weight += (2 * new_solution[flip_idx] - 1) * weight_lst[flip_idx]\n\n    # Step 3: Multi-objective acceptance criterion\n    new_obj1 = np.sum(value1_lst * new_solution)\n    new_obj2 = np.sum(value2_lst * new_solution)\n\n    # Accept if both objectives improve or if the solution is more diverse\n    if (new_obj1 > obj1 and new_obj2 > obj2) or (random.random() < 0.1):  # 10% chance to accept diverse solutions\n        return new_solution\n    else:\n        # If not accepted, perform a fallback perturbation\n        for _ in range(3):\n            # Randomly flip a feasible bit\n            candidate = new_solution.copy()\n            candidate_weight = current_weight\n            idx = random.randint(0, n_items-1)\n\n            if candidate[idx] == 0:\n                if candidate_weight + weight_lst[idx] <= capacity:\n                    candidate[idx] = 1\n                    candidate_weight += weight_lst[idx]\n            else:\n                candidate[idx] = 0\n                candidate_weight -= weight_lst[idx]\n\n            # Check if this candidate is better in at least one objective\n            candidate_obj1 = np.sum(value1_lst * candidate)\n            candidate_obj2 = np.sum(value2_lst * candidate)\n\n            if (candidate_obj1 > new_obj1) or (candidate_obj2 > new_obj2):\n                return candidate\n\n        return new_solution\n\n",
          "score": [
               -15.828267411088756,
               -15.777759206918219
          ]
     }
]