[
     {
          "algorithm": "{The novel \"Adaptive Pareto Frontier Exploration with Objective-Space Partitioning and Multi-Objective Greedy Perturbations\" algorithm begins by partitioning the objective space into regions and selecting a solution from the least explored region to balance exploration and exploitation, then applies a multi-phase approach that first performs a capacity-aware greedy improvement phase to maximize one objective while maintaining feasibility, followed by a dynamic objective fusion phase that refines the solution by selectively flipping items based on their combined marginal contributions to both objectives, with each phase dynamically adjusting its intensity based on the current solution's position in the objective space, culminating in a final refinement step that ensures the solution lies on or near the Pareto front by selectively adding items that provide the best combined improvement in both objectives without exceeding capacity, while also incorporating random perturbations to escape local optima and explore new regions of the solution space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Partition objective space and select solution from least explored region\n    def partition_archive(archive):\n        if len(archive) < 4:\n            return [archive]\n        # Sort by first objective\n        sorted_archive = sorted(archive, key=lambda x: x[1][0])\n        # Split into 4 partitions\n        split1 = len(sorted_archive) // 4\n        split2 = len(sorted_archive) // 2\n        split3 = 3 * len(sorted_archive) // 4\n        return [sorted_archive[:split1], sorted_archive[split1:split2],\n                sorted_archive[split2:split3], sorted_archive[split3:]]\n\n    partitions = partition_archive(archive)\n    selected_partition = min(partitions, key=lambda x: len(x))\n    if not selected_partition:\n        selected_partition = archive\n    selected_idx = np.random.randint(0, len(selected_partition))\n    base_solution, (obj1, obj2) = selected_partition[selected_idx]\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Multi-phase improvement with dynamic intensity\n    phases = 5\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Capacity-aware greedy improvement for one objective\n        if random.random() < phase_intensity:\n            if random.random() < 0.5:\n                # Improve objective 1\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                # Improve objective 2\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n        # Phase 2: Dynamic objective fusion refinement\n        if random.random() < phase_intensity:\n            # Calculate combined marginal improvements\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n        # Phase 3: Random perturbations with capacity check\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n    return new_solution\n\n",
          "score": [
               -17.005168901586927,
               -19.78378001221099
          ]
     },
     {
          "algorithm": "{The novel \"Multi-Objective Knapsack Exploration with Adaptive Objective Fusion and Dynamic Capacity-Aware Diversification\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply capacity-aware diversification with objective fusion\n    for _ in range(5):\n        # Select items to flip based on combined marginal contribution\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = obj1_weight * marginal1 + obj2_weight * marginal2\n\n        # Add items\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        # Remove items\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 4: Apply dynamic solution fusion with random swaps\n    for _ in range(3):\n        i, j = np.random.choice(len(weight_lst), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n\n    # Step 5: Final refinement with objective-aware perturbations\n    for _ in range(2):\n        if np.random.random() < obj1_weight:\n            # Focus on objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x] + 1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Focus on objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x] + 1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -19.832186630008383,
               -17.599380160700925
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Objective-Space Clustering with Multi-Phase Greedy Diversification and Dynamic Capacity-Adaptive Perturbations\" algorithm begins by clustering the archive solutions into regions based on their objective-space positions and selecting a solution from the least explored cluster to balance exploration and exploitation, then applies a multi-phase diversification process that alternates between capacity-aware greedy additions of items with high combined marginal contributions to both objectives, dynamic capacity-adaptive removals of items with low combined marginal contributions, and adaptive random perturbations that are guided by the current solution's position relative to the Pareto front, with each phase dynamically adjusting its intensity based on the solution's proximity to the front, culminating in a final refinement step that combines greedy value maximization in both objectives with capacity-preserving item replacements, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Cluster solutions and select from least explored cluster\n    def cluster_archive(archive):\n        if len(archive) < 3:\n            return [archive]\n        # Sort by both objectives\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n        # Split into 3 clusters\n        split1 = len(sorted_archive) // 3\n        split2 = 2 * len(sorted_archive) // 3\n        return [sorted_archive[:split1], sorted_archive[split1:split2], sorted_archive[split2:]]\n\n    clusters = cluster_archive(archive)\n    selected_cluster = min(clusters, key=lambda x: len(x))\n    if not selected_cluster:\n        selected_cluster = archive\n    selected_idx = np.random.randint(0, len(selected_cluster))\n    base_solution = selected_cluster[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    available_capacity = capacity - current_weight\n\n    # Step 2: Multi-phase diversification with dynamic intensity\n    phases = 4\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases\n\n        # Phase 1: Greedy addition of high-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n                for item in sorted_items:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-adaptive removal of low-marginal items\n        if random.random() < phase_intensity:\n            combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n                for item in sorted_items:\n                    if available_capacity + weight_lst[item] >= 0:\n                        new_solution[item] = 0\n                        available_capacity += weight_lst[item]\n                        break\n\n        # Phase 3: Dynamic random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if weight_lst[item] <= available_capacity:\n                        new_solution[item] = 1\n                        available_capacity -= weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    available_capacity += weight_lst[item]\n\n    # Final refinement: Greedy maximization of both objectives\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        potential_improvement = (value1_lst * (1 - new_solution) + value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if weight_lst[item] <= available_capacity:\n                    new_solution[item] = 1\n                    available_capacity -= weight_lst[item]\n                    break\n\n    return new_solution\n\n",
          "score": [
               -18.672353056969857,
               -19.073189253138064
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Optimization with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select the solution with the highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply a novel hybrid local search operator\n    # Phase 1: Capacity-aware item swaps\n    for _ in range(3):\n        i, j = np.random.choice(len(weight_lst), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n\n    # Phase 2: Objective-specific marginal contribution flips\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = obj1_weight * marginal1 + obj2_weight * marginal2\n\n    # Add items based on combined marginal contribution\n    candidate_items = np.where(new_solution == 0)[0]\n    if len(candidate_items) > 0:\n        sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Remove items based on combined marginal contribution\n    candidate_items = np.where(new_solution == 1)[0]\n    if len(candidate_items) > 0:\n        sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n        for item in sorted_items:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                break\n\n    # Phase 3: Adaptive random perturbations with cooling mechanism\n    cooling_factor = 0.9\n    temperature = 1.0\n\n    for _ in range(5):\n        item = np.random.randint(0, len(weight_lst))\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if np.random.random() < temperature:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        temperature *= cooling_factor\n\n    return new_solution\n\n",
          "score": [
               -19.360474058099307,
               -18.034558424433357
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Weighting and Solution Fusion\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -19.356478514057308,
               -18.42374679369928
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Search with Dynamic Objective Balancing and Capacity-Sensitive Evolutionary Perturbations\" algorithm begins by selecting a solution from the archive using a dynamic objective balancing mechanism that combines both objective values with adaptively adjusted weights based on their current relative importance, then applies a multi-phase evolutionary perturbation process that alternates between capacity-sensitive item flips, objective-specific marginal contribution swaps, and adaptive neighborhood exploration, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a hierarchical refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic objective balancing selection\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance with adaptive cooling\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.6\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.6\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2 + 0.2 * (norm_v1 + norm_v2)\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-phase evolutionary perturbations\n    phases = 3\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases  # Decreasing intensity\n\n        # Phase 1: Capacity-sensitive item flips\n        if phase % 2 == 0 and len(new_solution) > 1:\n            candidate_items = np.where(new_solution == 1)[0]\n            if len(candidate_items) > 0:\n                item = np.random.choice(candidate_items)\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                item = np.random.choice(candidate_items)\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n        # Phase 2: Objective-specific marginal contribution swaps\n        else:\n            if random.random() < phase_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Phase 3: Adaptive neighborhood exploration\n        if random.random() < phase_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Hierarchical refinement\n    for tier in range(2):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.6:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        else:\n            # Tier 2: Capacity-preserving item replacements\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.872508336569215,
               -18.801257865542038
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest sum of normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selected_sol = max(archive, key=lambda x: (x[1][0]/max_v1 + x[1][1]/max_v2))[0]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective weights based on current dominance\n    obj1_weight = 0.5 + 0.5 * (np.sum(value1_lst * base_solution) / (np.sum(value1_lst * base_solution) + np.sum(value2_lst * base_solution) + 1e-6))\n    obj2_weight = 1.0 - obj1_weight\n\n    # Step 3: Apply capacity-aware diversification with objective fusion\n    for _ in range(5):\n        # Select items to flip based on combined marginal contribution\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = obj1_weight * marginal1 + obj2_weight * marginal2\n\n        # Add items\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        # Remove items\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 4: Apply dynamic solution fusion with random swaps\n    for _ in range(3):\n        i, j = np.random.choice(len(weight_lst), size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n            if new_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n\n    # Step 5: Final refinement with objective-aware perturbations\n    for _ in range(2):\n        if np.random.random() < obj1_weight:\n            # Focus on objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x] + 1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Focus on objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x] + 1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -19.713091172291342,
               -17.928415178315884
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Exploration\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Hybrid selection with dynamic weights\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    selection_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        dominance_ratio = norm_v1 / (norm_v2 + 1e-6) if norm_v2 > 0 else 1.0\n        weight1 = 1.0 if dominance_ratio > 1.0 else 0.7\n        weight2 = 1.0 if dominance_ratio < 1.0 else 0.7\n        selection_score = weight1 * norm_v1 + weight2 * norm_v2\n        selection_scores.append((selection_score, sol))\n\n    selected_sol = max(selection_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Multi-stage exploration with dynamic intensity\n    stages = 5\n    for stage in range(stages):\n        stage_intensity = (stages - stage) / stages\n\n        if stage % 2 == 0:\n            # Stage 1: Capacity-aware item swaps with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n        else:\n            # Stage 2: Objective-specific marginal contribution flips with cooling\n            if random.random() < stage_intensity:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n                candidate_items = np.where(new_solution == 1)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x])\n                    for item in sorted_items[:1]:\n                        if current_weight - weight_lst[item] >= 0:\n                            new_solution[item] = 0\n                            current_weight -= weight_lst[item]\n\n        # Stage 3: Adaptive random perturbations with capacity check\n        if random.random() < stage_intensity * 0.5:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Tiered refinement with objective fusion\n    for tier in range(3):\n        if tier == 0:\n            # Tier 1: Greedy value maximization in most promising objective\n            if random.random() < 0.7:\n                if random.random() < 0.5:\n                    marginal = value1_lst / (weight_lst + 1e-6)\n                else:\n                    marginal = value2_lst / (weight_lst + 1e-6)\n\n                candidate_items = np.where(new_solution == 0)[0]\n                if len(candidate_items) > 0:\n                    sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                    for item in sorted_items[:1]:\n                        if current_weight + weight_lst[item] <= capacity:\n                            new_solution[item] = 1\n                            current_weight += weight_lst[item]\n\n        elif tier == 1:\n            # Tier 2: Capacity-preserving item replacements with objective balancing\n            if len(new_solution) >= 2:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n        else:\n            # Tier 3: Objective-specific swaps with adaptive cooling\n            if len(new_solution) >= 2 and random.random() < 0.3:\n                i, j = np.random.choice(len(new_solution), size=2, replace=False)\n                if new_solution[i] != new_solution[j]:\n                    new_weight = current_weight + (weight_lst[j] - weight_lst[i]) * (new_solution[i] - new_solution[j])\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(2):\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items[:1]:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
          "score": [
               -18.75670153390136,
               -18.94314768739674
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive based on a dynamic objective fusion metric that combines both objectives with adaptive weights determined by their current dominance relationships, then applies a capacity-aware perturbation mechanism that selectively flips items based on their marginal contributions to both objectives while maintaining feasibility, followed by a multi-phase refinement process that alternates between greedy value maximization in the most promising objective and capacity-preserving random perturbations, with each phase dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, culminating in a final solution that balances exploration of under-represented regions and exploitation of high-quality solutions, all within a controlled exploration-exploitation balance framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply a novel \"Dual-Objective Guided Perturbation\" with dynamic intensity\n    for _ in range(5):\n        # Calculate dynamic utility scores\n        utility1 = value1_lst / (weight_lst + 1e-6)\n        utility2 = value2_lst / (weight_lst + 1e-6)\n        combined_utility = (utility1 + utility2) / 2\n\n        # Select items to flip based on dynamic utility and capacity\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: combined_utility[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Apply a \"Feasibility-Preserving Random Walk\" with adaptive step size\n    for _ in range(3):\n        item = random.randint(0, len(weight_lst) - 1)\n        if new_solution[item] == 1:\n            if current_weight - weight_lst[item] >= 0:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Final refinement with objective-specific greedy improvements\n    for _ in range(2):\n        if random.random() < 0.5:\n            # Improve objective 1\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value1_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n        else:\n            # Improve objective 2\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: value2_lst[x]/(weight_lst[x]+1e-6), reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n    return new_solution\n\n",
          "score": [
               -19.239051632058647,
               -18.56045189976524
          ]
     },
     {
          "algorithm": "{The novel \"Adaptive Multi-Objective Knapsack Exploration with Dynamic Objective Fusion and Capacity-Aware Perturbation\" algorithm begins by selecting a solution from the archive using a hybrid selection mechanism that combines both objective values with dynamically adjusted weights based on their current dominance relationships, then applies a multi-stage exploration process that alternates between capacity-aware item swaps, objective-specific marginal contribution flips, and adaptive random perturbations, with each stage dynamically adjusting its intensity based on the current solution's position relative to the Pareto front, followed by a tiered refinement phase that combines greedy value maximization in the most promising objective with capacity-preserving item replacements, all within a controlled exploration-exploitation framework that ensures the generated neighbor solution remains feasible and shows potential for further improvement by balancing the exploration of under-represented regions with the exploitation of high-quality solutions, while incorporating a novel adaptive cooling mechanism that gradually reduces the intensity of perturbations as the solution approaches the Pareto front, thereby ensuring a more focused search in the final stages of the optimization process.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with dynamic objective fusion metric\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1.0\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1.0\n\n    fusion_scores = []\n    for sol, obj in archive:\n        norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        # Dynamic weight based on current dominance\n        weight1 = 1.0 if norm_v1 > norm_v2 else 0.7\n        weight2 = 1.0 if norm_v2 > norm_v1 else 0.7\n        fusion_score = weight1 * norm_v1 + weight2 * norm_v2\n        fusion_scores.append((fusion_score, sol))\n\n    selected_sol = max(fusion_scores, key=lambda x: x[0])[1]\n    base_solution = selected_sol.copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Apply capacity-aware perturbation mechanism\n    for _ in range(3):\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        combined_marginal = marginal1 + marginal2\n\n        # Select items to flip based on capacity and marginal contribution\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            # Sort by combined marginal contribution (descending)\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n        candidate_items = np.where(new_solution == 1)[0]\n        if len(candidate_items) > 0:\n            # Sort by combined marginal contribution (ascending)\n            sorted_items = sorted(candidate_items, key=lambda x: combined_marginal[x])\n            for item in sorted_items:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    # Step 3: Multi-phase refinement with dynamic intensity\n    phases = 5\n    for phase in range(phases):\n        phase_intensity = (phases - phase) / phases  # Decreasing intensity\n\n        # Phase 1: Greedy value maximization in most promising objective\n        if random.random() < phase_intensity:\n            if random.random() < 0.5:\n                # Maximize objective 1\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                # Maximize objective 2\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            candidate_items = np.where(new_solution == 0)[0]\n            if len(candidate_items) > 0:\n                sorted_items = sorted(candidate_items, key=lambda x: marginal[x], reverse=True)\n                for item in sorted_items:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                        break\n\n        # Phase 2: Capacity-preserving random perturbations\n        if random.random() < phase_intensity:\n            for _ in range(2):\n                item = random.randint(0, len(weight_lst) - 1)\n                if new_solution[item] == 0:\n                    if current_weight + weight_lst[item] <= capacity:\n                        new_solution[item] = 1\n                        current_weight += weight_lst[item]\n                else:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Final refinement: Ensure solution is on or near Pareto front\n    current_obj1 = np.sum(value1_lst * new_solution)\n    current_obj2 = np.sum(value2_lst * new_solution)\n\n    for _ in range(3):\n        # Find items that could potentially improve both objectives\n        potential_improvement = (value1_lst * (1 - new_solution)) + (value2_lst * (1 - new_solution))\n        candidate_items = np.where(new_solution == 0)[0]\n        if len(candidate_items) > 0:\n            sorted_items = sorted(candidate_items, key=lambda x: potential_improvement[x], reverse=True)\n            for item in sorted_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n\n    return new_solution\n\n",
          "score": [
               -19.07683380182865,
               -18.723969790458725
          ]
     }
]