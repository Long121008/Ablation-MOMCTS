[
    {
        "algorithm": "This algorithm combines Pareto dominance filtering with a value-weighted random walk and dynamic capacity-aware insertion to generate high-quality neighbor solutions for the BI-KP. It prioritizes solutions with the highest combined potential for improvement by evaluating marginal contributions of items relative to current objectives, while ensuring feasibility through capacity checks. The method selectively inserts top-marginal-value items and probabilistically removes low-value items to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate marginal contribution for each candidate item\n        marginal_contributions = []\n        for idx in not_in_sol:\n            if weight_lst[idx] <= remaining_capacity:\n                # Value-weighted marginal contribution\n                marginal_value1 = value1_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_value2 = value2_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_contributions.append(marginal_value1 + marginal_value2)\n            else:\n                marginal_contributions.append(0)\n\n        # Select top-k items based on marginal contribution\n        k = min(3, len(not_in_sol))\n        top_indices = np.argsort(marginal_contributions)[-k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Probabilistic removal of low-value items\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density for each item in solution\n        value_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / weight_lst[in_solution]\n\n        # Remove items below median value density with probability\n        median_density = np.median(value_densities)\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and random.random() < 0.3:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7288380710470508,
            0.43793824315071106
        ],
        "raw_score": [
            28.497859376231716,
            29.141616079332508
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements. It first identifies promising solutions from the archive, then applies cluster-aware perturbations (removing/addings entire value clusters) and greedy capacity-aware additions, while ensuring feasibility through value-density-based refinements. The approach balances exploration (via probabilistic cluster swaps) with exploitation (greedy high-value additions), with a focus on improving both objectives simultaneously.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9076216564059063,
            0.64542555809021
        ],
        "raw_score": [
            28.013844779263998,
            28.45696990371601
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements, while dynamically balancing capacity constraints and objective-specific value clustering. It intelligently selects promising solutions from the archive based on combined potential and applies a three-phase approach: clustering-based probabilistic swaps, greedy capacity-aware additions, and feasibility-preserving refinements. The method ensures feasibility by carefully considering weight constraints at each step and balances exploration of high-value items with exploitation of existing high-density clusters.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7578796122809431,
            0.6056255400180817
        ],
        "raw_score": [
            27.79683777677714,
            28.152085089337195
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8186783791152585,
            0.6194531917572021
        ],
        "raw_score": [
            27.751836714013184,
            28.43983241045889
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a Pareto-aware weighted selection based on potential value improvements, then applies a three-phase hybrid local search: (1) cluster-aware probabilistic perturbation to replace low-value clusters with high-value ones, (2) directed greedy insertion of high-value items based on combined value-to-weight ratios, and (3) feasibility-preserving refinement by removing low-value items when beneficial. The method prioritizes items with high combined value density while strictly maintaining feasibility through probabilistic and greedy selection.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic perturbation\n    combined_values = value1_lst + value2_lst\n    value_density = combined_values / weight_lst\n\n    # Cluster items based on value density\n    from sklearn.cluster import KMeans\n    n_clusters = min(5, len(np.unique(value_density)))\n    if n_clusters > 1:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        clusters = kmeans.fit_predict(value_density.reshape(-1, 1))\n\n        # Identify high and low density clusters\n        cluster_densities = [np.mean(value_density[clusters == i]) for i in range(n_clusters)]\n        high_density_clusters = np.argsort(cluster_densities)[-2:]  # Top 2 clusters\n        low_density_clusters = np.argsort(cluster_densities)[:2]    # Bottom 2 clusters\n\n        # Probabilistically replace low-density clusters with high-density ones\n        for cluster in low_density_clusters:\n            cluster_indices = np.where(clusters == cluster)[0]\n            if np.all(new_solution[cluster_indices] == 1):\n                replace_prob = 0.5 * (1 - cluster_densities[cluster] / np.max(cluster_densities))\n                if np.random.rand() < replace_prob:\n                    new_solution[cluster_indices] = 0\n                    current_weight -= np.sum(weight_lst[cluster_indices])\n                    remaining_capacity += np.sum(weight_lst[cluster_indices])\n\n    # Phase 2: Directed greedy insertion of high-value items\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability proportional to its ratio\n                add_prob = 0.8 * (combined_ratios[idx] / np.max(combined_ratios))\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    current_weight += weight_lst[global_idx]\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.2:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9964201697250465,
            4.176688760519028
        ],
        "raw_score": [
            27.28708230699197,
            28.227709029160614
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid search strategy that first probabilistically flips entire value clusters, then greedily adds high-value items, and finally refines the solution by removing low-density items while preserving feasibility. It prioritizes solutions with high combined potential for improvement and uses value clustering and density-based refinement to explore the solution space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic flips\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster flip\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                            (combined_values > combined_values[idx]) &\n                                            (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9451934946270497,
            0.6548592150211334
        ],
        "raw_score": [
            27.680831496635683,
            27.808755572256224
        ]
    },
    {
        "algorithm": "The algorithm employs a three-phase Pareto-aware hybrid search strategy: first selecting the most promising solution based on a weighted hybrid score combining normalized objectives and capacity utilization, then perturbing item clusters probabilistically based on their Pareto contribution and value density, followed by capacity-constrained greedy insertion of high-value items, and finally refining the solution by replacing underperforming items with Pareto-dominant alternatives while maintaining feasibility. The method prioritizes solutions with higher combined objective values (weighted 60-40) and better capacity utilization, using probabilistic cluster flips and Pareto-aware insertion to explore the solution space efficiently.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest Pareto-aware hybrid score\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    pareto_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n\n        # Calculate Pareto-aware score combining objectives and capacity\n        norm_obj1 = (obj[0] - min_obj[0]) / obj_ranges[0]\n        norm_obj2 = (obj[1] - min_obj[1]) / obj_ranges[1]\n        pareto_score = (0.6 * norm_obj1 + 0.4 * norm_obj2) * (1 - (current_weight / capacity)**2)\n        pareto_scores.append(pareto_score)\n\n    selected_idx = np.argmax(pareto_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware Pareto perturbation\n    combined_values = 0.5 * value1_lst + 0.5 * value2_lst\n    pareto_contributions = combined_values / (weight_lst + 1e-6)\n    contribution_bins = np.linspace(0, np.max(pareto_contributions), 5)\n\n    for i in range(len(contribution_bins)-1):\n        cluster_indices = np.where((pareto_contributions >= contribution_bins[i]) &\n                                 (pareto_contributions < contribution_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_contribution = cluster_value / (cluster_weight + 1e-6)\n\n            if np.random.rand() < 0.6:\n                if np.all(new_solution[cluster_indices] == 1):\n                    remove_prob = 0.8 * (1 - cluster_contribution / (np.max(pareto_contributions) + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    add_prob = 0.8 * (cluster_contribution / (np.max(pareto_contributions) + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Capacity-constrained Pareto insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate Pareto-aware value-to-weight ratios\n        pareto_ratios = (0.5 * value1_lst + 0.5 * value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(pareto_ratios[not_in_sol])[::-1]\n\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Pareto-aware refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate Pareto dominance scores\n        current_values = (value1_lst[in_solution] + value2_lst[in_solution])\n        mean_value = np.mean(current_values)\n\n        for i, idx in enumerate(in_solution):\n            if current_values[i] < mean_value and np.random.rand() < 0.5:\n                # Check for Pareto-improving swaps\n                potential_additions = np.where((new_solution == 0) &\n                                             (pareto_contributions > pareto_contributions[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.979641001671381,
            0.7899167239665985
        ],
        "raw_score": [
            27.592824508037737,
            27.64647335404669
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto-aware selection with a hybrid search strategy that first groups items into value clusters based on combined value-to-weight ratios, then performs probabilistic swaps between clusters while maintaining feasibility, and finally refines the solution by greedily adding high-value items from the best cluster while removing low-contribution items, all while adaptively balancing the two objectives with 60% weight on the first objective and 40% on the second. The selection prioritizes solutions with higher normalized combined objective values, and the search ensures feasibility through careful weight tracking and conditional swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection: choose solution with highest combined normalized objective value\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n    norm_value1 = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    norm_value2 = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n    combined_scores = norm_value1 * 0.6 + norm_value2 * 0.4  # Adaptive weighting\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value cluster identification and probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_ratios = combined_values / (weight_lst + 1e-6)\n    value_bins = np.linspace(np.min(value_ratios), np.max(value_ratios), 4)\n    cluster_indices = [np.where((value_ratios >= value_bins[i]) & (value_ratios < value_bins[i+1]))[0]\n                      for i in range(len(value_bins)-1)]\n\n    # Swap items between clusters probabilistically\n    for i in range(len(cluster_indices)-1):\n        if random.random() < 0.3:\n            from_cluster = cluster_indices[i]\n            to_cluster = cluster_indices[i+1]\n            from_items = np.where((new_solution[from_cluster] == 1) &\n                                (weight_lst[from_cluster] <= remaining_capacity +\n                                np.sum(weight_lst[to_cluster][new_solution[to_cluster] == 0])))[0]\n            to_items = np.where((new_solution[to_cluster] == 0) &\n                              (weight_lst[to_cluster] <= remaining_capacity +\n                              np.sum(weight_lst[from_cluster][new_solution[from_cluster] == 1])))[0]\n            if len(from_items) > 0 and len(to_items) > 0:\n                swap_from = np.random.choice(from_items)\n                swap_to = np.random.choice(to_items)\n                new_weight = current_weight - weight_lst[from_cluster[swap_from]] + weight_lst[to_cluster[swap_to]]\n                if new_weight <= capacity:\n                    new_solution[from_cluster[swap_from]] = 0\n                    new_solution[to_cluster[swap_to]] = 1\n                    current_weight = new_weight\n                    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Greedy cluster-based additions\n    best_cluster = np.argmax([np.mean(combined_values[cluster]) for cluster in cluster_indices])\n    best_cluster_items = cluster_indices[best_cluster]\n    for idx in best_cluster_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and random.random() < 0.2:\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.3012285233895833,
            0.5197923481464386
        ],
        "raw_score": [
            32.21798870570302,
            31.461922366447673
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and capacity utilization\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Find best swap candidates\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        if (value1_lst[out_idx] + value2_lst[out_idx]) > (value1_lst[in_idx] + value2_lst[in_idx]):\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items\n            if random.random() < 0.15 * (1 - (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))):\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items\n            if weight_lst[i] <= remaining_capacity:\n                if random.random() < 0.15 * (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst)):\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.971754987227483,
            3.0680152773857117
        ],
        "raw_score": [
            27.532413913689474,
            28.212249960363902
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a Pareto-aware weighted metric that balances both objectives and potential improvements, then applies a three-phase hybrid local search: first perturbing item clusters based on value-weight balance, second greedily inserting high-value items with strong combined ratios, and third refining by removing low-density items while ensuring feasibility. The solution prioritizes balanced objective improvements and maintains strict capacity constraints throughout.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware weighted selection\n    potentials = []\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    min_obj = np.min(objectives, axis=0)\n    obj_ranges = max_obj - min_obj + 1e-6\n\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Weighted Pareto selection metric\n        norm_obj = (obj - min_obj) / obj_ranges\n        combined_potential = (potential_value1 * (1 + norm_obj[0]) + potential_value2 * (1 + norm_obj[1])) / (1 + norm_obj[0] + norm_obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware value-based perturbations\n    combined_values = value1_lst + value2_lst\n    value_quantiles = np.percentile(combined_values, [33, 66])\n\n    for lower, upper in zip([0, value_quantiles[0], value_quantiles[1]], [value_quantiles[0], value_quantiles[1], np.inf]):\n        cluster_indices = np.where((combined_values >= lower) & (combined_values < upper))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n\n            # Objective balance factor\n            obj1_ratio = np.sum(value1_lst[cluster_indices]) / (cluster_value + 1e-6)\n            obj2_ratio = np.sum(value2_lst[cluster_indices]) / (cluster_value + 1e-6)\n            balance_factor = 1 - abs(obj1_ratio - obj2_ratio)\n\n            if np.random.rand() < 0.5 * balance_factor:\n                if np.all(new_solution[cluster_indices] == 1):\n                    remove_prob = 0.7 * (1 - balance_factor) * (1 - cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    add_prob = 0.7 * balance_factor * (cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Directed value-to-weight ratio insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices[:min(5, len(sorted_indices))]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 2:\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density * 0.8 and np.random.rand() < 0.4:\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n                    if potential_improvement1 > 0 or potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8998308348544921,
            1.7857818901538849
        ],
        "raw_score": [
            27.246667744332,
            27.723550271374588
        ]
    }
]