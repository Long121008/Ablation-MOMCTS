[
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and capacity utilization\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Find best swap candidates\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        if (value1_lst[out_idx] + value2_lst[out_idx]) > (value1_lst[in_idx] + value2_lst[in_idx]):\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items\n            if random.random() < 0.15 * (1 - (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))):\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items\n            if weight_lst[i] <= remaining_capacity:\n                if random.random() < 0.15 * (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst)):\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.971754987227483,
            3.0680152773857117
        ],
        "raw_score": [
            27.532413913689474,
            28.212249960363902
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto dominance filtering with a value-weighted random walk and dynamic capacity-aware insertion to generate high-quality neighbor solutions for the BI-KP. It prioritizes solutions with the highest combined potential for improvement by evaluating marginal contributions of items relative to current objectives, while ensuring feasibility through capacity checks. The method selectively inserts top-marginal-value items and probabilistically removes low-value items to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic capacity-aware insertion\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate marginal contribution for each candidate item\n        marginal_contributions = []\n        for idx in not_in_sol:\n            if weight_lst[idx] <= remaining_capacity:\n                # Value-weighted marginal contribution\n                marginal_value1 = value1_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_value2 = value2_lst[idx] / (current_weight + weight_lst[idx])\n                marginal_contributions.append(marginal_value1 + marginal_value2)\n            else:\n                marginal_contributions.append(0)\n\n        # Select top-k items based on marginal contribution\n        k = min(3, len(not_in_sol))\n        top_indices = np.argsort(marginal_contributions)[-k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Probabilistic removal of low-value items\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density for each item in solution\n        value_densities = (value1_lst[in_solution] + value2_lst[in_solution]) / weight_lst[in_solution]\n\n        # Remove items below median value density with probability\n        median_density = np.median(value_densities)\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and random.random() < 0.3:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7288380710470508,
            0.43793824315071106
        ],
        "raw_score": [
            28.497859376231716,
            29.141616079332508
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements. It first identifies promising solutions from the archive, then applies cluster-aware perturbations (removing/addings entire value clusters) and greedy capacity-aware additions, while ensuring feasibility through value-density-based refinements. The approach balances exploration (via probabilistic cluster swaps) with exploitation (greedy high-value additions), with a focus on improving both objectives simultaneously.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9076216564059063,
            0.64542555809021
        ],
        "raw_score": [
            28.013844779263998,
            28.45696990371601
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid local search strategy that prioritizes cluster-based probabilistic swaps, greedy high-value additions, and feasibility-preserving refinements, while dynamically balancing capacity constraints and objective-specific value clustering. It intelligently selects promising solutions from the archive based on combined potential and applies a three-phase approach: clustering-based probabilistic swaps, greedy capacity-aware additions, and feasibility-preserving refinements. The method ensures feasibility by carefully considering weight constraints at each step and balances exploration of high-value items with exploitation of existing high-density clusters.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    # Group items by combined value and perform probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    # Add top-k high-value items not in solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    # Remove low-value items if adding high-value items would improve both objectives\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.7578796122809431,
            0.6056255400180817
        ],
        "raw_score": [
            27.79683777677714,
            28.152085089337195
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto-aware selection with a three-stage local search: first flipping item clusters based on value-weight balance, then greedily inserting high-value items using a combined value-to-weight ratio, and finally refining by removing low-value items while ensuring feasibility. It prioritizes solutions with high potential improvements and uses adaptive probabilities to balance exploration and exploitation, focusing on both objectives through weighted ratios and density analysis.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Dynamic cluster-aware flips based on value and weight balance\n    combined_values = value1_lst + value2_lst\n    weight_bins = np.linspace(0, np.max(weight_lst), 5)\n    for i in range(len(weight_bins)-1):\n        cluster_indices = np.where((weight_lst >= weight_bins[i]) & (weight_lst < weight_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_value = np.sum(combined_values[cluster_indices])\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            if np.random.rand() < 0.4:  # Higher probability to consider flips\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster with probability based on value-weight balance\n                    remove_prob = 0.6 * (1 - cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < remove_prob:\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster with probability based on value-weight balance\n                    add_prob = 0.6 * (cluster_value / (cluster_weight + 1e-6))\n                    if np.random.rand() < add_prob:\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Adaptive value-to-weight ratio insertion with objective correlation\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items with adaptive probability\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability based on ratio and remaining capacity\n                add_prob = 0.7 * (combined_ratios[idx] / (np.max(combined_ratios) + 1e-6)) * (remaining_capacity / capacity)\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Objective-aware feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    # Calculate the potential improvement in both objectives\n                    potential_improvement1 = np.sum(value1_lst[potential_additions] - value1_lst[idx])\n                    potential_improvement2 = np.sum(value2_lst[potential_additions] - value2_lst[idx])\n\n                    # Only remove if both objectives improve\n                    if potential_improvement1 > 0 and potential_improvement2 > 0:\n                        new_solution[idx] = 0\n                        remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9715724260566645,
            1.5316701233386993
        ],
        "raw_score": [
            27.225662813595225,
            27.723490845037468
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster swap\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8186783791152585,
            0.6194531917572021
        ],
        "raw_score": [
            27.751836714013184,
            28.43983241045889
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a Pareto-aware weighted selection based on potential value improvements, then applies a three-phase hybrid local search: (1) cluster-aware probabilistic perturbation to replace low-value clusters with high-value ones, (2) directed greedy insertion of high-value items based on combined value-to-weight ratios, and (3) feasibility-preserving refinement by removing low-value items when beneficial. The method prioritizes items with high combined value density while strictly maintaining feasibility through probabilistic and greedy selection.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined potential using Pareto-aware weighted selection\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score with Pareto dominance consideration\n        combined_potential = (potential_value1 * (1 + obj[0]) + potential_value2 * (1 + obj[1])) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic perturbation\n    combined_values = value1_lst + value2_lst\n    value_density = combined_values / weight_lst\n\n    # Cluster items based on value density\n    from sklearn.cluster import KMeans\n    n_clusters = min(5, len(np.unique(value_density)))\n    if n_clusters > 1:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        clusters = kmeans.fit_predict(value_density.reshape(-1, 1))\n\n        # Identify high and low density clusters\n        cluster_densities = [np.mean(value_density[clusters == i]) for i in range(n_clusters)]\n        high_density_clusters = np.argsort(cluster_densities)[-2:]  # Top 2 clusters\n        low_density_clusters = np.argsort(cluster_densities)[:2]    # Bottom 2 clusters\n\n        # Probabilistically replace low-density clusters with high-density ones\n        for cluster in low_density_clusters:\n            cluster_indices = np.where(clusters == cluster)[0]\n            if np.all(new_solution[cluster_indices] == 1):\n                replace_prob = 0.5 * (1 - cluster_densities[cluster] / np.max(cluster_densities))\n                if np.random.rand() < replace_prob:\n                    new_solution[cluster_indices] = 0\n                    current_weight -= np.sum(weight_lst[cluster_indices])\n                    remaining_capacity += np.sum(weight_lst[cluster_indices])\n\n    # Phase 2: Directed greedy insertion of high-value items\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n\n        # Sort by combined ratio and select top items\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                # Add item with probability proportional to its ratio\n                add_prob = 0.8 * (combined_ratios[idx] / np.max(combined_ratios))\n                if np.random.rand() < add_prob:\n                    new_solution[global_idx] = 1\n                    current_weight += weight_lst[global_idx]\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinement\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density and remove items below median density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.2:\n                # Check if removing this item allows adding better items\n                potential_additions = np.where((new_solution == 0) &\n                                              (combined_values > combined_values[idx]) &\n                                              (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9964201697250465,
            4.176688760519028
        ],
        "raw_score": [
            27.28708230699197,
            28.227709029160614
        ]
    },
    {
        "algorithm": "This algorithm combines Pareto-aware selection with a hybrid search strategy that first probabilistically flips entire value clusters, then greedily adds high-value items, and finally refines the solution by removing low-density items while preserving feasibility. It prioritizes solutions with high combined potential for improvement and uses value clustering and density-based refinement to explore the solution space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto dominance filtering to select promising solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Select solution with highest combined potential\n    potentials = []\n    for sol, obj in pareto_front:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate potential improvements based on value-weighted random walk\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n\n        # Value-weighted potential score\n        combined_potential = (potential_value1 * obj[0] + potential_value2 * obj[1]) / (obj[0] + obj[1] + 1e-8)\n        potentials.append(combined_potential)\n\n    selected_idx = np.argmax(potentials)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Cluster-aware probabilistic flips\n    combined_values = value1_lst + value2_lst\n    value_bins = np.linspace(0, np.max(combined_values), 5)\n    for i in range(len(value_bins)-1):\n        cluster_indices = np.where((combined_values >= value_bins[i]) & (combined_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(combined_values[cluster_indices])\n            if np.random.rand() < 0.3:  # 30% chance to consider cluster flip\n                if np.all(new_solution[cluster_indices] == 1):\n                    # Remove entire cluster if beneficial\n                    if np.random.rand() < 0.2:  # 20% chance to remove\n                        new_solution[cluster_indices] = 0\n                        remaining_capacity += cluster_weight\n                elif np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    # Add entire cluster if beneficial\n                    if np.random.rand() < 0.5:  # 50% chance to add\n                        new_solution[cluster_indices] = 1\n                        remaining_capacity -= cluster_weight\n\n    # Phase 2: Greedy capacity-aware additions\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratio\n        ratios = combined_values[not_in_sol] / (weight_lst[not_in_sol] + 1e-6)\n        top_k = min(3, len(not_in_sol))\n        top_indices = np.argsort(ratios)[-top_k:][::-1]\n\n        for idx in top_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value density\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n\n        # Consider removing low-density items\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and np.random.rand() < 0.3:\n                # Check if adding high-value items would improve both objectives\n                potential_additions = np.where((new_solution == 0) &\n                                            (combined_values > combined_values[idx]) &\n                                            (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9451934946270497,
            0.6548592150211334
        ],
        "raw_score": [
            27.680831496635683,
            27.808755572256224
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto-aware selection with a hybrid search strategy that first groups items into value clusters based on combined value-to-weight ratios, then performs probabilistic swaps between clusters while maintaining feasibility, and finally refines the solution by greedily adding high-value items from the best cluster while removing low-contribution items, all while adaptively balancing the two objectives with 60% weight on the first objective and 40% on the second. The selection prioritizes solutions with higher normalized combined objective values, and the search ensures feasibility through careful weight tracking and conditional swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection: choose solution with highest combined normalized objective value\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n    norm_value1 = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    norm_value2 = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n    combined_scores = norm_value1 * 0.6 + norm_value2 * 0.4  # Adaptive weighting\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value cluster identification and probabilistic swaps\n    combined_values = value1_lst + value2_lst\n    value_ratios = combined_values / (weight_lst + 1e-6)\n    value_bins = np.linspace(np.min(value_ratios), np.max(value_ratios), 4)\n    cluster_indices = [np.where((value_ratios >= value_bins[i]) & (value_ratios < value_bins[i+1]))[0]\n                      for i in range(len(value_bins)-1)]\n\n    # Swap items between clusters probabilistically\n    for i in range(len(cluster_indices)-1):\n        if random.random() < 0.3:\n            from_cluster = cluster_indices[i]\n            to_cluster = cluster_indices[i+1]\n            from_items = np.where((new_solution[from_cluster] == 1) &\n                                (weight_lst[from_cluster] <= remaining_capacity +\n                                np.sum(weight_lst[to_cluster][new_solution[to_cluster] == 0])))[0]\n            to_items = np.where((new_solution[to_cluster] == 0) &\n                              (weight_lst[to_cluster] <= remaining_capacity +\n                              np.sum(weight_lst[from_cluster][new_solution[from_cluster] == 1])))[0]\n            if len(from_items) > 0 and len(to_items) > 0:\n                swap_from = np.random.choice(from_items)\n                swap_to = np.random.choice(to_items)\n                new_weight = current_weight - weight_lst[from_cluster[swap_from]] + weight_lst[to_cluster[swap_to]]\n                if new_weight <= capacity:\n                    new_solution[from_cluster[swap_from]] = 0\n                    new_solution[to_cluster[swap_to]] = 1\n                    current_weight = new_weight\n                    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Greedy cluster-based additions\n    best_cluster = np.argmax([np.mean(combined_values[cluster]) for cluster in cluster_indices])\n    best_cluster_items = cluster_indices[best_cluster]\n    for idx in best_cluster_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving refinements\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        value_densities = combined_values[in_solution] / weight_lst[in_solution]\n        median_density = np.median(value_densities)\n        for i, idx in enumerate(in_solution):\n            if value_densities[i] < median_density and random.random() < 0.2:\n                potential_additions = np.where((new_solution == 0) &\n                                             (combined_values > combined_values[idx]) &\n                                             (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n                if len(potential_additions) > 0:\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.3012285233895833,
            0.5197923481464386
        ],
        "raw_score": [
            32.21798870570302,
            31.461922366447673
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a dynamic fitness score that combines dominance, objective correlation, and potential improvement, then applies a three-phase local search: probabilistic item swaps weighted by objective-specific value ratios, biased insertion prioritizing underrepresented objectives, and trade-off refinement with adaptive weight adjustment. The selection prioritizes solutions with low dominance count, high objective diversity, and strong potential for improvement, while the local search phases progressively refine the solution by balancing objective trade-offs and maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic selection criterion combining dominance, objective correlation, and potential improvement\n    selection_scores = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n\n        # Calculate dominance rank (simplified)\n        dominated_count = sum(1 for s, o in archive if o[0] > obj[0] and o[1] > obj[1])\n        dominance_score = 1 / (1 + dominated_count)\n\n        # Calculate objective correlation (Pearson correlation coefficient)\n        in_sol = np.where(sol == 1)[0]\n        if len(in_sol) > 1:\n            correlation = np.corrcoef(value1_lst[in_sol], value2_lst[in_sol])[0, 1]\n            correlation_score = 1 - abs(correlation)  # Prefer solutions with low correlation\n        else:\n            correlation_score = 0\n\n        # Calculate potential improvement using weighted utility\n        if len(not_in_sol) > 0:\n            obj1_utility = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n            obj2_utility = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n            obj1_potential = np.sum(obj1_utility * (weight_lst[not_in_sol] <= remaining_capacity))\n            obj2_potential = np.sum(obj2_utility * (weight_lst[not_in_sol] <= remaining_capacity))\n            potential_score = 0.5 * obj1_potential + 0.5 * obj2_potential\n        else:\n            potential_score = 0\n\n        # Combined score with dynamic weighting\n        alpha = 0.6 if np.random.rand() < 0.4 else 0.3  # Randomly shift focus\n        score = alpha * dominance_score + (1 - alpha) * correlation_score + 0.2 * potential_score\n        selection_scores.append(score)\n\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Probabilistic item swaps weighted by objective-specific value ratios\n    in_sol = np.where(new_solution == 1)[0]\n    not_in_sol = np.where(new_solution == 0)[0]\n\n    if len(in_sol) > 0 and len(not_in_sol) > 0:\n        # Calculate normalized objective-specific utility ratios\n        obj1_utility = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-8)\n        obj2_utility = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-8)\n\n        # Combined utility with adaptive objective balance\n        beta = 0.6 if np.random.rand() < 0.5 else 0.4\n        combined_utility = beta * obj1_utility + (1 - beta) * obj2_utility\n\n        # Perform probabilistic swaps\n        for idx in in_sol:\n            swap_prob = 0.3 + 0.7 * (1 - combined_utility[idx])\n            if np.random.rand() < swap_prob:\n                potential_additions = not_in_sol[weight_lst[not_in_sol] <= remaining_capacity + weight_lst[idx]]\n                if len(potential_additions) > 0:\n                    # Select best replacement based on combined utility\n                    best_addition = potential_additions[np.argmax(combined_utility[potential_additions])]\n                    new_solution[idx] = 0\n                    new_solution[best_addition] = 1\n                    remaining_capacity = capacity - np.sum(weight_lst * new_solution)\n\n    # Phase 2: Biased insertion prioritizing underrepresented objectives\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate objective-specific insertion scores with diversity consideration\n        obj1_scores = (value1_lst[not_in_sol] / weight_lst[not_in_sol]) * (1 - np.std(value1_lst[not_in_sol]))\n        obj2_scores = (value2_lst[not_in_sol] / weight_lst[not_in_sol]) * (1 - np.std(value2_lst[not_in_sol]))\n\n        # Normalize scores\n        obj1_scores = (obj1_scores - np.min(obj1_scores)) / (np.max(obj1_scores) - np.min(obj1_scores) + 1e-8)\n        obj2_scores = (obj2_scores - np.min(obj2_scores)) / (np.max(obj2_scores) - np.min(obj2_scores) + 1e-8)\n\n        # Calculate objective dominance in current solution\n        current_obj1 = np.sum(value1_lst * new_solution)\n        current_obj2 = np.sum(value2_lst * new_solution)\n        obj1_bias = 1 if current_obj1 < current_obj2 else 0.7\n        obj2_bias = 1 if current_obj2 < current_obj1 else 0.7\n\n        # Combined insertion score with objective bias\n        insertion_scores = obj1_bias * obj1_scores + obj2_bias * obj2_scores\n\n        # Sort by insertion scores and select top candidates\n        sorted_indices = np.argsort(insertion_scores)[::-1]\n        for idx in sorted_indices[:max(1, len(sorted_indices)//3)]:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                if np.random.rand() < insertion_scores[idx]:\n                    new_solution[global_idx] = 1\n                    remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 3: Trade-off refinement with adaptive weight adjustment\n    in_solution = np.where(new_solution == 1)[0]\n    if len(in_solution) > 1:\n        # Calculate value trade-off for each item considering both objectives\n        value_tradeoffs = []\n        for idx in in_solution:\n            # Calculate potential value loss if removed\n            value_loss1 = value1_lst[idx]\n            value_loss2 = value2_lst[idx]\n\n            # Find best replacement items\n            potential_additions = np.where((new_solution == 0) &\n                                         (weight_lst <= remaining_capacity + weight_lst[idx]))[0]\n            if len(potential_additions) > 0:\n                # Calculate replacement value gain considering both objectives\n                replacement_values1 = value1_lst[potential_additions]\n                replacement_values2 = value2_lst[potential_additions]\n                value_gain1 = np.max(replacement_values1)\n                value_gain2 = np.max(replacement_values2)\n\n                # Calculate normalized trade-off\n                tradeoff1 = (value_loss1 - value_gain1) / (value_loss1 + 1e-8)\n                tradeoff2 = (value_loss2 - value_gain2) / (value_loss2 + 1e-8)\n                tradeoff = 0.5 * tradeoff1 + 0.5 * tradeoff2\n            else:\n                tradeoff = 1.0  # Always remove if no replacements\n\n            value_tradeoffs.append(tradeoff)\n\n        # Remove items with worst trade-offs\n        tradeoff_threshold = np.quantile(value_tradeoffs, 0.3)\n        for i, idx in enumerate(in_solution):\n            if value_tradeoffs[i] > tradeoff_threshold and np.random.rand() < 0.4:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9707458445923708,
            5.030361533164978
        ],
        "raw_score": [
            27.149469290672414,
            28.022493474161525
        ]
    }
]