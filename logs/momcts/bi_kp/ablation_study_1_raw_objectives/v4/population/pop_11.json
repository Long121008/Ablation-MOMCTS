[
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and capacity utilization\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Find best swap candidates\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        if (value1_lst[out_idx] + value2_lst[out_idx]) > (value1_lst[in_idx] + value2_lst[in_idx]):\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items\n            if random.random() < 0.15 * (1 - (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))):\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items\n            if weight_lst[i] <= remaining_capacity:\n                if random.random() < 0.15 * (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst)):\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.971754987227483,
            3.0680152773857117
        ],
        "raw_score": [
            27.532413913689474,
            28.212249960363902
        ]
    },
    {
        "algorithm": "The algorithm dynamically selects a base solution from the archive by balancing Pareto dominance (60% weight) and capacity utilization (40% weight), then applies a hybrid local search combining value-weighted swaps and adaptive flipping, ensuring feasibility by prioritizing items with higher value-to-weight ratios and adjusting probabilities based on remaining capacity. The selection favors non-dominated solutions with better capacity use, while the local search intelligently explores neighbors by probabilistically swapping and flipping items, with higher probabilities for items offering better value density and tighter capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic solution selection based on Pareto dominance and capacity\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Calculate dominance scores (1 if non-dominated, 0 otherwise)\n    dominance = np.ones(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (total_value1[i] <= total_value1[j] and total_value2[i] <= total_value2[j] and\n                    (total_value1[i] < total_value1[j] or total_value2[i] < total_value2[j])):\n                    dominance[i] = 0\n                    break\n\n    # Calculate capacity utilization scores\n    capacity_scores = (capacity - total_weights) / capacity\n\n    # Combine dominance and capacity scores for selection\n    combined_scores = dominance * 0.6 + capacity_scores * 0.4\n    selected_idx = np.random.choice(len(archive), p=combined_scores/combined_scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive probabilities\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Step 1: Value-weighted probabilistic swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value densities\n        included_densities = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        excluded_densities = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n\n        # Select items to swap with probability proportional to value density\n        swap_in = np.random.choice(included, p=included_densities/included_densities.sum())\n        swap_out = np.random.choice(excluded, p=excluded_densities/excluded_densities.sum())\n\n        # Check feasibility\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        new_weight = current_weight - weight_lst[swap_in] + weight_lst[swap_out]\n\n        if new_weight <= capacity:\n            new_solution[swap_in] = 0\n            new_solution[swap_out] = 1\n\n    # Step 2: Adaptive flipping based on value and capacity\n    for i in range(len(new_solution)):\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        remaining_capacity = capacity - current_weight\n\n        if new_solution[i] == 1:\n            # Probability to remove based on value density and remaining capacity\n            flip_prob = 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (1 - remaining_capacity/capacity)\n            if random.random() < flip_prob:\n                new_solution[i] = 0\n        else:\n            # Probability to add based on value density and capacity\n            if weight_lst[i] <= remaining_capacity:\n                flip_prob = 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (remaining_capacity/capacity)\n                if random.random() < flip_prob:\n                    new_solution[i] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9309139264047235,
            3.033954620361328
        ],
        "raw_score": [
            27.608625485677514,
            28.167711895727706
        ]
    },
    {
        "algorithm": "The algorithm combines probabilistic selection of promising solutions from the archive (based on normalized objective scores) with a two-phase local search: first applying objective-aware perturbations (flipping items with low marginal contribution or high potential) and then performing feasibility-aware greedy refinements (adding high-value items without exceeding capacity), while occasionally introducing random flips for diversification. It prioritizes items with high combined value-to-weight ratios while ensuring feasibility throughout the process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Probabilistic selection based on normalized objective scores\n    objectives = np.array([obj for _, obj in archive])\n    normalized_scores = objectives / (np.max(objectives, axis=0) + 1e-6)\n    combined_scores = np.sum(normalized_scores, axis=1)\n    selection_probs = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current state\n    current_weight = np.sum(weight_lst * new_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Two-phase local search\n    # Part A: Objective-aware perturbations\n    marginal_contrib = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    flip_prob = 0.2 * (1 - new_solution) + 0.1 * new_solution  # Higher for excluded items\n\n    for i in range(len(new_solution)):\n        if np.random.rand() < flip_prob[i]:\n            if new_solution[i] == 1:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n                    remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n                    remaining_capacity -= weight_lst[i]\n\n    # Part B: Feasibility-aware greedy refinements\n    if remaining_capacity > 0:\n        # Prioritize items with highest value-to-weight ratio\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        sorted_indices = np.argsort(value_ratios)[::-1]\n\n        for i in sorted_indices:\n            if new_solution[i] == 0 and weight_lst[i] <= remaining_capacity:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n                if remaining_capacity <= 0:\n                    break\n\n    # Additional diversification\n    if np.random.rand() < 0.2:  # 20% chance to perform a random flip\n        flip_idx = np.random.choice(len(new_solution))\n        if new_solution[flip_idx] == 1:\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                new_solution[flip_idx] = 0\n        else:\n            if weight_lst[flip_idx] <= capacity - np.sum(weight_lst * new_solution):\n                new_solution[flip_idx] = 1\n\n    return new_solution\n\n",
        "metric_score": [
            -0.40520448053214925,
            0.556917279958725
        ],
        "raw_score": [
            52.31080247199637,
            51.89192637766801
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate selection scores based on both objectives and normalized Pareto dominance\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize objectives\n    norm_value1 = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    norm_value2 = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score (Pareto-guided)\n    scores = 0.5 * norm_value1 + 0.5 * norm_value2\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-aware cluster flips\n    # Group items by value ranges and flip clusters probabilistically\n    value_bins = np.linspace(0, np.max(value1_lst + value2_lst), 5)\n    value_centers = (value_bins[:-1] + value_bins[1:]) / 2\n    item_values = value1_lst + value2_lst\n\n    for i in range(len(value_centers)):\n        cluster_indices = np.where((item_values >= value_bins[i]) & (item_values < value_bins[i+1]))[0]\n        if len(cluster_indices) > 0:\n            cluster_weight = np.sum(weight_lst[cluster_indices])\n            cluster_value = np.sum(item_values[cluster_indices])\n            flip_prob = 0.3 if np.random.rand() < 0.5 else 0.1  # Higher probability for some clusters\n\n            if np.random.rand() < flip_prob:\n                # Flip entire cluster if it fits\n                if np.all(new_solution[cluster_indices] == 0) and cluster_weight <= remaining_capacity:\n                    new_solution[cluster_indices] = 1\n                    remaining_capacity -= cluster_weight\n                elif np.all(new_solution[cluster_indices] == 1):\n                    new_solution[cluster_indices] = 0\n                    remaining_capacity += cluster_weight\n\n    # Phase 2: Capacity-aware sequential additions\n    # Add items in order of decreasing value-to-weight ratio, with probabilistic skip\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)[::-1]\n\n    for i in sorted_indices:\n        if new_solution[i] == 0 and weight_lst[i] <= remaining_capacity:\n            # Add with probability based on value\n            add_prob = 0.8 * (item_values[i] / np.max(item_values)) ** 2\n            if np.random.rand() < add_prob:\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Phase 3: Feasibility-preserving swaps\n    # Swap low-value items with high-value items when beneficial\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        low_value_included = included[np.argsort(item_values[included])[:len(included)//2]]\n        high_value_excluded = excluded[np.argsort(item_values[excluded])[::-1][:len(excluded)//2]]\n\n        for in_idx in low_value_included:\n            for out_idx in high_value_excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity and item_values[out_idx] > item_values[in_idx]:\n                        new_solution[in_idx] = 0\n                        new_solution[out_idx] = 1\n                        current_weight = new_weight\n                        remaining_capacity = capacity - current_weight\n                        break\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9225156853153487,
            0.8376076817512512
        ],
        "raw_score": [
            27.39273147521852,
            28.34011453450967
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid scoring mechanism that prioritizes high-value items, capacity utilization, and diversity, then applies a multi-phase local search: Phase 1 adds high-value items, Phase 2 performs capacity-aware swaps with a value improvement threshold, and Phase 3 uses adaptive probabilistic diversification to balance exploration and exploitation. The hybrid scoring gives higher weight to value objectives when solutions are above median quality, while diversity is always considered.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate hybrid selection scores combining objectives, capacity, and diversity\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Calculate diversity scores (distance to other solutions)\n    diversity_scores = []\n    for i, (sol, _) in enumerate(archive):\n        distances = [np.sum(sol != s[0]) for j, (s, _) in enumerate(archive) if j != i]\n        diversity_scores.append(np.mean(distances) if distances else 0)\n    diversity_scores = np.array(diversity_scores)\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n    diversity_scores = diversity_scores / (np.max(diversity_scores) + 1e-8)\n\n    # Combined selection score with adaptive weights\n    total_score = total_value1 + total_value2\n    avg_score = np.mean(total_score)\n    weight1 = 0.4 if avg_score > np.median(total_score) else 0.3\n    weight2 = 0.3 if avg_score > np.median(total_score) else 0.4\n    scores = weight_scores * 0.3 + value1_scores * weight1 + value2_scores * weight2 + diversity_scores * 0.2\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Value-driven selection with capacity constraint\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        # Calculate combined value-to-weight ratios with adaptive importance\n        importance = (value1_lst + value2_lst) / (np.max(value1_lst) + np.max(value2_lst))\n        ratios = (value1_lst[excluded] + value2_lst[excluded]) * importance[excluded] / weight_lst[excluded]\n        sorted_indices = np.argsort(ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = excluded[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    # Phase 2: Capacity-aware swaps with value improvement threshold\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate average value per weight for current solution\n        avg_value_per_weight = (total_value1[selected_idx] + total_value2[selected_idx]) / total_weights[selected_idx]\n\n        for in_idx in included:\n            for out_idx in excluded:\n                if weight_lst[out_idx] <= remaining_capacity + weight_lst[in_idx]:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        value_improvement = (value1_lst[out_idx] + value2_lst[out_idx]) - (value1_lst[in_idx] + value2_lst[in_idx])\n                        if value_improvement > 0.1 * avg_value_per_weight:  # Only accept if significant improvement\n                            new_solution[in_idx] = 0\n                            new_solution[out_idx] = 1\n                            current_weight = new_weight\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    # Phase 3: Adaptive probabilistic diversification\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Higher probability to remove low-value items with adaptive threshold\n            value_ratio = (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))\n            prob_threshold = 0.2 * (1 - value_ratio) * (1 - current_weight/capacity)\n            if random.random() < prob_threshold:\n                new_solution[i] = 0\n        else:\n            # Higher probability to add high-value items with capacity consideration\n            if weight_lst[i] <= remaining_capacity:\n                value_ratio = (value1_lst[i] + value2_lst[i]) / (np.max(value1_lst) + np.max(value2_lst))\n                prob_threshold = 0.2 * value_ratio * (remaining_capacity/capacity)\n                if random.random() < prob_threshold:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9383033652959242,
            9.323322862386703
        ],
        "raw_score": [
            27.29910286413015,
            27.850987440226127
        ]
    },
    {
        "algorithm": "The algorithm combines a weighted selection strategy (prioritizing solutions with high potential improvement based on normalized value and remaining capacity scores) with a hybrid local search that first probabilistically perturbs items based on their combined value-to-weight ratios and then greedily refines the solution by iteratively adding the most promising items until no further feasible improvements can be made. The selection emphasizes both objectives and capacity awareness, while the local search balances exploration (via probabilistic flips) with exploitation (via directed capacity-aware additions).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement scores\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score\n    scores = weight_scores * 0.4 + value1_scores * 0.3 + value2_scores * 0.3\n    selected_idx = np.random.choice(len(archive), p=scores/scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Value-aware perturbation phase\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove based on value density\n            if random.random() < 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8):\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n        else:\n            # Probability to add based on value density\n            if (remaining_capacity >= weight_lst[i]) and \\\n               (random.random() < 0.2 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8)):\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Capacity-aware refinement phase\n    added_items = []\n    while True:\n        # Calculate value-to-weight ratios for items not in solution\n        not_in_sol = np.where(new_solution == 0)[0]\n        if len(not_in_sol) == 0:\n            break\n\n        # Filter items that fit in remaining capacity\n        feasible_items = not_in_sol[weight_lst[not_in_sol] <= remaining_capacity]\n        if len(feasible_items) == 0:\n            break\n\n        # Select item with highest combined value-to-weight ratio\n        combined_ratios = (value1_lst[feasible_items] + value2_lst[feasible_items]) / weight_lst[feasible_items]\n        best_item = feasible_items[np.argmax(combined_ratios)]\n\n        # Add the best item\n        new_solution[best_item] = 1\n        remaining_capacity -= weight_lst[best_item]\n        added_items.append(best_item)\n\n        # Check if adding more items is beneficial\n        if len(added_items) > 3:  # Limit to prevent excessive computation\n            break\n\n    return new_solution\n\n",
        "metric_score": [
            -0.3888242975453579,
            0.6292133629322052
        ],
        "raw_score": [
            36.7724156200035,
            37.5580101790531
        ]
    },
    {
        "algorithm": "The algorithm combines probabilistic selection of solutions from the archive (prioritizing those with higher combined normalized objective scores) with a two-phase local search: Phase 1 probabilistically flips items based on their combined marginal contribution to both objectives, favoring inclusion of items with high value-to-weight ratios, while Phase 2 greedily adds high-value items that fit within remaining capacity. The method balances exploration (via probabilistic perturbations) with exploitation (via greedy refinements) while ensuring feasibility through dynamic capacity tracking.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Probabilistic selection based on normalized objective scores\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    normalized_scores = objectives / (max_obj + 1e-6)  # Avoid division by zero\n    combined_scores = np.sum(normalized_scores, axis=1)\n    selection_probs = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Objective-aware probabilistic perturbations\n    marginal_contrib1 = value1_lst / (weight_lst + 1e-6)\n    marginal_contrib2 = value2_lst / (weight_lst + 1e-6)\n    combined_contrib = marginal_contrib1 + marginal_contrib2\n    flip_probs = 0.5 * (1 - base_solution) * combined_contrib + 0.1 * base_solution  # Higher prob for 0s\n    flip_probs = flip_probs / (np.max(flip_probs) + 1e-6)  # Normalize to [0,1]\n\n    for i in range(len(new_solution)):\n        if np.random.rand() < flip_probs[i]:\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    new_solution[i] = 1\n                    remaining_capacity -= weight_lst[i]\n\n    # Phase 2: Greedy refinements for high-value items\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Sort by combined value-to-weight ratio\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n        combined_ratios = ratio1 + ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.38505190229076525,
            0.566772073507309
        ],
        "raw_score": [
            53.38986924694199,
            53.050468075553894
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive based on combined potential improvement in both objectives, then applies a hybrid local search: first performing random bit flips to escape local optima, followed by a directed search for items with high combined value-to-weight ratios for both objectives, ensuring feasibility by dynamically tracking remaining capacity. The selection prioritizes solutions with the highest normalized potential improvement, while the neighbor generation balances exploration (random flips) with exploitation (directed selection).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement for each solution\n    potentials = []\n    for sol, obj in archive:\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        # Potential items to add (not currently in solution)\n        not_in_sol = np.where(sol == 0)[0]\n        # Potential improvement in objective 1\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        # Potential improvement in objective 2\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        # Combined potential (normalized to avoid bias)\n        combined_potential = (potential_value1 + potential_value2) / (1 + obj[0] + obj[1])\n        potentials.append(combined_potential)\n\n    # Select solution with highest potential\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Generate neighbor solution\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: random flip with directed selection\n    # Step 1: Randomly flip a few bits (to escape local optima)\n    num_flips = min(3, len(new_solution))\n    flip_indices = random.sample(range(len(new_solution)), num_flips)\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            # Only add if it doesn't exceed capacity\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 2: Directed search for items that could improve either objective\n    # Consider items not in current solution\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        # Calculate value-to-weight ratios for both objectives\n        ratio1 = value1_lst[not_in_sol] / weight_lst[not_in_sol]\n        ratio2 = value2_lst[not_in_sol] / weight_lst[not_in_sol]\n\n        # Select items with highest combined ratio that fit in remaining capacity\n        combined_ratios = ratio1 + ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9079633024147904,
            1.0491748750209808
        ],
        "raw_score": [
            27.36814387336429,
            27.95142773130081
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate potential improvement scores\n    total_weights = np.array([np.sum(weight_lst[s[0] == 1]) for s in archive])\n    total_value1 = np.array([s[1][0] for s in archive])\n    total_value2 = np.array([s[1][1] for s in archive])\n\n    # Normalize scores\n    weight_scores = (capacity - total_weights) / capacity\n    value1_scores = (total_value1 - np.min(total_value1)) / (np.max(total_value1) - np.min(total_value1) + 1e-8)\n    value2_scores = (total_value2 - np.min(total_value2)) / (np.max(total_value2) - np.min(total_value2) + 1e-8)\n\n    # Combined selection score with emphasis on both objectives and capacity\n    scores = weight_scores * 0.5 + value1_scores * 0.25 + value2_scores * 0.25\n    selected_idx = np.random.choice(len(archive), p=scores/scores.sum())\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Novel hybrid local search: Value-aware cluster swapping with probabilistic refinement\n    # Step 1: Identify value clusters and swap between them\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 1 and len(excluded) > 1:\n        # Create value clusters based on combined value-to-weight ratios\n        included_ratios = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        excluded_ratios = (value1_lst[excluded] + value2_lst[excluded]) / weight_lst[excluded]\n\n        # Sort items in each cluster by their value-to-weight ratio\n        high_included = included[np.argsort(included_ratios)[-min(3, len(included)):]]\n        low_excluded = excluded[np.argsort(excluded_ratios)[:min(3, len(excluded))]]\n\n        # Attempt swaps between high-value included and low-value excluded items\n        for hi in high_included:\n            for lo in low_excluded:\n                new_weight = current_weight - weight_lst[hi] + weight_lst[lo]\n                if new_weight <= capacity:\n                    new_solution[hi] = 0\n                    new_solution[lo] = 1\n                    current_weight = new_weight\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Step 2: Probabilistic refinement with value-aware flips and capacity-aware additions\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove based on value density and capacity pressure\n            remove_prob = 0.1 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (1 - remaining_capacity/capacity)\n            if random.random() < remove_prob:\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n        else:\n            # Probability to add based on value density and capacity availability\n            add_prob = 0.1 * (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-8) * (remaining_capacity/weight_lst[i])\n            if (remaining_capacity >= weight_lst[i]) and (random.random() < add_prob):\n                new_solution[i] = 1\n                remaining_capacity -= weight_lst[i]\n\n    # Step 3: Greedy capacity-aware refinement\n    while True:\n        # Calculate value-to-weight ratios for items not in solution\n        not_in_sol = np.where(new_solution == 0)[0]\n        if len(not_in_sol) == 0:\n            break\n\n        # Filter items that fit in remaining capacity\n        feasible_items = not_in_sol[weight_lst[not_in_sol] <= remaining_capacity]\n        if len(feasible_items) == 0:\n            break\n\n        # Select item with highest combined value-to-weight ratio\n        combined_ratios = (value1_lst[feasible_items] + value2_lst[feasible_items]) / weight_lst[feasible_items]\n        best_item = feasible_items[np.argmax(combined_ratios)]\n\n        # Add the best item\n        new_solution[best_item] = 1\n        remaining_capacity -= weight_lst[best_item]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.43483846301394125,
            0.9316346347332001
        ],
        "raw_score": [
            43.37854733532269,
            43.24897827956235
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive by balancing diversity and potential improvement, then applies a hybrid local search that combines probabilistic bit flips (with higher probability for items with lower value-to-weight ratios) and a greedy selection of items with the highest normalized combined value-to-weight ratios, ensuring feasibility through dynamic capacity tracking. The selection prioritizes solutions with higher diversity and potential improvement, while the local search strategically explores the solution space by probabilistically flipping bits and greedily adding high-value items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate diversity and potential improvement for each solution\n    diversity_scores = []\n    potential_scores = []\n    for sol, obj in archive:\n        # Diversity: count of items not in other solutions\n        diversity = sum(1 for s, _ in archive if not np.array_equal(s, sol))\n        diversity_scores.append(diversity)\n\n        # Potential improvement\n        current_weight = np.sum(weight_lst * sol)\n        remaining_capacity = capacity - current_weight\n        not_in_sol = np.where(sol == 0)[0]\n        potential_value1 = np.sum(value1_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_value2 = np.sum(value2_lst[not_in_sol] * (weight_lst[not_in_sol] <= remaining_capacity))\n        potential_scores.append((potential_value1 + potential_value2) / (1 + np.sum(obj)))\n\n    # Combine diversity and potential scores (weighted)\n    combined_scores = [0.7 * d + 0.3 * p for d, p in zip(diversity_scores, potential_scores)]\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    remaining_capacity = capacity - current_weight\n\n    # Hybrid local search\n    new_solution = base_solution.copy()\n\n    # Step 1: Probabilistic bit flips (higher probability for worse items)\n    for i in range(len(new_solution)):\n        if random.random() < 0.3:  # Base probability\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                remaining_capacity += weight_lst[i]\n            else:\n                if weight_lst[i] <= remaining_capacity:\n                    # Higher probability to add items with lower combined ratio\n                    ratio1 = value1_lst[i] / weight_lst[i]\n                    ratio2 = value2_lst[i] / weight_lst[i]\n                    combined_ratio = ratio1 + ratio2\n                    if random.random() < 1 / (1 + combined_ratio):  # Inverse probability\n                        new_solution[i] = 1\n                        remaining_capacity -= weight_lst[i]\n\n    # Step 2: Greedy selection of items with highest normalized combined ratio\n    not_in_sol = np.where(new_solution == 0)[0]\n    if len(not_in_sol) > 0:\n        normalized_ratio1 = (value1_lst[not_in_sol] / weight_lst[not_in_sol]) / (1 + np.sum(value1_lst))\n        normalized_ratio2 = (value2_lst[not_in_sol] / weight_lst[not_in_sol]) / (1 + np.sum(value2_lst))\n        combined_ratios = normalized_ratio1 + normalized_ratio2\n        sorted_indices = np.argsort(combined_ratios)[::-1]\n\n        for idx in sorted_indices:\n            global_idx = not_in_sol[idx]\n            if weight_lst[global_idx] <= remaining_capacity:\n                new_solution[global_idx] = 1\n                remaining_capacity -= weight_lst[global_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.3426667545408827,
            0.7310943007469177
        ],
        "raw_score": [
            52.0280329321086,
            52.01414403529617
        ]
    }
]