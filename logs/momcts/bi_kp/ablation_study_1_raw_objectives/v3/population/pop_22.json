[
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution using a weighted random choice\n    weights = [obj[0] + obj[1] for (sol, obj) in archive]\n    selected_idx = random.choices(range(len(archive)), weights=weights, k=1)[0]\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n\n    # Step 4: Identify candidate items to flip\n    candidate_indices = np.where((base_solution == 0) & (weight_lst <= remaining_capacity))[0]\n    if len(candidate_indices) == 0:\n        # If no items can be added, try removing low-value items\n        candidate_indices = np.where(base_solution == 1)[0]\n        if len(candidate_indices) == 0:\n            return base_solution  # No change possible\n\n    # Step 5: Select top-k items based on combined marginal contributions\n    k = max(1, min(5, len(candidate_indices) // 2))\n    top_indices = np.argsort(combined_marginal[candidate_indices])[-k:][::-1]\n\n    # Step 6: Create new solution by flipping selected items\n    new_solution = base_solution.copy()\n    for idx in candidate_indices[top_indices]:\n        if new_solution[idx] == 0 and (current_weight + weight_lst[idx]) <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n        elif new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Step 7: Dynamic weight adjustment phase\n    while current_weight > capacity:\n        # Identify items to remove based on least combined marginal value\n        in_items = np.where(new_solution == 1)[0]\n        if len(in_items) == 0:\n            break\n        worst_item = in_items[np.argmin(combined_marginal[in_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.8379839795366024,
            0.18839186429977417
        ],
        "raw_score": [
            27.127153134971913,
            27.930397004202053
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive with a weighted random choice prioritizing solutions with higher combined objective values (70% weight for value1, 30% for value2), then employs a dynamic flipping strategy that considers both addition and removal of items based on their normalized marginal contributions (60% weight for value1, 40% for value2), while ensuring feasibility through capacity-aware adjustments and occasional random perturbations to escape local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution using a weighted random choice\n    weights = [obj[0] * 0.7 + obj[1] * 0.3 for (sol, obj) in archive]\n    selected_idx = random.choices(range(len(archive)), weights=weights, k=1)[0]\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate normalized marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2\n\n    # Step 4: Identify candidate items to flip\n    candidate_indices = np.where((base_solution == 0) & (weight_lst <= remaining_capacity))[0]\n    if len(candidate_indices) == 0:\n        candidate_indices = np.where(base_solution == 1)[0]\n        if len(candidate_indices) == 0:\n            return base_solution\n\n    # Step 5: Select top items based on combined marginal contributions\n    k = max(1, min(3, len(candidate_indices) // 3))\n    top_indices = np.argsort(combined_marginal[candidate_indices])[-k:][::-1]\n\n    # Step 6: Create new solution by flipping selected items\n    new_solution = base_solution.copy()\n    for idx in candidate_indices[top_indices]:\n        if new_solution[idx] == 0 and (current_weight + weight_lst[idx]) <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n        elif new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Step 7: Adaptive adjustment phase\n    while current_weight > capacity:\n        in_items = np.where(new_solution == 1)[0]\n        if len(in_items) == 0:\n            break\n        worst_item = in_items[np.argmin(combined_marginal[in_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Step 8: Random perturbation to escape local optima\n    if random.random() < 0.3:\n        perturb_idx = np.random.choice(np.where(base_solution != new_solution)[0])\n        new_solution[perturb_idx] = 1 - new_solution[perturb_idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9067608502595553,
            0.1949823498725891
        ],
        "raw_score": [
            27.17264943660627,
            27.61900648416579
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive based on combined objective values and weight efficiency, then applies a hybrid local search that prioritizes high-marginal-contribution items for swapping while maintaining feasibility, with occasional adaptive perturbations for exploration. It emphasizes both objective optimization and weight balance, using marginal contribution analysis to guide item selection and removal, and includes a final feasibility check to ensure the solution remains valid.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high combined objective value and near-capacity weight\n    selected = max(archive, key=lambda x: (x[1][0] + x[1][1]) * (1 - (np.sum(weight_lst[x[0] == 1]) / capacity)) if np.sum(weight_lst[x[0] == 1]) <= capacity else -np.inf)\n    selected_solution = selected[0].copy()\n    current_weight = np.sum(weight_lst[selected_solution == 1])\n\n    # Step 2: Calculate normalized marginal contributions\n    weight_norm = weight_lst + 1e-6\n    marginal1 = value1_lst / weight_norm\n    marginal2 = value2_lst / weight_norm\n    combined_marginal = (marginal1 + marginal2) * (1 + 0.1 * np.random.rand(len(weight_lst)))\n\n    # Step 3: Identify candidate items for hybrid local search\n    out_items = np.where(selected_solution == 0)[0]\n    in_items = np.where(selected_solution == 1)[0]\n\n    # Step 4: Apply hybrid local search (item swapping + marginal contribution)\n    new_solution = selected_solution.copy()\n    if len(out_items) > 0 and len(in_items) > 0:\n        # Select top marginal items to add\n        add_candidates = out_items[np.argsort(combined_marginal[out_items])[-min(3, len(out_items)):]]\n        # Select worst marginal items to remove\n        remove_candidates = in_items[np.argsort(combined_marginal[in_items])[:min(3, len(in_items))]]\n\n        # Perform swaps while maintaining feasibility\n        for add_idx in add_candidates:\n            if current_weight + weight_lst[add_idx] <= capacity:\n                new_solution[add_idx] = 1\n                current_weight += weight_lst[add_idx]\n\n        for remove_idx in remove_candidates:\n            if new_solution[remove_idx] == 1:\n                new_solution[remove_idx] = 0\n                current_weight -= weight_lst[remove_idx]\n\n    # Step 5: Adaptive perturbation for exploration\n    if random.random() < 0.3:\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 1:\n            flip_idx1, flip_idx2 = random.sample(list(flip_candidates), 2)\n            if weight_lst[flip_idx1] + weight_lst[flip_idx2] <= capacity:\n                new_solution[flip_idx1], new_solution[flip_idx2] = new_solution[flip_idx2], new_solution[flip_idx1]\n\n    # Step 6: Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        worst_item = np.argmin(combined_marginal[new_solution == 1])\n        new_solution[np.where(new_solution == 1)[0][worst_item]] = 0\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9414381367377272,
            0.6581200361251831
        ],
        "raw_score": [
            27.463074193494123,
            28.070324531844413
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive by prioritizing those with high entropy (diversity), near-capacity weight utilization, and high Pareto dominance, then applies a hybrid local search that flips items based on weighted marginal contributions (balancing both objectives) while dynamically adjusting perturbation intensity and ensuring feasibility by removing low-value items if capacity is exceeded. The selection score balances entropy, weight utilization, and dominance, while the local search prioritizes high-marginal-value items and adaptively adjusts flips to avoid local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    def selection_score(sol_obj):\n        sol = sol_obj[0]\n        entropy = -np.sum(sol * np.log(sol + 1e-6) + (1 - sol) * np.log(1 - sol + 1e-6))\n        weight_util = np.sum(weight_lst[sol == 1]) / capacity\n        # Pareto dominance factor: count how many solutions this dominates\n        dominates = sum(1 for (_, (v1, v2)) in archive if (v1 <= sol_obj[1][0] and v2 <= sol_obj[1][1]) and (v1 < sol_obj[1][0] or v2 < sol_obj[1][1]))\n        return entropy * (1 - abs(weight_util - 0.9)) * (1 + dominates)\n\n    selected_solution = max(archive, key=selection_score)[0].copy()\n    new_solution = selected_solution.copy()\n\n    # Hybrid local search: flip items based on weighted marginal contributions\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    marginal_contributions = np.sqrt(norm_value1 * norm_value2) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_contributions)[::-1]\n\n    # Dynamic perturbation: adjust number of flips based on local optima\n    k = max(1, len(new_solution) // 5)\n    for idx in sorted_indices[:k]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Feasibility check: remove items with lowest combined value-to-weight ratios if needed\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_indices = np.where(new_solution == 1)[0]\n        value_ratios = (value1_lst[included_indices] + value2_lst[included_indices]) / (weight_lst[included_indices] + 1e-6)\n        sorted_least_val = included_indices[np.argsort(value_ratios)]\n        for idx in sorted_least_val:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9450169886182402,
            1.6589842736721039
        ],
        "raw_score": [
            27.461915009353113,
            27.871701693291868
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive using a hybrid scoring system that balances Pareto dominance and trade-off sensitivity, then applies a multi-phase local search with strategic item swaps, capacity-aware additions, marginal utility thresholding removals, and dynamic perturbations, while ensuring feasibility through a greedy removal mechanism. It prioritizes balanced trade-offs by dynamically adjusting the weighting of objectives (\u03b2) and uses marginal utility thresholds to remove low-value items, with higher emphasis on solutions with promising potential for improvement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection combining Pareto dominance and trade-off sensitivity\n    dominance_scores = []\n    tradeoff_scores = []\n    for (sol, obj) in archive:\n        # Calculate Pareto dominance score\n        dom_count = sum(1 for (_, o) in archive if o[0] >= obj[0] and o[1] >= obj[1] and (o[0] > obj[0] or o[1] > obj[1]))\n        dominance_scores.append(1.0 / (1.0 + dom_count))\n\n        # Calculate trade-off sensitivity score\n        if obj[1] > 0:\n            tradeoff = obj[0] / obj[1]\n            tradeoff_scores.append(1.0 / (1.0 + abs(tradeoff - 0.5)))  # Prefer balanced trade-offs\n        else:\n            tradeoff_scores.append(1.0)\n\n    # Combine scores with adaptive weights\n    alpha = 0.7 if len(archive) > 5 else 0.3  # More dominance focus with larger archive\n    combined_scores = [alpha * d + (1 - alpha) * t for d, t in zip(dominance_scores, tradeoff_scores)]\n    selected_idx = random.choices(range(len(archive)), weights=combined_scores, k=1)[0]\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state and marginal utilities\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n    value1_total = np.sum(value1_lst[base_solution == 1])\n    value2_total = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Calculate adaptive marginal utilities with capacity awareness\n    weight_norm = weight_lst + 1e-6\n    marginal1 = value1_lst / weight_norm\n    marginal2 = value2_lst / weight_norm\n\n    # Dynamic trade-off coefficient based on current solution's balance\n    if value2_total > 0:\n        current_tradeoff = value1_total / value2_total\n        beta = min(1.0, max(0.0, 1.0 - 0.5 * abs(current_tradeoff - 1.0)))  # Prefer balanced trade-offs\n    else:\n        beta = 0.5\n\n    combined_marginal = (1 - beta) * marginal1 + beta * marginal2\n\n    # Step 4: Multi-phase local search with adaptive operations\n    new_solution = base_solution.copy()\n\n    # Phase 1: Strategic item swaps\n    swap_candidates = np.where(new_solution == 1)[0]\n    if len(swap_candidates) > 1:\n        for _ in range(min(2, len(swap_candidates))):\n            i, j = random.sample(list(swap_candidates), 2)\n            if (current_weight - weight_lst[i] + weight_lst[j]) <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Phase 2: Capacity-aware additions\n    candidate_indices = np.where(new_solution == 0)[0]\n    sorted_candidates = candidate_indices[np.argsort(combined_marginal[candidate_indices])[-min(3, len(candidate_indices)):]]\n    for idx in sorted_candidates:\n        if (np.sum(weight_lst[new_solution == 1]) + weight_lst[idx]) <= capacity:\n            new_solution[idx] = 1\n\n    # Phase 3: Marginal utility thresholding removals\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        threshold = np.percentile(combined_marginal[in_items], 25)  # Remove bottom 25% marginal items\n        for idx in in_items:\n            if combined_marginal[idx] < threshold:\n                new_solution[idx] = 0\n\n    # Step 5: Dynamic perturbation based on solution quality\n    if random.random() < 0.3:  # 30% perturbation chance\n        perturbation_size = min(3, len(np.where(new_solution == 1)[0]))\n        flip_indices = random.sample(list(np.where(new_solution == 1)[0]), perturbation_size)\n        for idx in flip_indices:\n            if (np.sum(weight_lst[new_solution == 1]) - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n\n    # Step 6: Final feasibility check with greedy removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        worst_item = np.argmin(combined_marginal[new_solution == 1])\n        new_solution[np.where(new_solution == 1)[0][worst_item]] = 0\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9260302099970282,
            0.5997877717018127
        ],
        "raw_score": [
            27.26532074552231,
            27.741456717172184
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid scoring system that prioritizes Pareto dominance (60%), trade-off balance (30%), and weight efficiency (10%), then applies a multi-phase local search with adaptive marginal flips, dynamic trade-off-aware swaps, and feasibility-preserving removals, while dynamically adjusting perturbation intensity to maintain high-quality neighbors while ensuring feasibility. The selection balances exploration (via marginal utilities) and exploitation (via solution quality), while the local search intelligently balances objective trade-offs and capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection combining Pareto dominance, trade-off sensitivity, and weight efficiency\n    dominance_scores = []\n    tradeoff_scores = []\n    weight_util_scores = []\n    for (sol, obj) in archive:\n        # Pareto dominance score\n        dom_count = sum(1 for (_, o) in archive if o[0] >= obj[0] and o[1] >= obj[1] and (o[0] > obj[0] or o[1] > obj[1]))\n        dominance_scores.append(1.0 / (1.0 + dom_count))\n\n        # Trade-off sensitivity score (prefer balanced trade-offs)\n        if obj[1] > 0:\n            tradeoff = obj[0] / obj[1]\n            tradeoff_scores.append(1.0 / (1.0 + abs(tradeoff - 1.0)))\n        else:\n            tradeoff_scores.append(1.0)\n\n        # Weight efficiency score (prefer near-capacity solutions)\n        current_weight = np.sum(weight_lst[sol == 1])\n        weight_util_scores.append(1.0 / (1.0 + abs(current_weight / capacity - 0.9)))\n\n    # Combine scores with adaptive weights\n    alpha = 0.6  # Dominance weight\n    beta = 0.3   # Trade-off weight\n    gamma = 0.1  # Weight efficiency weight\n    combined_scores = [alpha * d + beta * t + gamma * w for d, t, w in zip(dominance_scores, tradeoff_scores, weight_util_scores)]\n    selected_idx = random.choices(range(len(archive)), weights=combined_scores, k=1)[0]\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state and marginal utilities\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n    value1_total = np.sum(value1_lst[base_solution == 1])\n    value2_total = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Calculate adaptive marginal utilities with dynamic trade-off awareness\n    weight_norm = weight_lst + 1e-6\n    marginal1 = value1_lst / weight_norm\n    marginal2 = value2_lst / weight_norm\n\n    # Dynamic trade-off coefficient based on current solution's balance\n    if value2_total > 0:\n        current_tradeoff = value1_total / value2_total\n        beta_tradeoff = min(1.0, max(0.0, 1.0 - 0.5 * abs(current_tradeoff - 1.0)))\n    else:\n        beta_tradeoff = 0.5\n\n    combined_marginal = (1 - beta_tradeoff) * marginal1 + beta_tradeoff * marginal2\n\n    # Step 4: Multi-phase hybrid local search\n    new_solution = base_solution.copy()\n\n    # Phase 1: Adaptive marginal flips (add high-marginal items, remove low-marginal items)\n    candidate_add = np.where(new_solution == 0)[0]\n    candidate_remove = np.where(new_solution == 1)[0]\n\n    # Add top-k most promising items\n    k_add = max(1, min(3, len(candidate_add)))\n    top_add = candidate_add[np.argsort(combined_marginal[candidate_add])[-k_add:]]\n    for idx in top_add:\n        if (current_weight + weight_lst[idx]) <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Remove bottom-k least promising items\n    k_remove = max(1, min(2, len(candidate_remove)))\n    bottom_remove = candidate_remove[np.argsort(combined_marginal[candidate_remove])[:k_remove]]\n    for idx in bottom_remove:\n        new_solution[idx] = 0\n        current_weight -= weight_lst[idx]\n\n    # Phase 2: Dynamic trade-off-aware swaps\n    if len(candidate_remove) > 1:\n        swap_candidates = np.random.choice(candidate_remove, min(2, len(candidate_remove)), replace=False)\n        i, j = swap_candidates\n        if (current_weight - weight_lst[i] + weight_lst[j]) <= capacity:\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Step 5: Dynamic perturbation based on local optima\n    perturbation_intensity = 0.2 if len(archive) > 5 else 0.4\n    if random.random() < perturbation_intensity:\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = np.random.choice(flip_candidates)\n            if (current_weight - weight_lst[flip_idx]) <= capacity:\n                new_solution[flip_idx] = 0\n\n    # Step 6: Final feasibility check with greedy removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        worst_item = np.argmin(combined_marginal[new_solution == 1])\n        new_solution[np.where(new_solution == 1)[0][worst_item]] = 0\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9302281787463444,
            0.6324405372142792
        ],
        "raw_score": [
            27.49335678217982,
            28.223380086518368
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected = max(archive, key=lambda x: np.std(x[0]) * (1 - (np.sum(weight_lst[x[0] == 1]) / capacity)))[0].copy()\n\n    # Calculate Pareto-aware marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2  # Balanced trade-off\n\n    # Dynamic perturbation: flip top-k items with highest combined marginal\n    new_solution = selected.copy()\n    sorted_indices = np.argsort(combined_marginal)[::-1]\n    k = max(1, len(new_solution) // 8)  # Adaptive perturbation size\n    for idx in sorted_indices[:k]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Feasibility check and adaptive removal\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        # Remove items with lowest combined value-to-weight ratio\n        included = np.where(new_solution == 1)[0]\n        value_ratios = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        sorted_removal = included[np.argsort(value_ratios)]\n        for idx in sorted_removal:\n            if total_weight <= capacity:\n                break\n            new_solution[idx] = 0\n            total_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9368280378974417,
            1.1944421529769897
        ],
        "raw_score": [
            27.504629144873487,
            27.97143083249673
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto-dominance-aware selection with a dynamic marginal contribution strategy to generate feasible neighbor solutions, prioritizing non-dominated solutions and adaptively balancing trade-offs between objectives while enforcing feasibility through iterative removal and probabilistic perturbation. It dynamically adjusts the selection of items to flip based on trade-off weights and ensures feasibility through iterative removal of low-marginal items, with occasional perturbations to escape local optima. The selection of items to flip is guided by a weighted combination of marginal contributions for both objectives, with non-dominated solutions receiving higher selection weights.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-dominance-aware weighted random selection\n    pareto_weights = []\n    for (sol, obj) in archive:\n        dominated = False\n        for (_, other_obj) in archive:\n            if (other_obj[0] >= obj[0] and other_obj[1] > obj[1]) or (other_obj[0] > obj[0] and other_obj[1] >= obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_weights.append(obj[0] * 0.6 + obj[1] * 0.4)\n        else:\n            pareto_weights.append(0.1 * (obj[0] + obj[1]))\n\n    selected_idx = random.choices(range(len(archive)), weights=pareto_weights, k=1)[0]\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Dynamic marginal contribution calculation\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    trade_off = np.sum(value1_lst) / (np.sum(value2_lst) + 1e-6)\n    combined_marginal = (trade_off * marginal1) + ((1 - trade_off) * marginal2)\n\n    # Step 4: Identify candidate items to flip\n    candidate_indices = np.where((base_solution == 0) & (weight_lst <= remaining_capacity))[0]\n    if len(candidate_indices) == 0:\n        candidate_indices = np.where(base_solution == 1)[0]\n        if len(candidate_indices) == 0:\n            return base_solution\n\n    # Step 5: Select top-k items based on dynamic marginal contributions\n    k = max(1, min(5, len(candidate_indices) // 2))\n    top_indices = np.argsort(combined_marginal[candidate_indices])[-k:][::-1]\n\n    # Step 6: Create new solution by flipping selected items\n    new_solution = base_solution.copy()\n    for idx in candidate_indices[top_indices]:\n        if new_solution[idx] == 0 and (current_weight + weight_lst[idx]) <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n        elif new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Step 7: Adaptive feasibility enforcement\n    while current_weight > capacity:\n        in_items = np.where(new_solution == 1)[0]\n        if len(in_items) == 0:\n            break\n        worst_item = in_items[np.argmin(combined_marginal[in_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Step 8: Dynamic perturbation scaling\n    perturbation_prob = min(0.5, 0.1 + 0.05 * len(archive))\n    if random.random() < perturbation_prob:\n        perturb_indices = np.where(base_solution != new_solution)[0]\n        if len(perturb_indices) > 0:\n            perturb_idx = np.random.choice(perturb_indices)\n            new_solution[perturb_idx] = 1 - new_solution[perturb_idx]\n            if new_solution[perturb_idx] == 1:\n                current_weight += weight_lst[perturb_idx]\n            else:\n                current_weight -= weight_lst[perturb_idx]\n\n    # Step 9: Final feasibility check\n    while current_weight > capacity:\n        in_items = np.where(new_solution == 1)[0]\n        if len(in_items) == 0:\n            break\n        worst_item = in_items[np.argmin(combined_marginal[in_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.772012137365541,
            0.24318775534629822
        ],
        "raw_score": [
            27.421736819659714,
            27.936880974691945
        ]
    },
    {
        "algorithm": "This algorithm selects a promising solution from the archive using Pareto-dominance-aware weighted selection, then applies a hybrid flip strategy that prioritizes adding high-marginal-value items and removing low-marginal-value items while dynamically adjusting perturbations based on local optima density to maintain feasibility. The method combines greedy selection with controlled randomness to explore trade-offs between objectives while ensuring solutions remain feasible.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-dominance-aware weighted selection\n    pareto_front = [sol_obj for sol_obj in archive if not any(\n        (sol_obj[1][0] < other[1][0] and sol_obj[1][1] <= other[1][1]) or\n        (sol_obj[1][0] <= other[1][0] and sol_obj[1][1] < other[1][1])\n        for other in archive\n    )]\n\n    if not pareto_front:\n        pareto_front = archive\n\n    weights = [obj[0] + obj[1] for (sol, obj) in pareto_front]\n    selected_idx = random.choices(range(len(pareto_front)), weights=weights, k=1)[0]\n    base_solution = pareto_front[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) * (1 + 0.1 * np.random.rand(len(weight_lst)))\n\n    # Step 4: Hybrid flip strategy\n    new_solution = base_solution.copy()\n    candidate_indices = np.where((base_solution == 0) & (weight_lst <= remaining_capacity))[0]\n\n    if len(candidate_indices) > 0:\n        # Add items with high marginal value\n        top_add = np.argsort(combined_marginal[candidate_indices])[-min(3, len(candidate_indices)):]\n        for idx in candidate_indices[top_add]:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Remove items with low marginal value\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        bottom_remove = np.argsort(combined_marginal[in_items])[:min(2, len(in_items))]\n        for idx in in_items[bottom_remove]:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Step 5: Dynamic perturbation scaling\n    perturbation_intensity = 0.1 if len(pareto_front) > 2 else 0.3\n    if random.random() < perturbation_intensity:\n        flip_candidates = np.where(base_solution != new_solution)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = np.random.choice(flip_candidates)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Step 6: Feasibility enforcement\n    while current_weight > capacity:\n        in_items = np.where(new_solution == 1)[0]\n        if len(in_items) == 0:\n            break\n        worst_item = in_items[np.argmin(combined_marginal[in_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.9033869833134398,
            0.27192428708076477
        ],
        "raw_score": [
            27.304415110215366,
            28.04960465379954
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid score balancing objective diversity, item diversity, and quality, then applies a multi-phase local search that prioritizes adding high-value items, removing low-value items, and performing objective-aware swaps while dynamically adjusting exploration through perturbation scaling and enforcing feasibility through adaptive removal of over-weighted items. The selection score combines objective diversity (abs(obj[0]-obj[1])), item diversity (standard deviation of the solution vector), and solution quality (normalized sum of objectives), while the local search phases target value maximization with dynamic value-weight ratios and weighted random swaps to balance multi-objective trade-offs.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Solution selection based on hybrid score\n    def selection_score(sol_obj):\n        sol, obj = sol_obj\n        obj_diversity = abs(obj[0] - obj[1]) / (max(obj[0], obj[1]) + 1e-6)\n        item_diversity = np.std(sol)\n        solution_quality = (obj[0] + obj[1]) / (2 * np.sum(weight_lst))\n        return obj_diversity * item_diversity * solution_quality\n\n    selected_sol_obj = max(archive, key=selection_score)\n    base_solution = selected_sol_obj[0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 3: Calculate dynamic value-weight ratios with objective trade-off awareness\n    obj1_ratio = value1_lst / (weight_lst + 1e-6)\n    obj2_ratio = value2_lst / (weight_lst + 1e-6)\n    dynamic_ratio = (obj1_ratio + obj2_ratio) * (1 + 0.3 * np.random.rand(len(weight_lst)))\n\n    # Step 4: Multi-phase local search with dynamic perturbation scaling\n    new_solution = base_solution.copy()\n\n    # Phase 1: Add high-value items with capacity check\n    candidate_add = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n    if len(candidate_add) > 0:\n        top_add = np.argsort(dynamic_ratio[candidate_add])[-min(5, len(candidate_add)):]\n        for idx in candidate_add[top_add]:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Remove low-value items with objective-aware selection\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        bottom_remove = np.argsort(dynamic_ratio[in_items])[:min(3, len(in_items))]\n        for idx in in_items[bottom_remove]:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Phase 3: Objective-aware swap operation with weighted random selection\n    if len(in_items) > 0 and random.random() < 0.3:\n        obj1, obj2 = selected_sol_obj[1]\n        if obj1 > obj2:\n            # Prefer swaps that improve value2\n            swap_candidates = np.where((new_solution == 1) & (obj2_ratio > np.median(obj2_ratio)))[0]\n        else:\n            # Prefer swaps that improve value1\n            swap_candidates = np.where((new_solution == 1) & (obj1_ratio > np.median(obj1_ratio)))[0]\n\n        if len(swap_candidates) > 0:\n            swap_idx = np.random.choice(swap_candidates)\n            new_solution[swap_idx] = 1 - new_solution[swap_idx]\n            if new_solution[swap_idx] == 1:\n                current_weight += weight_lst[swap_idx]\n            else:\n                current_weight -= weight_lst[swap_idx]\n\n    # Step 5: Dynamic perturbation scaling based on local optima\n    if random.random() < 0.2:\n        perturbation_candidates = np.where(new_solution == 1)[0]\n        if len(perturbation_candidates) > 0:\n            flip_idx = np.random.choice(perturbation_candidates)\n            new_solution[flip_idx] = 1 - new_solution[flip_idx]\n            if new_solution[flip_idx] == 1:\n                current_weight += weight_lst[flip_idx]\n            else:\n                current_weight -= weight_lst[flip_idx]\n\n    # Step 6: Feasibility enforcement with adaptive removal\n    while current_weight > capacity:\n        in_items = np.where(new_solution == 1)[0]\n        if len(in_items) == 0:\n            break\n\n        # Remove items with lowest dynamic ratio\n        worst_item = in_items[np.argmin(dynamic_ratio[in_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "metric_score": [
            -0.904424958466408,
            1.030183345079422
        ],
        "raw_score": [
            27.32580425782588,
            27.67023728745683
        ]
    }
]