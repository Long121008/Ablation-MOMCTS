[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 5 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm selects a promising solution from an archive using a hybrid metric of dominance rank and solution density, then applies a tiered local search combining marginal gain analysis, adaptive swaps, and probabilistic perturbations to generate a neighbor solution while ensuring feasibility through dynamic weight tracking. The search prioritizes items with high combined marginal gains (value1 + value2) and uses adaptive swaps between high and low marginal items, followed by probabilistic flips weighted by normalized marginal values.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection: Combine dominance rank and solution density\n    objectives = np.array([obj for _, obj in archive])\n    ranks = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        ranks[sorted_idx] += np.arange(len(archive))\n    density_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dist = np.linalg.norm(objectives - objectives[i], axis=1)\n        density_scores[i] = 1 / (np.sum(dist) + 1e-6)\n    combined_scores = ranks + density_scores\n    selected_idx = np.argmin(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Objective-aware marginal flips\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n    sorted_indices = np.argsort(-combined_marginal)\n    for idx in sorted_indices[:min(3, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive objective-aware swaps\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        if new_solution[high_idx] != new_solution[low_idx]:\n            weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n            if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n               (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n                new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Phase 3: Probabilistic objective-aware perturbation\n    for idx in np.random.permutation(len(new_solution)):\n        if np.random.rand() < 0.2:\n            prob = 0.5 + 0.5 * (marginal1[idx] + marginal2[idx]) / (np.max(marginal1) + np.max(marginal2) + 1e-6)\n            if np.random.rand() < prob:\n                if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThis algorithm selects the most crowded solution from the archive (based on crowding distance) and applies a two-phase local search: first flipping high-marginal-value items and then swapping high/low-marginal items while ensuring feasibility. The selection prioritizes solutions in dense regions of the Pareto front, and the local search balances both objectives through marginal gains and adaptive swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for each solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: High-marginal flips\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n    sorted_indices = np.argsort(-combined_marginal)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive objective-aware swaps\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n        if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n           (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n            new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive using a weighted objective (40% value1, 60% value2), then applies a hybrid local search that flips up to 3 high-marginal-contribution items (prioritizing value1 over value2), performs probabilistic 3-item swaps among top marginal contributors, and has a 30% chance to remove a low-marginal item, all while ensuring feasibility. Marginal contributions are calculated as 70% value1 + 30% value2, balancing both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using weighted objective (0.4*value1 + 0.6*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.4 * obj[0] + 0.6 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item (0.7*value1 + 0.3*value2)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Hybrid flip strategy: flip up to 3 items with highest marginal contribution\n    flip_count = min(3, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Probabilistic swap of three items to explore diverse neighborhoods\n    if len(sorted_indices) >= 3:\n        swap_candidates = sorted_indices[:6]  # Consider top 6 items for swap\n        if len(swap_candidates) >= 3:\n            i, j, k = np.random.choice(swap_candidates, size=3, replace=False)\n            # Check if swapping is feasible\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                elif new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[i] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n            if new_solution[j] != new_solution[k]:\n                if new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[k] <= capacity:\n                    new_solution[j], new_solution[k] = new_solution[k], new_solution[j]\n                elif new_solution[k] == 1 and current_weight - weight_lst[k] + weight_lst[j] <= capacity:\n                    new_solution[j], new_solution[k] = new_solution[k], new_solution[j]\n\n    # 30% chance to remove a random low-marginal item if possible\n    if np.random.rand() < 0.3:\n        low_marginal_indices = np.where((combined_marginal <= np.percentile(combined_marginal, 30)) & (new_solution == 1))[0]\n        if len(low_marginal_indices) > 0:\n            random_idx = np.random.choice(low_marginal_indices)\n            if current_weight - weight_lst[random_idx] <= capacity:\n                new_solution[random_idx] = 0\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest crowding distance (promising for improvement)\n    if len(archive) > 1:\n        objectives = np.array([obj for _, obj in archive])\n        crowding = np.zeros(len(archive))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(archive)-1):\n                crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective dominance\n    obj1_dominance = np.mean(value1_lst) / (np.mean(value2_lst) + 1e-6)\n    obj2_dominance = np.mean(value2_lst) / (np.mean(value1_lst) + 1e-6)\n\n    # Step 3: Dynamic flip operation based on dominance\n    if obj1_dominance > obj2_dominance:\n        # Prioritize objective 1 when more dominant\n        marginal = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    else:\n        # Prioritize objective 2 when more dominant\n        marginal = (value1_lst * 0.3 + value2_lst * 0.7) / (weight_lst + 1e-6)\n\n    sorted_indices = np.argsort(-marginal)\n    flip_candidates = sorted_indices[:min(5, len(sorted_indices))]\n\n    # Step 4: Perform flips with feasibility checks\n    for idx in flip_candidates:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 5: Adaptive perturbation based on solution density\n    density = np.sum(new_solution) / len(new_solution)\n    if density < 0.3:  # Sparse solution - add more items\n        available = np.where(new_solution == 0)[0]\n        if len(available) > 0:\n            perturb_candidates = np.argsort(-marginal)[-min(3, len(available)):]\n            for idx in perturb_candidates:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n    elif density > 0.7:  # Dense solution - remove some items\n        included = np.where(new_solution == 1)[0]\n        if len(included) > 1:\n            perturb_candidates = np.argsort(marginal)[:min(3, len(included)-1)]\n            for idx in perturb_candidates:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here's the ultra-concise response (exactly 98 words):\n\n- **Keywords**: Adaptive dominance, hybrid local search, marginal gains, dynamic feasibility.\n- **Advice**: Select solutions with high crowding distance, then apply tiered flips (high-marginal first) and adaptive swaps to escape local optima.\n- **Avoid**: Static objective prioritization; late feasibility checks.\n- **Explanation**: Dynamic adaptation balances exploration/exploitation, while early feasibility checks ensure robust neighbors.\n\nThe heuristic will:\n1. Select solutions with high crowding distance to focus improvement efforts\n2. Apply tiered flips prioritizing high-marginal items for exploitation\n3. Use adaptive swaps between high/low marginal items for exploration\n4. Maintain feasibility through early weight checks at each operation\n\nThis balances both objectives while avoiding common pitfalls.\n\n*(98 words exactly)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 301,
        "algorithm": "This algorithm selects the most crowded solution from the archive (based on crowding distance) and applies a three-phase local search: (1) flips top 5 high-marginal items, (2) swaps high/low marginal items adaptively, and (3) probabilistically perturbs items based on normalized marginal gains, all while ensuring feasibility through dynamic weight tracking. The method prioritizes items with combined high marginal gains for both objectives, balancing exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for each solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate normalized marginal gains\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / (np.max(marginal1) + np.max(marginal2) + 1e-6)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Tiered local search\n    # Phase 1: High-marginal flips (top 5 items)\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive swaps (high/low marginal)\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        if new_solution[high_idx] != new_solution[low_idx]:\n            weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n            if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n               (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n                new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Phase 3: Probabilistic perturbations\n    for idx in np.random.permutation(len(new_solution)):\n        if np.random.rand() < 0.2:\n            prob = 0.5 * combined_marginal[idx] + 0.5 * np.random.rand()\n            if np.random.rand() < prob:\n                if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8624175606459911,
            2.855442076921463
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for each solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate normalized marginal gains\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / (np.max(marginal1) + np.max(marginal2) + 1e-6)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Tiered local search\n    # Phase 1: High-marginal flips (top 5 items)\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive swaps (high/low marginal)\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        if new_solution[high_idx] != new_solution[low_idx]:\n            weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n            if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n               (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n                new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Phase 3: Probabilistic perturbations\n    for idx in np.random.permutation(len(new_solution)):\n        if np.random.rand() < 0.2:\n            prob = 0.5 * combined_marginal[idx] + 0.5 * np.random.rand()\n            if np.random.rand() < prob:\n                if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 302,
        "algorithm": "The algorithm selects the top-performing solution from the archive (weighted 70% on value1 and 30% on value2) and performs a greedy flip of up to 5 items with the highest marginal contribution (prioritizing value1 over value2) while ensuring feasibility. It precomputes sorted indices and checks flips in a single pass, making it efficient and effective for local improvement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Precompute marginal contributions and sort indices once\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Select the top solution from the archive (O(1) if archive is small)\n    selected_idx = np.argmax([0.7 * obj[0] + 0.3 * obj[1] for _, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.743173855611309,
            1.1570338308811188
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Precompute marginal contributions and sort indices once\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Select the top solution from the archive (O(1) if archive is small)\n    selected_idx = np.argmax([0.7 * obj[0] + 0.3 * obj[1] for _, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "r1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 303,
        "algorithm": "The algorithm selects a base solution from the archive, precomputes marginal contributions (weighted 70% for value1 and 30% for value2) for all items, identifies the top 5 items with highest marginal contributions, and flips their inclusion status while ensuring feasibility. It prioritizes items with higher combined marginal utility and performs a single-pass flip operation for efficiency.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the first solution (no need to loop through all)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n\n    # Precompute marginal contributions once\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Get top 5 indices with highest marginal contributions\n    top_indices = np.argpartition(-combined_marginal, 5)[:5]\n\n    # Flip status of top 5 items in a single pass\n    current_weight = np.sum(weight_lst * new_solution)\n    for idx in top_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8738947779010506,
            1.025781512260437
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the first solution (no need to loop through all)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n\n    # Precompute marginal contributions once\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Get top 5 indices with highest marginal contributions\n    top_indices = np.argpartition(-combined_marginal, 5)[:5]\n\n    # Flip status of top 5 items in a single pass\n    current_weight = np.sum(weight_lst * new_solution)\n    for idx in top_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "r1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest crowding distance (promising for improvement)\n    if len(archive) > 1:\n        objectives = np.array([obj for _, obj in archive])\n        crowding = np.zeros(len(archive))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(archive)-1):\n                crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective dominance\n    obj1_dominance = np.mean(value1_lst) / (np.mean(value2_lst) + 1e-6)\n    obj2_dominance = np.mean(value2_lst) / (np.mean(value1_lst) + 1e-6)\n\n    # Step 3: Dynamic flip operation based on dominance\n    if obj1_dominance > obj2_dominance:\n        # Prioritize objective 1 when more dominant\n        marginal = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    else:\n        # Prioritize objective 2 when more dominant\n        marginal = (value1_lst * 0.3 + value2_lst * 0.7) / (weight_lst + 1e-6)\n\n    sorted_indices = np.argsort(-marginal)\n    flip_candidates = sorted_indices[:min(5, len(sorted_indices))]\n\n    # Step 4: Perform flips with feasibility checks\n    for idx in flip_candidates:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 5: Adaptive perturbation based on solution density\n    density = np.sum(new_solution) / len(new_solution)\n    if density < 0.3:  # Sparse solution - add more items\n        available = np.where(new_solution == 0)[0]\n        if len(available) > 0:\n            perturb_candidates = np.argsort(-marginal)[-min(3, len(available)):]\n            for idx in perturb_candidates:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n    elif density > 0.7:  # Dense solution - remove some items\n        included = np.where(new_solution == 1)[0]\n        if len(included) > 1:\n            perturb_candidates = np.argsort(marginal)[:min(3, len(included)-1)]\n            for idx in perturb_candidates:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines crowding-distance-based selection with a three-phase local search: (1) tiered value-density flips prioritizing high-marginal items, (2) dynamic objective-balanced swaps using weighted random selection, and (3) capacity-aware mutation that adjusts operations based on remaining capacity. It ensures feasibility through early checks and balances objectives dynamically without static prioritization. The solution prioritizes items with higher combined density (value1 + value2) in the first phase, then uses normalized weights to probabilistically swap items in the second phase, and finally performs capacity-aware mutations to explore further.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Tiered value-density flips\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n    combined_density = value_density1 + value_density2\n    sorted_indices = np.argsort(-combined_density)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic objective-balanced swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined density\n            remove_idx = random.choices(included, weights=combined_density[included], k=1)[0]\n\n            # Select item to add based on both objectives\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = 0.7 * value1_norm + 0.3 * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Capacity-aware mutation\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        mutation_strength = min(1.0, remaining_capacity / capacity)\n        mutation_count = max(1, int(mutation_strength * len(new_solution) * 0.2))\n\n        for _ in range(mutation_count):\n            if random.random() < 0.5:\n                # Add operation\n                available = np.where(new_solution == 0)[0]\n                if len(available) > 0:\n                    candidate_weights = weight_lst[available]\n                    candidate_mask = candidate_weights <= remaining_capacity\n                    valid_candidates = available[candidate_mask]\n\n                    if len(valid_candidates) > 0:\n                        idx = random.choice(valid_candidates)\n                        new_solution[idx] = 1\n                        remaining_capacity -= weight_lst[idx]\n            else:\n                # Remove operation\n                included = np.where(new_solution == 1)[0]\n                if len(included) > 1:  # Keep at least one item\n                    idx = random.choice(included)\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here's the ultra-concise response (exactly 98 words):\n\n- **Keywords**: Adaptive dominance, hybrid local search, marginal gains, dynamic feasibility.\n- **Advice**: Select solutions with high crowding distance, then apply tiered flips (high-marginal first) and adaptive swaps to escape local optima.\n- **Avoid**: Static objective prioritization; late feasibility checks.\n- **Explanation**: Dynamic adaptation balances exploration/exploitation, while early feasibility checks ensure robust neighbors.\n\nThe heuristic will:\n1. Select solutions with high crowding distance to focus improvement efforts\n2. Apply tiered flips prioritizing high-marginal items for exploitation\n3. Use adaptive swaps between high/low marginal items for exploration\n4. Maintain feasibility through early weight checks at each operation\n\nThis balances both objectives while avoiding common pitfalls.\n\n*(98 words exactly)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 304,
        "algorithm": "The algorithm combines crowding-distance-based selection with a three-phase hybrid local search: (1) prioritizes high-value-density flips, (2) performs dynamic objective-balanced swaps with weighted randomness, and (3) applies capacity-aware mutations that adaptively adjust operation intensity. It ensures feasibility through early weight checks and balances exploration/exploitation by dynamically prioritizing marginal gains and probabilistic selections, avoiding static objective prioritization. The solution selection focuses on high-crowding-distance solutions to target improvement hotspots, while the local search phases progressively refine solutions through increasing exploration levels.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection phase: Choose solution with highest crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n    selected_idx = np.argmax(crowding)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Tiered value-density flips\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n    combined_density = value_density1 + value_density2\n    sorted_indices = np.argsort(-combined_density)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic objective-balanced swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined density\n            remove_idx = random.choices(included, weights=combined_density[included], k=1)[0]\n\n            # Select item to add based on both objectives\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = 0.7 * value1_norm + 0.3 * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Capacity-aware mutation\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        mutation_strength = min(1.0, remaining_capacity / capacity)\n        mutation_count = max(1, int(mutation_strength * len(new_solution) * 0.2))\n\n        for _ in range(mutation_count):\n            if random.random() < 0.5:\n                # Add operation\n                available = np.where(new_solution == 0)[0]\n                if len(available) > 0:\n                    candidate_weights = weight_lst[available]\n                    candidate_mask = candidate_weights <= remaining_capacity\n                    valid_candidates = available[candidate_mask]\n\n                    if len(valid_candidates) > 0:\n                        idx = random.choice(valid_candidates)\n                        new_solution[idx] = 1\n                        remaining_capacity -= weight_lst[idx]\n            else:\n                # Remove operation\n                included = np.where(new_solution == 1)[0]\n                if len(included) > 1:  # Keep at least one item\n                    idx = random.choice(included)\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.5920316565409158,
            2.2742727398872375
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection phase: Choose solution with highest crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n    selected_idx = np.argmax(crowding)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Tiered value-density flips\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n    combined_density = value_density1 + value_density2\n    sorted_indices = np.argsort(-combined_density)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic objective-balanced swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined density\n            remove_idx = random.choices(included, weights=combined_density[included], k=1)[0]\n\n            # Select item to add based on both objectives\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = 0.7 * value1_norm + 0.3 * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Capacity-aware mutation\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        mutation_strength = min(1.0, remaining_capacity / capacity)\n        mutation_count = max(1, int(mutation_strength * len(new_solution) * 0.2))\n\n        for _ in range(mutation_count):\n            if random.random() < 0.5:\n                # Add operation\n                available = np.where(new_solution == 0)[0]\n                if len(available) > 0:\n                    candidate_weights = weight_lst[available]\n                    candidate_mask = candidate_weights <= remaining_capacity\n                    valid_candidates = available[candidate_mask]\n\n                    if len(valid_candidates) > 0:\n                        idx = random.choice(valid_candidates)\n                        new_solution[idx] = 1\n                        remaining_capacity -= weight_lst[idx]\n            else:\n                # Remove operation\n                included = np.where(new_solution == 1)[0]\n                if len(included) > 1:  # Keep at least one item\n                    idx = random.choice(included)\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines crowding-distance-based selection with a three-phase local search: (1) tiered value-density flips prioritizing high-marginal items, (2) dynamic objective-balanced swaps using weighted random selection, and (3) capacity-aware mutation that adjusts operations based on remaining capacity. It ensures feasibility through early checks and balances objectives dynamically without static prioritization. The solution prioritizes items with higher combined density (value1 + value2) in the first phase, then uses normalized weights to probabilistically swap items in the second phase, and finally performs capacity-aware mutations to explore further.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Tiered value-density flips\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n    combined_density = value_density1 + value_density2\n    sorted_indices = np.argsort(-combined_density)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic objective-balanced swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined density\n            remove_idx = random.choices(included, weights=combined_density[included], k=1)[0]\n\n            # Select item to add based on both objectives\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = 0.7 * value1_norm + 0.3 * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Capacity-aware mutation\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        mutation_strength = min(1.0, remaining_capacity / capacity)\n        mutation_count = max(1, int(mutation_strength * len(new_solution) * 0.2))\n\n        for _ in range(mutation_count):\n            if random.random() < 0.5:\n                # Add operation\n                available = np.where(new_solution == 0)[0]\n                if len(available) > 0:\n                    candidate_weights = weight_lst[available]\n                    candidate_mask = candidate_weights <= remaining_capacity\n                    valid_candidates = available[candidate_mask]\n\n                    if len(valid_candidates) > 0:\n                        idx = random.choice(valid_candidates)\n                        new_solution[idx] = 1\n                        remaining_capacity -= weight_lst[idx]\n            else:\n                # Remove operation\n                included = np.where(new_solution == 1)[0]\n                if len(included) > 1:  # Keep at least one item\n                    idx = random.choice(included)\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 305,
        "algorithm": "The algorithm combines adaptive objective prioritization with a hybrid local search that dynamically balances exploration and exploitation, using tiered operations including marginal contribution-based flips, capacity-aware probabilistic swaps, and dynamic mutations that adjust intensity based on solution quality and remaining capacity. It prioritizes items with higher combined marginal contributions for both objectives and selectively mutates lower-quality solutions more aggressively, ensuring feasibility through capacity checks at each step. The selection process is guided by solution quality, with higher-quality solutions undergoing more exploitation while lower-quality ones receive more exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive objective prioritization\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-6)\n    objective_weights = 0.5 + 0.5 * (1 - normalized_obj)  # Prioritize underperforming objectives\n\n    # Select solution with highest combined potential\n    combined_scores = np.sum(objective_weights * objectives, axis=1)\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Adaptive marginal contribution flips\n    marginal_contrib1 = value1_lst / (weight_lst + 1e-6)\n    marginal_contrib2 = value2_lst / (weight_lst + 1e-6)\n    combined_contrib = marginal_contrib1 + marginal_contrib2\n\n    # Calculate exploitation probability based on solution quality\n    solution_quality = (np.sum(value1_lst * new_solution) + np.sum(value2_lst * new_solution)) / (2 * capacity)\n    exploitation_prob = 0.7 * solution_quality  # Higher quality solutions exploit more\n\n    for idx in np.argsort(-combined_contrib):\n        if random.random() < exploitation_prob and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif random.random() < exploitation_prob and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Capacity-aware probabilistic swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Weight swaps by marginal contribution and remaining capacity\n        swap_weights = np.zeros(len(included))\n        for i, idx in enumerate(included):\n            potential_add = excluded[weight_lst[excluded] <= (capacity - current_weight + weight_lst[idx])]\n            if len(potential_add) > 0:\n                add_contrib = combined_contrib[potential_add]\n                swap_weights[i] = combined_contrib[idx] * np.mean(add_contrib)\n\n        if np.sum(swap_weights) > 0:\n            remove_idx = random.choices(included, weights=swap_weights, k=1)[0]\n            potential_add = excluded[weight_lst[excluded] <= (capacity - current_weight + weight_lst[remove_idx])]\n\n            if len(potential_add) > 0:\n                add_weights = combined_contrib[potential_add]\n                add_idx = random.choices(potential_add, weights=add_weights, k=1)[0]\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Dynamic mutation based on solution quality\n    remaining_capacity = capacity - np.sum(weight_lst * new_solution)\n    mutation_intensity = 0.3 + 0.7 * (1 - solution_quality)  # Lower quality solutions mutate more\n    mutation_count = max(1, int(mutation_intensity * len(new_solution) * 0.1))\n\n    for _ in range(mutation_count):\n        if random.random() < 0.5 and remaining_capacity > 0:\n            # Add operation with value-aware selection\n            available = np.where(new_solution == 0)[0]\n            available = available[weight_lst[available] <= remaining_capacity]\n\n            if len(available) > 0:\n                selection_weights = combined_contrib[available]\n                idx = random.choices(available, weights=selection_weights, k=1)[0]\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n        else:\n            # Remove operation with quality-aware selection\n            included = np.where(new_solution == 1)[0]\n            if len(included) > 1:\n                # Prefer removing low-contribution items\n                removal_weights = 1 / (combined_contrib[included] + 1e-6)\n                idx = random.choices(included, weights=removal_weights, k=1)[0]\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.658319576321087,
            8.360369741916656
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive objective prioritization\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-6)\n    objective_weights = 0.5 + 0.5 * (1 - normalized_obj)  # Prioritize underperforming objectives\n\n    # Select solution with highest combined potential\n    combined_scores = np.sum(objective_weights * objectives, axis=1)\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Adaptive marginal contribution flips\n    marginal_contrib1 = value1_lst / (weight_lst + 1e-6)\n    marginal_contrib2 = value2_lst / (weight_lst + 1e-6)\n    combined_contrib = marginal_contrib1 + marginal_contrib2\n\n    # Calculate exploitation probability based on solution quality\n    solution_quality = (np.sum(value1_lst * new_solution) + np.sum(value2_lst * new_solution)) / (2 * capacity)\n    exploitation_prob = 0.7 * solution_quality  # Higher quality solutions exploit more\n\n    for idx in np.argsort(-combined_contrib):\n        if random.random() < exploitation_prob and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif random.random() < exploitation_prob and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Capacity-aware probabilistic swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Weight swaps by marginal contribution and remaining capacity\n        swap_weights = np.zeros(len(included))\n        for i, idx in enumerate(included):\n            potential_add = excluded[weight_lst[excluded] <= (capacity - current_weight + weight_lst[idx])]\n            if len(potential_add) > 0:\n                add_contrib = combined_contrib[potential_add]\n                swap_weights[i] = combined_contrib[idx] * np.mean(add_contrib)\n\n        if np.sum(swap_weights) > 0:\n            remove_idx = random.choices(included, weights=swap_weights, k=1)[0]\n            potential_add = excluded[weight_lst[excluded] <= (capacity - current_weight + weight_lst[remove_idx])]\n\n            if len(potential_add) > 0:\n                add_weights = combined_contrib[potential_add]\n                add_idx = random.choices(potential_add, weights=add_weights, k=1)[0]\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Dynamic mutation based on solution quality\n    remaining_capacity = capacity - np.sum(weight_lst * new_solution)\n    mutation_intensity = 0.3 + 0.7 * (1 - solution_quality)  # Lower quality solutions mutate more\n    mutation_count = max(1, int(mutation_intensity * len(new_solution) * 0.1))\n\n    for _ in range(mutation_count):\n        if random.random() < 0.5 and remaining_capacity > 0:\n            # Add operation with value-aware selection\n            available = np.where(new_solution == 0)[0]\n            available = available[weight_lst[available] <= remaining_capacity]\n\n            if len(available) > 0:\n                selection_weights = combined_contrib[available]\n                idx = random.choices(available, weights=selection_weights, k=1)[0]\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n        else:\n            # Remove operation with quality-aware selection\n            included = np.where(new_solution == 1)[0]\n            if len(included) > 1:\n                # Prefer removing low-contribution items\n                removal_weights = 1 / (combined_contrib[included] + 1e-6)\n                idx = random.choices(included, weights=removal_weights, k=1)[0]\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines crowding-distance-based selection with a three-phase local search: (1) tiered value-density flips prioritizing high-marginal items, (2) dynamic objective-balanced swaps using weighted random selection, and (3) capacity-aware mutation that adjusts operations based on remaining capacity. It ensures feasibility through early checks and balances objectives dynamically without static prioritization. The solution prioritizes items with higher combined density (value1 + value2) in the first phase, then uses normalized weights to probabilistically swap items in the second phase, and finally performs capacity-aware mutations to explore further.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Tiered value-density flips\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n    combined_density = value_density1 + value_density2\n    sorted_indices = np.argsort(-combined_density)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic objective-balanced swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined density\n            remove_idx = random.choices(included, weights=combined_density[included], k=1)[0]\n\n            # Select item to add based on both objectives\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = 0.7 * value1_norm + 0.3 * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    # Phase 3: Capacity-aware mutation\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        mutation_strength = min(1.0, remaining_capacity / capacity)\n        mutation_count = max(1, int(mutation_strength * len(new_solution) * 0.2))\n\n        for _ in range(mutation_count):\n            if random.random() < 0.5:\n                # Add operation\n                available = np.where(new_solution == 0)[0]\n                if len(available) > 0:\n                    candidate_weights = weight_lst[available]\n                    candidate_mask = candidate_weights <= remaining_capacity\n                    valid_candidates = available[candidate_mask]\n\n                    if len(valid_candidates) > 0:\n                        idx = random.choice(valid_candidates)\n                        new_solution[idx] = 1\n                        remaining_capacity -= weight_lst[idx]\n            else:\n                # Remove operation\n                included = np.where(new_solution == 1)[0]\n                if len(included) > 1:  # Keep at least one item\n                    idx = random.choice(included)\n                    new_solution[idx] = 0\n                    remaining_capacity += weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 306,
        "algorithm": "The algorithm combines adaptive dominance-based selection with a two-phase local search: first prioritizing flips of high-marginal-contribution items (weighted by objective importance) and then performing capacity-aware swaps between included and excluded items, balancing exploration and exploitation while ensuring feasibility. Value1 is given 60% weight and value2 40% in the combined objective, with marginal contributions calculated as normalized value-to-weight ratios. The solution selection favors less-dominated archive members, and swaps use normalized objective weights for probabilistic selection of items to exchange.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive dominance-based selection\n    objectives = np.array([obj for _, obj in archive])\n    dominance_counts = np.zeros(len(archive))\n\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] >= objectives[j, 1]) and (objectives[i, 0] > objectives[j, 0] or objectives[i, 1] > objectives[j, 1]):\n                    dominance_counts[i] += 1\n\n    selected_idx = np.argmin(dominance_counts) if np.max(dominance_counts) > 0 else np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Objective-weighted flips\n    value_weights = np.array([0.6, 0.4])  # Weight for value1 and value2\n    combined_value = value_weights[0] * value1_lst + value_weights[1] * value2_lst\n    marginal_contributions = combined_value / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-marginal_contributions)\n\n    for idx in sorted_indices[:min(3, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined value\n            remove_idx = random.choices(included, weights=combined_value[included], k=1)[0]\n\n            # Select item to add based on normalized objective weights\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = value_weights[0] * value1_norm + value_weights[1] * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    return new_solution\n\n",
        "score": [
            -0.8691617503951263,
            3.663556456565857
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive dominance-based selection\n    objectives = np.array([obj for _, obj in archive])\n    dominance_counts = np.zeros(len(archive))\n\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] >= objectives[j, 1]) and (objectives[i, 0] > objectives[j, 0] or objectives[i, 1] > objectives[j, 1]):\n                    dominance_counts[i] += 1\n\n    selected_idx = np.argmin(dominance_counts) if np.max(dominance_counts) > 0 else np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Objective-weighted flips\n    value_weights = np.array([0.6, 0.4])  # Weight for value1 and value2\n    combined_value = value_weights[0] * value1_lst + value_weights[1] * value2_lst\n    marginal_contributions = combined_value / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-marginal_contributions)\n\n    for idx in sorted_indices[:min(3, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps\n    if len(new_solution) >= 2:\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove based on combined value\n            remove_idx = random.choices(included, weights=combined_value[included], k=1)[0]\n\n            # Select item to add based on normalized objective weights\n            value1_norm = (value1_lst[excluded] - np.min(value1_lst[excluded])) / (np.max(value1_lst[excluded]) - np.min(value1_lst[excluded]) + 1e-6)\n            value2_norm = (value2_lst[excluded] - np.min(value2_lst[excluded])) / (np.max(value2_lst[excluded]) - np.min(value2_lst[excluded]) + 1e-6)\n            combined_norm = value_weights[0] * value1_norm + value_weights[1] * value2_norm\n            add_idx = random.choices(excluded, weights=combined_norm, k=1)[0]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx], new_solution[add_idx] = 0, 1\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 5 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm selects a promising solution from an archive using a hybrid metric of dominance rank and solution density, then applies a tiered local search combining marginal gain analysis, adaptive swaps, and probabilistic perturbations to generate a neighbor solution while ensuring feasibility through dynamic weight tracking. The search prioritizes items with high combined marginal gains (value1 + value2) and uses adaptive swaps between high and low marginal items, followed by probabilistic flips weighted by normalized marginal values.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection: Combine dominance rank and solution density\n    objectives = np.array([obj for _, obj in archive])\n    ranks = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        ranks[sorted_idx] += np.arange(len(archive))\n    density_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dist = np.linalg.norm(objectives - objectives[i], axis=1)\n        density_scores[i] = 1 / (np.sum(dist) + 1e-6)\n    combined_scores = ranks + density_scores\n    selected_idx = np.argmin(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Objective-aware marginal flips\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n    sorted_indices = np.argsort(-combined_marginal)\n    for idx in sorted_indices[:min(3, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive objective-aware swaps\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        if new_solution[high_idx] != new_solution[low_idx]:\n            weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n            if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n               (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n                new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Phase 3: Probabilistic objective-aware perturbation\n    for idx in np.random.permutation(len(new_solution)):\n        if np.random.rand() < 0.2:\n            prob = 0.5 + 0.5 * (marginal1[idx] + marginal2[idx]) / (np.max(marginal1) + np.max(marginal2) + 1e-6)\n            if np.random.rand() < prob:\n                if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThis algorithm selects the most crowded solution from the archive (based on crowding distance) and applies a two-phase local search: first flipping high-marginal-value items and then swapping high/low-marginal items while ensuring feasibility. The selection prioritizes solutions in dense regions of the Pareto front, and the local search balances both objectives through marginal gains and adaptive swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances for each solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_distances[sorted_idx[0]] = np.inf\n        crowding_distances[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Select solution with maximum crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: High-marginal flips\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n    sorted_indices = np.argsort(-combined_marginal)\n\n    for idx in sorted_indices[:min(5, len(sorted_indices))]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive objective-aware swaps\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n        if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n           (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n            new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive using a weighted objective (40% value1, 60% value2), then applies a hybrid local search that flips up to 3 high-marginal-contribution items (prioritizing value1 over value2), performs probabilistic 3-item swaps among top marginal contributors, and has a 30% chance to remove a low-marginal item, all while ensuring feasibility. Marginal contributions are calculated as 70% value1 + 30% value2, balancing both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using weighted objective (0.4*value1 + 0.6*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.4 * obj[0] + 0.6 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item (0.7*value1 + 0.3*value2)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Hybrid flip strategy: flip up to 3 items with highest marginal contribution\n    flip_count = min(3, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Probabilistic swap of three items to explore diverse neighborhoods\n    if len(sorted_indices) >= 3:\n        swap_candidates = sorted_indices[:6]  # Consider top 6 items for swap\n        if len(swap_candidates) >= 3:\n            i, j, k = np.random.choice(swap_candidates, size=3, replace=False)\n            # Check if swapping is feasible\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                elif new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[i] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n            if new_solution[j] != new_solution[k]:\n                if new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[k] <= capacity:\n                    new_solution[j], new_solution[k] = new_solution[k], new_solution[j]\n                elif new_solution[k] == 1 and current_weight - weight_lst[k] + weight_lst[j] <= capacity:\n                    new_solution[j], new_solution[k] = new_solution[k], new_solution[j]\n\n    # 30% chance to remove a random low-marginal item if possible\n    if np.random.rand() < 0.3:\n        low_marginal_indices = np.where((combined_marginal <= np.percentile(combined_marginal, 30)) & (new_solution == 1))[0]\n        if len(low_marginal_indices) > 0:\n            random_idx = np.random.choice(low_marginal_indices)\n            if current_weight - weight_lst[random_idx] <= capacity:\n                new_solution[random_idx] = 0\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest crowding distance (promising for improvement)\n    if len(archive) > 1:\n        objectives = np.array([obj for _, obj in archive])\n        crowding = np.zeros(len(archive))\n        for m in range(2):\n            sorted_idx = np.argsort(objectives[:, m])\n            crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n            for i in range(1, len(archive)-1):\n                crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective dominance\n    obj1_dominance = np.mean(value1_lst) / (np.mean(value2_lst) + 1e-6)\n    obj2_dominance = np.mean(value2_lst) / (np.mean(value1_lst) + 1e-6)\n\n    # Step 3: Dynamic flip operation based on dominance\n    if obj1_dominance > obj2_dominance:\n        # Prioritize objective 1 when more dominant\n        marginal = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    else:\n        # Prioritize objective 2 when more dominant\n        marginal = (value1_lst * 0.3 + value2_lst * 0.7) / (weight_lst + 1e-6)\n\n    sorted_indices = np.argsort(-marginal)\n    flip_candidates = sorted_indices[:min(5, len(sorted_indices))]\n\n    # Step 4: Perform flips with feasibility checks\n    for idx in flip_candidates:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 5: Adaptive perturbation based on solution density\n    density = np.sum(new_solution) / len(new_solution)\n    if density < 0.3:  # Sparse solution - add more items\n        available = np.where(new_solution == 0)[0]\n        if len(available) > 0:\n            perturb_candidates = np.argsort(-marginal)[-min(3, len(available)):]\n            for idx in perturb_candidates:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n    elif density > 0.7:  # Dense solution - remove some items\n        included = np.where(new_solution == 1)[0]\n        if len(included) > 1:\n            perturb_candidates = np.argsort(marginal)[:min(3, len(included)-1)]\n            for idx in perturb_candidates:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here's the ultra-concise response (exactly 98 words):\n\n- **Keywords**: Adaptive dominance, hybrid local search, marginal gains, dynamic feasibility.\n- **Advice**: Select solutions with high crowding distance, then apply tiered flips (high-marginal first) and adaptive swaps to escape local optima.\n- **Avoid**: Static objective prioritization; late feasibility checks.\n- **Explanation**: Dynamic adaptation balances exploration/exploitation, while early feasibility checks ensure robust neighbors.\n\nThe heuristic will:\n1. Select solutions with high crowding distance to focus improvement efforts\n2. Apply tiered flips prioritizing high-marginal items for exploitation\n3. Use adaptive swaps between high/low marginal items for exploration\n4. Maintain feasibility through early weight checks at each operation\n\nThis balances both objectives while avoiding common pitfalls.\n\n*(98 words exactly)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 307,
        "algorithm": "The algorithm selects a promising solution from the archive using crowding distance, then applies a tiered flip operation based on dynamically weighted marginal values, followed by adaptive probabilistic swaps and density adjustments to generate a feasible neighbor solution while balancing both objectives. It prioritizes items with higher marginal utility (weighted by objective dominance) and adjusts solution density probabilistically to maintain exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n    selected_idx = np.argmax(crowding)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective dominance\n    obj1_dominance = np.mean(value1_lst) / (np.mean(value2_lst) + 1e-6)\n    obj2_dominance = np.mean(value2_lst) / (np.mean(value1_lst) + 1e-6)\n\n    # Step 3: Dynamic marginal calculation based on dominance\n    if obj1_dominance > obj2_dominance:\n        marginal = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    else:\n        marginal = (value1_lst * 0.3 + value2_lst * 0.7) / (weight_lst + 1e-6)\n\n    # Step 4: Tiered flip operation\n    sorted_indices = np.argsort(-marginal)\n    flip_candidates = sorted_indices[:min(5, len(sorted_indices))]\n\n    for idx in flip_candidates:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 5: Adaptive probabilistic swaps\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        if new_solution[high_idx] != new_solution[low_idx]:\n            weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n            if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n               (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n                new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Step 6: Density adjustment\n    density = np.sum(new_solution) / len(new_solution)\n    if density < 0.3 and np.random.rand() < 0.4:\n        available = np.where(new_solution == 0)[0]\n        if len(available) > 0:\n            perturb_candidates = np.argsort(-marginal)[-min(2, len(available)):]\n            for idx in perturb_candidates:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n    elif density > 0.7 and np.random.rand() < 0.3:\n        included = np.where(new_solution == 1)[0]\n        if len(included) > 1:\n            perturb_candidates = np.argsort(marginal)[:min(2, len(included)-1)]\n            for idx in perturb_candidates:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8014130122075602,
            1.9094721674919128
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n    selected_idx = np.argmax(crowding)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Calculate objective dominance\n    obj1_dominance = np.mean(value1_lst) / (np.mean(value2_lst) + 1e-6)\n    obj2_dominance = np.mean(value2_lst) / (np.mean(value1_lst) + 1e-6)\n\n    # Step 3: Dynamic marginal calculation based on dominance\n    if obj1_dominance > obj2_dominance:\n        marginal = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    else:\n        marginal = (value1_lst * 0.3 + value2_lst * 0.7) / (weight_lst + 1e-6)\n\n    # Step 4: Tiered flip operation\n    sorted_indices = np.argsort(-marginal)\n    flip_candidates = sorted_indices[:min(5, len(sorted_indices))]\n\n    for idx in flip_candidates:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Step 5: Adaptive probabilistic swaps\n    if len(sorted_indices) >= 2:\n        high_idx, low_idx = sorted_indices[0], sorted_indices[-1]\n        if new_solution[high_idx] != new_solution[low_idx]:\n            weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n            if (new_solution[high_idx] == 1 and current_weight + weight_diff <= capacity) or \\\n               (new_solution[high_idx] == 0 and current_weight - weight_diff <= capacity):\n                new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Step 6: Density adjustment\n    density = np.sum(new_solution) / len(new_solution)\n    if density < 0.3 and np.random.rand() < 0.4:\n        available = np.where(new_solution == 0)[0]\n        if len(available) > 0:\n            perturb_candidates = np.argsort(-marginal)[-min(2, len(available)):]\n            for idx in perturb_candidates:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n    elif density > 0.7 and np.random.rand() < 0.3:\n        included = np.where(new_solution == 1)[0]\n        if len(included) > 1:\n            perturb_candidates = np.argsort(marginal)[:min(2, len(included)-1)]\n            for idx in perturb_candidates:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 308,
        "algorithm": "The algorithm selects the best solution from the archive (assumed pre-sorted) and performs a targeted local search by flipping the top 5 items with the highest combined marginal contributions (weighted 70% for value1 and 30% for value2), ensuring feasibility by checking weight constraints. It prioritizes items with better overall value-to-weight ratios while maintaining computational efficiency through precomputation and a single pass.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Precompute weighted marginal contributions once\n    combined_marginal = 0.7 * (value1_lst / (weight_lst + 1e-6)) + 0.3 * (value2_lst / (weight_lst + 1e-6))\n\n    # Find best solution in O(1) time (assuming archive is pre-sorted)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Get indices of top 5 items by marginal contribution\n    top_indices = np.argpartition(-combined_marginal, 5)[:5]\n\n    # Flip each of the top 5 items if feasible\n    for idx in top_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9210394265410156,
            0.9670766294002533
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Precompute weighted marginal contributions once\n    combined_marginal = 0.7 * (value1_lst / (weight_lst + 1e-6)) + 0.3 * (value2_lst / (weight_lst + 1e-6))\n\n    # Find best solution in O(1) time (assuming archive is pre-sorted)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Get indices of top 5 items by marginal contribution\n    top_indices = np.argpartition(-combined_marginal, 5)[:5]\n\n    # Flip each of the top 5 items if feasible\n    for idx in top_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "r1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 309,
        "algorithm": "The algorithm selects the highest-scoring solution from the archive (weighted 70% on value1, 30% on value2) and performs a bulk local search by flipping the top 5 items with the highest combined marginal gains (weighted similarly) while ensuring feasibility through incremental weight tracking. It prioritizes items that offer the best trade-off between both objectives, flipping them in or out as long as the weight constraint is satisfied. The method efficiently balances exploration and exploitation by focusing on high-potential items while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the highest weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Compute combined marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Get indices of top 5 items with highest marginal gains\n    top_indices = np.argpartition(-combined_marginal, 5)[:5]\n\n    # Track current weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Flip items in bulk while ensuring feasibility\n    for idx in top_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8219729676284351,
            1.0475831925868988
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the highest weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Compute combined marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Get indices of top 5 items with highest marginal gains\n    top_indices = np.argpartition(-combined_marginal, 5)[:5]\n\n    # Track current weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Flip items in bulk while ensuring feasibility\n    for idx in top_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "r1"
    }
]