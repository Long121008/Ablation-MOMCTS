[
    {
        "algorithm": "The algorithm selects a solution from the archive based on a weighted objective score (70% value1 + 30% value2) and applies a local search by flipping up to 5 items with the highest marginal contribution (also weighted 70-30) while ensuring feasibility. It dynamically adjusts the solution by adding or removing items to maximize the combined objective without exceeding the knapsack capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item\n    marginal1 = value1_lst / (weight_lst + 1e-6)  # Avoid division by zero\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 5 items with highest marginal contribution\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8742819369527933,
            1.159465342760086
        ]
    },
    {
        "algorithm": "The algorithm selects the most crowded solution in the archive (highest dominance count) and applies a hybrid local search combining adaptive item replacement (prioritizing high-impact items) and probabilistic insertion/removal, while ensuring feasibility through dynamic weight adjustment. It prioritizes items with high normalized impact scores (combining both objectives) and replaces low-impact items with high-impact ones, while probabilistically adding/removing high-impact items. Feasibility is maintained by removing low-impact items if needed.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate dominance counts for each solution\n    dominance_counts = []\n    for i, (sol_i, obj_i) in enumerate(archive):\n        count = 0\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j and (obj_j[0] >= obj_i[0] and obj_j[1] >= obj_i[1]) and (obj_j[0] > obj_i[0] or obj_j[1] > obj_i[1]):\n                count += 1\n        dominance_counts.append(count)\n\n    # Select solution with highest dominance count (most crowded)\n    selected_idx = np.argmax(dominance_counts)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute normalized impact scores (considering both objectives and weight efficiency)\n    impact_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    normalized_scores = impact_scores / (np.max(impact_scores) + 1e-6)\n\n    # Adaptive replacement strategy: replace items with low impact with high impact items\n    low_impact_mask = normalized_scores < 0.3\n    high_impact_items = np.where(normalized_scores >= 0.7)[0]\n\n    if np.any(low_impact_mask) and len(high_impact_items) > 0:\n        low_impact_indices = np.where(low_impact_mask)[0]\n        for idx in low_impact_indices:\n            if new_solution[idx] == 1:\n                # Try to replace with a high impact item\n                for high_idx in high_impact_items:\n                    if new_solution[high_idx] == 0 and current_weight - weight_lst[idx] + weight_lst[high_idx] <= capacity:\n                        new_solution[idx], new_solution[high_idx] = new_solution[high_idx], new_solution[idx]\n                        current_weight = current_weight - weight_lst[idx] + weight_lst[high_idx]\n                        break\n\n    # Probabilistic insertion/removal of high impact items\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and np.random.rand() < 0.4 * normalized_scores[idx]:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n        elif new_solution[idx] == 1 and np.random.rand() < 0.2 * (1 - normalized_scores[idx]):\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n\n    # Dynamic weight adjustment to maintain feasibility\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        # Remove items with lowest impact until feasible\n        sorted_indices = np.argsort(normalized_scores)\n        for idx in sorted_indices:\n            if new_solution[idx] == 1:\n                new_solution[idx] = 0\n                excess_weight -= weight_lst[idx]\n                if excess_weight <= 0:\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.9084257576837889,
            2.1438385248184204
        ]
    },
    {
        "algorithm": "The algorithm combines crowding-distance selection with a tiered local search that prioritizes high-marginal-gain items, uses a weighted swap mechanism, and probabilistically flips items based on their combined normalized impact scores, ensuring feasibility through dynamic weight checks. It first selects the most crowded solution in the archive, then applies three refinement tiers: flipping top marginal items, swapping high/low marginal items, and probabilistic flips based on normalized scores, all while maintaining capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distance for each solution\n    crowding_distances = []\n    for i, (sol_i, obj_i) in enumerate(archive):\n        distances = []\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j:\n                dist = np.sqrt((obj_j[0] - obj_i[0])**2 + (obj_j[1] - obj_i[1])**2)\n                distances.append(dist)\n        crowding_distances.append(np.min(distances) if distances else float('inf'))\n\n    # Select solution with minimum crowding distance (most crowded)\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate combined marginal gains\n    marginal_value1 = value1_lst / (weight_lst + 1e-6)\n    marginal_value2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal_value1 + marginal_value2\n\n    # Tier 1: Flip top marginal items for both objectives\n    sorted_indices = np.argsort(-combined_marginal)\n    flip_count = min(5, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Tier 2: Weighted swap mechanism\n    if len(sorted_indices) >= 2:\n        # Select items with highest and lowest marginal gains\n        high_idx = sorted_indices[0]\n        low_idx = sorted_indices[-1]\n\n        # Calculate weight difference\n        weight_diff = weight_lst[high_idx] - weight_lst[low_idx]\n\n        # Perform swap if feasible\n        if new_solution[high_idx] != new_solution[low_idx]:\n            if new_solution[high_idx] == 1:\n                if current_weight + weight_diff <= capacity:\n                    new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n            else:\n                if current_weight - weight_diff <= capacity:\n                    new_solution[high_idx], new_solution[low_idx] = new_solution[low_idx], new_solution[high_idx]\n\n    # Tier 3: Probabilistic flip based on combined impact\n    normalized_scores = combined_marginal / (np.max(combined_marginal) + 1e-6)\n    for idx in range(len(normalized_scores)):\n        if np.random.rand() < 0.3 * normalized_scores[idx]:\n            if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9215646050728925,
            9.20160436630249
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive prioritizing higher scores (60% value2 + 40% value1), then applies a hybrid local search that flips up to 3 high-marginal-contribution items (weighted 60% value2 + 40% value1) while ensuring feasibility, followed by a probabilistic swap of two items to escape local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.4*value1 + 0.6*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.4 * obj[0] + 0.6 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item (0.6*value2 + 0.4*value1)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal2 + 0.4 * marginal1\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 3 items with highest marginal contribution\n    flip_count = min(3, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Probabilistic swap of two items to escape local optima\n    if len(sorted_indices) >= 2:\n        swap_candidates = sorted_indices[:5]  # Consider top 5 items for swap\n        if len(swap_candidates) >= 2:\n            i, j = np.random.choice(swap_candidates, size=2, replace=False)\n            # Check if swapping is feasible\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                elif new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[i] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "score": [
            -0.7724345802780868,
            1.3127074539661407
        ]
    },
    {
        "algorithm": "The algorithm selects a high-scoring solution from the archive (prioritizing 70% value1 + 30% value2) and generates a neighbor by flipping up to 4 high-marginal-contribution items (weighted similarly) while ensuring feasibility, followed by a probabilistic 3-item swap to explore diverse neighborhoods. It dynamically balances exploitation (flipping high-marginal items) and exploration (random swaps) while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution using a weighted objective score (0.7*value1 + 0.3*value2)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        score = 0.7 * obj[0] + 0.3 * obj[1]\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Compute marginal contributions for each item (0.7*value1 + 0.3*value2)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.7 * marginal1 + 0.3 * marginal2\n\n    # Sort items by marginal contribution (descending)\n    sorted_indices = np.argsort(-combined_marginal)\n\n    # Dynamic flip strategy: flip up to 4 items with highest marginal contribution\n    flip_count = min(4, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Probabilistic swap of three items to explore diverse neighborhoods\n    if len(sorted_indices) >= 3:\n        swap_candidates = sorted_indices[:6]  # Consider top 6 items for swap\n        if len(swap_candidates) >= 3:\n            i, j, k = np.random.choice(swap_candidates, size=3, replace=False)\n            # Check if swapping is feasible\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                elif new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[i] <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n            if new_solution[j] != new_solution[k]:\n                if new_solution[j] == 1 and current_weight - weight_lst[j] + weight_lst[k] <= capacity:\n                    new_solution[j], new_solution[k] = new_solution[k], new_solution[j]\n                elif new_solution[k] == 1 and current_weight - weight_lst[k] + weight_lst[j] <= capacity:\n                    new_solution[j], new_solution[k] = new_solution[k], new_solution[j]\n\n    return new_solution\n\n",
        "score": [
            -0.8631499479602416,
            1.3316587507724762
        ]
    },
    {
        "algorithm": "The algorithm dynamically selects a solution from the archive based on alternating priorities (value1, value2, or a hybrid) and applies a two-stage local search: first flipping items with the highest normalized marginal gains (weighted by a time-varying factor) to improve both objectives, then performing a probabilistic swap of items to explore the solution space while maintaining feasibility. The selection strategy cycles through different objective preferences, and the local search uses both deterministic and stochastic operations to balance exploitation and exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic selection based on time-varying weights (cyclic between objectives)\n    current_time = len(archive) % 4\n    if current_time == 0:\n        # Focus on value2\n        selected_idx = np.argmax([obj[1] for _, obj in archive])\n    elif current_time == 1:\n        # Focus on value1\n        selected_idx = np.argmax([obj[0] for _, obj in archive])\n    elif current_time == 2:\n        # Hybrid approach (weighted sum)\n        selected_idx = np.argmax([0.6 * obj[0] + 0.4 * obj[1] for _, obj in archive])\n    else:\n        # Random selection to encourage diversity\n        selected_idx = np.random.randint(0, len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Time-varying marginal contribution calculation\n    beta = 0.8 if (current_time % 2 == 0) else 0.2  # Alternate between objectives\n    combined_marginal = beta * (value1_lst / (weight_lst + 1e-6)) + (1 - beta) * (value2_lst / (weight_lst + 1e-6))\n\n    # Stage 1: Flip top items with highest marginal contribution\n    sorted_indices = np.argsort(-combined_marginal)\n    flip_count = min(3, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Stage 2: Probabilistic item swap\n    if np.random.random() < 0.6:  # 60% chance to perform swap\n        swap_candidates = sorted_indices[:5]  # Consider top 5 items\n        if len(swap_candidates) >= 2:\n            i, j = np.random.choice(swap_candidates, size=2, replace=False)\n            if new_solution[i] != new_solution[j]:\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if new_solution[i] == 1:\n                    if current_weight + weight_diff <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                else:\n                    if current_weight - weight_diff <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "score": [
            -0.7210599016669208,
            1.2667514085769653
        ]
    },
    {
        "algorithm": "The algorithm selects the most crowded solution from the archive based on dominance counts, then applies a hybrid local search combining greedy marginal improvement (prioritizing items with high combined impact scores) and probabilistic neighborhood exploration (adjusting inclusion/exclusion probabilistically with temperature-based weighting). It ensures feasibility by dynamically adjusting item selections while respecting the capacity constraint. The method balances exploitation (greedy phase) and exploration (probabilistic phase) while maintaining multi-objective optimization through weighted impact scores.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate dominance-based crowding metric\n    dominance_counts = []\n    for i, (sol_i, obj_i) in enumerate(archive):\n        count = 0\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j and (obj_j[0] >= obj_i[0] and obj_j[1] >= obj_i[1]) and (obj_j[0] > obj_i[0] or obj_j[1] > obj_i[1]):\n                count += 1\n        dominance_counts.append(count)\n\n    # Select solution with highest dominance count (most crowded in Pareto front)\n    selected_idx = np.argmax(dominance_counts)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate objective-specific impact scores\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (impact1 + impact2) / 2\n\n    # Greedy marginal improvement phase\n    sorted_indices = np.argsort(-combined_impact)\n    for idx in sorted_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Probabilistic neighborhood exploration\n    temp = 0.7 * (1 - current_weight/capacity)\n    for idx in range(len(combined_impact)):\n        if new_solution[idx] == 1:\n            prob = np.exp(-impact1[idx] / temp) * np.exp(-impact2[idx] / temp)\n            if np.random.rand() < prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            prob = np.exp(impact1[idx] / temp) * np.exp(impact2[idx] / temp)\n            if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.889904361836914,
            11.143534690141678
        ]
    },
    {
        "algorithm": "The heuristic selects a high-value solution from the archive (based on the maximum of either objective value) and applies a hybrid local search by flipping items prioritized by their combined value-to-weight ratios (sum of ratios for both objectives), ensuring feasibility by checking weight constraints. It randomly selects up to 5 high-priority items to flip, removing or adding them if the operation doesn\u2019t exceed capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (e.g., those with high diversity or low dominance)\n    # Here, we randomly select a solution with high value in at least one objective\n    max_value = -1\n    selected_idx = 0\n    for i, (sol, obj) in enumerate(archive):\n        if max(obj[0], obj[1]) > max_value:\n            max_value = max(obj[0], obj[1])\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: flip items with high value-to-weight ratio in a random order\n    # First, compute value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n\n    # Combine ratios and sort by descending order\n    combined_ratio = ratio1 + ratio2\n    sorted_indices = np.argsort(-combined_ratio)\n\n    # Randomly select a subset of items to flip\n    flip_indices = np.random.choice(sorted_indices, size=min(5, len(sorted_indices)), replace=False)\n\n    # Flip selected items while ensuring feasibility\n    total_weight = np.sum(weight_lst * new_solution)\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            # Try to remove if feasible\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                total_weight -= weight_lst[idx]\n        else:\n            # Try to add if feasible\n            if total_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.30518390165972703,
            1.3124490082263947
        ]
    },
    {
        "algorithm": "The algorithm selects the most crowded solution from the archive (based on crowding distance) and applies a tiered local search: first removing low-marginal items, then adding high-marginal items, and finally performing probabilistic swaps between items with normalized impact scores, all while ensuring feasibility by dynamically checking weight constraints. The combined marginal gains (sum of normalized value1 and value2 per weight) guide the selection and swapping priorities.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distance for each solution\n    crowding_distances = []\n    for i, (sol_i, obj_i) in enumerate(archive):\n        distances = []\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j:\n                dist = np.sqrt((obj_j[0] - obj_i[0])**2 + (obj_j[1] - obj_i[1])**2)\n                distances.append(dist)\n        crowding_distances.append(np.min(distances) if distances else float('inf'))\n\n    # Select solution with minimum crowding distance (most crowded)\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate combined marginal gains\n    marginal_value1 = value1_lst / (weight_lst + 1e-6)\n    marginal_value2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal_value1 + marginal_value2\n\n    # Tier 1: Remove low marginal items\n    sorted_indices = np.argsort(combined_marginal)\n    for idx in sorted_indices[:min(3, len(sorted_indices))]:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Tier 2: Add high marginal items\n    sorted_indices = np.argsort(-combined_marginal)\n    for idx in sorted_indices[:min(3, len(sorted_indices))]:\n        if new_solution[idx] == 0:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Tier 3: Probabilistic swap based on normalized impact\n    normalized_scores = combined_marginal / (np.max(combined_marginal) + 1e-6)\n    swap_indices = np.random.choice(len(normalized_scores), size=min(5, len(normalized_scores)), replace=False)\n    for i in range(0, len(swap_indices) - 1, 2):\n        idx1, idx2 = swap_indices[i], swap_indices[i+1]\n        weight_diff = weight_lst[idx1] - weight_lst[idx2]\n        if new_solution[idx1] != new_solution[idx2]:\n            if new_solution[idx1] == 1:\n                if current_weight + weight_diff <= capacity:\n                    new_solution[idx1], new_solution[idx2] = new_solution[idx2], new_solution[idx1]\n            else:\n                if current_weight - weight_diff <= capacity:\n                    new_solution[idx1], new_solution[idx2] = new_solution[idx2], new_solution[idx1]\n\n    return new_solution\n\n",
        "score": [
            -0.8624499829105705,
            9.329465210437775
        ]
    },
    {
        "algorithm": "The algorithm dynamically selects a solution from the archive based on alternating priorities (value1, value2, or balanced) and applies a multi-stage local search: first flipping top marginal-contribution items (adaptively weighted by solution density) to improve both objectives, then probabilistically replacing items to escape local optima while maintaining feasibility. It prioritizes items with higher combined marginal gains (value1 and value2 normalized by weight) and adjusts flipping/replacement based on current solution density.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic selection based on time-varying weights (cyclic between objectives)\n    current_time = len(archive) % 3\n    if current_time == 0:\n        # Focus on value2\n        selected_idx = np.argmax([obj[1] for _, obj in archive])\n    elif current_time == 1:\n        # Focus on value1\n        selected_idx = np.argmax([obj[0] for _, obj in archive])\n    else:\n        # Balanced approach\n        selected_idx = np.argmax([0.5 * obj[0] + 0.5 * obj[1] for _, obj in archive])\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Adaptive marginal contribution calculation\n    density = np.sum(new_solution) / len(new_solution)\n    alpha = 0.7 if density > 0.5 else 0.3  # Adjust based on solution density\n    combined_marginal = alpha * (value2_lst / (weight_lst + 1e-6)) + (1 - alpha) * (value1_lst / (weight_lst + 1e-6))\n\n    # Multi-stage local search\n    # Stage 1: Flip top items with highest marginal contribution\n    sorted_indices = np.argsort(-combined_marginal)\n    flip_count = min(4, len(sorted_indices))\n    for idx in sorted_indices[:flip_count]:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Stage 2: Probabilistic item replacement\n    if np.random.random() < 0.7:  # 70% chance to perform replacement\n        replace_candidates = sorted_indices[:6]  # Consider top 6 items\n        if len(replace_candidates) >= 2:\n            i, j = np.random.choice(replace_candidates, size=2, replace=False)\n            if new_solution[i] != new_solution[j]:\n                # Calculate weight difference\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if new_solution[i] == 1:\n                    if current_weight + weight_diff <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                else:\n                    if current_weight - weight_diff <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    return new_solution\n\n",
        "score": [
            -0.8334009459878748,
            1.40790656208992
        ]
    }
]