[
    {
        "algorithm": "This algorithm employs a diversity-aware selection mechanism that prioritizes solutions with high variance in marginal value-to-weight ratios, followed by a hybrid local search that alternates between targeted inclusion of high-value items and strategic swaps of low-value items with high-weight alternatives, while dynamically balancing exploration and exploitation based on solution quality and ensuring feasibility through a greedy removal process prioritizing high weight-to-value ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection: prioritize solutions with high marginal contribution variance\n    diversity_scores = []\n    for sol, _ in archive:\n        included = np.where(sol == 1)[0]\n        if len(included) == 0:\n            diversity_scores.append(0)\n            continue\n\n        # Calculate marginal contributions for included items\n        marginal_contribs1 = value1_lst[included] / weight_lst[included]\n        marginal_contribs2 = value2_lst[included] / weight_lst[included]\n        combined_contribs = marginal_contribs1 + marginal_contribs2\n\n        # Diversity score is the variance of marginal contributions\n        diversity = np.var(combined_contribs)\n        diversity_scores.append(diversity)\n\n    # Select solution with highest diversity score\n    selected_idx = np.argmax(diversity_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate solution quality score (combined value-to-weight ratio)\n    included = np.where(base_solution == 1)[0]\n    if len(included) > 0:\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n        total_weight = np.sum(weight_lst[included])\n        quality_score = (total_value1 + total_value2) / (total_weight + 1e-8)\n    else:\n        quality_score = 0\n\n    # Dynamic exploration/exploitation trade-off (inverse of original)\n    exploitation_prob = 0.3 + 0.7 * (quality_score / (np.max([obj[0] + obj[1] for _, obj in archive]) + 1e-8))\n\n    # Hybrid local search with different strategy\n    if np.random.rand() < exploitation_prob:\n        # Targeted inclusion of high-value items with low weight\n        not_included = np.where(base_solution == 0)[0]\n        if len(not_included) > 0:\n            # Calculate value-to-weight ratio for not-included items\n            vw_ratios = (value1_lst[not_included] + value2_lst[not_included]) / (weight_lst[not_included] + 1e-8)\n\n            # Select top 20% items with highest value-to-weight ratio\n            top_candidates = not_included[np.argsort(vw_ratios)[-max(1, len(not_included)//5):]]\n\n            for item in top_candidates:\n                if np.random.rand() < 0.6:  # 60% chance to include each top candidate\n                    if (np.sum(weight_lst[new_solution == 1]) + weight_lst[item]) <= capacity:\n                        new_solution[item] = 1\n    else:\n        # Probabilistic swap of low-value items with high-weight items\n        included = np.where(base_solution == 1)[0]\n        not_included = np.where(base_solution == 0)[0]\n\n        if len(included) > 0 and len(not_included) > 0:\n            # Find low-value items to potentially remove\n            removal_ratios = (value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-8)\n            low_value_items = included[np.argsort(removal_ratios)[:max(1, len(included)//4)]]\n\n            # Find high-weight items to potentially add\n            high_weight_items = not_included[np.argsort(weight_lst[not_included])[-max(1, len(not_included)//4):]]\n\n            for remove_item in low_value_items:\n                for add_item in high_weight_items:\n                    if (np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_item] + weight_lst[add_item]) <= capacity:\n                        new_solution[remove_item] = 0\n                        new_solution[add_item] = 1\n                        break\n\n    # Feasibility enforcement (different priority calculation)\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Calculate removal priorities (highest weight-to-value ratio)\n        removal_priorities = (weight_lst[included_items] + 1e-8) / (value1_lst[included_items] + value2_lst[included_items] + 1e-8)\n        sorted_indices = included_items[np.argsort(removal_priorities)[::-1]]  # Descending order\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.999853639804997,
            9.46109089255333
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted sum of normalized objectives (70% value1, 30% value2), then applies a hybrid local search combining probabilistic swaps, marginal contribution-based flips, and a greedy removal step to ensure feasibility. It dynamically balances exploration/exploitation based on archive size, with higher probabilities for marginal improvements and feasibility checks. The method prioritizes solutions with better combined objective scores while intelligently modifying them to improve both objectives while respecting capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Normalize objectives and compute combined scores\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    norm_obj1 = (obj1 - np.min(obj1)) / (np.max(obj1) - np.min(obj1) + 1e-8)\n    norm_obj2 = (obj2 - np.min(obj2)) / (np.max(obj2) - np.min(obj2) + 1e-8)\n    combined_scores = 0.7 * norm_obj1 + 0.3 * norm_obj2  # Weighted sum\n\n    # Select solution with highest combined score\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic exploration/exploitation trade-off\n    exploration_prob = 0.5 + 0.5 * (1 - len(archive) / 100)  # Higher exploration for small archives\n\n    # Strategy 1: Probabilistic item swap with feasibility check\n    if np.random.rand() < exploration_prob and n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        if new_solution[swap_indices[0]] != new_solution[swap_indices[1]]:\n            # Check feasibility\n            delta = (weight_lst[swap_indices[1]] - weight_lst[swap_indices[0]]) if new_solution[swap_indices[0]] == 1 else (weight_lst[swap_indices[0]] - weight_lst[swap_indices[1]])\n            if current_weight + delta <= capacity:\n                new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    # Strategy 2: Marginal contribution-based flips\n    for i in range(n_items):\n        if np.random.rand() < 0.3:  # Higher probability than original\n            if new_solution[i] == 1:\n                # Check if removing improves both objectives\n                marginal1 = -value1_lst[i]\n                marginal2 = -value2_lst[i]\n                if marginal1 < 0 or marginal2 < 0:  # Negative marginal\n                    new_weight = current_weight - weight_lst[i]\n                    if new_weight >= 0:\n                        new_solution[i] = 0\n            else:\n                # Check if adding improves both objectives\n                marginal1 = value1_lst[i]\n                marginal2 = value2_lst[i]\n                if marginal1 > 0 and marginal2 > 0:  # Positive marginal\n                    new_weight = current_weight + weight_lst[i]\n                    if new_weight <= capacity:\n                        new_solution[i] = 1\n\n    # Strategy 3: Greedy removal for feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        # Sort items by weight/value ratio and remove until feasible\n        included_items = np.where(new_solution == 1)[0]\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(ratios)]\n        excess = total_weight - capacity\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9450039798002199,
            2.6820522248744965
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto filtering with objective-space partitioning to select a diverse, high-potential solution from the archive, then applies a hybrid local search featuring adaptive flipping (based on negative objective gradients) and correlation-aware swapping (probabilistically guided by objective correlation) to generate neighbors while maintaining feasibility through probabilistic capacity adjustment. It prioritizes items with high value-to-weight ratios during removal when capacity is exceeded.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection with Pareto filtering\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Objective-space partitioning\n    obj1 = np.array([obj[0] for _, obj in pareto_front])\n    obj2 = np.array([obj[1] for _, obj in pareto_front])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    min_obj1, min_obj2 = np.min(obj1), np.min(obj2)\n\n    # Select solution from under-represented region\n    density = np.zeros(len(pareto_front))\n    for i in range(len(pareto_front)):\n        dist1 = (obj1[i] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n        dist2 = (obj2[i] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n        density[i] = dist1 + dist2\n\n    selected_idx = np.argmin(density)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive flipping based on objective gradients\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0:\n        # Calculate objective gradients\n        obj1_grad = (value1_lst[in_items] - np.mean(value1_lst[in_items])) / (np.std(value1_lst[in_items]) + 1e-10)\n        obj2_grad = (value2_lst[in_items] - np.mean(value2_lst[in_items])) / (np.std(value2_lst[in_items]) + 1e-10)\n\n        # Flip items with high negative gradients\n        flip_mask = (obj1_grad < -0.5) | (obj2_grad < -0.5)\n        for i in in_items[flip_mask]:\n            if current_weight - weight_lst[i] >= 0:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Correlation-aware swapping\n    if len(out_items) > 0 and len(in_items) > 0:\n        # Calculate correlation between objectives\n        obj_corr = np.corrcoef(value1_lst[in_items], value2_lst[in_items])[0, 1]\n        swap_prob = 0.5 * (1 + obj_corr)\n\n        if random.random() < swap_prob:\n            # Select item to remove based on combined value\n            combined_value = value1_lst[in_items] + value2_lst[in_items]\n            remove_idx = in_items[np.argmin(combined_value)]\n\n            # Select item to add based on marginal gain\n            marginal_gain = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n            add_idx = out_items[np.argmax(marginal_gain)]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n\n    # Probabilistic capacity adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Calculate removal probabilities based on value-to-weight ratio\n        value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        removal_probs = value_ratio / np.sum(value_ratio)\n\n        while excess > 0 and len(included_items) > 0:\n            removal_idx = np.random.choice(included_items, p=removal_probs)\n            excess -= weight_lst[removal_idx]\n            new_solution[removal_idx] = 0\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) > 0:\n                value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n                removal_probs = value_ratio / np.sum(value_ratio)\n\n    return new_solution\n\n",
        "score": [
            -0.9689916212137121,
            2.895742356777191
        ]
    },
    {
        "algorithm": "This heuristic algorithm combines novelty-aware selection with an adaptive hybrid local search that prioritizes solutions with high objective diversity and crowding distance, using dynamic flipping probabilities and novelty-aware insertion metrics to generate improved neighbors while maintaining feasibility through weighted removal. The algorithm balances exploration and exploitation by emphasizing solutions with balanced objectives and leveraging objective-specific novelty metrics to guide local search, ensuring high-quality solutions across multiple objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Extract objectives and compute normalization factors\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    min_obj1, min_obj2 = np.min(obj1), np.min(obj2)\n\n    # Calculate normalized novelty metrics\n    norm_obj1 = (obj1 - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n    norm_obj2 = (obj2 - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n    novelty_score = (1 - np.abs(norm_obj1 - norm_obj2)) * (norm_obj1 + norm_obj2) / 2\n\n    # Calculate crowding distance\n    sorted_indices = np.argsort(obj1)\n    crowding = np.zeros(len(archive))\n    crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n    for i in range(1, len(archive)-1):\n        crowding[sorted_indices[i]] = abs(obj1[sorted_indices[i+1]] - obj1[sorted_indices[i-1]]) / (max_obj1 - min_obj1 + 1e-10)\n\n    # Combine novelty and crowding distance with adaptive weights\n    selection_score = novelty_score * (1 + 0.5 * crowding)\n    selected_idx = np.argmax(selection_score)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Dynamic objective balance factor\n    obj_balance = (obj1[selected_idx] * max_obj2 + obj2[selected_idx] * max_obj1) / (max_obj1 * max_obj2 + 1e-10)\n\n    # Novelty-aware flipping with adaptive probabilities\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        flip_prob = 0.2 + 0.6 * obj_balance + 0.2 * (1 - novelty_score[selected_idx])\n        for i in in_items:\n            if random.random() < flip_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n\n    # Crowding-aware insertion with novelty consideration\n    out_items = np.where(new_solution == 0)[0]\n    if len(out_items) > 0:\n        # Calculate novelty-aware insertion metrics\n        insertion_metric = (value1_lst[out_items] * norm_obj1[selected_idx] + value2_lst[out_items] * norm_obj2[selected_idx]) / (weight_lst[out_items] + 1e-10)\n        insertion_idx = np.argmax(insertion_metric)\n        best_insert = out_items[insertion_idx]\n\n        # Check feasibility and perform insertion\n        if (current_weight + weight_lst[best_insert]) <= capacity:\n            new_solution[best_insert] = 1\n            current_weight += weight_lst[best_insert]\n\n    # Feasibility enforcement with weighted removal\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Calculate weighted removal metric\n        removal_metric = (value1_lst[included_items] * norm_obj1[selected_idx] + value2_lst[included_items] * norm_obj2[selected_idx]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(removal_metric)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9327023772208678,
            1.7781460285186768
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using adaptive weights that prioritize solutions with balanced objectives, then applies a hybrid local search that probabilistically removes items based on objective balance and strategically inserts new items using crowding-aware metrics, while ensuring feasibility through weighted removal. The method balances exploration and exploitation by dynamically adjusting probabilities and metrics based on the current solution's objective values.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Extract objectives and compute normalization factors\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    min_obj1, min_obj2 = np.min(obj1), np.min(obj2)\n\n    # Calculate adaptive selection weights\n    obj_ratio = (obj1 / (max_obj1 + 1e-10)) / (obj2 / (max_obj2 + 1e-10) + 1e-10)\n    selection_weights = (1 - np.abs(obj_ratio - 1)) * (obj1 * max_obj2 + obj2 * max_obj1) / (max_obj1 * max_obj2 + 1e-10)\n    selected_idx = np.argmax(selection_weights)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Objective-balanced flipping probabilities\n    obj_balance = (obj1[selected_idx] / (max_obj1 + 1e-10)) * (obj2[selected_idx] / (max_obj2 + 1e-10))\n    flip_prob = 0.3 + 0.5 * obj_balance\n\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        for i in in_items:\n            if random.random() < flip_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n\n    # Hybrid local search with objective-specific metrics\n    out_items = np.where(new_solution == 0)[0]\n    if len(out_items) > 0:\n        # Calculate combined insertion metric\n        insertion_metric = (value1_lst[out_items] * (max_obj2 - obj2[selected_idx]) / (max_obj2 - min_obj2 + 1e-10) +\n                          value2_lst[out_items] * (max_obj1 - obj1[selected_idx]) / (max_obj1 - min_obj1 + 1e-10)) / (weight_lst[out_items] + 1e-10)\n        insertion_idx = np.argmax(insertion_metric)\n        best_insert = out_items[insertion_idx]\n\n        if (current_weight + weight_lst[best_insert]) <= capacity:\n            new_solution[best_insert] = 1\n            current_weight += weight_lst[best_insert]\n\n    # Objective-aware weighted removal\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        removal_metric = (value1_lst[included_items] * (obj2[selected_idx] / (max_obj2 + 1e-10)) +\n                         value2_lst[included_items] * (obj1[selected_idx] / (max_obj1 + 1e-10))) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(removal_metric)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9120491472890997,
            1.6299539804458618
        ]
    },
    {
        "algorithm": "The algorithm prioritizes solutions with high combined objective values and crowding distance, applying a hybrid local search that adaptively flips items with probabilities based on objective balance and novelty scores, while ensuring feasibility through a multi-criteria removal strategy that balances value1 and value2 with different weightings. It dynamically adjusts flip probabilities and selection criteria, favoring value1 in novelty calculations and value2 in quality evaluations, and enforces feasibility by removing items with the lowest combined novelty and quality metrics.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novelty-driven selection with different weightings\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n\n    # Calculate novelty score with different balancing\n    novelty_score = (obj1 / (max_obj1 + 1e-10))**0.8 * (obj2 / (max_obj2 + 1e-10))**0.8 * (1 - np.abs(obj1 - obj2) / (max_obj1 + max_obj2 + 1e-10))\n\n    # Calculate crowding distance with different metric\n    sorted_indices = np.argsort(obj1)\n    crowding = np.zeros(len(archive))\n    crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n    for i in range(1, len(archive)-1):\n        crowding[sorted_indices[i]] = abs(obj1[sorted_indices[i+1]] - obj1[sorted_indices[i-1]]) / (max_obj1 + 1e-10)\n\n    # Combine novelty and crowding with different weights\n    selection_score = 0.7 * novelty_score + 0.3 * crowding\n    selected_idx = np.argmax(selection_score)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive local search with different parameters\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Objective balance factor with different calculation\n        obj_balance = (obj1[selected_idx] + 0.4 * obj2[selected_idx]) / (max_obj1 + max_obj2 + 1e-10)\n\n        # Dynamic flip probabilities with different formula\n        flip_prob = 0.3 + 0.4 * obj_balance + 0.3 * (1 - novelty_score[selected_idx])\n\n        # Novelty-aware flip\n        for i in in_items:\n            if random.random() < flip_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n\n        # Targeted swap with different selection criteria\n        if len(out_items) > 0:\n            # Calculate novelty-aware marginal gains with different weights\n            novelty_gain = (value1_lst[out_items] * 0.8 + value2_lst[out_items] * 0.2) / (weight_lst[out_items] + 1e-10)\n            quality_gain = (value1_lst[out_items] * 0.2 + value2_lst[out_items] * 0.8) / (weight_lst[out_items] + 1e-10)\n\n            # Combined metric for swap selection with different weights\n            combined_gain = 0.6 * novelty_gain + 0.4 * quality_gain\n            swap_in = out_items[np.argmax(combined_gain)]\n\n            # Calculate marginal gains for removal with different metric\n            if len(in_items) > 0:\n                marginal_gain = (value1_lst[in_items] * 0.3 + value2_lst[in_items] * 0.7) / weight_lst[in_items]\n                swap_out = in_items[np.argmin(marginal_gain)]\n\n                if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                    new_solution[swap_out] = 0\n                    new_solution[swap_in] = 1\n\n    # Multi-criteria feasibility enforcement with different weights\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest combined novelty and quality with different weights\n        combined_metric = (value1_lst[included_items] * 0.7 + value2_lst[included_items] * 0.3) / (weight_lst[included_items] + 1e-10)\n        novelty_metric = (value1_lst[included_items] * 0.3 + value2_lst[included_items] * 0.7) / (weight_lst[included_items] + 1e-10)\n        removal_metric = 0.7 * combined_metric + 0.3 * novelty_metric\n\n        sorted_indices = included_items[np.argsort(removal_metric)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8755752154159082,
            0.642067164182663
        ]
    },
    {
        "algorithm": "The algorithm combines a diversity-aware selection strategy with a hybrid local search that dynamically balances marginal contribution flips (70% probability) and value-aware swaps (30% probability), prioritizing solutions with high combined objective ratios while ensuring feasibility through greedy excess removal. It intelligently samples from the archive using a combined objective ratio to guide exploration, dynamically adjusting between exploration and exploitation based on archive size, and always maintains feasibility through strict capacity checks and greedy removal steps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate combined objective ratios for selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    combined_ratios = (obj1 + obj2) / (np.sum(weight_lst) + 1e-8)\n\n    # Select solution with highest combined ratio\n    selected_idx = np.argmax(combined_ratios)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Dynamic exploration/exploitation trade-off\n    exploration_prob = 0.5 + 0.5 * (1 - len(archive) / 100)\n\n    if random.random() < exploration_prob:\n        # Marginal contribution flips (70% probability)\n        for i in range(len(weight_lst)):\n            if random.random() < 0.7:\n                if new_solution[i] == 1:\n                    # Remove if negative marginal contribution\n                    marginal1 = -value1_lst[i]\n                    marginal2 = -value2_lst[i]\n                    if marginal1 < 0 or marginal2 < 0:\n                        new_weight = current_weight - weight_lst[i]\n                        if new_weight >= 0:\n                            new_solution[i] = 0\n                            current_weight = new_weight\n                else:\n                    # Add if positive marginal contribution\n                    marginal1 = value1_lst[i]\n                    marginal2 = value2_lst[i]\n                    if marginal1 > 0 and marginal2 > 0:\n                        new_weight = current_weight + weight_lst[i]\n                        if new_weight <= capacity:\n                            new_solution[i] = 1\n                            current_weight = new_weight\n    else:\n        # Value-aware swaps (30% probability)\n        not_in_solution = np.where(new_solution == 0)[0]\n        if len(not_in_solution) > 0:\n            # Calculate value ratios\n            ratios1 = value1_lst[not_in_solution] / weight_lst[not_in_solution]\n            ratios2 = value2_lst[not_in_solution] / weight_lst[not_in_solution]\n            combined_ratios = ratios1 + ratios2\n\n            # Select top 30% candidates\n            top_candidates = not_in_solution[np.argsort(combined_ratios)[-max(1, len(combined_ratios)//3):]]\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find worst item to remove\n                in_solution = np.where(new_solution == 1)[0]\n                if len(in_solution) > 0:\n                    ratios1_in = value1_lst[in_solution] / weight_lst[in_solution]\n                    ratios2_in = value2_lst[in_solution] / weight_lst[in_solution]\n                    combined_ratios_in = ratios1_in + ratios2_in\n                    swap_out = in_solution[np.argmin(combined_ratios_in)]\n\n                    # Perform swap if feasible\n                    if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                        new_solution[swap_out] = 0\n                        new_solution[swap_in] = 1\n                        current_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n\n    # Greedy excess removal if still over capacity\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(ratios)]\n        excess = total_weight - capacity\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.39200351044471826,
            0.8863400220870972
        ]
    },
    {
        "algorithm": "The algorithm combines hypervolume-guided selection with correlation-aware local search that dynamically adjusts flip probabilities based on objective ratios and correlations, prioritizing high-marginal-gain items in targeted swaps while enforcing feasibility through greedy excess removal. It balances exploration and exploitation by alternating between probabilistic flips and marginal-gain swaps, adjusting behavior based on objective correlations to maintain diversity. The selection mechanism prioritizes solutions with high hypervolume and correlation factors, while the local search avoids overfitting by dynamically adapting flip probabilities and swap mechanisms.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-guided selection with correlation awareness\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    hypervolume = (max_obj1 - obj1) * (max_obj2 - obj2)\n\n    # Objective correlation analysis\n    obj_correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    correlation_factor = 1 + abs(obj_correlation)\n\n    # Combined selection score\n    selection_score = hypervolume * correlation_factor\n    selected_idx = np.argmax(selection_score)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Correlation-aware local search\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Dynamic flip probability based on objective correlation\n        obj1_ratio = obj1[selected_idx] / (obj1[selected_idx] + obj2[selected_idx] + 1e-10)\n        obj2_ratio = obj2[selected_idx] / (obj1[selected_idx] + obj2[selected_idx] + 1e-10)\n        flip_prob = 0.5 * (1 + obj1_ratio - obj2_ratio) * (1 + obj_correlation)\n\n        # Probabilistic marginal-gain flips\n        if len(in_items) > 1:\n            marginal_gains = (value1_lst[in_items] * obj1_ratio + value2_lst[in_items] * obj2_ratio) / weight_lst[in_items]\n            flip_candidates = in_items[np.argsort(marginal_gains)[:max(1, len(in_items)//3)]]\n            for i in flip_candidates:\n                if random.random() < flip_prob:\n                    new_weight = current_weight - weight_lst[i]\n                    if new_weight >= 0:\n                        new_solution[i] = 0\n\n        # Targeted probabilistic swap\n        if len(out_items) > 0:\n            # Prioritize items with high combined marginal gains\n            combined_ratios = (value1_lst[out_items] * obj1_ratio + value2_lst[out_items] * obj2_ratio) / weight_lst[out_items]\n            top_candidates = out_items[np.argsort(combined_ratios)[-max(1, len(out_items)//4):]]\n\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find item to remove with lowest combined marginal gain\n                marginal_gains_out = (value1_lst[in_items] * obj1_ratio + value2_lst[in_items] * obj2_ratio) / weight_lst[in_items]\n                swap_out = in_items[np.argmin(marginal_gains_out)]\n\n                if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                    new_solution[swap_out] = 0\n                    new_solution[swap_in] = 1\n\n    # Feasibility enforcement\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest combined marginal gain\n        combined_marginal = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(combined_marginal)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9240962978312129,
            2.3544531762599945
        ]
    },
    {
        "algorithm": "The algorithm implements a diversity-aware selection strategy that prioritizes solutions with high potential for improvement, followed by a hybrid local search combining random flips (60% chance) for items already in the solution and a value-aware swap (40% chance) that intelligently selects high-value items to add while removing low-value items to maintain feasibility. The value-aware swap specifically targets items with top 40% combined value-to-weight ratios, ensuring both objectives are balanced.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high potential for improvement (diversity-aware)\n    selected_idx = random.choices(range(len(archive)), weights=[1/(1 + i) for i in range(len(archive))])[0]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: random flip with value-aware swap\n    if random.random() < 0.6:  # 60% chance for random flip\n        # Randomly flip items that could improve either objective\n        candidates = np.where(base_solution == 1)[0]\n        if len(candidates) > 0:\n            flip_idx = random.choice(candidates)\n            new_solution[flip_idx] = 0\n    else:  # 40% chance for value-aware swap\n        # Find items not in solution with high value ratios\n        not_in_solution = np.where(base_solution == 0)[0]\n        if len(not_in_solution) > 0:\n            # Calculate value ratios (value1/weight and value2/weight)\n            ratios1 = value1_lst[not_in_solution] / weight_lst[not_in_solution]\n            ratios2 = value2_lst[not_in_solution] / weight_lst[not_in_solution]\n            combined_ratios = ratios1 + ratios2\n\n            # Select top 40% candidates by combined ratio\n            top_candidates = not_in_solution[np.argsort(combined_ratios)[-max(1, len(combined_ratios)//2):]]\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find items in solution with low value ratios to swap out\n                in_solution = np.where(base_solution == 1)[0]\n                if len(in_solution) > 0:\n                    ratios1_in = value1_lst[in_solution] / weight_lst[in_solution]\n                    ratios2_in = value2_lst[in_solution] / weight_lst[in_solution]\n                    combined_ratios_in = ratios1_in + ratios2_in\n                    swap_out = in_solution[np.argmin(combined_ratios_in)]\n\n                    # Perform swap if feasible\n                    if (np.sum(weight_lst[new_solution == 1]) - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                        new_solution[swap_out] = 0\n                        new_solution[swap_in] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.5493558789167777,
            1.034000277519226
        ]
    },
    {
        "algorithm": "This algorithm employs a diversity-aware selection of solutions from the archive, prioritizing those with high value2 and low value1, then applies a hybrid local search with adaptive flip probabilities and targeted swaps guided by objective balance factors, while ensuring feasibility through a multi-criteria removal strategy that balances diversity and quality metrics. The selection process uses a weighted combination of diversity scores (favoring high value2 and low value1) and crowding distances to identify promising solutions, followed by a local search that probabilistically flips items out of the knapsack and strategically swaps items based on combined gain metrics, with a final feasibility check that removes items in order of decreasing weighted diversity/quality scores.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n\n    diversity_score = (obj2 / (max_obj2 + 1e-10))**0.8 * (1 - obj1 / (max_obj1 + 1e-10))**0.6 * (np.abs(obj1 - obj2) / (max_obj1 + max_obj2 + 1e-10))\n\n    sorted_indices = np.argsort(obj2)\n    crowding = np.zeros(len(archive))\n    crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n    for i in range(1, len(archive)-1):\n        crowding[sorted_indices[i]] = abs(obj2[sorted_indices[i+1]] - obj2[sorted_indices[i-1]]) / (max_obj2 + 1e-10)\n\n    selection_score = 0.5 * diversity_score + 0.5 * crowding\n    selected_idx = np.argmax(selection_score)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        obj_balance = (0.6 * obj1[selected_idx] + 0.4 * obj2[selected_idx]) / (max_obj1 + max_obj2 + 1e-10)\n\n        flip_prob = 0.3 + 0.4 * obj_balance + 0.3 * diversity_score[selected_idx]\n\n        for i in in_items:\n            if random.random() < flip_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n\n        if len(out_items) > 0:\n            diversity_gain = (value1_lst[out_items] * 0.3 + value2_lst[out_items] * 0.7) / (weight_lst[out_items] + 1e-10)\n            quality_gain = (value1_lst[out_items] * 0.7 + value2_lst[out_items] * 0.3) / (weight_lst[out_items] + 1e-10)\n\n            combined_gain = 0.4 * diversity_gain + 0.6 * quality_gain\n            swap_in = out_items[np.argmax(combined_gain)]\n\n            if len(in_items) > 0:\n                marginal_gain = (value1_lst[in_items] * 0.6 + value2_lst[in_items] * 0.4) / weight_lst[in_items]\n                swap_out = in_items[np.argmin(marginal_gain)]\n\n                if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                    new_solution[swap_out] = 0\n                    new_solution[swap_in] = 1\n\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        diversity_metric = (value1_lst[included_items] * 0.2 + value2_lst[included_items] * 0.8) / (weight_lst[included_items] + 1e-10)\n        quality_metric = (value1_lst[included_items] * 0.8 + value2_lst[included_items] * 0.2) / (weight_lst[included_items] + 1e-10)\n        removal_metric = 0.7 * diversity_metric + 0.3 * quality_metric\n\n        sorted_indices = included_items[np.argsort(removal_metric)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8412409577719013,
            1.3900923430919647
        ]
    }
]