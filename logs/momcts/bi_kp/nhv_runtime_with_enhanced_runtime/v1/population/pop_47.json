[
    {
        "algorithm": "The algorithm selects the most promising solution from the archive (based on the product of its two objective values) and applies a hybrid local search strategy: it first tries a random item swap (if feasible) and then probabilistically flips items with low marginal contribution, ensuring feasibility by removing excess items if needed. The approach balances exploration (via random swaps and flips) and exploitation (focusing on high-potential solutions) while strictly enforcing capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the solution with the highest potential (product of objectives)\n    potentials = [obj[0] * obj[1] for _, obj in archive]\n    selected_idx = np.argmax(potentials)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap two items (if feasible)\n    if n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        if new_solution[swap_indices[0]] != new_solution[swap_indices[1]]:\n            # Check feasibility\n            if new_solution[swap_indices[0]] == 1:\n                new_weight = current_weight - weight_lst[swap_indices[0]] + weight_lst[swap_indices[1]]\n            else:\n                new_weight = current_weight + weight_lst[swap_indices[0]] - weight_lst[swap_indices[1]]\n            if new_weight <= capacity:\n                new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    # Strategy 2: Probabilistic flip of items with low marginal contribution\n    for i in range(n_items):\n        if np.random.rand() < 0.2:  # 20% chance to flip\n            if new_solution[i] == 1:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:  # Ensure non-negative weight (though capacity check is more important)\n                    new_solution[i] = 0\n            else:\n                new_weight = current_weight + weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i] = 1\n\n    # Ensure feasibility (in case of multiple changes)\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        # Remove items until feasible\n        excess = total_weight - capacity\n        while excess > 0:\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) == 0:\n                break\n            remove_idx = np.random.choice(included_items)\n            excess -= weight_lst[remove_idx]\n            new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8969946250886272,
            2.1612739861011505
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted sum of normalized objectives (70% value1, 30% value2), then applies a hybrid local search combining probabilistic swaps, marginal contribution-based flips, and a greedy removal step to ensure feasibility. It dynamically balances exploration/exploitation based on archive size, with higher probabilities for marginal improvements and feasibility checks. The method prioritizes solutions with better combined objective scores while intelligently modifying them to improve both objectives while respecting capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Normalize objectives and compute combined scores\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    norm_obj1 = (obj1 - np.min(obj1)) / (np.max(obj1) - np.min(obj1) + 1e-8)\n    norm_obj2 = (obj2 - np.min(obj2)) / (np.max(obj2) - np.min(obj2) + 1e-8)\n    combined_scores = 0.7 * norm_obj1 + 0.3 * norm_obj2  # Weighted sum\n\n    # Select solution with highest combined score\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic exploration/exploitation trade-off\n    exploration_prob = 0.5 + 0.5 * (1 - len(archive) / 100)  # Higher exploration for small archives\n\n    # Strategy 1: Probabilistic item swap with feasibility check\n    if np.random.rand() < exploration_prob and n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        if new_solution[swap_indices[0]] != new_solution[swap_indices[1]]:\n            # Check feasibility\n            delta = (weight_lst[swap_indices[1]] - weight_lst[swap_indices[0]]) if new_solution[swap_indices[0]] == 1 else (weight_lst[swap_indices[0]] - weight_lst[swap_indices[1]])\n            if current_weight + delta <= capacity:\n                new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    # Strategy 2: Marginal contribution-based flips\n    for i in range(n_items):\n        if np.random.rand() < 0.3:  # Higher probability than original\n            if new_solution[i] == 1:\n                # Check if removing improves both objectives\n                marginal1 = -value1_lst[i]\n                marginal2 = -value2_lst[i]\n                if marginal1 < 0 or marginal2 < 0:  # Negative marginal\n                    new_weight = current_weight - weight_lst[i]\n                    if new_weight >= 0:\n                        new_solution[i] = 0\n            else:\n                # Check if adding improves both objectives\n                marginal1 = value1_lst[i]\n                marginal2 = value2_lst[i]\n                if marginal1 > 0 and marginal2 > 0:  # Positive marginal\n                    new_weight = current_weight + weight_lst[i]\n                    if new_weight <= capacity:\n                        new_solution[i] = 1\n\n    # Strategy 3: Greedy removal for feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        # Sort items by weight/value ratio and remove until feasible\n        included_items = np.where(new_solution == 1)[0]\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(ratios)]\n        excess = total_weight - capacity\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9450039798002199,
            2.6820522248744965
        ]
    },
    {
        "algorithm": "The algorithm implements a diversity-aware selection strategy that prioritizes solutions with high potential for improvement, followed by a hybrid local search combining random flips (60% chance) for items already in the solution and a value-aware swap (40% chance) that intelligently selects high-value items to add while removing low-value items to maintain feasibility. The value-aware swap specifically targets items with top 40% combined value-to-weight ratios, ensuring both objectives are balanced.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high potential for improvement (diversity-aware)\n    selected_idx = random.choices(range(len(archive)), weights=[1/(1 + i) for i in range(len(archive))])[0]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: random flip with value-aware swap\n    if random.random() < 0.6:  # 60% chance for random flip\n        # Randomly flip items that could improve either objective\n        candidates = np.where(base_solution == 1)[0]\n        if len(candidates) > 0:\n            flip_idx = random.choice(candidates)\n            new_solution[flip_idx] = 0\n    else:  # 40% chance for value-aware swap\n        # Find items not in solution with high value ratios\n        not_in_solution = np.where(base_solution == 0)[0]\n        if len(not_in_solution) > 0:\n            # Calculate value ratios (value1/weight and value2/weight)\n            ratios1 = value1_lst[not_in_solution] / weight_lst[not_in_solution]\n            ratios2 = value2_lst[not_in_solution] / weight_lst[not_in_solution]\n            combined_ratios = ratios1 + ratios2\n\n            # Select top 40% candidates by combined ratio\n            top_candidates = not_in_solution[np.argsort(combined_ratios)[-max(1, len(combined_ratios)//2):]]\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find items in solution with low value ratios to swap out\n                in_solution = np.where(base_solution == 1)[0]\n                if len(in_solution) > 0:\n                    ratios1_in = value1_lst[in_solution] / weight_lst[in_solution]\n                    ratios2_in = value2_lst[in_solution] / weight_lst[in_solution]\n                    combined_ratios_in = ratios1_in + ratios2_in\n                    swap_out = in_solution[np.argmin(combined_ratios_in)]\n\n                    # Perform swap if feasible\n                    if (np.sum(weight_lst[new_solution == 1]) - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                        new_solution[swap_out] = 0\n                        new_solution[swap_in] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.5493558789167777,
            1.034000277519226
        ]
    },
    {
        "algorithm": "This algorithm implements a crowding-distance-aware selection strategy that prioritizes solutions near the Pareto front boundaries, followed by a hybrid local search combining probabilistic swaps (70% chance) that favor high-margin items and a value-aware removal (30% chance) that intelligently eliminates low-impact items while maintaining feasibility. The selection process uses crowding distance to identify crowded regions of the Pareto front, while the local search alternates between swapping high-margin items and removing low-value items to explore the solution space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near Pareto front boundaries using crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) == 1:\n        selected_idx = 0\n    else:\n        # Calculate crowding distance for each objective\n        crowding_dist = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding_dist[sorted_idx[0]] = np.inf\n            crowding_dist[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                crowding_dist[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n\n        # Select solution with lowest crowding distance (most crowded)\n        selected_idx = np.argmin(crowding_dist)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: probabilistic swap with value-aware removal\n    if random.random() < 0.7:  # 70% chance for probabilistic swap\n        # Calculate value margins for both objectives\n        in_items = np.where(base_solution == 1)[0]\n        not_in_items = np.where(base_solution == 0)[0]\n\n        if len(in_items) > 0 and len(not_in_items) > 0:\n            # Calculate marginal gains for not-in items\n            marginal_gain1 = value1_lst[not_in_items] / weight_lst[not_in_items]\n            marginal_gain2 = value2_lst[not_in_items] / weight_lst[not_in_items]\n            combined_gains = marginal_gain1 + marginal_gain2\n\n            # Select item with highest combined gain\n            swap_in = not_in_items[np.argmax(combined_gains)]\n\n            # Calculate marginal losses for in items\n            marginal_loss1 = value1_lst[in_items] / weight_lst[in_items]\n            marginal_loss2 = value2_lst[in_items] / weight_lst[in_items]\n            combined_losses = marginal_loss1 + marginal_loss2\n\n            # Select item with lowest combined loss\n            swap_out = in_items[np.argmin(combined_losses)]\n\n            # Perform swap if feasible\n            if (np.sum(weight_lst[new_solution == 1]) - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n    else:  # 30% chance for value-aware removal\n        # Remove items with lowest combined value-to-weight ratio\n        in_items = np.where(base_solution == 1)[0]\n        if len(in_items) > 1:  # Need at least one item to remain\n            ratios1 = value1_lst[in_items] / weight_lst[in_items]\n            ratios2 = value2_lst[in_items] / weight_lst[in_items]\n            combined_ratios = ratios1 + ratios2\n            remove_idx = in_items[np.argmin(combined_ratios)]\n            new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9585816545487339,
            3.386291205883026
        ]
    },
    {
        "algorithm": "The algorithm combines a diversity-aware selection strategy with a hybrid local search that dynamically balances marginal contribution flips (70% probability) and value-aware swaps (30% probability), prioritizing solutions with high combined objective ratios while ensuring feasibility through greedy excess removal. It intelligently samples from the archive using a combined objective ratio to guide exploration, dynamically adjusting between exploration and exploitation based on archive size, and always maintains feasibility through strict capacity checks and greedy removal steps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate combined objective ratios for selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    combined_ratios = (obj1 + obj2) / (np.sum(weight_lst) + 1e-8)\n\n    # Select solution with highest combined ratio\n    selected_idx = np.argmax(combined_ratios)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Dynamic exploration/exploitation trade-off\n    exploration_prob = 0.5 + 0.5 * (1 - len(archive) / 100)\n\n    if random.random() < exploration_prob:\n        # Marginal contribution flips (70% probability)\n        for i in range(len(weight_lst)):\n            if random.random() < 0.7:\n                if new_solution[i] == 1:\n                    # Remove if negative marginal contribution\n                    marginal1 = -value1_lst[i]\n                    marginal2 = -value2_lst[i]\n                    if marginal1 < 0 or marginal2 < 0:\n                        new_weight = current_weight - weight_lst[i]\n                        if new_weight >= 0:\n                            new_solution[i] = 0\n                            current_weight = new_weight\n                else:\n                    # Add if positive marginal contribution\n                    marginal1 = value1_lst[i]\n                    marginal2 = value2_lst[i]\n                    if marginal1 > 0 and marginal2 > 0:\n                        new_weight = current_weight + weight_lst[i]\n                        if new_weight <= capacity:\n                            new_solution[i] = 1\n                            current_weight = new_weight\n    else:\n        # Value-aware swaps (30% probability)\n        not_in_solution = np.where(new_solution == 0)[0]\n        if len(not_in_solution) > 0:\n            # Calculate value ratios\n            ratios1 = value1_lst[not_in_solution] / weight_lst[not_in_solution]\n            ratios2 = value2_lst[not_in_solution] / weight_lst[not_in_solution]\n            combined_ratios = ratios1 + ratios2\n\n            # Select top 30% candidates\n            top_candidates = not_in_solution[np.argsort(combined_ratios)[-max(1, len(combined_ratios)//3):]]\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find worst item to remove\n                in_solution = np.where(new_solution == 1)[0]\n                if len(in_solution) > 0:\n                    ratios1_in = value1_lst[in_solution] / weight_lst[in_solution]\n                    ratios2_in = value2_lst[in_solution] / weight_lst[in_solution]\n                    combined_ratios_in = ratios1_in + ratios2_in\n                    swap_out = in_solution[np.argmin(combined_ratios_in)]\n\n                    # Perform swap if feasible\n                    if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                        new_solution[swap_out] = 0\n                        new_solution[swap_in] = 1\n                        current_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n\n    # Greedy excess removal if still over capacity\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(ratios)]\n        excess = total_weight - capacity\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.39200351044471826,
            0.8863400220870972
        ]
    },
    {
        "algorithm": "This algorithm employs a diversity-aware selection mechanism that prioritizes solutions with high variance in marginal value-to-weight ratios, followed by a hybrid local search that alternates between targeted inclusion of high-value items and strategic swaps of low-value items with high-weight alternatives, while dynamically balancing exploration and exploitation based on solution quality and ensuring feasibility through a greedy removal process prioritizing high weight-to-value ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection: prioritize solutions with high marginal contribution variance\n    diversity_scores = []\n    for sol, _ in archive:\n        included = np.where(sol == 1)[0]\n        if len(included) == 0:\n            diversity_scores.append(0)\n            continue\n\n        # Calculate marginal contributions for included items\n        marginal_contribs1 = value1_lst[included] / weight_lst[included]\n        marginal_contribs2 = value2_lst[included] / weight_lst[included]\n        combined_contribs = marginal_contribs1 + marginal_contribs2\n\n        # Diversity score is the variance of marginal contributions\n        diversity = np.var(combined_contribs)\n        diversity_scores.append(diversity)\n\n    # Select solution with highest diversity score\n    selected_idx = np.argmax(diversity_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate solution quality score (combined value-to-weight ratio)\n    included = np.where(base_solution == 1)[0]\n    if len(included) > 0:\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n        total_weight = np.sum(weight_lst[included])\n        quality_score = (total_value1 + total_value2) / (total_weight + 1e-8)\n    else:\n        quality_score = 0\n\n    # Dynamic exploration/exploitation trade-off (inverse of original)\n    exploitation_prob = 0.3 + 0.7 * (quality_score / (np.max([obj[0] + obj[1] for _, obj in archive]) + 1e-8))\n\n    # Hybrid local search with different strategy\n    if np.random.rand() < exploitation_prob:\n        # Targeted inclusion of high-value items with low weight\n        not_included = np.where(base_solution == 0)[0]\n        if len(not_included) > 0:\n            # Calculate value-to-weight ratio for not-included items\n            vw_ratios = (value1_lst[not_included] + value2_lst[not_included]) / (weight_lst[not_included] + 1e-8)\n\n            # Select top 20% items with highest value-to-weight ratio\n            top_candidates = not_included[np.argsort(vw_ratios)[-max(1, len(not_included)//5):]]\n\n            for item in top_candidates:\n                if np.random.rand() < 0.6:  # 60% chance to include each top candidate\n                    if (np.sum(weight_lst[new_solution == 1]) + weight_lst[item]) <= capacity:\n                        new_solution[item] = 1\n    else:\n        # Probabilistic swap of low-value items with high-weight items\n        included = np.where(base_solution == 1)[0]\n        not_included = np.where(base_solution == 0)[0]\n\n        if len(included) > 0 and len(not_included) > 0:\n            # Find low-value items to potentially remove\n            removal_ratios = (value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-8)\n            low_value_items = included[np.argsort(removal_ratios)[:max(1, len(included)//4)]]\n\n            # Find high-weight items to potentially add\n            high_weight_items = not_included[np.argsort(weight_lst[not_included])[-max(1, len(not_included)//4):]]\n\n            for remove_item in low_value_items:\n                for add_item in high_weight_items:\n                    if (np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_item] + weight_lst[add_item]) <= capacity:\n                        new_solution[remove_item] = 0\n                        new_solution[add_item] = 1\n                        break\n\n    # Feasibility enforcement (different priority calculation)\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Calculate removal priorities (highest weight-to-value ratio)\n        removal_priorities = (weight_lst[included_items] + 1e-8) / (value1_lst[included_items] + value2_lst[included_items] + 1e-8)\n        sorted_indices = included_items[np.argsort(removal_priorities)[::-1]]  # Descending order\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.999853639804997,
            9.46109089255333
        ]
    },
    {
        "algorithm": "The algorithm implements an adaptive local search strategy that dynamically selects a solution from the archive based on diversity-aware metrics, then applies objective-specific flip probabilities and marginal contribution analysis to generate neighbors while maintaining feasibility through a hybrid restoration mechanism. It prioritizes items with high combined value-weight ratios for both objectives, using correlation between objectives to adjust flip probabilities, and ensures solutions remain feasible by iteratively removing the least impactful items when capacity is exceeded. The selection of solutions is biased towards those with high diversity contributions to the archive, balancing exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate diversity-aware selection metric\n    obj1_values = [obj[0] for _, obj in archive]\n    obj2_values = [obj[1] for _, obj in archive]\n    obj1_std = np.std(obj1_values)\n    obj2_std = np.std(obj2_values)\n\n    # Select solution with highest diversity contribution\n    diversity_scores = []\n    for sol, obj in archive:\n        included = np.where(sol == 1)[0]\n        if len(included) == 0:\n            diversity_scores.append(0)\n            continue\n        # Normalized diversity contribution\n        obj1_contrib = (obj[0] - np.mean(obj1_values)) / (obj1_std + 1e-8)\n        obj2_contrib = (obj[1] - np.mean(obj2_values)) / (obj2_std + 1e-8)\n        diversity_scores.append(obj1_contrib * obj2_contrib)\n\n    selected_idx = np.argmax(diversity_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current solution metrics\n    included = np.where(base_solution == 1)[0]\n    current_weight = np.sum(weight_lst[included]) if len(included) > 0 else 0\n    current_value1 = np.sum(value1_lst[included]) if len(included) > 0 else 0\n    current_value2 = np.sum(value2_lst[included]) if len(included) > 0 else 0\n\n    # Adaptive flip probabilities based on objective correlation\n    obj_corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    flip_prob1 = 0.3 + 0.6 * (1 - obj_corr)  # Higher for independent objectives\n    flip_prob2 = 0.3 + 0.6 * obj_corr      # Higher for correlated objectives\n\n    # Weighted marginal contribution analysis\n    for i in range(len(weight_lst)):\n        if np.random.rand() < flip_prob1:\n            if base_solution[i] == 1:\n                # Remove low-impact items\n                if (value1_lst[i] / (weight_lst[i] + 1e-8)) < (current_value1 / (current_weight + 1e-8)):\n                    new_solution[i] = 0\n            else:\n                # Add high-impact items\n                if (current_weight + weight_lst[i]) <= capacity and \\\n                   (value1_lst[i] / (weight_lst[i] + 1e-8)) > (current_value1 / (current_weight + 1e-8)):\n                    new_solution[i] = 1\n\n        if np.random.rand() < flip_prob2:\n            if base_solution[i] == 1:\n                # Remove low-impact items for second objective\n                if (value2_lst[i] / (weight_lst[i] + 1e-8)) < (current_value2 / (current_weight + 1e-8)):\n                    new_solution[i] = 0\n            else:\n                # Add high-impact items for second objective\n                if (current_weight + weight_lst[i]) <= capacity and \\\n                   (value2_lst[i] / (weight_lst[i] + 1e-8)) > (current_value2 / (current_weight + 1e-8)):\n                    new_solution[i] = 1\n\n    # Hybrid feasibility restoration\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        if len(included_items) > 0:\n            # Combined value-weight ratio considering both objectives\n            ratios = (value1_lst[included_items] + value2_lst[included_items]) / \\\n                    (weight_lst[included_items] * (1 + np.abs(value1_lst[included_items] - value2_lst[included_items])))\n\n            sorted_indices = included_items[np.argsort(ratios)]\n            for i in sorted_indices:\n                if excess <= 0:\n                    break\n                excess -= weight_lst[i]\n                new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.966762674068027,
            8.966729640960693
        ]
    },
    {
        "algorithm": "The algorithm selects solutions from the archive using hypervolume contributions, then applies a three-stage local search: (1) probabilistic item flips based on objective-specific marginal gains, (2) correlation-aware swaps between objectives, and (3) adaptive removal of least beneficial items to enforce feasibility. It prioritizes items with high marginal gains and considers objective correlations to make informed swaps, while dynamically adjusting parameters based on solution quality and archive diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic hypervolume-based selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n\n    # Calculate hypervolume contributions\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    hypervolume = (max_obj1 - obj1) * (max_obj2 - obj2)\n    selected_idx = np.argmax(hypervolume)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate objective-specific marginal gains\n    marginal1 = value1_lst / weight_lst\n    marginal2 = value2_lst / weight_lst\n    avg_marginal1 = np.mean(marginal1[base_solution == 1]) if np.sum(base_solution) > 0 else 0\n    avg_marginal2 = np.mean(marginal2[base_solution == 1]) if np.sum(base_solution) > 0 else 0\n\n    # Stage 1: Probabilistic flips based on objective-specific criteria\n    for i in range(len(weight_lst)):\n        flip_prob = 0.4  # Higher exploration probability\n        if random.random() < flip_prob:\n            if new_solution[i] == 1:\n                # Remove if below both objectives' average marginal gain\n                if (marginal1[i] < avg_marginal1 * 0.8) and (marginal2[i] < avg_marginal2 * 0.8):\n                    new_weight = current_weight - weight_lst[i]\n                    if new_weight >= 0:\n                        new_solution[i] = 0\n            else:\n                # Add if above at least one objective's average marginal gain and fits\n                if ((marginal1[i] > avg_marginal1 * 1.2) or (marginal2[i] > avg_marginal2 * 1.2)) and (current_weight + weight_lst[i]) <= capacity:\n                    new_solution[i] = 1\n\n    # Stage 2: Correlation-aware swaps between objectives\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Calculate objective correlations\n        corr1 = np.corrcoef(value1_lst[in_items], value2_lst[in_items])[0, 1]\n        corr_threshold = 0.6  # Higher correlation threshold\n\n        if abs(corr1) > corr_threshold:\n            # Select top 15% of items to consider for swap\n            top_candidates = out_items[np.argsort(marginal1[out_items] + marginal2[out_items])[-max(1, len(out_items)//7):]]\n\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find worst marginal impact items to swap out\n                marginal_impact = marginal1[in_items] + marginal2[in_items]\n                swap_out = in_items[np.argmin(marginal_impact)]\n\n                if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                    new_solution[swap_out] = 0\n                    new_solution[swap_in] = 1\n\n    # Stage 3: Adaptive removal for feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Remove items with worst combined marginal gain until feasible\n        combined_marginal = marginal1[included_items] + marginal2[included_items]\n        sorted_indices = included_items[np.argsort(combined_marginal)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9143953005420267,
            2.704031527042389
        ]
    },
    {
        "algorithm": "This algorithm combines crowding-distance-aware selection with a hybrid local search strategy, prioritizing items with high combined value-to-weight ratios while adaptively mutating solutions to balance exploration and exploitation. It uses objective-weighted item removal and insertion to maintain feasibility, with higher weight given to value2 in removal priorities (0.6 vs. 0.4 for value1) and a 30% top-item insertion threshold. The approach intelligently selects solutions from the archive, mutates them probabilistically, and enforces feasibility through marginal contribution analysis.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Crowding-distance-aware solution selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    sorted_indices1 = np.argsort(obj1)\n    sorted_indices2 = np.argsort(obj2)\n\n    crowding_distances = np.zeros(len(archive))\n    for i in range(1, len(archive)-1):\n        crowding_distances[sorted_indices1[i]] += (obj1[sorted_indices1[i+1]] - obj1[sorted_indices1[i-1]]) / (obj1[-1] - obj1[0] + 1e-8)\n        crowding_distances[sorted_indices2[i]] += (obj2[sorted_indices2[i+1]] - obj2[sorted_indices2[i-1]]) / (obj2[-1] - obj2[0] + 1e-8)\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Adaptive mutation probabilities\n    mutation_prob = 0.5 + 0.5 * (1 - len(archive) / 100)  # Higher mutation for small archives\n\n    # Hybrid neighborhood exploration\n    if np.random.rand() < mutation_prob:\n        # Multi-objective bit-flip with probabilistic selection\n        flip_candidates = np.where(base_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            # Select items with lowest combined marginal contribution\n            marginal_contribs = (value1_lst[flip_candidates] + value2_lst[flip_candidates]) / weight_lst[flip_candidates]\n            flip_indices = flip_candidates[np.argsort(marginal_contribs)[:max(1, len(flip_candidates)//4)]]\n\n            for i in flip_indices:\n                if np.random.rand() < 0.4:  # Higher flip probability\n                    new_solution[i] = 0\n    else:\n        # Objective-weighted item insertion\n        not_in_solution = np.where(base_solution == 0)[0]\n        if len(not_in_solution) > 0:\n            # Select top 30% items by weighted value-to-weight ratio\n            combined_ratios = (0.6*value1_lst + 0.4*value2_lst) / weight_lst\n            top_candidates = not_in_solution[np.argsort(combined_ratios[not_in_solution])[-max(1, len(not_in_solution)//3):]]\n\n            if len(top_candidates) > 0:\n                insert_item = np.random.choice(top_candidates)\n                current_weight = np.sum(weight_lst[new_solution == 1])\n\n                if current_weight + weight_lst[insert_item] <= capacity:\n                    new_solution[insert_item] = 1\n                else:\n                    # If insertion violates capacity, remove worst item\n                    included_items = np.where(new_solution == 1)[0]\n                    if len(included_items) > 0:\n                        removal_priorities = weight_lst[included_items] / (0.7*value1_lst[included_items] + 0.3*value2_lst[included_items] + 1e-8)\n                        remove_item = included_items[np.argmax(removal_priorities)]\n\n                        if (current_weight - weight_lst[remove_item] + weight_lst[insert_item]) <= capacity:\n                            new_solution[remove_item] = 0\n                            new_solution[insert_item] = 1\n\n    # Feasibility enforcement with objective-weighted removal\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        removal_priorities = weight_lst[included_items] / (0.4*value1_lst[included_items] + 0.6*value2_lst[included_items] + 1e-8)\n        sorted_indices = included_items[np.argsort(removal_priorities)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.46623446803638885,
            1.4170138537883759
        ]
    },
    {
        "algorithm": "This algorithm selects a solution from the archive based on diversity in marginal value-to-weight ratios, then applies a hybrid local search that either exploits high-value items or strategically swaps low-value items with high-weight alternatives, dynamically balancing exploration and exploitation while ensuring feasibility through greedy removal of high weight-to-value items. The selection prioritizes solutions with diverse item contributions, and the local search alternates between adding top candidates and swapping items based on quality scores.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    diversity_scores = []\n    for sol, _ in archive:\n        included = np.where(sol == 1)[0]\n        if len(included) == 0:\n            diversity_scores.append(0)\n            continue\n        marginal_contribs1 = value1_lst[included] / weight_lst[included]\n        marginal_contribs2 = value2_lst[included] / weight_lst[included]\n        combined_contribs = marginal_contribs1 + marginal_contribs2\n        diversity_scores.append(np.var(combined_contribs))\n\n    selected_idx = np.argmax(diversity_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    included = np.where(base_solution == 1)[0]\n    if len(included) > 0:\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n        total_weight = np.sum(weight_lst[included])\n        quality_score = (total_value1 + total_value2) / (total_weight + 1e-8)\n    else:\n        quality_score = 0\n\n    exploitation_prob = 0.3 + 0.7 * (quality_score / (np.max(objectives[:, 0] + objectives[:, 1]) + 1e-8))\n\n    if np.random.rand() < exploitation_prob:\n        not_included = np.where(base_solution == 0)[0]\n        if len(not_included) > 0:\n            vw_ratios = (value1_lst[not_included] + value2_lst[not_included]) / (weight_lst[not_included] + 1e-8)\n            top_candidates = not_included[np.argsort(vw_ratios)[-max(1, len(not_included)//5):]]\n            for item in top_candidates:\n                if np.random.rand() < 0.6:\n                    if (np.sum(weight_lst[new_solution == 1]) + weight_lst[item]) <= capacity:\n                        new_solution[item] = 1\n    else:\n        included = np.where(base_solution == 1)[0]\n        not_included = np.where(base_solution == 0)[0]\n        if len(included) > 0 and len(not_included) > 0:\n            removal_ratios = (value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-8)\n            low_value_items = included[np.argsort(removal_ratios)[:max(1, len(included)//4)]]\n            high_weight_items = not_included[np.argsort(weight_lst[not_included])[-max(1, len(not_included)//4):]]\n            for remove_item in low_value_items:\n                for add_item in high_weight_items:\n                    if (np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_item] + weight_lst[add_item]) <= capacity:\n                        new_solution[remove_item] = 0\n                        new_solution[add_item] = 1\n                        break\n\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        removal_priorities = (weight_lst[included_items] + 1e-8) / (value1_lst[included_items] + value2_lst[included_items] + 1e-8)\n        sorted_indices = included_items[np.argsort(removal_priorities)[::-1]]\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9545062780826046,
            9.477187544107437
        ]
    }
]