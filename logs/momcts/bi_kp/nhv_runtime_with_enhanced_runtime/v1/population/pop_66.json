[
    {
        "algorithm": "This algorithm employs a diversity-aware selection mechanism that prioritizes solutions with high variance in marginal value-to-weight ratios, followed by a hybrid local search that alternates between targeted inclusion of high-value items and strategic swaps of low-value items with high-weight alternatives, while dynamically balancing exploration and exploitation based on solution quality and ensuring feasibility through a greedy removal process prioritizing high weight-to-value ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection: prioritize solutions with high marginal contribution variance\n    diversity_scores = []\n    for sol, _ in archive:\n        included = np.where(sol == 1)[0]\n        if len(included) == 0:\n            diversity_scores.append(0)\n            continue\n\n        # Calculate marginal contributions for included items\n        marginal_contribs1 = value1_lst[included] / weight_lst[included]\n        marginal_contribs2 = value2_lst[included] / weight_lst[included]\n        combined_contribs = marginal_contribs1 + marginal_contribs2\n\n        # Diversity score is the variance of marginal contributions\n        diversity = np.var(combined_contribs)\n        diversity_scores.append(diversity)\n\n    # Select solution with highest diversity score\n    selected_idx = np.argmax(diversity_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate solution quality score (combined value-to-weight ratio)\n    included = np.where(base_solution == 1)[0]\n    if len(included) > 0:\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n        total_weight = np.sum(weight_lst[included])\n        quality_score = (total_value1 + total_value2) / (total_weight + 1e-8)\n    else:\n        quality_score = 0\n\n    # Dynamic exploration/exploitation trade-off (inverse of original)\n    exploitation_prob = 0.3 + 0.7 * (quality_score / (np.max([obj[0] + obj[1] for _, obj in archive]) + 1e-8))\n\n    # Hybrid local search with different strategy\n    if np.random.rand() < exploitation_prob:\n        # Targeted inclusion of high-value items with low weight\n        not_included = np.where(base_solution == 0)[0]\n        if len(not_included) > 0:\n            # Calculate value-to-weight ratio for not-included items\n            vw_ratios = (value1_lst[not_included] + value2_lst[not_included]) / (weight_lst[not_included] + 1e-8)\n\n            # Select top 20% items with highest value-to-weight ratio\n            top_candidates = not_included[np.argsort(vw_ratios)[-max(1, len(not_included)//5):]]\n\n            for item in top_candidates:\n                if np.random.rand() < 0.6:  # 60% chance to include each top candidate\n                    if (np.sum(weight_lst[new_solution == 1]) + weight_lst[item]) <= capacity:\n                        new_solution[item] = 1\n    else:\n        # Probabilistic swap of low-value items with high-weight items\n        included = np.where(base_solution == 1)[0]\n        not_included = np.where(base_solution == 0)[0]\n\n        if len(included) > 0 and len(not_included) > 0:\n            # Find low-value items to potentially remove\n            removal_ratios = (value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-8)\n            low_value_items = included[np.argsort(removal_ratios)[:max(1, len(included)//4)]]\n\n            # Find high-weight items to potentially add\n            high_weight_items = not_included[np.argsort(weight_lst[not_included])[-max(1, len(not_included)//4):]]\n\n            for remove_item in low_value_items:\n                for add_item in high_weight_items:\n                    if (np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_item] + weight_lst[add_item]) <= capacity:\n                        new_solution[remove_item] = 0\n                        new_solution[add_item] = 1\n                        break\n\n    # Feasibility enforcement (different priority calculation)\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Calculate removal priorities (highest weight-to-value ratio)\n        removal_priorities = (weight_lst[included_items] + 1e-8) / (value1_lst[included_items] + value2_lst[included_items] + 1e-8)\n        sorted_indices = included_items[np.argsort(removal_priorities)[::-1]]  # Descending order\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.999853639804997,
            9.46109089255333
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto filtering with objective-space partitioning to select a diverse, high-potential solution from the archive, then applies a hybrid local search featuring adaptive flipping (based on negative objective gradients) and correlation-aware swapping (probabilistically guided by objective correlation) to generate neighbors while maintaining feasibility through probabilistic capacity adjustment. It prioritizes items with high value-to-weight ratios during removal when capacity is exceeded.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection with Pareto filtering\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive\n\n    # Objective-space partitioning\n    obj1 = np.array([obj[0] for _, obj in pareto_front])\n    obj2 = np.array([obj[1] for _, obj in pareto_front])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    min_obj1, min_obj2 = np.min(obj1), np.min(obj2)\n\n    # Select solution from under-represented region\n    density = np.zeros(len(pareto_front))\n    for i in range(len(pareto_front)):\n        dist1 = (obj1[i] - min_obj1) / (max_obj1 - min_obj1 + 1e-10)\n        dist2 = (obj2[i] - min_obj2) / (max_obj2 - min_obj2 + 1e-10)\n        density[i] = dist1 + dist2\n\n    selected_idx = np.argmin(density)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive flipping based on objective gradients\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0:\n        # Calculate objective gradients\n        obj1_grad = (value1_lst[in_items] - np.mean(value1_lst[in_items])) / (np.std(value1_lst[in_items]) + 1e-10)\n        obj2_grad = (value2_lst[in_items] - np.mean(value2_lst[in_items])) / (np.std(value2_lst[in_items]) + 1e-10)\n\n        # Flip items with high negative gradients\n        flip_mask = (obj1_grad < -0.5) | (obj2_grad < -0.5)\n        for i in in_items[flip_mask]:\n            if current_weight - weight_lst[i] >= 0:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Correlation-aware swapping\n    if len(out_items) > 0 and len(in_items) > 0:\n        # Calculate correlation between objectives\n        obj_corr = np.corrcoef(value1_lst[in_items], value2_lst[in_items])[0, 1]\n        swap_prob = 0.5 * (1 + obj_corr)\n\n        if random.random() < swap_prob:\n            # Select item to remove based on combined value\n            combined_value = value1_lst[in_items] + value2_lst[in_items]\n            remove_idx = in_items[np.argmin(combined_value)]\n\n            # Select item to add based on marginal gain\n            marginal_gain = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n            add_idx = out_items[np.argmax(marginal_gain)]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n\n    # Probabilistic capacity adjustment\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Calculate removal probabilities based on value-to-weight ratio\n        value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        removal_probs = value_ratio / np.sum(value_ratio)\n\n        while excess > 0 and len(included_items) > 0:\n            removal_idx = np.random.choice(included_items, p=removal_probs)\n            excess -= weight_lst[removal_idx]\n            new_solution[removal_idx] = 0\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) > 0:\n                value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n                removal_probs = value_ratio / np.sum(value_ratio)\n\n    return new_solution\n\n",
        "score": [
            -0.9689916212137121,
            2.895742356777191
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive based on its trade-off between objectives (prioritizing extreme ratios), then applies a hybrid local search that probabilistically removes items (with higher probability for solutions with better value ratios) and inserts items using a utility metric that balances both objectives while considering weight efficiency, all while ensuring feasibility through adaptive removal of heaviest items if necessary. The utility metric dynamically adjusts based on the selected solution's objective trade-off, favoring items that complement the current solution's weaknesses.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    norm_obj1 = obj1 / (max_obj1 + 1e-10)\n    norm_obj2 = obj2 / (max_obj2 + 1e-10)\n\n    tradeoff_scores = np.abs(norm_obj1 - norm_obj2)\n    selected_idx = np.argmax(tradeoff_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    obj_ratio = norm_obj1[selected_idx] / (norm_obj2[selected_idx] + 1e-10)\n    removal_prob = 0.3 + 0.5 * (1 - np.exp(-obj_ratio))\n\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        for i in in_items:\n            if random.random() < removal_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n\n    out_items = np.where(new_solution == 0)[0]\n    if len(out_items) > 0:\n        utility_metric = (value1_lst[out_items] * (1 - norm_obj1[selected_idx]) + value2_lst[out_items] * (1 - norm_obj2[selected_idx])) / (weight_lst[out_items] + 1e-10)\n        utility_metric += 0.2 * np.random.rand(len(out_items))\n        best_insert = out_items[np.argmax(utility_metric)]\n\n        if (current_weight + weight_lst[best_insert]) <= capacity:\n            new_solution[best_insert] = 1\n            current_weight += weight_lst[best_insert]\n\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        sorted_indices = included_items[np.argsort(-weight_lst[included_items])]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9549209161973649,
            0.4467619061470032
        ]
    },
    {
        "algorithm": "The algorithm prioritizes solutions with high combined objective values and crowding distance, applying a hybrid local search that adaptively flips items with probabilities based on objective balance and novelty scores, while ensuring feasibility through a multi-criteria removal strategy that balances value1 and value2 with different weightings. It dynamically adjusts flip probabilities and selection criteria, favoring value1 in novelty calculations and value2 in quality evaluations, and enforces feasibility by removing items with the lowest combined novelty and quality metrics.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novelty-driven selection with different weightings\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n\n    # Calculate novelty score with different balancing\n    novelty_score = (obj1 / (max_obj1 + 1e-10))**0.8 * (obj2 / (max_obj2 + 1e-10))**0.8 * (1 - np.abs(obj1 - obj2) / (max_obj1 + max_obj2 + 1e-10))\n\n    # Calculate crowding distance with different metric\n    sorted_indices = np.argsort(obj1)\n    crowding = np.zeros(len(archive))\n    crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n    for i in range(1, len(archive)-1):\n        crowding[sorted_indices[i]] = abs(obj1[sorted_indices[i+1]] - obj1[sorted_indices[i-1]]) / (max_obj1 + 1e-10)\n\n    # Combine novelty and crowding with different weights\n    selection_score = 0.7 * novelty_score + 0.3 * crowding\n    selected_idx = np.argmax(selection_score)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive local search with different parameters\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Objective balance factor with different calculation\n        obj_balance = (obj1[selected_idx] + 0.4 * obj2[selected_idx]) / (max_obj1 + max_obj2 + 1e-10)\n\n        # Dynamic flip probabilities with different formula\n        flip_prob = 0.3 + 0.4 * obj_balance + 0.3 * (1 - novelty_score[selected_idx])\n\n        # Novelty-aware flip\n        for i in in_items:\n            if random.random() < flip_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n\n        # Targeted swap with different selection criteria\n        if len(out_items) > 0:\n            # Calculate novelty-aware marginal gains with different weights\n            novelty_gain = (value1_lst[out_items] * 0.8 + value2_lst[out_items] * 0.2) / (weight_lst[out_items] + 1e-10)\n            quality_gain = (value1_lst[out_items] * 0.2 + value2_lst[out_items] * 0.8) / (weight_lst[out_items] + 1e-10)\n\n            # Combined metric for swap selection with different weights\n            combined_gain = 0.6 * novelty_gain + 0.4 * quality_gain\n            swap_in = out_items[np.argmax(combined_gain)]\n\n            # Calculate marginal gains for removal with different metric\n            if len(in_items) > 0:\n                marginal_gain = (value1_lst[in_items] * 0.3 + value2_lst[in_items] * 0.7) / weight_lst[in_items]\n                swap_out = in_items[np.argmin(marginal_gain)]\n\n                if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                    new_solution[swap_out] = 0\n                    new_solution[swap_in] = 1\n\n    # Multi-criteria feasibility enforcement with different weights\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest combined novelty and quality with different weights\n        combined_metric = (value1_lst[included_items] * 0.7 + value2_lst[included_items] * 0.3) / (weight_lst[included_items] + 1e-10)\n        novelty_metric = (value1_lst[included_items] * 0.3 + value2_lst[included_items] * 0.7) / (weight_lst[included_items] + 1e-10)\n        removal_metric = 0.7 * combined_metric + 0.3 * novelty_metric\n\n        sorted_indices = included_items[np.argsort(removal_metric)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8755752154159082,
            0.642067164182663
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive using a dominance-based scoring system that prioritizes solutions with balanced objective values, then applies a hybrid local search that probabilistically removes items based on objective dominance and strategically inserts items using a utility metric that considers both objectives and weight efficiency, while ensuring feasibility through adaptive weight-based removal. The selection process prioritizes solutions with higher combined normalized objective values, while the local search dynamically adjusts removal probabilities and insertion criteria based on the solution's objective ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    norm_obj1 = obj1 / (max_obj1 + 1e-10)\n    norm_obj2 = obj2 / (max_obj2 + 1e-10)\n\n    dominance_scores = (norm_obj1 + norm_obj2) / (1 + np.abs(norm_obj1 - norm_obj2))\n    selected_idx = np.argmax(dominance_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    obj_ratio = norm_obj1[selected_idx] / (norm_obj2[selected_idx] + 1e-10)\n    removal_prob = 0.2 + 0.6 * (1 - np.exp(-obj_ratio))\n\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        for i in in_items:\n            if random.random() < removal_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n\n    out_items = np.where(new_solution == 0)[0]\n    if len(out_items) > 0:\n        utility_metric = (value1_lst[out_items] * (1 - norm_obj1[selected_idx]) + value2_lst[out_items] * (1 - norm_obj2[selected_idx])) / (weight_lst[out_items] + 1e-10)\n        utility_metric += 0.1 * np.random.rand(len(out_items))\n        best_insert = out_items[np.argmax(utility_metric)]\n\n        if (current_weight + weight_lst[best_insert]) <= capacity:\n            new_solution[best_insert] = 1\n            current_weight += weight_lst[best_insert]\n\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        removal_value = (value1_lst[included_items] * norm_obj2[selected_idx] + value2_lst[included_items] * norm_obj1[selected_idx]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(removal_value)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9482264101160008,
            1.0599313974380493
        ]
    },
    {
        "algorithm": "The algorithm combines hypervolume-guided selection with correlation-aware local search, prioritizing solutions with high hypervolume contribution and strong objective correlations. It uses adaptive marginal thresholds to flip low-value items and correlation-aware probabilistic swaps to improve solution quality, while ensuring feasibility through greedy excess removal of least valuable items. The selection score balances hypervolume and correlation factors, and the local search dynamically adjusts based on objective relationships and item marginal gains.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-guided selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    hypervolume = (max_obj1 - obj1) * (max_obj2 - obj2)\n\n    # Objective correlation analysis\n    obj_corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    corr_factor = 1 + abs(obj_corr)\n\n    # Combined selection score with diversity consideration\n    selection_score = hypervolume * corr_factor\n    selected_idx = np.argmax(selection_score)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive marginal thresholding\n    in_items = np.where(new_solution == 1)[0]\n    out_items = np.where(new_solution == 0)[0]\n\n    if len(in_items) > 0:\n        # Calculate marginal gains for included items\n        marginal_gains = (value1_lst[in_items] + value2_lst[in_items]) / weight_lst[in_items]\n\n        # Dynamic threshold based on correlation\n        threshold_factor = 0.5 * (1 + obj_corr)\n        threshold = np.percentile(marginal_gains, 100 * (1 - threshold_factor))\n\n        # Flip items below threshold\n        flip_candidates = in_items[marginal_gains < threshold]\n        for i in flip_candidates:\n            if current_weight - weight_lst[i] >= 0:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Correlation-aware probabilistic swaps\n    if len(out_items) > 0 and len(in_items) > 0:\n        # Calculate swap probabilities based on correlation\n        swap_prob = 0.5 * (1 + obj_corr)\n\n        if random.random() < swap_prob:\n            # Select candidate items to swap\n            candidate_out = out_items[np.random.choice(len(out_items), size=min(3, len(out_items)), replace=False)]\n            candidate_in = in_items[np.random.choice(len(in_items), size=min(3, len(in_items)), replace=False)]\n\n            # Evaluate potential swaps\n            best_swap = None\n            best_gain = -float('inf')\n\n            for out_idx in candidate_out:\n                for in_idx in candidate_in:\n                    new_weight = current_weight - weight_lst[in_idx] + weight_lst[out_idx]\n                    if new_weight <= capacity:\n                        gain = (value1_lst[out_idx] + value2_lst[out_idx] - value1_lst[in_idx] - value2_lst[in_idx]) / (weight_lst[out_idx] - weight_lst[in_idx] + 1e-10)\n                        if gain > best_gain:\n                            best_gain = gain\n                            best_swap = (in_idx, out_idx)\n\n            if best_swap is not None:\n                new_solution[best_swap[0]] = 0\n                new_solution[best_swap[1]] = 1\n\n    # Feasibility enforcement\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest marginal gains\n        marginal_gains = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(marginal_gains)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9502506734553036,
            4.966159850358963
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive using a novelty score that prioritizes solutions with diverse objective trade-offs, then applies a hybrid local search that adaptively removes items with high probability and inserts new items based on a multi-criteria utility metric, while ensuring feasibility through dynamic weight adjustment. It balances exploration and exploitation by using adaptive probabilities and randomized insertion, with a final step to fix infeasible solutions by removing least valuable items. The utility metric combines both objectives and weight efficiency, with added randomness to avoid local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n    norm_obj1 = obj1 / (max_obj1 + 1e-10)\n    norm_obj2 = obj2 / (max_obj2 + 1e-10)\n\n    novelty_scores = (1 - norm_obj1) * norm_obj2 + norm_obj1 * (1 - norm_obj2)\n    selected_idx = np.argmax(novelty_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    obj_ratio = norm_obj1[selected_idx] / (norm_obj2[selected_idx] + 1e-10)\n    removal_prob = 0.3 * (1 - np.tanh(obj_ratio - 1))\n\n    in_items = np.where(new_solution == 1)[0]\n    if len(in_items) > 0:\n        for i in in_items:\n            if random.random() < removal_prob:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n\n    out_items = np.where(new_solution == 0)[0]\n    if len(out_items) > 0:\n        utility_metric = (value1_lst[out_items] * (1 - norm_obj1[selected_idx]) + value2_lst[out_items] * (1 - norm_obj2[selected_idx])) / (weight_lst[out_items] + 1e-10)\n        utility_metric += 0.2 * np.random.rand(len(out_items))\n        candidate_insert = out_items[np.argsort(utility_metric)[-3:]]\n        np.random.shuffle(candidate_insert)\n\n        for i in candidate_insert:\n            if (current_weight + weight_lst[i]) <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n\n        removal_value = (value1_lst[included_items] * (1 - norm_obj1[selected_idx]) + value2_lst[included_items] * (1 - norm_obj2[selected_idx])) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(removal_value)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7561370125518683,
            0.54945969581604
        ]
    },
    {
        "algorithm": "The algorithm selects a solution from the archive using hypervolume-guided diversity scoring, then applies a hybrid local search that alternates between correlation-aware swaps (prioritizing items with high value-weight ratios) and marginal-gain flips (removing low-value items), with adaptive probabilities based on objective correlations. It ensures feasibility by greedily removing items with the lowest marginal value-to-weight ratios when the capacity is exceeded.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-guided selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    max_obj1, max_obj2 = np.max(obj1), np.max(obj2)\n\n    # Calculate hypervolume contribution\n    sorted_indices = np.lexsort((obj2, obj1))\n    hv_contribs = np.zeros(len(archive))\n    for i in range(len(archive)):\n        if i == 0:\n            hv_contribs[sorted_indices[i]] = (max_obj1 - obj1[sorted_indices[i]]) * (max_obj2 - obj2[sorted_indices[i]])\n        else:\n            hv_contribs[sorted_indices[i]] = (max_obj1 - obj1[sorted_indices[i]]) * (max_obj2 - obj2[sorted_indices[i]]) - \\\n                                             (max_obj1 - obj1[sorted_indices[i-1]]) * (max_obj2 - obj2[sorted_indices[i-1]])\n\n    selected_idx = np.argmax(hv_contribs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate objective correlations\n    in_items = np.where(base_solution == 1)[0]\n    if len(in_items) > 1:\n        corr = np.corrcoef(value1_lst[in_items], value2_lst[in_items])[0, 1]\n    else:\n        corr = 0\n\n    # Hybrid local search\n    swap_prob = 0.7 if corr > 0.5 else 0.3\n    if random.random() < swap_prob and len(in_items) > 0 and len(np.where(base_solution == 0)[0]) > 0:\n        # Correlation-aware swap\n        utility = (value1_lst * (1 - corr) + value2_lst * (1 - corr)) / (weight_lst + 1e-10)\n        swap_in = np.argmax(utility)\n        swap_out = in_items[np.argmin((value1_lst[in_items] / weight_lst[in_items]) + (value2_lst[in_items] / weight_lst[in_items]))]\n\n        if (np.sum(weight_lst[new_solution == 1]) - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n            new_solution[swap_out] = 0\n            new_solution[swap_in] = 1\n    else:\n        # Marginal-gain flip\n        marginal_gain = (value1_lst + value2_lst) / (weight_lst + 1e-10)\n        flip_candidates = np.where(base_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_item = flip_candidates[np.argmin(marginal_gain[flip_candidates])]\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[flip_item] >= 0:\n                new_solution[flip_item] = 0\n\n    # Feasibility enforcement\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        included_items = np.where(new_solution == 1)[0]\n        removal_value = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(removal_value)]\n\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8490731465916118,
            0.6228833496570587
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted sum of normalized objectives (70% value1, 30% value2), then applies a hybrid local search combining probabilistic swaps, marginal contribution-based flips, and a greedy removal step to ensure feasibility. It dynamically balances exploration/exploitation based on archive size, with higher probabilities for marginal improvements and feasibility checks. The method prioritizes solutions with better combined objective scores while intelligently modifying them to improve both objectives while respecting capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Normalize objectives and compute combined scores\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    norm_obj1 = (obj1 - np.min(obj1)) / (np.max(obj1) - np.min(obj1) + 1e-8)\n    norm_obj2 = (obj2 - np.min(obj2)) / (np.max(obj2) - np.min(obj2) + 1e-8)\n    combined_scores = 0.7 * norm_obj1 + 0.3 * norm_obj2  # Weighted sum\n\n    # Select solution with highest combined score\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic exploration/exploitation trade-off\n    exploration_prob = 0.5 + 0.5 * (1 - len(archive) / 100)  # Higher exploration for small archives\n\n    # Strategy 1: Probabilistic item swap with feasibility check\n    if np.random.rand() < exploration_prob and n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        if new_solution[swap_indices[0]] != new_solution[swap_indices[1]]:\n            # Check feasibility\n            delta = (weight_lst[swap_indices[1]] - weight_lst[swap_indices[0]]) if new_solution[swap_indices[0]] == 1 else (weight_lst[swap_indices[0]] - weight_lst[swap_indices[1]])\n            if current_weight + delta <= capacity:\n                new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    # Strategy 2: Marginal contribution-based flips\n    for i in range(n_items):\n        if np.random.rand() < 0.3:  # Higher probability than original\n            if new_solution[i] == 1:\n                # Check if removing improves both objectives\n                marginal1 = -value1_lst[i]\n                marginal2 = -value2_lst[i]\n                if marginal1 < 0 or marginal2 < 0:  # Negative marginal\n                    new_weight = current_weight - weight_lst[i]\n                    if new_weight >= 0:\n                        new_solution[i] = 0\n            else:\n                # Check if adding improves both objectives\n                marginal1 = value1_lst[i]\n                marginal2 = value2_lst[i]\n                if marginal1 > 0 and marginal2 > 0:  # Positive marginal\n                    new_weight = current_weight + weight_lst[i]\n                    if new_weight <= capacity:\n                        new_solution[i] = 1\n\n    # Strategy 3: Greedy removal for feasibility\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        # Sort items by weight/value ratio and remove until feasible\n        included_items = np.where(new_solution == 1)[0]\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(ratios)]\n        excess = total_weight - capacity\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9450039798002199,
            2.6820522248744965
        ]
    },
    {
        "algorithm": "The algorithm combines a diversity-aware selection strategy with a hybrid local search that dynamically balances marginal contribution flips (70% probability) and value-aware swaps (30% probability), prioritizing solutions with high combined objective ratios while ensuring feasibility through greedy excess removal. It intelligently samples from the archive using a combined objective ratio to guide exploration, dynamically adjusting between exploration and exploitation based on archive size, and always maintains feasibility through strict capacity checks and greedy removal steps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate combined objective ratios for selection\n    obj1 = np.array([obj[0] for _, obj in archive])\n    obj2 = np.array([obj[1] for _, obj in archive])\n    combined_ratios = (obj1 + obj2) / (np.sum(weight_lst) + 1e-8)\n\n    # Select solution with highest combined ratio\n    selected_idx = np.argmax(combined_ratios)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Dynamic exploration/exploitation trade-off\n    exploration_prob = 0.5 + 0.5 * (1 - len(archive) / 100)\n\n    if random.random() < exploration_prob:\n        # Marginal contribution flips (70% probability)\n        for i in range(len(weight_lst)):\n            if random.random() < 0.7:\n                if new_solution[i] == 1:\n                    # Remove if negative marginal contribution\n                    marginal1 = -value1_lst[i]\n                    marginal2 = -value2_lst[i]\n                    if marginal1 < 0 or marginal2 < 0:\n                        new_weight = current_weight - weight_lst[i]\n                        if new_weight >= 0:\n                            new_solution[i] = 0\n                            current_weight = new_weight\n                else:\n                    # Add if positive marginal contribution\n                    marginal1 = value1_lst[i]\n                    marginal2 = value2_lst[i]\n                    if marginal1 > 0 and marginal2 > 0:\n                        new_weight = current_weight + weight_lst[i]\n                        if new_weight <= capacity:\n                            new_solution[i] = 1\n                            current_weight = new_weight\n    else:\n        # Value-aware swaps (30% probability)\n        not_in_solution = np.where(new_solution == 0)[0]\n        if len(not_in_solution) > 0:\n            # Calculate value ratios\n            ratios1 = value1_lst[not_in_solution] / weight_lst[not_in_solution]\n            ratios2 = value2_lst[not_in_solution] / weight_lst[not_in_solution]\n            combined_ratios = ratios1 + ratios2\n\n            # Select top 30% candidates\n            top_candidates = not_in_solution[np.argsort(combined_ratios)[-max(1, len(combined_ratios)//3):]]\n            if len(top_candidates) > 0:\n                swap_in = random.choice(top_candidates)\n\n                # Find worst item to remove\n                in_solution = np.where(new_solution == 1)[0]\n                if len(in_solution) > 0:\n                    ratios1_in = value1_lst[in_solution] / weight_lst[in_solution]\n                    ratios2_in = value2_lst[in_solution] / weight_lst[in_solution]\n                    combined_ratios_in = ratios1_in + ratios2_in\n                    swap_out = in_solution[np.argmin(combined_ratios_in)]\n\n                    # Perform swap if feasible\n                    if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n                        new_solution[swap_out] = 0\n                        new_solution[swap_in] = 1\n                        current_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n\n    # Greedy excess removal if still over capacity\n    total_weight = np.sum(weight_lst[new_solution == 1])\n    if total_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        sorted_indices = included_items[np.argsort(ratios)]\n        excess = total_weight - capacity\n        for i in sorted_indices:\n            if excess <= 0:\n                break\n            excess -= weight_lst[i]\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.39200351044471826,
            0.8863400220870972
        ]
    }
]