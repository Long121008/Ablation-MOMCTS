[
    {
        "algorithm": "The algorithm selects the most promising solution from the archive (prioritizing high combined normalized objective value), applies a random swap mutation, and then performs a weighted greedy local search to add/remove items, favoring objective 1 (60%) over objective 2 (40%) while ensuring feasibility. The weighted approach balances improvements across objectives, and the random shuffling ensures exploration. The structure combines mutation and greedy search to efficiently explore the neighborhood of the selected solution.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_score = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        score = (v1 + v2) / (np.sum(weight_lst * sol) + 1e-6)  # Normalized by weight\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Randomly swap two items (swap mutation)\n    if n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Greedy addition based on weighted objective improvement\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            # Calculate weighted objective improvement\n            weight_improvement = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Greedy removal based on weighted objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        # Calculate weighted objective loss\n        weight_loss = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n        new_solution[item] = 0\n        current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.9780105623667606,
            0.5062039494514465
        ]
    },
    {
        "algorithm": "The algorithm selects a younger solution from the archive, prioritizes critical high-value items for flipping with age-based probability, and performs guided probabilistic swaps to improve objective values while maintaining feasibility through incremental weight checks and a repair mechanism that removes low-value items first. It dynamically adjusts perturbation intensity based on solution age and combines value-to-weight ratios for intelligent item selection.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Age-based selection: prioritize younger solutions (lower index)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)  # Select from younger 25% or last if fewer than 4\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 20):]  # Top 5% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with probability inversely proportional to age\n    flip_prob = 1.0 / (selected_idx + 1)  # Younger solutions have higher flip probability\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.3 / (selected_idx + 1)  # Younger solutions have higher swap probability\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n",
        "score": [
            -0.8087162914292674,
            0.16034522652626038
        ]
    },
    {
        "algorithm": "The algorithm prioritizes solutions with high variance in objective values, applies a random bit-flip mutation, and performs a dynamic weighted greedy search that alternates between objectives based on their current dominance, ensuring feasibility through adaptive weight adjustments. The selection of solutions is biased toward those with significant differences between the two objectives, while the local search dynamically adjusts weights to balance improvements in either objective, with occasional removals to maintain diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_variance = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        variance = abs(v1 - v2)\n        if variance > max_variance:\n            max_variance = variance\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Random bit-flip mutation\n    flip_index = np.random.randint(n_items)\n    new_solution[flip_index] = 1 - new_solution[flip_index]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic weighted greedy search\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    # Determine objective weights dynamically\n    total_v1 = np.sum(value1_lst * new_solution)\n    total_v2 = np.sum(value2_lst * new_solution)\n    w1 = 0.7 if total_v1 < total_v2 else 0.3\n    w2 = 1 - w1\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            improvement = w1 * value1_lst[item] + w2 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Dynamic weighted greedy removal\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        loss = w1 * value1_lst[item] + w2 * value2_lst[item]\n        if np.random.rand() < 0.3:  # 30% chance to remove to maintain diversity\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8841680503524515,
            0.35383936762809753
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a combined score of value dominance and weight utilization, then generates a neighbor by adaptively swapping high-value items with biased probabilities, probabilistically removing low-value items, and finally repairing feasibility by removing items with the smallest normalized marginal value-to-weight ratio. It prioritizes high-value items through value biases and ensures feasibility by guided repair, making it effective for bi-objective knapsack problems.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined value dominance and weight utilization\n    objectives = np.array([obj for _, obj in archive])\n    value_dominance = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    combined_scores = value_dominance[0] * value_dominance[1] * weight_utilization\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate adaptive value biases\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    value_biases = (value1_lst / (total_value1 + 1e-6) + value2_lst / (total_value2 + 1e-6)) / 2\n    value_biases = np.power(value_biases, 1.5)  # Stronger bias towards high-value items\n\n    # Adaptive value-biased swaps\n    swap_prob = 0.4 * (1 - selected_idx / len(archive))  # Higher probability for earlier solutions\n    for i in range(n_items):\n        if np.random.rand() < swap_prob * value_biases[i]:\n            if new_solution[i] == 0:\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n            else:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Probabilistic removal of low-value items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_ranks = np.argsort(value_biases[included_items])\n        for i in range(min(2, len(included_items))):  # Remove up to 2 lowest-value items\n            remove_idx = included_items[value_ranks[i]]\n            if np.random.rand() < 0.6:  # 60% chance to remove\n                new_solution[remove_idx] = 0\n                current_weight -= weight_lst[remove_idx]\n\n    # Guided repair mechanism\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with smallest normalized marginal value-to-weight ratio\n            marginal_ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n            normalized_ratios = marginal_ratios / np.max(marginal_ratios + 1e-6)\n            remove_idx = excess_items[np.argmin(normalized_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -1.0101433422973065,
            0.6888655126094818
        ]
    },
    {
        "algorithm": "The algorithm adaptively selects a solution from the archive by prioritizing younger, more diverse solutions (based on objective trade-offs), then applies a hybrid local search that dynamically flips critical items (top 10% by combined value-to-weight ratio) with age-based probabilities and performs guided probabilistic swaps (removing low-value items and adding high-value ones) while ensuring feasibility through incremental weight checks and targeted removals. The method balances exploration (via probabilistic flips) and exploitation (via critical item focus) while maintaining solution quality across both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize younger solutions with high objective diversity\n    selected_idx = min(int(len(archive) * 0.3), len(archive) - 1)  # Select from younger 30%\n    max_diversity = -1\n    for i in range(selected_idx, len(archive)):\n        sol, (v1, v2) = archive[i]\n        diversity = abs(v1 - v2) / (np.sum(weight_lst * sol) + 1e-6)\n        if diversity > max_diversity:\n            max_diversity = diversity\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 10):]  # Top 10% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with age-based probability\n    flip_prob = 0.5 / (selected_idx + 1)\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.4 / (selected_idx + 1)\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n                current_weight = potential_weight\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n",
        "score": [
            -0.8529030041131614,
            0.2645938992500305
        ]
    },
    {
        "algorithm": "The algorithm combines adaptive solution selection (prioritizing diverse, high-improvement-potential solutions) with a hybrid local search that alternates between critical-item flips (top 10% value-to-weight ratio items), guided swaps (balanced marginal contributions), and dynamic intensity adjustments (increased randomness when stuck). It ensures feasibility through incremental capacity checks and value-to-weight ratio-based repairs, dynamically adjusting perturbation intensity to balance exploration and exploitation. The selection mechanism emphasizes diversity and improvement potential, while the local search prioritizes high-value items while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on age-weighted diversity\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Identify critical items (top 10% combined ratio)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 10):]\n\n    # Hybrid perturbation: critical flips, guided swaps, and dynamic intensity\n    for _ in range(3):  # Three perturbation rounds\n        # Critical item flip with probability based on diversity\n        if len(critical_items) > 0 and np.random.rand() < 0.7:\n            item = np.random.choice(critical_items)\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n        # Guided swap: items with balanced marginal contributions\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Find item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Find item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Dynamic intensity adjustment: increase randomness if stuck\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.4:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.8970407631406285,
            0.3557857871055603
        ]
    },
    {
        "algorithm": "This heuristic combines adaptive solution selection with a multi-phase perturbation strategy that prioritizes critical items (top 10% value-to-weight ratio), objective-correlated swaps (guided by value correlation), and segment-based diversification, while strictly maintaining feasibility through weight checks and excess-item removal. The algorithm dynamically balances exploitation (focused flips and swaps) with exploration (random segment flips) and adjusts perturbation intensity based on objective correlation, ensuring both local optimization and global search capabilities.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or recent improvement\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives[i+1, 0] - objectives[i-1, 0]) + (objectives[i+1, 1] - objectives[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 10% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 10):]\n    for item in critical_items:\n        if np.random.rand() < 0.4:  # 40% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.5:  # Strong correlation\n        if np.random.rand() < 0.6:  # 60% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 40% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.median(value1_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.median(value2_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.3:  # 30% chance for segment flip\n        segment_length = np.random.randint(2, min(7, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.9196129097821386,
            0.3892856538295746
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using adaptive weighted scoring based on diversity, age, and weight utilization, then applies a hybrid local search with critical-item flips (top 5% value-to-weight ratio), objective-correlated swaps, and segment-based diversification, while ensuring feasibility through capacity checks and dynamic adjustments. It prioritizes high-value-to-weight items and balances optimization between objectives based on their correlation, with randomized segment flips and dynamic intensity adjustments for exploration. The solution is always repaired to ensure feasibility if capacity constraints are violated.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive weighted selection\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    age_weights = np.linspace(1.0, 0.5, len(archive))  # Younger solutions have higher weight\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    selection_scores = diversity[0] * diversity[1] * weight_utilization * age_weights\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.6:  # Strong correlation\n        if np.random.rand() < 0.7:  # 70% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_ratio[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_ratio[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 30% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_ratio[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_ratio[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_ratio > np.median(value1_ratio))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_ratio > np.median(value2_ratio))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Dynamic intensity adjustment\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.5:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.9679278702220337,
            0.6522309482097626
        ]
    },
    {
        "algorithm": "The algorithm selects promising solutions from the archive using an age-weighted diversity metric (prioritizing younger, more diverse solutions) and applies a hybrid local search combining critical item flips and guided random swaps, with dynamic perturbation intensity based on search progress. It ensures feasibility through incremental checks and lightweight repair, focusing on high-value items while maintaining balance between exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high diversity and younger age\n    current_time = len(archive)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        diversity = abs(obj[0] - obj[1])\n        age = current_time - i\n        score = (diversity + 1) / (age + 1)\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Dynamic perturbation intensity\n    perturbation_intensity = min(0.3 + (len(archive) / 1000), 1.0)\n\n    # Critical items (top 5% by combined value-to-weight)\n    combined_ratio = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(-combined_ratio)[:max(1, int(len(weight_lst) * 0.05))]\n\n    # Hybrid perturbation\n    for _ in range(2):\n        # Critical item flips\n        if np.random.rand() < perturbation_intensity:\n            for item in critical_items:\n                if np.random.rand() < 0.5:\n                    temp_solution = new_solution.copy()\n                    temp_solution[item] = 1 - temp_solution[item]\n                    if np.sum(temp_solution * weight_lst) <= capacity:\n                        new_solution = temp_solution\n\n        # Guided random swaps (higher-value items)\n        if np.random.rand() < 0.5 * perturbation_intensity:\n            high_value_items = np.argsort(-(value1_lst + value2_lst))[:max(2, int(len(weight_lst) * 0.2))]\n            i, j = np.random.choice(high_value_items, size=2, replace=False)\n            temp_solution = new_solution.copy()\n            temp_solution[i], temp_solution[j] = temp_solution[j], temp_solution[i]\n            if np.sum(temp_solution * weight_lst) <= capacity:\n                new_solution = temp_solution\n\n    # Lightweight repair\n    if np.sum(new_solution * weight_lst) > capacity:\n        excess = np.sum(new_solution * weight_lst) - capacity\n        removable_items = np.where(new_solution == 1)[0]\n        np.random.shuffle(removable_items)\n        for item in removable_items:\n            if excess <= 0:\n                break\n            temp_solution = new_solution.copy()\n            temp_solution[item] = 0\n            excess -= weight_lst[item]\n            if excess >= 0:\n                new_solution = temp_solution\n\n    return new_solution\n\n",
        "score": [
            -0.6901818589045985,
            0.24805396795272827
        ]
    },
    {
        "algorithm": "The algorithm adaptively selects a solution from the archive by prioritizing diverse, high-improvement-potential candidates, then applies a hybrid local search combining critical-item flips (top 5% value-to-weight ratio items), guided swaps (balancing marginal contributions), and dynamic intensity adjustments (increased randomness when improvement plateaus). It ensures feasibility through incremental capacity checks and value-to-weight ratio-based repairs, dynamically adjusting perturbation intensity to balance exploration and exploitation while prioritizing high-value items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on diversity and improvement potential\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Identify critical items (top 5% combined ratio)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n\n    # Hybrid perturbation: critical flips, guided swaps, and dynamic intensity\n    for _ in range(3):  # Three perturbation rounds\n        # Critical item flip with probability based on diversity\n        if len(critical_items) > 0 and np.random.rand() < 0.6:\n            item = np.random.choice(critical_items)\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n        # Guided swap: items with balanced marginal contributions\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Find item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Find item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Dynamic intensity adjustment: increase randomness if improvement potential is low\n    if np.random.rand() < 0.3 and np.sum(improvement_potential[selected_idx]) < np.mean(improvement_potential):\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.9022090237294293,
            0.4394521415233612
        ]
    }
]