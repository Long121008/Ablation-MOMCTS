[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and diversity (crowding distance), then applies a hybrid local search strategy that includes adaptive item swaps, marginal contribution analysis, and dynamic objective-specific perturbations to generate a high-quality neighbor solution while ensuring feasibility. It prioritizes items with high marginal contributions for each objective and dynamically adjusts the focus between objectives during perturbation, followed by a feasibility repair step to handle any weight violations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 + normalized_crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Objective-Specific Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:  # Value1 focus\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Weighted Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 133,
        "algorithm": "The algorithm selects a promising solution from the archive by prioritizing those with high combined marginal contributions for both objectives and low crowding distance, then applies a hybrid local search combining probabilistic item flips weighted by marginal contributions with a Pareto-guided swap operator, while dynamically adjusting objective focus and ensuring feasibility through marginal contribution-based repairs. The selection prioritizes solutions with balanced marginal gains, while the local search probabilistically flips items based on their marginal contributions to both objectives, with dynamic weighting between them, and includes targeted additions of high-marginal items. Feasibility is maintained by sequentially removing items with the lowest marginal utility when capacity is exceeded.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        scores = objectives[:, 0] * 0.5 + objectives[:, 1] * 0.5 + crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8878794376162581,
            0.3844030797481537
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        scores = objectives[:, 0] * 0.5 + objectives[:, 1] * 0.5 + crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (prioritizing high combined normalized objective value), applies a random swap mutation, and then performs a weighted greedy local search to add/remove items, favoring objective 1 (60%) over objective 2 (40%) while ensuring feasibility. The weighted approach balances improvements across objectives, and the random shuffling ensures exploration. The structure combines mutation and greedy search to efficiently explore the neighborhood of the selected solution.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_score = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        score = (v1 + v2) / (np.sum(weight_lst * sol) + 1e-6)  # Normalized by weight\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Randomly swap two items (swap mutation)\n    if n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Greedy addition based on weighted objective improvement\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            # Calculate weighted objective improvement\n            weight_improvement = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Greedy removal based on weighted objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        # Calculate weighted objective loss\n        weight_loss = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n        new_solution[item] = 0\n        current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a younger solution from the archive, prioritizes critical high-value items for flipping with age-based probability, and performs guided probabilistic swaps to improve objective values while maintaining feasibility through incremental weight checks and a repair mechanism that removes low-value items first. It dynamically adjusts perturbation intensity based on solution age and combines value-to-weight ratios for intelligent item selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Age-based selection: prioritize younger solutions (lower index)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)  # Select from younger 25% or last if fewer than 4\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 20):]  # Top 5% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with probability inversely proportional to age\n    flip_prob = 1.0 / (selected_idx + 1)  # Younger solutions have higher flip probability\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.3 / (selected_idx + 1)  # Younger solutions have higher swap probability\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm prioritizes solutions with high variance in objective values, applies a random bit-flip mutation, and performs a dynamic weighted greedy search that alternates between objectives based on their current dominance, ensuring feasibility through adaptive weight adjustments. The selection of solutions is biased toward those with significant differences between the two objectives, while the local search dynamically adjusts weights to balance improvements in either objective, with occasional removals to maintain diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_variance = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        variance = abs(v1 - v2)\n        if variance > max_variance:\n            max_variance = variance\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Random bit-flip mutation\n    flip_index = np.random.randint(n_items)\n    new_solution[flip_index] = 1 - new_solution[flip_index]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic weighted greedy search\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    # Determine objective weights dynamically\n    total_v1 = np.sum(value1_lst * new_solution)\n    total_v2 = np.sum(value2_lst * new_solution)\n    w1 = 0.7 if total_v1 < total_v2 else 0.3\n    w2 = 1 - w1\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            improvement = w1 * value1_lst[item] + w2 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Dynamic weighted greedy removal\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        loss = w1 * value1_lst[item] + w2 * value2_lst[item]\n        if np.random.rand() < 0.3:  # 30% chance to remove to maintain diversity\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a combined score of value dominance and weight utilization, then generates a neighbor by adaptively swapping high-value items with biased probabilities, probabilistically removing low-value items, and finally repairing feasibility by removing items with the smallest normalized marginal value-to-weight ratio. It prioritizes high-value items through value biases and ensures feasibility by guided repair, making it effective for bi-objective knapsack problems.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined value dominance and weight utilization\n    objectives = np.array([obj for _, obj in archive])\n    value_dominance = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    combined_scores = value_dominance[0] * value_dominance[1] * weight_utilization\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate adaptive value biases\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    value_biases = (value1_lst / (total_value1 + 1e-6) + value2_lst / (total_value2 + 1e-6)) / 2\n    value_biases = np.power(value_biases, 1.5)  # Stronger bias towards high-value items\n\n    # Adaptive value-biased swaps\n    swap_prob = 0.4 * (1 - selected_idx / len(archive))  # Higher probability for earlier solutions\n    for i in range(n_items):\n        if np.random.rand() < swap_prob * value_biases[i]:\n            if new_solution[i] == 0:\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n            else:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Probabilistic removal of low-value items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_ranks = np.argsort(value_biases[included_items])\n        for i in range(min(2, len(included_items))):  # Remove up to 2 lowest-value items\n            remove_idx = included_items[value_ranks[i]]\n            if np.random.rand() < 0.6:  # 60% chance to remove\n                new_solution[remove_idx] = 0\n                current_weight -= weight_lst[remove_idx]\n\n    # Guided repair mechanism\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with smallest normalized marginal value-to-weight ratio\n            marginal_ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n            normalized_ratios = marginal_ratios / np.max(marginal_ratios + 1e-6)\n            remove_idx = excess_items[np.argmin(normalized_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm adaptively selects a solution from the archive by prioritizing younger, more diverse solutions (based on objective trade-offs), then applies a hybrid local search that dynamically flips critical items (top 10% by combined value-to-weight ratio) with age-based probabilities and performs guided probabilistic swaps (removing low-value items and adding high-value ones) while ensuring feasibility through incremental weight checks and targeted removals. The method balances exploration (via probabilistic flips) and exploitation (via critical item focus) while maintaining solution quality across both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize younger solutions with high objective diversity\n    selected_idx = min(int(len(archive) * 0.3), len(archive) - 1)  # Select from younger 30%\n    max_diversity = -1\n    for i in range(selected_idx, len(archive)):\n        sol, (v1, v2) = archive[i]\n        diversity = abs(v1 - v2) / (np.sum(weight_lst * sol) + 1e-6)\n        if diversity > max_diversity:\n            max_diversity = diversity\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 10):]  # Top 10% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with age-based probability\n    flip_prob = 0.5 / (selected_idx + 1)\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.4 / (selected_idx + 1)\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n                current_weight = potential_weight\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive solution selection (prioritizing diverse, high-improvement-potential solutions) with a hybrid local search that alternates between critical-item flips (top 10% value-to-weight ratio items), guided swaps (balanced marginal contributions), and dynamic intensity adjustments (increased randomness when stuck). It ensures feasibility through incremental capacity checks and value-to-weight ratio-based repairs, dynamically adjusting perturbation intensity to balance exploration and exploitation. The selection mechanism emphasizes diversity and improvement potential, while the local search prioritizes high-value items while maintaining feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on age-weighted diversity\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Identify critical items (top 10% combined ratio)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 10):]\n\n    # Hybrid perturbation: critical flips, guided swaps, and dynamic intensity\n    for _ in range(3):  # Three perturbation rounds\n        # Critical item flip with probability based on diversity\n        if len(critical_items) > 0 and np.random.rand() < 0.7:\n            item = np.random.choice(critical_items)\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n        # Guided swap: items with balanced marginal contributions\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Find item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Find item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Dynamic intensity adjustment: increase randomness if stuck\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.4:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThis heuristic combines adaptive solution selection with a multi-phase perturbation strategy that prioritizes critical items (top 10% value-to-weight ratio), objective-correlated swaps (guided by value correlation), and segment-based diversification, while strictly maintaining feasibility through weight checks and excess-item removal. The algorithm dynamically balances exploitation (focused flips and swaps) with exploration (random segment flips) and adjusts perturbation intensity based on objective correlation, ensuring both local optimization and global search capabilities.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or recent improvement\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives[i+1, 0] - objectives[i-1, 0]) + (objectives[i+1, 1] - objectives[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 10% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 10):]\n    for item in critical_items:\n        if np.random.rand() < 0.4:  # 40% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.5:  # Strong correlation\n        if np.random.rand() < 0.6:  # 60% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 40% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.median(value1_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.median(value2_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.3:  # 30% chance for segment flip\n        segment_length = np.random.randint(2, min(7, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 147,
        "algorithm": "This algorithm selects a promising solution from the archive by prioritizing those with high objective diversity and low redundancy, then applies a hybrid local search combining critical-item flips (top 5% value-to-weight ratio), guided probabilistic swaps (balancing marginal contributions), and dynamic segment flips (adaptive intensity), while strictly maintaining feasibility through incremental weight checks and value-to-weight ratio-based repairs. The method emphasizes high-value items, strategic swaps, and localized segment adjustments to improve both objectives while ensuring capacity constraints are respected.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest diversity and lowest redundancy\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    redundancy = np.array([np.sum(sol[0] == 1) for sol in archive]) / len(weight_lst)\n    selection_scores = diversity[0] * diversity[1] * (1 - redundancy)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Guided probabilistic swaps\n    if np.random.rand() < 0.7:  # 70% chance for guided swap\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Select item to remove (lowest marginal contribution)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Select item to add (highest marginal contribution among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Phase 3: Dynamic segment flips\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.8569176149065492,
            0.42228320240974426
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest diversity and lowest redundancy\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    redundancy = np.array([np.sum(sol[0] == 1) for sol in archive]) / len(weight_lst)\n    selection_scores = diversity[0] * diversity[1] * (1 - redundancy)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Guided probabilistic swaps\n    if np.random.rand() < 0.7:  # 70% chance for guided swap\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Select item to remove (lowest marginal contribution)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Select item to add (highest marginal contribution among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Phase 3: Dynamic segment flips\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a weighted combination of objective values and diversity (crowding distance), then applies a hybrid local search strategy that includes adaptive item swaps, marginal contribution analysis, and dynamic objective-specific perturbations to generate a high-quality neighbor solution while ensuring feasibility. It prioritizes items with high marginal contributions for each objective and dynamically adjusts the focus between objectives during perturbation, followed by a feasibility repair step to handle any weight violations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 + normalized_crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Objective-Specific Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:  # Value1 focus\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Weighted Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 134,
        "algorithm": "The algorithm dynamically selects a promising solution from the archive by balancing objective values and diversity, then applies a hybrid local search combining rarity-based perturbations, objective-balanced insertions, and feasibility repairs, prioritizing items with high marginal contributions while dynamically adjusting weights between objectives. It ensures feasibility by strategically removing low-contribution items when needed. The selection process favors solutions in underrepresented regions of the objective space, while the local search intelligently explores the solution space by considering item rarity and objective-specific priorities.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection based on objective balance and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance and objective balance\n        crowding = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.linalg.norm(objectives - objectives[i], axis=1)\n            crowding[i] = np.mean(np.sort(distances)[1:min(4, len(distances))])\n\n        # Dynamic weights based on objective balance\n        obj1_mean = np.mean(objectives[:, 0])\n        obj2_mean = np.mean(objectives[:, 1])\n        obj1_weight = obj2_mean / (obj1_mean + obj2_mean + 1e-10)\n        obj2_weight = obj1_mean / (obj1_mean + obj2_mean + 1e-10)\n\n        # Combine metrics with dynamic weights\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * obj1_weight + objectives[:, 1] * obj2_weight + normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate weighted marginal contributions\n    obj1_weight = 0.6 if np.random.rand() < 0.5 else 0.4\n    obj2_weight = 1 - obj1_weight\n    marginal = (value1_lst * obj1_weight + value2_lst * obj2_weight) / (weight_lst + 1e-10)\n\n    # Phase 1: Rarity-based Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate item rarity (inverse of how common items are in archive)\n    item_presence = np.zeros(len(weight_lst))\n    for sol, _ in archive:\n        item_presence += sol\n    rarity = 1 / (item_presence + 1)\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = (1 - np.tanh(marginal[included] * 0.3)) * rarity[included] * 0.7\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal[excluded] * 0.3) * rarity[excluded] * 0.7) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-balanced Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-marginal[excluded])]\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective focus\n                if np.random.rand() < obj1_weight:\n                    if value1_lst[add_idx] > np.percentile(value1_lst[included], 75):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:\n                    if value2_lst[add_idx] > np.percentile(value2_lst[included], 75):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Dynamic Priority\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest weighted contribution\n        remove_contributions = (value1_lst[candidates] * obj1_weight + value2_lst[candidates] * obj2_weight) / (weight_lst[candidates] + 1e-10)\n        remove_idx = candidates[np.argmin(remove_contributions)]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.35228424551436116,
            0.6216509640216827
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection based on objective balance and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance and objective balance\n        crowding = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.linalg.norm(objectives - objectives[i], axis=1)\n            crowding[i] = np.mean(np.sort(distances)[1:min(4, len(distances))])\n\n        # Dynamic weights based on objective balance\n        obj1_mean = np.mean(objectives[:, 0])\n        obj2_mean = np.mean(objectives[:, 1])\n        obj1_weight = obj2_mean / (obj1_mean + obj2_mean + 1e-10)\n        obj2_weight = obj1_mean / (obj1_mean + obj2_mean + 1e-10)\n\n        # Combine metrics with dynamic weights\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * obj1_weight + objectives[:, 1] * obj2_weight + normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate weighted marginal contributions\n    obj1_weight = 0.6 if np.random.rand() < 0.5 else 0.4\n    obj2_weight = 1 - obj1_weight\n    marginal = (value1_lst * obj1_weight + value2_lst * obj2_weight) / (weight_lst + 1e-10)\n\n    # Phase 1: Rarity-based Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate item rarity (inverse of how common items are in archive)\n    item_presence = np.zeros(len(weight_lst))\n    for sol, _ in archive:\n        item_presence += sol\n    rarity = 1 / (item_presence + 1)\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = (1 - np.tanh(marginal[included] * 0.3)) * rarity[included] * 0.7\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal[excluded] * 0.3) * rarity[excluded] * 0.7) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-balanced Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-marginal[excluded])]\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective focus\n                if np.random.rand() < obj1_weight:\n                    if value1_lst[add_idx] > np.percentile(value1_lst[included], 75):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:\n                    if value2_lst[add_idx] > np.percentile(value2_lst[included], 75):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Dynamic Priority\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest weighted contribution\n        remove_contributions = (value1_lst[candidates] * obj1_weight + value2_lst[candidates] * obj2_weight) / (weight_lst[candidates] + 1e-10)\n        remove_idx = candidates[np.argmin(remove_contributions)]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a younger solution from the archive, prioritizes critical high-value items for flipping with age-based probability, and performs guided probabilistic swaps to improve objective values while maintaining feasibility through incremental weight checks and a repair mechanism that removes low-value items first. It dynamically adjusts perturbation intensity based on solution age and combines value-to-weight ratios for intelligent item selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Age-based selection: prioritize younger solutions (lower index)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)  # Select from younger 25% or last if fewer than 4\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 20):]  # Top 5% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with probability inversely proportional to age\n    flip_prob = 1.0 / (selected_idx + 1)  # Younger solutions have higher flip probability\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.3 / (selected_idx + 1)  # Younger solutions have higher swap probability\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm implements an adaptive selection mechanism that prioritizes solutions with high objective diversity and younger age, followed by a hybrid local search that uses dynamic perturbation intensity based on Pareto front position to guide additions and removals of items with high combined value-to-weight ratios, while ensuring feasibility through targeted removals of low-value items. The selection scores combine objective diversity (70% weight for value1, 30% for value2) and solution age (inverse of index), and the perturbation intensity scales inversely with the solution's front position. The local search first adds high-ratio items probabilistically, then removes low-ratio items, and finally enforces feasibility by removing the least valuable items if needed.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 148,
        "algorithm": "The algorithm selects a promising solution from the archive using a tiered selection mechanism that prioritizes solutions with high objective diversity and younger age, then applies a dynamic perturbation strategy alternating between targeted item additions (based on value-to-weight ratios and correlation awareness) and strategic removals (based on marginal contributions), with perturbation intensity scaled by the solution's position in the Pareto front. It ensures feasibility through incremental weight checks and a hierarchical repair process that removes items with the least combined impact on both objectives. The approach emphasizes objective balance and solution diversity while maintaining feasibility through creative local search operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Tiered selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))\n    selection_scores = (diversity[0] * 0.6 + diversity[1] * 0.4) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios and correlation coefficients\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = ratio1 + ratio2\n    correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1)\n\n    # Value frontier heuristic: identify items with high combined ratio and low correlation\n    value_frontier = np.where(new_solution == 0)[0]\n    frontier_scores = combined_ratios[value_frontier] * (1 - np.abs(correlation))\n    value_frontier = value_frontier[np.argsort(frontier_scores)[::-1]]\n\n    # Correlation-aware removal strategy\n    included_items = np.where(new_solution == 1)[0]\n    removal_scores = (combined_ratios[included_items] * (1 - np.abs(correlation))) / weight_lst[included_items]\n    included_items = included_items[np.argsort(removal_scores)]\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Alternating perturbation strategy\n    for item in value_frontier:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity * 0.5:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Hierarchical repair: remove items with least impact on both objectives\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        repair_scores = (ratio1[excess_items] + ratio2[excess_items]) * (1 - np.abs(correlation))\n        remove_item = excess_items[np.argmin(repair_scores)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8957797412226749,
            0.566279411315918
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Tiered selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))\n    selection_scores = (diversity[0] * 0.6 + diversity[1] * 0.4) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios and correlation coefficients\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = ratio1 + ratio2\n    correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1)\n\n    # Value frontier heuristic: identify items with high combined ratio and low correlation\n    value_frontier = np.where(new_solution == 0)[0]\n    frontier_scores = combined_ratios[value_frontier] * (1 - np.abs(correlation))\n    value_frontier = value_frontier[np.argsort(frontier_scores)[::-1]]\n\n    # Correlation-aware removal strategy\n    included_items = np.where(new_solution == 1)[0]\n    removal_scores = (combined_ratios[included_items] * (1 - np.abs(correlation))) / weight_lst[included_items]\n    included_items = included_items[np.argsort(removal_scores)]\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Alternating perturbation strategy\n    for item in value_frontier:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity * 0.5:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Hierarchical repair: remove items with least impact on both objectives\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        repair_scores = (ratio1[excess_items] + ratio2[excess_items]) * (1 - np.abs(correlation))\n        remove_item = excess_items[np.argmin(repair_scores)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a weighted combination of objective values and diversity (crowding distance), then applies a hybrid local search strategy that includes adaptive item swaps, marginal contribution analysis, and dynamic objective-specific perturbations to generate a high-quality neighbor solution while ensuring feasibility. It prioritizes items with high marginal contributions for each objective and dynamically adjusts the focus between objectives during perturbation, followed by a feasibility repair step to handle any weight violations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 + normalized_crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Objective-Specific Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:  # Value1 focus\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Weighted Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 135,
        "algorithm": "The algorithm selects a promising solution from the archive using a dynamic objective weighting scheme that balances solution quality and diversity, then applies a hybrid local search combining adaptive item swaps (prioritizing high-margin items) and dynamic objective-specific perturbations (alternating between value1 and value2 focus), while ensuring feasibility through marginal contribution analysis and feasibility repair. It dynamically adjusts objective priorities during perturbation and insertion phases, with value1 given higher initial weight (0.6) but allowing probabilistic shifts to value2 (0.4) for exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection based on combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance with different normalization\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / (objectives[sorted_indices[-1], 0] - objectives[sorted_indices[0], 0] + 1e-10)\n\n        # Dynamic objective weighting\n        obj_weights = np.array([0.4, 0.6]) if np.random.rand() < 0.7 else np.array([0.7, 0.3])\n        scores = objectives[:, 0] * obj_weights[0] + objectives[:, 1] * obj_weights[1] + crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 * 0.6 + marginal2 * 0.4)\n\n    # Phase 1: Adaptive Objective-Specific Perturbation with different probability model\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.8 if np.random.rand() < 0.6 else 0.2\n\n    perturbation_prob[included] = 1 - np.exp(-marginal1[included] * 0.3) * objective_weight + np.exp(-marginal2[included] * 0.3) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.exp(-marginal1[excluded] * 0.3) * objective_weight + np.exp(-marginal2[excluded] * 0.3) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion with different criteria\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.4:  # Value1 focus\n                    if marginal1[add_idx] > np.median(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.median(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with different criteria\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.4 + value2_lst[candidates] * 0.6 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7478807457416358,
            0.4959564208984375
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection based on combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance with different normalization\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / (objectives[sorted_indices[-1], 0] - objectives[sorted_indices[0], 0] + 1e-10)\n\n        # Dynamic objective weighting\n        obj_weights = np.array([0.4, 0.6]) if np.random.rand() < 0.7 else np.array([0.7, 0.3])\n        scores = objectives[:, 0] * obj_weights[0] + objectives[:, 1] * obj_weights[1] + crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 * 0.6 + marginal2 * 0.4)\n\n    # Phase 1: Adaptive Objective-Specific Perturbation with different probability model\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.8 if np.random.rand() < 0.6 else 0.2\n\n    perturbation_prob[included] = 1 - np.exp(-marginal1[included] * 0.3) * objective_weight + np.exp(-marginal2[included] * 0.3) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.exp(-marginal1[excluded] * 0.3) * objective_weight + np.exp(-marginal2[excluded] * 0.3) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion with different criteria\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.4:  # Value1 focus\n                    if marginal1[add_idx] > np.median(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.median(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with different criteria\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.4 + value2_lst[candidates] * 0.6 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and diversity (crowding distance), then applies a hybrid local search strategy that includes adaptive item swaps, marginal contribution analysis, and dynamic objective-specific perturbations to generate a high-quality neighbor solution while ensuring feasibility. It prioritizes items with high marginal contributions for each objective and dynamically adjusts the focus between objectives during perturbation, followed by a feasibility repair step to handle any weight violations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 + normalized_crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Objective-Specific Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:  # Value1 focus\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Weighted Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and high diversity (crowding distance)\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Objective-Driven Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    if np.random.rand() < 0.5:  # Focus on value1\n        perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal1[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n    else:  # Focus on value2\n        perturbation_prob[included] = 1 - np.tanh(marginal2[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal2[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 136,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on combined objectives and diversity (crowding distance)\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 + normalized_crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Objective-Specific Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:  # Value1 focus\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Weighted Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    # Phase 4: Novel Pareto-Guided Swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7749152882669507,
            0.4442054033279419
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on combined objectives and diversity (crowding distance)\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 + normalized_crowding * 0.2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Objective-Specific Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    objective_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n\n    perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.2) * objective_weight + np.tanh(marginal2[included] * 0.2) * (1 - objective_weight)\n    mask = (weight_lst[excluded] <= (capacity - current_weight))\n    perturbation_prob[excluded] = (np.tanh(marginal1[excluded] * 0.2) * objective_weight + np.tanh(marginal2[excluded] * 0.2) * (1 - objective_weight)) * mask\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                if np.random.rand() < 0.5:  # Value1 focus\n                    if marginal1[add_idx] > np.mean(marginal1[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n                else:  # Value2 focus\n                    if marginal2[add_idx] > np.mean(marginal2[included]):\n                        new_solution[add_idx] = 1\n                        current_weight += weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Weighted Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] * 0.5 + value2_lst[candidates] * 0.5 + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    # Phase 4: Novel Pareto-Guided Swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 137,
        "algorithm": "The algorithm selects a promising solution from the archive by combining objective values, crowding distance, and marginal contributions, then applies a hybrid local search that first probabilistically flips items based on their marginal contributions, followed by Pareto-guided swaps between included and excluded items, with a final feasibility repair step. It prioritizes solutions with high combined objectives and low crowding distance while balancing exploration (via probabilistic flips) and exploitation (via Pareto-guided swaps) to generate high-quality neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives, low crowding distance, and high marginal contributions\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-10)\n        marginal2 = value2_lst / (weight_lst + 1e-10)\n        combined_marginal = marginal1 + marginal2\n\n        # Combine scores\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding + np.array([np.sum(combined_marginal * sol) for sol, _ in archive])\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps between included and excluded items\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9006178453843704,
            0.5026294887065887
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives, low crowding distance, and high marginal contributions\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-10)\n        marginal2 = value2_lst / (weight_lst + 1e-10)\n        combined_marginal = marginal1 + marginal2\n\n        # Combine scores\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding + np.array([np.sum(combined_marginal * sol) for sol, _ in archive])\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps between included and excluded items\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 138,
        "algorithm": "This algorithm combines adaptive selection with a multi-phase local search to generate high-quality neighbors for the BI-KP. It first selects promising solutions based on crowding distance and marginal contributions to both objectives, then applies probabilistic flips (weighted by marginal gains) and Pareto-guided swaps, while dynamically adjusting exploration-exploitation balance and ensuring feasibility through marginal checks and feasibility repair. The method prioritizes items with higher marginal contributions while maintaining diversity and solution quality through careful perturbation and targeted swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.38889370596649586,
            0.40150997042655945
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 139,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection: Choose solution with highest hypervolume contribution and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute hypervolume contributions\n        sorted_indices = np.lexsort((-objectives[:, 0], -objectives[:, 1]))\n        hypervolume = np.zeros(len(archive))\n        hypervolume[sorted_indices[0]] = float('inf')\n        for i in range(1, len(archive)):\n            hypervolume[sorted_indices[i]] = (objectives[sorted_indices[i], 0] - objectives[sorted_indices[i-1], 0]) * (objectives[sorted_indices[i], 1] - objectives[sorted_indices[i-1], 1])\n\n        # Compute crowding distance\n        crowding = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                crowding[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n\n        # Combine scores\n        scores = hypervolume + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive value-weight ratios\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation with Value-Weight Ratio Bias\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.6, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.6, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis and Diversity Consideration\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal and Diversity Preservation\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution while preserving diversity\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8584173393612764,
            0.44093063473701477
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection: Choose solution with highest hypervolume contribution and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute hypervolume contributions\n        sorted_indices = np.lexsort((-objectives[:, 0], -objectives[:, 1]))\n        hypervolume = np.zeros(len(archive))\n        hypervolume[sorted_indices[0]] = float('inf')\n        for i in range(1, len(archive)):\n            hypervolume[sorted_indices[i]] = (objectives[sorted_indices[i], 0] - objectives[sorted_indices[i-1], 0]) * (objectives[sorted_indices[i], 1] - objectives[sorted_indices[i-1], 1])\n\n        # Compute crowding distance\n        crowding = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                crowding[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n\n        # Combine scores\n        scores = hypervolume + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive value-weight ratios\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation with Value-Weight Ratio Bias\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.6, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.6, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis and Diversity Consideration\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal and Diversity Preservation\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution while preserving diversity\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 140,
        "algorithm": "The algorithm employs a three-phase hybrid local search strategy that first perturbs solutions probabilistically based on marginal contributions, then performs targeted swaps to improve objective values, and finally ensures feasibility through marginal contribution-based repairs. It dynamically balances exploration and exploitation by combining adaptive objective weighting with diversity-aware selection, prioritizing items with higher combined marginal gains while maintaining feasibility constraints. The solution selection process considers both objective scores and solution diversity to guide the search toward promising regions of the Pareto front.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution with higher probability\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.3)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.6, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.3)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.6, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9207493722356912,
            0.4220876693725586
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution with higher probability\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.3)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.6, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.3)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.6, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic combines adaptive solution selection with a multi-phase perturbation strategy that prioritizes critical items (top 10% value-to-weight ratio), objective-correlated swaps (guided by value correlation), and segment-based diversification, while strictly maintaining feasibility through weight checks and excess-item removal. The algorithm dynamically balances exploitation (focused flips and swaps) with exploration (random segment flips) and adjusts perturbation intensity based on objective correlation, ensuring both local optimization and global search capabilities.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or recent improvement\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives[i+1, 0] - objectives[i-1, 0]) + (objectives[i+1, 1] - objectives[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 10% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 10):]\n    for item in critical_items:\n        if np.random.rand() < 0.4:  # 40% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.5:  # Strong correlation\n        if np.random.rand() < 0.6:  # 60% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 40% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.median(value1_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.median(value2_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.3:  # 30% chance for segment flip\n        segment_length = np.random.randint(2, min(7, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm implements an adaptive selection mechanism that prioritizes solutions with high objective diversity and younger age, followed by a hybrid local search that uses dynamic perturbation intensity based on Pareto front position to guide additions and removals of items with high combined value-to-weight ratios, while ensuring feasibility through targeted removals of low-value items. The selection scores combine objective diversity (70% weight for value1, 30% for value2) and solution age (inverse of index), and the perturbation intensity scales inversely with the solution's front position. The local search first adds high-ratio items probabilistically, then removes low-ratio items, and finally enforces feasibility by removing the least valuable items if needed.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 149,
        "algorithm": "The algorithm intelligently selects a promising solution from the archive based on Pareto front position and age, then applies a multi-phase perturbation strategy that dynamically adjusts segment-based flips and critical item operations guided by objective correlation, while maintaining feasibility through incremental checks and excess-item removal. It prioritizes younger solutions on higher fronts and adapts perturbation intensity based on domination status, balancing exploration and exploitation through correlated objective-aware perturbations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high Pareto front position and younger age\n    objectives = np.array([obj for _, obj in archive])\n    front_positions = np.zeros(len(archive))\n    for i in range(len(archive)):\n        front_positions[i] = np.sum(np.all(objectives >= objectives[i], axis=1))\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (front_positions + 1) / (ages + 1)  # Prefer younger solutions on higher fronts\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios and objective correlation\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n\n    # Dynamic perturbation intensity based on improvement trend\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Check if solution is dominated by any other in archive\n    is_dominated = np.any(np.all(objectives >= (current_value1, current_value2), axis=1) &\n                          np.any(objectives > (current_value1, current_value2), axis=1))\n    perturbation_intensity = 0.7 if is_dominated else 0.3\n\n    # Phase 1: Segment-based perturbations guided by objective correlation\n    n_items = len(weight_lst)\n    segment_length = max(2, min(5, n_items // 5))  # Dynamic segment length\n    n_segments = max(1, n_items // segment_length)\n\n    for _ in range(n_segments):\n        if np.random.rand() < perturbation_intensity:\n            start_idx = np.random.randint(0, n_items - segment_length)\n            segment = new_solution[start_idx:start_idx+segment_length]\n            new_segment = 1 - segment\n            temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n\n            if temp_weight <= capacity:\n                # Guided flip based on objective correlation\n                if corr > 0.5:  # Strong correlation - flip high value segment\n                    segment_value = np.sum(value1_lst[start_idx:start_idx+segment_length] + value2_lst[start_idx:start_idx+segment_length])\n                    if segment_value > np.median(value1_lst + value2_lst):\n                        new_solution[start_idx:start_idx+segment_length] = new_segment\n                        current_weight = temp_weight\n                else:  # Weak correlation - flip balanced segment\n                    if np.random.rand() < 0.5:\n                        segment_value1 = np.sum(value1_lst[start_idx:start_idx+segment_length])\n                        if segment_value1 > np.median(value1_lst):\n                            new_solution[start_idx:start_idx+segment_length] = new_segment\n                            current_weight = temp_weight\n                    else:\n                        segment_value2 = np.sum(value2_lst[start_idx:start_idx+segment_length])\n                        if segment_value2 > np.median(value2_lst):\n                            new_solution[start_idx:start_idx+segment_length] = new_segment\n                            current_weight = temp_weight\n\n    # Phase 2: Critical item additions/removals\n    if np.random.rand() < perturbation_intensity:\n        # Add critical items (top 5% combined ratio)\n        combined_ratios = (ratio1 + ratio2) / 2\n        critical_items = np.argsort(combined_ratios)[-max(1, n_items // 20):]\n        for item in critical_items:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 3: Feasibility enforcement\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "score": [
            -0.42398500843440035,
            3.4038612842559814
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high Pareto front position and younger age\n    objectives = np.array([obj for _, obj in archive])\n    front_positions = np.zeros(len(archive))\n    for i in range(len(archive)):\n        front_positions[i] = np.sum(np.all(objectives >= objectives[i], axis=1))\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (front_positions + 1) / (ages + 1)  # Prefer younger solutions on higher fronts\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios and objective correlation\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n\n    # Dynamic perturbation intensity based on improvement trend\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Check if solution is dominated by any other in archive\n    is_dominated = np.any(np.all(objectives >= (current_value1, current_value2), axis=1) &\n                          np.any(objectives > (current_value1, current_value2), axis=1))\n    perturbation_intensity = 0.7 if is_dominated else 0.3\n\n    # Phase 1: Segment-based perturbations guided by objective correlation\n    n_items = len(weight_lst)\n    segment_length = max(2, min(5, n_items // 5))  # Dynamic segment length\n    n_segments = max(1, n_items // segment_length)\n\n    for _ in range(n_segments):\n        if np.random.rand() < perturbation_intensity:\n            start_idx = np.random.randint(0, n_items - segment_length)\n            segment = new_solution[start_idx:start_idx+segment_length]\n            new_segment = 1 - segment\n            temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n\n            if temp_weight <= capacity:\n                # Guided flip based on objective correlation\n                if corr > 0.5:  # Strong correlation - flip high value segment\n                    segment_value = np.sum(value1_lst[start_idx:start_idx+segment_length] + value2_lst[start_idx:start_idx+segment_length])\n                    if segment_value > np.median(value1_lst + value2_lst):\n                        new_solution[start_idx:start_idx+segment_length] = new_segment\n                        current_weight = temp_weight\n                else:  # Weak correlation - flip balanced segment\n                    if np.random.rand() < 0.5:\n                        segment_value1 = np.sum(value1_lst[start_idx:start_idx+segment_length])\n                        if segment_value1 > np.median(value1_lst):\n                            new_solution[start_idx:start_idx+segment_length] = new_segment\n                            current_weight = temp_weight\n                    else:\n                        segment_value2 = np.sum(value2_lst[start_idx:start_idx+segment_length])\n                        if segment_value2 > np.median(value2_lst):\n                            new_solution[start_idx:start_idx+segment_length] = new_segment\n                            current_weight = temp_weight\n\n    # Phase 2: Critical item additions/removals\n    if np.random.rand() < perturbation_intensity:\n        # Add critical items (top 5% combined ratio)\n        combined_ratios = (ratio1 + ratio2) / 2\n        critical_items = np.argsort(combined_ratios)[-max(1, n_items // 20):]\n        for item in critical_items:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 3: Feasibility enforcement\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 141,
        "algorithm": "The algorithm combines diversity-aware selection with adaptive marginal contribution analysis to guide local search, alternating between dynamic perturbations (biased by marginal gains) and targeted exchanges (weighted by objective trade-offs), while ensuring feasibility through capacity-constrained adjustments and adaptive removal of low-contribution items. It prioritizes solutions with high combined objective scores and diversity, using dynamic objective weighting (\u03b1) to balance exploration and exploitation, with marginal contributions calculated as weighted averages of normalized profit-to-weight ratios. The local search alternates between perturbation-based exploration and targeted exchange-based exploitation, with feasibility maintained through capacity checks and adaptive removal of over-capacity items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.3 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.4, 0.6)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Dynamic Perturbation with Marginal Contribution Bias\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.exp(-combined_marginal[included] * 0.1)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.6, 0.9, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.exp(-combined_marginal[excluded] * 0.1)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.6, 0.9, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Targeted Exchange with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8893585443424679,
            0.40514111518859863
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.3 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.4, 0.6)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Dynamic Perturbation with Marginal Contribution Bias\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.exp(-combined_marginal[included] * 0.1)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.6, 0.9, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.exp(-combined_marginal[excluded] * 0.1)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.6, 0.9, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Targeted Exchange with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and high diversity (crowding distance)\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Objective-Driven Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    if np.random.rand() < 0.5:  # Focus on value1\n        perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal1[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n    else:  # Focus on value2\n        perturbation_prob[included] = 1 - np.tanh(marginal2[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal2[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 142,
        "algorithm": "This algorithm intelligently selects a solution from the archive by combining objective scores, crowding distance, and marginal contributions, then applies a hybrid local search that first performs probabilistic flips weighted by marginal contributions, followed by Pareto-guided swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items. The algorithm dynamically balances exploration and exploitation through adaptive objective weighting and marginal contribution analysis, ensuring feasibility at every step while prioritizing high-quality neighbor solutions that simultaneously improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores, diversity, and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        crowding = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                crowding[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_crowding)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Probabilistic Flips with Marginal Contribution Weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7248927108613124,
            0.5226570069789886
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores, diversity, and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        crowding = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                crowding[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_crowding)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Probabilistic Flips with Marginal Contribution Weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 143,
        "algorithm": "The algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by marginal contributions and Pareto-guided swaps to generate neighbors while ensuring feasibility. It prioritizes items with high combined marginal value for inclusion and low-marginal-value items for exclusion, with special attention to Pareto-optimal swaps that improve both objectives simultaneously. The process includes three phases: probabilistic flips, Pareto-guided swaps, and feasibility repair to maintain solution validity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9430225621392785,
            0.39643803238868713
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm implements an adaptive selection mechanism that prioritizes solutions with high objective diversity and younger age, followed by a hybrid local search that uses dynamic perturbation intensity based on Pareto front position to guide additions and removals of items with high combined value-to-weight ratios, while ensuring feasibility through targeted removals of low-value items. The selection scores combine objective diversity (70% weight for value1, 30% for value2) and solution age (inverse of index), and the perturbation intensity scales inversely with the solution's front position. The local search first adds high-ratio items probabilistically, then removes low-ratio items, and finally enforces feasibility by removing the least valuable items if needed.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 150,
        "algorithm": "This algorithm implements a hybrid local search for the BI-KP that adaptively selects solutions based on objective diversity and age, then applies a dynamic perturbation strategy guided by Pareto front position to intelligently add high-value items and remove low-value ones, while ensuring feasibility through targeted removals of least valuable items. The selection prioritizes younger, more diverse solutions, and the perturbation intensity scales inversely with the solution's front position, balancing exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8918953695646187,
            0.550024151802063
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by marginal contributions and Pareto-guided swaps to generate neighbors while ensuring feasibility. It prioritizes items with high combined marginal value for inclusion and low-marginal-value items for exclusion, with special attention to Pareto-optimal swaps that improve both objectives simultaneously. The process includes three phases: probabilistic flips, Pareto-guided swaps, and feasibility repair to maintain solution validity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 144,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions and diversity scores\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + diversity-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions and diversity\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if weight_lst[idx] <= (capacity - np.sum(weight_lst[new_solution == 1])):\n                new_solution[idx] = 1\n\n    # Phase 2: Diversity-guided swaps (replace worst marginal items with best excluded items)\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by ascending marginal (worst first), excluded by descending marginal (best first)\n        included_sorted = included[np.argsort(combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n\n    # Phase 3: Feasibility repair (remove least valuable items first)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(value1_lst[candidates] + value2_lst[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8982294963800608,
            0.35623520612716675
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions and diversity scores\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + diversity-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions and diversity\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if weight_lst[idx] <= (capacity - np.sum(weight_lst[new_solution == 1])):\n                new_solution[idx] = 1\n\n    # Phase 2: Diversity-guided swaps (replace worst marginal items with best excluded items)\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by ascending marginal (worst first), excluded by descending marginal (best first)\n        included_sorted = included[np.argsort(combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n\n    # Phase 3: Feasibility repair (remove least valuable items first)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(value1_lst[candidates] + value2_lst[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by marginal contributions and Pareto-guided swaps to generate neighbors while ensuring feasibility. It prioritizes items with high combined marginal value for inclusion and low-marginal-value items for exclusion, with special attention to Pareto-optimal swaps that improve both objectives simultaneously. The process includes three phases: probabilistic flips, Pareto-guided swaps, and feasibility repair to maintain solution validity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 145,
        "algorithm": "The algorithm selects a promising solution from the archive based on combined marginal contributions to both objectives, then probabilistically flips item selections weighted by their marginal values while ensuring feasibility. It performs targeted swaps between high-marginal included and low-marginal excluded items to simultaneously improve both objectives, followed by a dynamic repair mechanism that removes least marginal items if the solution exceeds capacity. The approach balances exploration of high-value items with exploitation of complementary item pairs, prioritizing solutions with strong marginal contributions while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select a solution with high combined marginal contribution\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    selected_idx = np.argmax([np.sum(combined_marginal * sol) for sol, _ in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 2: Probabilistic inclusion/exclusion of high-marginal items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Weighted probability for flipping items\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 3: Targeted swaps between complementary items\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 4: Dynamic feasibility repair\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess = np.sum(weight_lst * new_solution) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with least marginal contribution to both objectives\n        remove_idx = candidates[np.argmin((value1_lst[candidates] + value2_lst[candidates]) / (weight_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9507414327730717,
            0.41735026240348816
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select a solution with high combined marginal contribution\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    selected_idx = np.argmax([np.sum(combined_marginal * sol) for sol, _ in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 2: Probabilistic inclusion/exclusion of high-marginal items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Weighted probability for flipping items\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 3: Targeted swaps between complementary items\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 4: Dynamic feasibility repair\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess = np.sum(weight_lst * new_solution) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with least marginal contribution to both objectives\n        remove_idx = candidates[np.argmin((value1_lst[candidates] + value2_lst[candidates]) / (weight_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by marginal contributions and Pareto-guided swaps to generate neighbors while ensuring feasibility. It prioritizes items with high combined marginal value for inclusion and low-marginal-value items for exclusion, with special attention to Pareto-optimal swaps that improve both objectives simultaneously. The process includes three phases: probabilistic flips, Pareto-guided swaps, and feasibility repair to maintain solution validity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 146,
        "algorithm": "The algorithm selects a diverse solution from the archive by combining objective scores with novelty metrics, then applies a four-phase local search: probabilistic flips weighted by value ratios, dominance-based swaps, a value-balancing operator, and feasibility repair. It prioritizes items with high value ratios, maintains balance between objectives, and ensures feasibility through strategic removals. The selection process favors solutions with both good objective values and unique structures, while the local search intelligently explores the neighborhood while preserving diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high diversity and balanced objectives\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Calculate novelty scores based on Hamming distance\n        solutions = np.array([sol for (sol, obj) in archive])\n        novelty_scores = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.sum(np.abs(solutions - solutions[i]), axis=1)\n            novelty_scores[i] = np.mean(distances)\n\n        # Combine novelty with objective scores\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + novelty_scores / np.max(novelty_scores))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized value ratios\n    value_ratio = (value1_lst + 1e-10) / (value2_lst + 1e-10)\n    normalized_ratio = (value_ratio - np.min(value_ratio)) / (np.max(value_ratio) - np.min(value_ratio) + 1e-10)\n\n    # Hybrid local search: probabilistic flips + dominance-based swaps + value-balancing\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips with value ratio weighting\n    flip_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        flip_prob[included] = 0.3 + 0.7 * (1 - normalized_ratio[included])\n    if len(excluded) > 0:\n        flip_prob[excluded] = 0.3 * normalized_ratio[excluded] * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dominance-based swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending value ratio, excluded by ascending\n        included_sorted = included[np.argsort(-value_ratio[included])]\n        excluded_sorted = excluded[np.argsort(value_ratio[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Value-balancing operator\n    if len(included) > 1:\n        # Calculate current balance\n        current_v1 = np.sum(value1_lst[included])\n        current_v2 = np.sum(value2_lst[included])\n        target_ratio = 0.7 if current_v1 > 1.3 * current_v2 else 0.3 if current_v2 > 1.3 * current_v1 else 0.5\n\n        # Try to balance by swapping items\n        for _ in range(3):\n            if np.random.rand() < 0.7:\n                # Remove an item that's too dominant\n                if current_v1 > target_ratio * current_v2:\n                    remove_idx = included[np.argmax(value1_lst[included] / (value2_lst[included] + 1e-10))]\n                else:\n                    remove_idx = included[np.argmax(value2_lst[included] / (value1_lst[included] + 1e-10))]\n\n                # Find an item to add that would improve balance\n                possible_adds = excluded[weight_lst[excluded] <= (capacity - current_weight + weight_lst[remove_idx])]\n                if len(possible_adds) > 0:\n                    add_idx = possible_adds[np.argmin(np.abs(value_ratio[possible_adds] - target_ratio))]\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 4: Feasibility repair with value-aware removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with lowest value density\n        remove_idx = candidates[np.argmin((value1_lst[candidates] + value2_lst[candidates]) / (weight_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8854969169974459,
            1.0249180495738983
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high diversity and balanced objectives\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Calculate novelty scores based on Hamming distance\n        solutions = np.array([sol for (sol, obj) in archive])\n        novelty_scores = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.sum(np.abs(solutions - solutions[i]), axis=1)\n            novelty_scores[i] = np.mean(distances)\n\n        # Combine novelty with objective scores\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + novelty_scores / np.max(novelty_scores))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized value ratios\n    value_ratio = (value1_lst + 1e-10) / (value2_lst + 1e-10)\n    normalized_ratio = (value_ratio - np.min(value_ratio)) / (np.max(value_ratio) - np.min(value_ratio) + 1e-10)\n\n    # Hybrid local search: probabilistic flips + dominance-based swaps + value-balancing\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips with value ratio weighting\n    flip_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        flip_prob[included] = 0.3 + 0.7 * (1 - normalized_ratio[included])\n    if len(excluded) > 0:\n        flip_prob[excluded] = 0.3 * normalized_ratio[excluded] * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dominance-based swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending value ratio, excluded by ascending\n        included_sorted = included[np.argsort(-value_ratio[included])]\n        excluded_sorted = excluded[np.argsort(value_ratio[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Value-balancing operator\n    if len(included) > 1:\n        # Calculate current balance\n        current_v1 = np.sum(value1_lst[included])\n        current_v2 = np.sum(value2_lst[included])\n        target_ratio = 0.7 if current_v1 > 1.3 * current_v2 else 0.3 if current_v2 > 1.3 * current_v1 else 0.5\n\n        # Try to balance by swapping items\n        for _ in range(3):\n            if np.random.rand() < 0.7:\n                # Remove an item that's too dominant\n                if current_v1 > target_ratio * current_v2:\n                    remove_idx = included[np.argmax(value1_lst[included] / (value2_lst[included] + 1e-10))]\n                else:\n                    remove_idx = included[np.argmax(value2_lst[included] / (value1_lst[included] + 1e-10))]\n\n                # Find an item to add that would improve balance\n                possible_adds = excluded[weight_lst[excluded] <= (capacity - current_weight + weight_lst[remove_idx])]\n                if len(possible_adds) > 0:\n                    add_idx = possible_adds[np.argmin(np.abs(value_ratio[possible_adds] - target_ratio))]\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 4: Feasibility repair with value-aware removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with lowest value density\n        remove_idx = candidates[np.argmin((value1_lst[candidates] + value2_lst[candidates]) / (weight_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by marginal contributions and Pareto-guided swaps to generate neighbors while ensuring feasibility. It prioritizes items with high combined marginal value for inclusion and low-marginal-value items for exclusion, with special attention to Pareto-optimal swaps that improve both objectives simultaneously. The process includes three phases: probabilistic flips, Pareto-guided swaps, and feasibility repair to maintain solution validity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 147,
        "algorithm": "The algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by normalized marginal contributions and Pareto-guided swaps, prioritizing high-marginal-value items for inclusion and low-marginal-value items for exclusion while ensuring feasibility. The method balances exploration (via probabilistic flips) and exploitation (via targeted swaps) to efficiently navigate the solution space while maintaining diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - 0.7 * normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n    normalized_marginal = combined_marginal / (np.max(combined_marginal) + 1e-10)\n\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-2 * normalized_marginal[included])\n    flip_prob[excluded] = np.exp(-1.5 * normalized_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-normalized_marginal[included])]\n        excluded_sorted = excluded[np.argsort(normalized_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] > 0.8 * value1_lst[remove_idx] and value2_lst[add_idx] > 0.8 * value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.4395422455845357,
            0.3459703028202057
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - 0.7 * normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n    normalized_marginal = combined_marginal / (np.max(combined_marginal) + 1e-10)\n\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-2 * normalized_marginal[included])\n    flip_prob[excluded] = np.exp(-1.5 * normalized_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-normalized_marginal[included])]\n        excluded_sorted = excluded[np.argsort(normalized_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] > 0.8 * value1_lst[remove_idx] and value2_lst[add_idx] > 0.8 * value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on high combined objectives and low crowding distance, then applies a hybrid local search combining probabilistic flips weighted by marginal contributions and Pareto-guided swaps to generate neighbors while ensuring feasibility. It prioritizes items with high combined marginal value for inclusion and low-marginal-value items for exclusion, with special attention to Pareto-optimal swaps that improve both objectives simultaneously. The process includes three phases: probabilistic flips, Pareto-guided swaps, and feasibility repair to maintain solution validity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if weight_lst[idx] <= (capacity - current_weight):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and high diversity (crowding distance)\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Objective-Driven Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    if np.random.rand() < 0.5:  # Focus on value1\n        perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal1[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n    else:  # Focus on value2\n        perturbation_prob[included] = 1 - np.tanh(marginal2[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal2[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 148,
        "algorithm": "The algorithm selects a high-quality solution from the archive (prioritizing those with high objective values and low crowding distance) and applies a hybrid local search combining: 1) adaptive probabilistic flips (weighted by marginal contributions), 2) Pareto-guided swaps (targeting high-gain improvements), and 3) feasibility repair (removing low-marginal items when needed). It dynamically balances exploration (perturbations) and exploitation (targeted swaps) using adaptive objective weighting (alpha) to ensure both objectives are improved while maintaining feasibility. The algorithm prioritizes items with high marginal contributions (combined from both objectives) and performs guided swaps between high and low marginal items to enhance solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        crowding = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                crowding[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_crowding)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7443442826688416,
            0.42574068903923035
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        crowding = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                crowding[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_crowding)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(3, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 149,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        scores = objectives[:, 0] + objectives[:, 1] - crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[new_solution == 1] = 1 - np.tanh(combined_marginal[new_solution == 1] * 0.1)\n    perturbation_prob[new_solution == 0] = np.tanh(combined_marginal[new_solution == 0] * 0.1) * (weight_lst[new_solution == 0] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if current_weight - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7823628310212614,
            0.340983510017395
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        scores = objectives[:, 0] + objectives[:, 1] - crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[new_solution == 1] = 1 - np.tanh(combined_marginal[new_solution == 1] * 0.1)\n    perturbation_prob[new_solution == 0] = np.tanh(combined_marginal[new_solution == 0] * 0.1) * (weight_lst[new_solution == 0] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if current_weight - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 150,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding distance\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        # Combine objectives and crowding distance\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Adaptive Value-Weighted Perturbations\n    perturbation_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Higher probability for items with low marginal contribution\n    perturbation_prob[included] = 0.5 * (1 - np.tanh(combined_marginal[included] * 0.1))\n    perturbation_prob[excluded] = 0.5 * np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Aware Pareto-Optimal Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is feasible\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Perform swap based on objective dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Dynamic Weight Adjustment for Feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest combined marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8959196666303096,
            0.33285123109817505
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding distance\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        # Combine objectives and crowding distance\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Adaptive Value-Weighted Perturbations\n    perturbation_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Higher probability for items with low marginal contribution\n    perturbation_prob[included] = 0.5 * (1 - np.tanh(combined_marginal[included] * 0.1))\n    perturbation_prob[excluded] = 0.5 * np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Aware Pareto-Optimal Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is feasible\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Perform swap based on objective dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Dynamic Weight Adjustment for Feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest combined marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe heuristic selects a random solution from the archive and applies a hybrid local search combining random item swaps (to explore the solution space) and greedy marginal-value selection (to exploit high-value items), ensuring feasibility by checking weights at each step. The algorithm prioritizes items with combined high value-to-weight ratios while maintaining the knapsack capacity constraint.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = random.randint(0, len(archive) - 1)\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Hybrid local search: random swaps with greedy selection\n    for _ in range(10):  # Number of iterations\n        # Randomly select two items to swap\n        item1, item2 = random.sample(range(len(weight_lst)), 2)\n\n        # Calculate new weights if we swap the items\n        new_weight = current_weight - weight_lst[item1] * new_solution[item1] + weight_lst[item1] * (1 - new_solution[item1])\n        new_weight = new_weight - weight_lst[item2] * new_solution[item2] + weight_lst[item2] * (1 - new_solution[item2])\n\n        if new_weight <= capacity:\n            # Accept the swap if feasible\n            new_solution[item1] = 1 - new_solution[item1]\n            new_solution[item2] = 1 - new_solution[item2]\n            current_weight = new_weight\n\n    # Additional greedy improvement: add items with highest marginal value\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        marginal_values = (value1_lst + value2_lst) / weight_lst\n        feasible_items = (weight_lst <= remaining_capacity) & (new_solution == 0)\n        if np.any(feasible_items):\n            best_item = np.argmax(marginal_values * feasible_items)\n            if marginal_values[best_item] > 0:\n                new_solution[best_item] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 151,
        "algorithm": "The algorithm combines multi-phase local search with adaptive objective prioritization, selecting solutions with high marginal contributions or low crowding distance, then applies a probabilistic flip mechanism weighted by item importance (derived from both objectives), followed by a Pareto-guided swap phase that evaluates item exchanges based on their potential to improve both objectives simultaneously, and finally performs a lightweight feasibility repair using a value-weighted removal strategy to ensure constraint satisfaction. The algorithm prioritizes items with higher combined marginal contributions (value1 + value2) for inclusion, while maintaining feasibility through dynamic weight adjustments and targeted removals.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high marginal contributions or low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic Flip with Adaptive Objective Prioritization\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 0.3 * np.exp(-combined_marginal[new_solution == 1] * 0.1)\n    flip_prob[new_solution == 0] = 0.7 * np.exp(combined_marginal[new_solution == 0] * 0.1) * (weight_lst[new_solution == 0] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Multi-Objective Awareness\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Multi-objective aware swap\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Value-Weighted Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin((value1_lst[candidates] + value2_lst[candidates]) / (weight_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8440321039657415,
            0.3508290648460388
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high marginal contributions or low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic Flip with Adaptive Objective Prioritization\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 0.3 * np.exp(-combined_marginal[new_solution == 1] * 0.1)\n    flip_prob[new_solution == 0] = 0.7 * np.exp(combined_marginal[new_solution == 0] * 0.1) * (weight_lst[new_solution == 0] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Multi-Objective Awareness\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Multi-objective aware swap\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Value-Weighted Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin((value1_lst[candidates] + value2_lst[candidates]) / (weight_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm implements an adaptive selection mechanism that prioritizes solutions with high objective diversity and younger age, followed by a hybrid local search that uses dynamic perturbation intensity based on Pareto front position to guide additions and removals of items with high combined value-to-weight ratios, while ensuring feasibility through targeted removals of low-value items. The selection scores combine objective diversity (70% weight for value1, 30% for value2) and solution age (inverse of index), and the perturbation intensity scales inversely with the solution's front position. The local search first adds high-ratio items probabilistically, then removes low-ratio items, and finally enforces feasibility by removing the least valuable items if needed.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 151,
        "algorithm": "The algorithm adaptively selects a solution from the archive based on objective diversity (60% weight for value1, 40% for value2) and age (prioritizing younger solutions), then applies a hybrid local search with dynamic perturbation intensity (scaling inversely with the solution's Pareto front position) to probabilistically add high-value-to-weight-ratio items and remove low-value items, ensuring feasibility through targeted removals of least valuable items. The selection scores balance diversity and age, while the local search uses combined ratios to guide additions/removals and enforces feasibility through excess item removal.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.6 + diversity[1] * 0.4) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9530262093394266,
            0.520490825176239
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.6 + diversity[1] * 0.4) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 152,
        "algorithm": "This heuristic algorithm selects a promising solution from the archive by balancing dominance and diversity, then applies a three-phase local search: 1) value-weighted random perturbations to explore the solution space, 2) objective-specific multi-swaps to improve each objective's value, and 3) diversity-aware feasibility repair to maintain solution quality while ensuring feasibility. The algorithm prioritizes items with high combined value-weight ratios while adaptively adjusting exploration based on the current solution's characteristics.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection: Hybrid of dominance and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n        # Calculate dominance scores (higher is better)\n        dominance = np.sum(norm_obj, axis=1)\n        # Calculate diversity scores (higher is better for diversity)\n        diversity = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.sum((norm_obj - norm_obj[i])**2, axis=1)\n            diversity[i] = np.mean(distances)\n        # Combine scores with adaptive weighting\n        alpha = 0.7 if np.random.rand() < 0.5 else 0.3\n        scores = alpha * dominance + (1 - alpha) * diversity\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized value-weight ratios\n    norm_v1 = value1_lst / (weight_lst + 1e-10)\n    norm_v2 = value2_lst / (weight_lst + 1e-10)\n    combined_ratio = (norm_v1 + norm_v2) * (1 + 0.1 * np.random.rand(len(weight_lst)))\n\n    # Phase 1: Value-Weight Ratio Guided Multi-Objective Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate perturbation probabilities\n    if len(included) > 0:\n        remove_probs = np.exp(-combined_ratio[included]) * (1 - np.tanh(0.5 * combined_ratio[included]))\n    else:\n        remove_probs = np.array([])\n\n    if len(excluded) > 0:\n        add_probs = np.tanh(1.5 * combined_ratio[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n    else:\n        add_probs = np.array([])\n\n    # Apply perturbations\n    if len(remove_probs) > 0:\n        for idx in included[np.random.rand(len(included)) < remove_probs]:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    if len(add_probs) > 0:\n        for idx in excluded[np.random.rand(len(excluded)) < add_probs]:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Multi-Swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by objective-specific ratios\n        v1_sorted = np.argsort(-norm_v1)\n        v2_sorted = np.argsort(-norm_v2)\n\n        # Select top candidates for each objective\n        v1_candidates = [i for i in v1_sorted if new_solution[i] == 1][:min(3, len(included))]\n        v2_candidates = [i for i in v2_sorted if new_solution[i] == 1][:min(3, len(included))]\n\n        # Perform objective-specific swaps\n        for obj, candidates in [(1, v1_candidates), (2, v2_candidates)]:\n            for remove_idx in candidates:\n                # Find best alternative item\n                if obj == 1:\n                    potential_adds = [i for i in excluded if (current_weight - weight_lst[remove_idx] + weight_lst[i]) <= capacity]\n                    if potential_adds:\n                        add_idx = potential_adds[np.argmax(norm_v1[potential_adds])]\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:\n                    potential_adds = [i for i in excluded if (current_weight - weight_lst[remove_idx] + weight_lst[i]) <= capacity]\n                    if potential_adds:\n                        add_idx = potential_adds[np.argmax(norm_v2[potential_adds])]\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Diversity-Aware Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n\n        # Select items to remove based on diversity contribution\n        if len(archive) > 1:\n            # Calculate diversity contribution of each item\n            diversity_contrib = np.zeros(len(candidates))\n            for i, idx in enumerate(candidates):\n                temp_sol = new_solution.copy()\n                temp_sol[idx] = 0\n                temp_obj = (np.sum(value1_lst[temp_sol == 1]), np.sum(value2_lst[temp_sol == 1]))\n                distances = [np.linalg.norm(np.array(temp_obj) - np.array(obj)) for (_, obj) in archive]\n                diversity_contrib[i] = np.mean(distances)\n\n            # Remove item with lowest diversity contribution\n            remove_idx = candidates[np.argmin(diversity_contrib)]\n        else:\n            # Fallback to value-weighted removal\n            remove_idx = candidates[np.argmin((value1_lst + value2_lst)[candidates] / (weight_lst[candidates] + 1e-10))]\n\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.6132153611179945,
            1.7093928754329681
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection: Hybrid of dominance and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n        # Calculate dominance scores (higher is better)\n        dominance = np.sum(norm_obj, axis=1)\n        # Calculate diversity scores (higher is better for diversity)\n        diversity = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.sum((norm_obj - norm_obj[i])**2, axis=1)\n            diversity[i] = np.mean(distances)\n        # Combine scores with adaptive weighting\n        alpha = 0.7 if np.random.rand() < 0.5 else 0.3\n        scores = alpha * dominance + (1 - alpha) * diversity\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized value-weight ratios\n    norm_v1 = value1_lst / (weight_lst + 1e-10)\n    norm_v2 = value2_lst / (weight_lst + 1e-10)\n    combined_ratio = (norm_v1 + norm_v2) * (1 + 0.1 * np.random.rand(len(weight_lst)))\n\n    # Phase 1: Value-Weight Ratio Guided Multi-Objective Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate perturbation probabilities\n    if len(included) > 0:\n        remove_probs = np.exp(-combined_ratio[included]) * (1 - np.tanh(0.5 * combined_ratio[included]))\n    else:\n        remove_probs = np.array([])\n\n    if len(excluded) > 0:\n        add_probs = np.tanh(1.5 * combined_ratio[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n    else:\n        add_probs = np.array([])\n\n    # Apply perturbations\n    if len(remove_probs) > 0:\n        for idx in included[np.random.rand(len(included)) < remove_probs]:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    if len(add_probs) > 0:\n        for idx in excluded[np.random.rand(len(excluded)) < add_probs]:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Multi-Swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by objective-specific ratios\n        v1_sorted = np.argsort(-norm_v1)\n        v2_sorted = np.argsort(-norm_v2)\n\n        # Select top candidates for each objective\n        v1_candidates = [i for i in v1_sorted if new_solution[i] == 1][:min(3, len(included))]\n        v2_candidates = [i for i in v2_sorted if new_solution[i] == 1][:min(3, len(included))]\n\n        # Perform objective-specific swaps\n        for obj, candidates in [(1, v1_candidates), (2, v2_candidates)]:\n            for remove_idx in candidates:\n                # Find best alternative item\n                if obj == 1:\n                    potential_adds = [i for i in excluded if (current_weight - weight_lst[remove_idx] + weight_lst[i]) <= capacity]\n                    if potential_adds:\n                        add_idx = potential_adds[np.argmax(norm_v1[potential_adds])]\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:\n                    potential_adds = [i for i in excluded if (current_weight - weight_lst[remove_idx] + weight_lst[i]) <= capacity]\n                    if potential_adds:\n                        add_idx = potential_adds[np.argmax(norm_v2[potential_adds])]\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Diversity-Aware Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n\n        # Select items to remove based on diversity contribution\n        if len(archive) > 1:\n            # Calculate diversity contribution of each item\n            diversity_contrib = np.zeros(len(candidates))\n            for i, idx in enumerate(candidates):\n                temp_sol = new_solution.copy()\n                temp_sol[idx] = 0\n                temp_obj = (np.sum(value1_lst[temp_sol == 1]), np.sum(value2_lst[temp_sol == 1]))\n                distances = [np.linalg.norm(np.array(temp_obj) - np.array(obj)) for (_, obj) in archive]\n                diversity_contrib[i] = np.mean(distances)\n\n            # Remove item with lowest diversity contribution\n            remove_idx = candidates[np.argmin(diversity_contrib)]\n        else:\n            # Fallback to value-weighted removal\n            remove_idx = candidates[np.argmin((value1_lst + value2_lst)[candidates] / (weight_lst[candidates] + 1e-10))]\n\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 153,
        "algorithm": "The algorithm selects a promising solution from the archive by balancing objective values and diversity, then applies a two-phase local search: first, it probabilistically perturbs items based on normalized value-to-weight ratios, and second, it performs targeted swaps between low-value included items and high-value excluded items while prioritizing either objective with equal probability. The method ensures feasibility through probabilistic operations and final capacity adjustments.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: balance between objective values and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n        # Calculate diversity (distance to nearest neighbor)\n        diversity = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.sum((objectives - objectives[i])**2, axis=1)\n            diversity[i] = np.min(distances[distances > 0] if np.sum(distances > 0) > 0 else [0])\n        # Combine scores\n        scores = 0.6 * (norm_obj[:, 0] + norm_obj[:, 1]) + 0.4 * (diversity / (np.max(diversity) + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate normalized value-weight ratios\n    norm_vw1 = value1_lst / (weight_lst + 1e-10)\n    norm_vw2 = value2_lst / (weight_lst + 1e-10)\n    combined_norm = (norm_vw1 + norm_vw2) / 2\n\n    # Create perturbation probabilities\n    prob_remove = np.zeros(len(weight_lst))\n    prob_add = np.zeros(len(weight_lst))\n\n    if len(included) > 0:\n        # Items to potentially remove: low value-weight ratio and not critical\n        prob_remove[included] = np.exp(-combined_norm[included] * 0.5) * (weight_lst[included] <= capacity * 0.3)\n\n    if len(excluded) > 0:\n        # Items to potentially add: high value-weight ratio and fits capacity\n        prob_add[excluded] = np.tanh(combined_norm[excluded] * 0.3) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    # Apply perturbations\n    for idx in np.where((np.random.rand(len(weight_lst)) < prob_remove) | (np.random.rand(len(weight_lst)) < prob_add))[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Targeted Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined value-weight ratio (ascending)\n        included_sorted = included[np.argsort(combined_norm[included])]\n        # Sort excluded items by combined value-weight ratio (descending)\n        excluded_sorted = excluded[np.argsort(-combined_norm[excluded])]\n\n        # Focus on swapping low-value items with high-value items\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Decide which objective to prioritize\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx] * 1.2:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx] * 1.2:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8315075669485148,
            0.9474883675575256
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: balance between objective values and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n        # Calculate diversity (distance to nearest neighbor)\n        diversity = np.zeros(len(archive))\n        for i in range(len(archive)):\n            distances = np.sum((objectives - objectives[i])**2, axis=1)\n            diversity[i] = np.min(distances[distances > 0] if np.sum(distances > 0) > 0 else [0])\n        # Combine scores\n        scores = 0.6 * (norm_obj[:, 0] + norm_obj[:, 1]) + 0.4 * (diversity / (np.max(diversity) + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate normalized value-weight ratios\n    norm_vw1 = value1_lst / (weight_lst + 1e-10)\n    norm_vw2 = value2_lst / (weight_lst + 1e-10)\n    combined_norm = (norm_vw1 + norm_vw2) / 2\n\n    # Create perturbation probabilities\n    prob_remove = np.zeros(len(weight_lst))\n    prob_add = np.zeros(len(weight_lst))\n\n    if len(included) > 0:\n        # Items to potentially remove: low value-weight ratio and not critical\n        prob_remove[included] = np.exp(-combined_norm[included] * 0.5) * (weight_lst[included] <= capacity * 0.3)\n\n    if len(excluded) > 0:\n        # Items to potentially add: high value-weight ratio and fits capacity\n        prob_add[excluded] = np.tanh(combined_norm[excluded] * 0.3) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    # Apply perturbations\n    for idx in np.where((np.random.rand(len(weight_lst)) < prob_remove) | (np.random.rand(len(weight_lst)) < prob_add))[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Targeted Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined value-weight ratio (ascending)\n        included_sorted = included[np.argsort(combined_norm[included])]\n        # Sort excluded items by combined value-weight ratio (descending)\n        excluded_sorted = excluded[np.argsort(-combined_norm[excluded])]\n\n        # Focus on swapping low-value items with high-value items\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Decide which objective to prioritize\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx] * 1.2:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx] * 1.2:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 6 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by combining objective values and crowding distance, then applies a hybrid local search that probabilistically flips items based on their marginal contributions to both objectives while ensuring feasibility, followed by a guided swap operation that intelligently exchanges items between included and excluded sets to improve both objectives, prioritizing items with higher combined value-to-weight ratios. The approach adaptively adjusts the search based on solution quality and diversity, with critical design elements including marginal contribution-based flipping probabilities and objective-guided swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first selects a promising solution from the archive by combining objective values and crowding distance, then applies a hybrid local search operator that probabilistically flips items based on their marginal contributions in both objectives while ensuring feasibility, followed by a Pareto-guided swap to further improve both objectives. It prioritizes items with high marginal contributions, uses non-linear probabilities for flips, and adaptively adjusts the search based on solution quality and diversity. The method intelligently balances exploration and exploitation by favoring items with significant combined value-to-weight ratios in both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then applies a value-weighted perturbation strategy that probabilistically flips items based on their combined marginal contributions in both objectives, followed by a multi-objective guided insertion to add high-value items while ensuring feasibility. The perturbation prioritizes flipping items with high marginal contributions (included items have higher removal probability, excluded items have higher addition probability), while the insertion focuses on adding items that improve both objectives simultaneously.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided insertion\n    if len(excluded) > 0:\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        # Select top items to try adding\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                # Check if adding this item improves both objectives\n                if value1_lst[add_idx] > 0 and value2_lst[add_idx] > 0:\n                    new_solution[add_idx] = 1\n                    current_weight += weight_lst[add_idx]\n                    break\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by balancing high objective values and diversity (crowding distance), then generates a neighbor solution through a value-weighted perturbation strategy that flips items probabilistically based on their combined marginal contributions, while maintaining feasibility through adaptive capacity checks. It further refines the solution with a multi-objective guided swap operation that intelligently exchanges items between included and excluded sets to improve both objectives. The approach prioritizes items with higher value-to-weight ratios and ensures the neighbor solution remains feasible by carefully adjusting the knapsack capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Value-weighted perturbation strategy\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Select items to flip based on marginal contribution and current inclusion\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Probabilistic flip based on marginal contribution\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded])\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 154,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then generates a neighbor through three phases: (1) a dynamic flip operator that probabilistically toggles items based on their combined value-to-weight ratios and inclusion status, (2) a Pareto-guided swap that prioritizes high-marginal-contribution items to improve both objectives, and (3) a lightweight repair that removes low-marginal-contribution items to restore feasibility. The method balances exploration and exploitation by focusing on items with significant combined marginal contributions while maintaining feasibility through continuous capacity checks and marginal-based removals.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7882380350437681,
            0.3304981291294098
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm implements an adaptive selection mechanism that prioritizes solutions with high objective diversity and younger age, followed by a hybrid local search that uses dynamic perturbation intensity based on Pareto front position to guide additions and removals of items with high combined value-to-weight ratios, while ensuring feasibility through targeted removals of low-value items. The selection scores combine objective diversity (70% weight for value1, 30% for value2) and solution age (inverse of index), and the perturbation intensity scales inversely with the solution's front position. The local search first adds high-ratio items probabilistically, then removes low-ratio items, and finally enforces feasibility by removing the least valuable items if needed.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1) ** 0.5\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    candidates = candidates[np.argsort(combined_ratios[candidates])[::-1]]\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    included_items = included_items[np.argsort(combined_ratios[included_items])]\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive solution selection based on objective diversity and age with a hybrid local search that prioritizes items with high value-to-weight ratios for both objectives, dynamically adjusting perturbation intensity based on the solution's position in the Pareto front while ensuring feasibility through incremental capacity checks and a repair mechanism that removes items with the lowest combined ratios. The selection process favors younger, more diverse solutions, while the local search intelligently adds/removes items with probabilistic guidance and enforces feasibility through targeted removals.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = diversity[0] * diversity[1] / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n\n    # Hybrid local search: guided additions and removals\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1)\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    np.random.shuffle(candidates)\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        combined_ratios = (ratio1 + ratio2)[excess_items]\n        remove_item = excess_items[np.argmin(combined_ratios)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive, applies a flip mutation to a random item, and then attempts to iteratively add or remove items to improve both objectives while ensuring feasibility. It prioritizes flipping a single item first, followed by greedy additions and removals of items that don't worsen either objective. The heuristic balances exploration (random selection and flipping) with exploitation (greedy improvements) to generate diverse, high-quality neighbors.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hybrid local search: Flip a randomly selected item and attempt to add/remove items to improve both objectives\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Randomly flip one item (basic local search)\n    flip_idx = np.random.choice(n_items)\n    new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Ensure feasibility\n    if np.sum(weight_lst * new_solution) > capacity:\n        # If flipping violates capacity, undo the flip\n        new_solution[flip_idx] = base_solution[flip_idx]\n\n    # Step 2: Attempt to add items not in the solution to improve both objectives\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            # Temporarily add the item\n            temp_solution = new_solution.copy()\n            temp_solution[item] = 1\n            temp_weight = np.sum(weight_lst * temp_solution)\n\n            # Check if adding improves both objectives (approximate)\n            if temp_weight <= capacity:\n                # If no objective worsens, keep the change\n                new_solution = temp_solution\n                current_weight = temp_weight\n\n    # Step 3: Attempt to remove items to improve both objectives\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        temp_solution = new_solution.copy()\n        temp_solution[item] = 0\n        temp_weight = np.sum(weight_lst * temp_solution)\n\n        # Check if removing improves both objectives (approximate)\n        if temp_weight <= capacity:\n            # If no objective worsens, keep the change\n            new_solution = temp_solution\n            current_weight = temp_weight\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 152,
        "algorithm": "The algorithm selects a promising solution from an archive using age-weighted diversity scores (prioritizing younger, more diverse solutions), then applies a hybrid local search that first flips high-value items (top 5% combined ratio) with adaptive probability, followed by guided random swaps to improve both objectives while dynamically adjusting perturbation intensity based on recent improvements. It ensures feasibility by incrementally checking capacity constraints and repairing solutions by removing least valuable items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Dynamic perturbation intensity based on solution quality and improvement history\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = min(1.0, 0.5 + 0.5 * (1.0 / (front_pos + 1)))\n\n    # Track improvement history (simplified for this example)\n    improvement_history = [0]  # In a real implementation, this would track recent improvements\n\n    if len(improvement_history) > 1 and improvement_history[-1] > improvement_history[-2]:\n        perturbation_intensity = min(1.0, perturbation_intensity * 1.2)  # Increase intensity if improving\n\n    # Step 1: Flip critical items (top 5% combined ratio) with high probability\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Select top 5% critical items to potentially flip\n    critical_threshold = np.percentile(combined_ratios, 95)\n    critical_items = np.where(combined_ratios >= critical_threshold)[0]\n\n    for item in critical_items:\n        if np.random.rand() < perturbation_intensity * 0.8:  # Higher probability for critical items\n            if new_solution[item] == 1:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n            else:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Step 2: Guided random swaps to improve both objectives\n    for _ in range(int(len(base_solution) * perturbation_intensity)):\n        if np.random.rand() < 0.5:  # 50% chance to add an item\n            candidates = excluded_items[combined_ratios[excluded_items] > np.mean(combined_ratios)]\n            if len(candidates) > 0:\n                item = np.random.choice(candidates)\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    excluded_items = excluded_items[excluded_items != item]\n        else:  # 50% chance to remove an item\n            candidates = included_items[combined_ratios[included_items] < np.mean(combined_ratios)]\n            if len(candidates) > 0:\n                item = np.random.choice(candidates)\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                included_items = included_items[included_items != item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9000751536221656,
            3.1309657990932465
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high objective diversity and younger age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = (diversity[0] * 0.7 + diversity[1] * 0.3) / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratios = (ratio1 + ratio2) / 2\n\n    # Dynamic perturbation intensity based on solution quality and improvement history\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = min(1.0, 0.5 + 0.5 * (1.0 / (front_pos + 1)))\n\n    # Track improvement history (simplified for this example)\n    improvement_history = [0]  # In a real implementation, this would track recent improvements\n\n    if len(improvement_history) > 1 and improvement_history[-1] > improvement_history[-2]:\n        perturbation_intensity = min(1.0, perturbation_intensity * 1.2)  # Increase intensity if improving\n\n    # Step 1: Flip critical items (top 5% combined ratio) with high probability\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Select top 5% critical items to potentially flip\n    critical_threshold = np.percentile(combined_ratios, 95)\n    critical_items = np.where(combined_ratios >= critical_threshold)[0]\n\n    for item in critical_items:\n        if np.random.rand() < perturbation_intensity * 0.8:  # Higher probability for critical items\n            if new_solution[item] == 1:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n            else:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Step 2: Guided random swaps to improve both objectives\n    for _ in range(int(len(base_solution) * perturbation_intensity)):\n        if np.random.rand() < 0.5:  # 50% chance to add an item\n            candidates = excluded_items[combined_ratios[excluded_items] > np.mean(combined_ratios)]\n            if len(candidates) > 0:\n                item = np.random.choice(candidates)\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    excluded_items = excluded_items[excluded_items != item]\n        else:  # 50% chance to remove an item\n            candidates = included_items[combined_ratios[included_items] < np.mean(combined_ratios)]\n            if len(candidates) > 0:\n                item = np.random.choice(candidates)\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                included_items = included_items[included_items != item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        remove_item = excess_items[np.argmin(combined_ratios[excess_items])]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 155,
        "algorithm": "This algorithm selects a high-quality solution from the archive by prioritizing both objective values and diversity (crowding distance), then generates a neighbor through three phases: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The approach balances exploration and exploitation by combining marginal-value-based flips, Pareto-optimal swaps, and random perturbations, ensuring diverse and high-quality neighbor solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check Pareto dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8809383682645143,
            0.3627064824104309
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check Pareto dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (prioritizing high combined normalized objective value), applies a random swap mutation, and then performs a weighted greedy local search to add/remove items, favoring objective 1 (60%) over objective 2 (40%) while ensuring feasibility. The weighted approach balances improvements across objectives, and the random shuffling ensures exploration. The structure combines mutation and greedy search to efficiently explore the neighborhood of the selected solution.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_score = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        score = (v1 + v2) / (np.sum(weight_lst * sol) + 1e-6)  # Normalized by weight\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Randomly swap two items (swap mutation)\n    if n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Greedy addition based on weighted objective improvement\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            # Calculate weighted objective improvement\n            weight_improvement = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Greedy removal based on weighted objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        # Calculate weighted objective loss\n        weight_loss = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n        new_solution[item] = 0\n        current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a younger solution from the archive, prioritizes critical high-value items for flipping with age-based probability, and performs guided probabilistic swaps to improve objective values while maintaining feasibility through incremental weight checks and a repair mechanism that removes low-value items first. It dynamically adjusts perturbation intensity based on solution age and combines value-to-weight ratios for intelligent item selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Age-based selection: prioritize younger solutions (lower index)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)  # Select from younger 25% or last if fewer than 4\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 20):]  # Top 5% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with probability inversely proportional to age\n    flip_prob = 1.0 / (selected_idx + 1)  # Younger solutions have higher flip probability\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.3 / (selected_idx + 1)  # Younger solutions have higher swap probability\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm prioritizes solutions with high variance in objective values, applies a random bit-flip mutation, and performs a dynamic weighted greedy search that alternates between objectives based on their current dominance, ensuring feasibility through adaptive weight adjustments. The selection of solutions is biased toward those with significant differences between the two objectives, while the local search dynamically adjusts weights to balance improvements in either objective, with occasional removals to maintain diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_variance = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        variance = abs(v1 - v2)\n        if variance > max_variance:\n            max_variance = variance\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Random bit-flip mutation\n    flip_index = np.random.randint(n_items)\n    new_solution[flip_index] = 1 - new_solution[flip_index]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic weighted greedy search\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    # Determine objective weights dynamically\n    total_v1 = np.sum(value1_lst * new_solution)\n    total_v2 = np.sum(value2_lst * new_solution)\n    w1 = 0.7 if total_v1 < total_v2 else 0.3\n    w2 = 1 - w1\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            improvement = w1 * value1_lst[item] + w2 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Dynamic weighted greedy removal\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        loss = w1 * value1_lst[item] + w2 * value2_lst[item]\n        if np.random.rand() < 0.3:  # 30% chance to remove to maintain diversity\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a combined score of value dominance and weight utilization, then generates a neighbor by adaptively swapping high-value items with biased probabilities, probabilistically removing low-value items, and finally repairing feasibility by removing items with the smallest normalized marginal value-to-weight ratio. It prioritizes high-value items through value biases and ensures feasibility by guided repair, making it effective for bi-objective knapsack problems.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined value dominance and weight utilization\n    objectives = np.array([obj for _, obj in archive])\n    value_dominance = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    combined_scores = value_dominance[0] * value_dominance[1] * weight_utilization\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate adaptive value biases\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    value_biases = (value1_lst / (total_value1 + 1e-6) + value2_lst / (total_value2 + 1e-6)) / 2\n    value_biases = np.power(value_biases, 1.5)  # Stronger bias towards high-value items\n\n    # Adaptive value-biased swaps\n    swap_prob = 0.4 * (1 - selected_idx / len(archive))  # Higher probability for earlier solutions\n    for i in range(n_items):\n        if np.random.rand() < swap_prob * value_biases[i]:\n            if new_solution[i] == 0:\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n            else:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Probabilistic removal of low-value items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_ranks = np.argsort(value_biases[included_items])\n        for i in range(min(2, len(included_items))):  # Remove up to 2 lowest-value items\n            remove_idx = included_items[value_ranks[i]]\n            if np.random.rand() < 0.6:  # 60% chance to remove\n                new_solution[remove_idx] = 0\n                current_weight -= weight_lst[remove_idx]\n\n    # Guided repair mechanism\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with smallest normalized marginal value-to-weight ratio\n            marginal_ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n            normalized_ratios = marginal_ratios / np.max(marginal_ratios + 1e-6)\n            remove_idx = excess_items[np.argmin(normalized_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm adaptively selects a solution from the archive by prioritizing younger, more diverse solutions (based on objective trade-offs), then applies a hybrid local search that dynamically flips critical items (top 10% by combined value-to-weight ratio) with age-based probabilities and performs guided probabilistic swaps (removing low-value items and adding high-value ones) while ensuring feasibility through incremental weight checks and targeted removals. The method balances exploration (via probabilistic flips) and exploitation (via critical item focus) while maintaining solution quality across both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize younger solutions with high objective diversity\n    selected_idx = min(int(len(archive) * 0.3), len(archive) - 1)  # Select from younger 30%\n    max_diversity = -1\n    for i in range(selected_idx, len(archive)):\n        sol, (v1, v2) = archive[i]\n        diversity = abs(v1 - v2) / (np.sum(weight_lst * sol) + 1e-6)\n        if diversity > max_diversity:\n            max_diversity = diversity\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 10):]  # Top 10% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with age-based probability\n    flip_prob = 0.5 / (selected_idx + 1)\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.4 / (selected_idx + 1)\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n                current_weight = potential_weight\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive solution selection (prioritizing diverse, high-improvement-potential solutions) with a hybrid local search that alternates between critical-item flips (top 10% value-to-weight ratio items), guided swaps (balanced marginal contributions), and dynamic intensity adjustments (increased randomness when stuck). It ensures feasibility through incremental capacity checks and value-to-weight ratio-based repairs, dynamically adjusting perturbation intensity to balance exploration and exploitation. The selection mechanism emphasizes diversity and improvement potential, while the local search prioritizes high-value items while maintaining feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on age-weighted diversity\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Identify critical items (top 10% combined ratio)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 10):]\n\n    # Hybrid perturbation: critical flips, guided swaps, and dynamic intensity\n    for _ in range(3):  # Three perturbation rounds\n        # Critical item flip with probability based on diversity\n        if len(critical_items) > 0 and np.random.rand() < 0.7:\n            item = np.random.choice(critical_items)\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n        # Guided swap: items with balanced marginal contributions\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Find item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Find item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Dynamic intensity adjustment: increase randomness if stuck\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.4:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThis heuristic combines adaptive solution selection with a multi-phase perturbation strategy that prioritizes critical items (top 10% value-to-weight ratio), objective-correlated swaps (guided by value correlation), and segment-based diversification, while strictly maintaining feasibility through weight checks and excess-item removal. The algorithm dynamically balances exploitation (focused flips and swaps) with exploration (random segment flips) and adjusts perturbation intensity based on objective correlation, ensuring both local optimization and global search capabilities.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or recent improvement\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives[i+1, 0] - objectives[i-1, 0]) + (objectives[i+1, 1] - objectives[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 10% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 10):]\n    for item in critical_items:\n        if np.random.rand() < 0.4:  # 40% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.5:  # Strong correlation\n        if np.random.rand() < 0.6:  # 60% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 40% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.median(value1_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.median(value2_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.3:  # 30% chance for segment flip\n        segment_length = np.random.randint(2, min(7, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 153,
        "algorithm": "This algorithm selects a promising solution from the archive using an adaptive metric that combines diversity and improvement potential, then applies a three-phase perturbation strategy: critical-item flips prioritize high-value-to-weight items, objective-balanced swaps ensure multi-objective trade-offs, and segment-based diversification introduces randomness while maintaining feasibility. The method dynamically adjusts perturbation intensity and enforces feasibility through incremental checks, with critical items and multi-objective ratios guiding all operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high diversity and improvement potential\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-balanced swaps\n    if np.random.rand() < 0.7:  # 70% chance for balanced swaps\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Select item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Select item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Phase 3: Segment-based diversification\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Dynamic intensity adjustment\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.3:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.8201746496546032,
            0.2754799723625183
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high diversity and improvement potential\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-balanced swaps\n    if np.random.rand() < 0.7:  # 70% chance for balanced swaps\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Select item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Select item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Phase 3: Segment-based diversification\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Dynamic intensity adjustment\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.3:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis heuristic algorithm first selects a promising solution from the archive by balancing objective values and diversity (crowding distance), then applies a dynamic local search that probabilistically flips items based on their marginal contributions to both objectives, with higher probability for less valuable items and lower probability for more valuable ones. Finally, it performs an enhanced Pareto-guided swap to improve solution quality by prioritizing high-margin items, ensuring feasibility throughout the process. The algorithm adapts search intensity based on solution quality and diversity, with included items given higher priority in the swap phase.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Dynamic flip probability based on solution quality\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.5 + solution_quality * 0.5))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.8 - solution_quality * 0.3))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 156,
        "algorithm": "The algorithm combines marginal contribution analysis with crowding distance to select a high-quality solution from the archive, then applies a three-phase local search: value-balanced perturbation (probabilistically flipping items based on weighted marginal contributions), diversity-guided swap (prioritizing high-marginal items for replacement), and contextual perturbation (random flips weighted by marginal contributions) while ensuring feasibility through capacity checks. The method balances exploration and exploitation by adaptively weighting objective-specific contributions and maintaining solution diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9753773244681556,
            0.5580992698669434
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on high marginal contributions (value-to-weight ratios) and low crowding distance, then applies a hybrid local search combining probabilistic flips and Pareto-guided swaps to generate feasible neighbors that balance both objectives. It prioritizes items with high combined marginal values for flips and performs targeted swaps between included and excluded items that improve both objectives simultaneously. The method ensures feasibility by checking weight constraints at each operation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high marginal contributions and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1])\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0])\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Pareto-guided swap: prioritize items that improve both objectives\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis heuristic algorithm first selects a promising solution from the archive by balancing objective values and diversity (crowding distance), then applies a dynamic local search that probabilistically flips items based on their marginal contributions to both objectives, with higher probability for less valuable items and lower probability for more valuable ones. Finally, it performs an enhanced Pareto-guided swap to improve solution quality by prioritizing high-margin items, ensuring feasibility throughout the process. The algorithm adapts search intensity based on solution quality and diversity, with included items given higher priority in the swap phase.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Dynamic flip probability based on solution quality\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.5 + solution_quality * 0.5))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.8 - solution_quality * 0.3))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 157,
        "algorithm": "The algorithm selects a promising solution from the archive based on combined marginal contributions and crowding distance, then applies a dynamic local search that probabilistically flips items with higher marginal contributions while ensuring feasibility, followed by a value-balancing swap that prioritizes high-marginal items for potential Pareto improvements. The method balances exploration (via probabilistic flips) and exploitation (via targeted swaps) while maintaining feasibility constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic flip probability based on marginal contributions and remaining capacity\n    remaining_capacity = capacity - current_weight\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (0.5 + remaining_capacity / capacity))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (1.0 - remaining_capacity / capacity))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Value-balancing swap: prioritize items with high marginal contributions for both objectives\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending combined marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending combined marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9567621966329822,
            0.39527174830436707
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic flip probability based on marginal contributions and remaining capacity\n    remaining_capacity = capacity - current_weight\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (0.5 + remaining_capacity / capacity))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (1.0 - remaining_capacity / capacity))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Value-balancing swap: prioritize items with high marginal contributions for both objectives\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending combined marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending combined marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects promising solutions from the archive using an age-weighted diversity metric (prioritizing younger, more diverse solutions) and applies a hybrid local search combining critical item flips and guided random swaps, with dynamic perturbation intensity based on search progress. It ensures feasibility through incremental checks and lightweight repair, focusing on high-value items while maintaining balance between exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize solutions with high diversity and younger age\n    current_time = len(archive)\n    selected_idx = 0\n    max_score = -1\n    for i, (sol, obj) in enumerate(archive):\n        diversity = abs(obj[0] - obj[1])\n        age = current_time - i\n        score = (diversity + 1) / (age + 1)\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Dynamic perturbation intensity\n    perturbation_intensity = min(0.3 + (len(archive) / 1000), 1.0)\n\n    # Critical items (top 5% by combined value-to-weight)\n    combined_ratio = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(-combined_ratio)[:max(1, int(len(weight_lst) * 0.05))]\n\n    # Hybrid perturbation\n    for _ in range(2):\n        # Critical item flips\n        if np.random.rand() < perturbation_intensity:\n            for item in critical_items:\n                if np.random.rand() < 0.5:\n                    temp_solution = new_solution.copy()\n                    temp_solution[item] = 1 - temp_solution[item]\n                    if np.sum(temp_solution * weight_lst) <= capacity:\n                        new_solution = temp_solution\n\n        # Guided random swaps (higher-value items)\n        if np.random.rand() < 0.5 * perturbation_intensity:\n            high_value_items = np.argsort(-(value1_lst + value2_lst))[:max(2, int(len(weight_lst) * 0.2))]\n            i, j = np.random.choice(high_value_items, size=2, replace=False)\n            temp_solution = new_solution.copy()\n            temp_solution[i], temp_solution[j] = temp_solution[j], temp_solution[i]\n            if np.sum(temp_solution * weight_lst) <= capacity:\n                new_solution = temp_solution\n\n    # Lightweight repair\n    if np.sum(new_solution * weight_lst) > capacity:\n        excess = np.sum(new_solution * weight_lst) - capacity\n        removable_items = np.where(new_solution == 1)[0]\n        np.random.shuffle(removable_items)\n        for item in removable_items:\n            if excess <= 0:\n                break\n            temp_solution = new_solution.copy()\n            temp_solution[item] = 0\n            excess -= weight_lst[item]\n            if excess >= 0:\n                new_solution = temp_solution\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic algorithm combines dominance-aware selection with crowding distance to choose diverse Pareto-optimal solutions, then applies a multi-phase local search that prioritizes critical items (top 12.5% by combined value-to-weight ratio) with higher refinement probabilities, followed by probabilistic perturbations of non-critical items with adaptive intensity based on recent improvement trends, while ensuring feasibility through a value-to-weight-based repair mechanism. The algorithm balances exploration and exploitation by dynamically adjusting perturbation probabilities and focusing refinement on promising items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining dominance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n\n    # Identify Pareto front solutions\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if len(pareto_front) > 1:\n        # Calculate diversity among Pareto solutions\n        pareto_obj = objectives[pareto_front]\n        normalized_obj = (pareto_obj - pareto_obj.min(axis=0)) / (pareto_obj.max(axis=0) - pareto_obj.min(axis=0) + 1e-10)\n        crowding_dist = np.zeros(len(pareto_front))\n\n        for m in range(2):\n            sorted_idx = np.argsort(normalized_obj[:, m])\n            crowding_dist[sorted_idx[0]] = np.inf\n            crowding_dist[sorted_idx[-1]] = np.inf\n            for i in range(1, len(pareto_front)-1):\n                crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n        selected_idx = pareto_front[np.argmax(crowding_dist)]\n    else:\n        selected_idx = pareto_front[0] if pareto_front else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Multi-phase local search with adaptive intensity\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = (value_to_weight1 + value_to_weight2) * (1 + base_solution)  # Prioritize included items\n\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//8):]  # Top 12.5% items\n\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.7:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Probabilistic perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Adaptive perturbation intensity\n        recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                              for i, j in zip(range(len(archive)), range(1, len(archive)))]\n\n        avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n        perturbation_prob = min(0.7, max(0.1, 0.3 + 0.4 * (1 - avg_improvement)))\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.6:  # Higher probability to remove non-critical items\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 154,
        "algorithm": "The algorithm combines hierarchical selection (prioritizing non-dominated solutions with high value-to-weight balance) with a multi-stage local search: first refining critical items (top 15% by combined value-to-weight ratio) through probabilistic inclusion/exclusion, then applying adaptive guided perturbations to non-critical items based on archive diversity, while ensuring feasibility through a two-phase repair process that prioritizes removing low-value items and redistributing capacity to high-value items. The search intensity dynamically adjusts based on convergence metrics to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hierarchical selection based on dominance rank and value-to-weight balance\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance ranks\n    dominated_counts = np.zeros(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated_counts[i] += 1\n\n    # Calculate value-to-weight balance\n    value_to_weight_balance = np.zeros(len(archive))\n    for i in range(len(archive)):\n        selected_items = solutions[i] == 1\n        if np.any(selected_items):\n            v1_balance = np.mean(value1_lst[selected_items] / weight_lst[selected_items])\n            v2_balance = np.mean(value2_lst[selected_items] / weight_lst[selected_items])\n            value_to_weight_balance[i] = v1_balance * v2_balance\n\n    # Combined score: inverse dominance rank + value-to-weight balance\n    scores = (1 / (dominated_counts + 1)) + value_to_weight_balance\n    selected_idx = np.argmax(scores)\n    base_solution = solutions[selected_idx].copy()\n\n    # Multi-stage local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Stage 1: Critical items refinement (top 15%)\n    combined_value_to_weight = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(-combined_value_to_weight)[:max(1, int(len(weight_lst) * 0.15))]\n\n    for item in critical_items:\n        # Probabilistic inclusion/exclusion based on marginal contribution\n        if new_solution[item] == 0:\n            marginal_v1 = value1_lst[item] if current_weight + weight_lst[item] <= capacity else 0\n            marginal_v2 = value2_lst[item] if current_weight + weight_lst[item] <= capacity else 0\n            if marginal_v1 + marginal_v2 > 0 and np.random.rand() < 0.8:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if np.random.rand() < 0.6:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Stage 2: Guided perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Calculate archive diversity\n        archive_diversity = np.std(objectives, axis=0).mean()\n\n        # Adaptive perturbation probability\n        perturbation_prob = min(0.6, max(0.1, 0.3 + 0.3 * (1 - archive_diversity)))\n\n        for item in non_critical_items:\n            if np.random.rand() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and np.random.rand() < 0.7:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove low-value items first\n        removable_value = (value1_lst + value2_lst)[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_value)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n        # Redistribute capacity to high-value items if possible\n        if excess_weight > 0:\n            available_weight = capacity - np.sum(weight_lst[new_solution == 1])\n            if available_weight > 0:\n                potential_items = np.where(new_solution == 0)[0]\n                high_value_items = potential_items[np.argsort(-(value1_lst + value2_lst)[potential_items])]\n\n                for item in high_value_items:\n                    if available_weight <= 0:\n                        break\n                    if weight_lst[item] <= available_weight:\n                        new_solution[item] = 1\n                        available_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.6360897212188901,
            1.5260004103183746
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hierarchical selection based on dominance rank and value-to-weight balance\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance ranks\n    dominated_counts = np.zeros(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated_counts[i] += 1\n\n    # Calculate value-to-weight balance\n    value_to_weight_balance = np.zeros(len(archive))\n    for i in range(len(archive)):\n        selected_items = solutions[i] == 1\n        if np.any(selected_items):\n            v1_balance = np.mean(value1_lst[selected_items] / weight_lst[selected_items])\n            v2_balance = np.mean(value2_lst[selected_items] / weight_lst[selected_items])\n            value_to_weight_balance[i] = v1_balance * v2_balance\n\n    # Combined score: inverse dominance rank + value-to-weight balance\n    scores = (1 / (dominated_counts + 1)) + value_to_weight_balance\n    selected_idx = np.argmax(scores)\n    base_solution = solutions[selected_idx].copy()\n\n    # Multi-stage local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Stage 1: Critical items refinement (top 15%)\n    combined_value_to_weight = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(-combined_value_to_weight)[:max(1, int(len(weight_lst) * 0.15))]\n\n    for item in critical_items:\n        # Probabilistic inclusion/exclusion based on marginal contribution\n        if new_solution[item] == 0:\n            marginal_v1 = value1_lst[item] if current_weight + weight_lst[item] <= capacity else 0\n            marginal_v2 = value2_lst[item] if current_weight + weight_lst[item] <= capacity else 0\n            if marginal_v1 + marginal_v2 > 0 and np.random.rand() < 0.8:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if np.random.rand() < 0.6:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Stage 2: Guided perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Calculate archive diversity\n        archive_diversity = np.std(objectives, axis=0).mean()\n\n        # Adaptive perturbation probability\n        perturbation_prob = min(0.6, max(0.1, 0.3 + 0.3 * (1 - archive_diversity)))\n\n        for item in non_critical_items:\n            if np.random.rand() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and np.random.rand() < 0.7:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove low-value items first\n        removable_value = (value1_lst + value2_lst)[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_value)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n        # Redistribute capacity to high-value items if possible\n        if excess_weight > 0:\n            available_weight = capacity - np.sum(weight_lst[new_solution == 1])\n            if available_weight > 0:\n                potential_items = np.where(new_solution == 0)[0]\n                high_value_items = potential_items[np.argsort(-(value1_lst + value2_lst)[potential_items])]\n\n                for item in high_value_items:\n                    if available_weight <= 0:\n                        break\n                    if weight_lst[item] <= available_weight:\n                        new_solution[item] = 1\n                        available_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This heuristic algorithm first selects a promising solution from the archive by balancing objective values and diversity (crowding distance), then applies a dynamic local search that probabilistically flips items based on their marginal contributions to both objectives, with higher probability for less valuable items and lower probability for more valuable ones. Finally, it performs an enhanced Pareto-guided swap to improve solution quality by prioritizing high-margin items, ensuring feasibility throughout the process. The algorithm adapts search intensity based on solution quality and diversity, with included items given higher priority in the swap phase.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Dynamic flip probability based on solution quality\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.5 + solution_quality * 0.5))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.8 - solution_quality * 0.3))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 158,
        "algorithm": "The algorithm selects a promising solution from the archive using a multi-criteria approach that prioritizes Pareto dominance (40%), objective values (30% each), and diversity (10%), then applies a hybrid local search that probabilistically flips items based on their marginal contributions (weighted 60%/40% for objectives) and performs dynamic item swaps to improve both objectives while maintaining feasibility. The solution quality dynamically adjusts flip probabilities and swap counts to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Multi-criteria selection: combine Pareto dominance, solution quality, and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    solutions = np.array([sol for (sol, obj) in archive])\n\n    # Compute Pareto dominance ranks\n    dominance = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1] and\n                          (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1])):\n                dominated = True\n                break\n        if not dominated:\n            dominance[i] = 1\n\n    # Compute crowding distance\n    crowding = np.zeros(len(archive))\n    for m in range(2):  # For each objective\n        sorted_indices = np.argsort(objectives[:, m])\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_indices[i]] += (objectives[sorted_indices[i+1], m] - objectives[sorted_indices[i-1], m])\n\n    # Normalize and combine metrics\n    normalized_dominance = (dominance - np.min(dominance)) / (np.max(dominance) - np.min(dominance) + 1e-10)\n    normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n    normalized_objectives = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n\n    scores = (normalized_dominance * 0.4 +\n              normalized_objectives[:, 0] * 0.3 +\n              normalized_objectives[:, 1] * 0.2 +\n              normalized_crowding * 0.1)\n\n    selected_idx = np.argmax(scores)\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate dynamic marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 * 0.6 + marginal2 * 0.4)\n\n    # Adaptive probability model\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n\n    # Higher probability for items with low marginal contribution\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.2 + solution_quality * 0.3))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.7 - solution_quality * 0.2))\n\n    # Perform flips with calculated probabilities\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Hybrid local search: dynamic item swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by decreasing marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded by increasing marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Dynamic swap count based on solution quality\n        swap_count = max(1, int(3 * (1 - solution_quality)))\n\n        for i in range(min(swap_count, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check capacity and Pareto improvement\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if at least one objective improves\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8918843423964581,
            0.7452880442142487
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Multi-criteria selection: combine Pareto dominance, solution quality, and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    solutions = np.array([sol for (sol, obj) in archive])\n\n    # Compute Pareto dominance ranks\n    dominance = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1] and\n                          (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1])):\n                dominated = True\n                break\n        if not dominated:\n            dominance[i] = 1\n\n    # Compute crowding distance\n    crowding = np.zeros(len(archive))\n    for m in range(2):  # For each objective\n        sorted_indices = np.argsort(objectives[:, m])\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_indices[i]] += (objectives[sorted_indices[i+1], m] - objectives[sorted_indices[i-1], m])\n\n    # Normalize and combine metrics\n    normalized_dominance = (dominance - np.min(dominance)) / (np.max(dominance) - np.min(dominance) + 1e-10)\n    normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n    normalized_objectives = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n\n    scores = (normalized_dominance * 0.4 +\n              normalized_objectives[:, 0] * 0.3 +\n              normalized_objectives[:, 1] * 0.2 +\n              normalized_crowding * 0.1)\n\n    selected_idx = np.argmax(scores)\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate dynamic marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 * 0.6 + marginal2 * 0.4)\n\n    # Adaptive probability model\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n\n    # Higher probability for items with low marginal contribution\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.2 + solution_quality * 0.3))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.7 - solution_quality * 0.2))\n\n    # Perform flips with calculated probabilities\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Hybrid local search: dynamic item swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by decreasing marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded by increasing marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Dynamic swap count based on solution quality\n        swap_count = max(1, int(3 * (1 - solution_quality)))\n\n        for i in range(min(swap_count, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check capacity and Pareto improvement\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if at least one objective improves\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This heuristic algorithm first selects a promising solution from the archive by balancing objective values and diversity (crowding distance), then applies a dynamic local search that probabilistically flips items based on their marginal contributions to both objectives, with higher probability for less valuable items and lower probability for more valuable ones. Finally, it performs an enhanced Pareto-guided swap to improve solution quality by prioritizing high-margin items, ensuring feasibility throughout the process. The algorithm adapts search intensity based on solution quality and diversity, with included items given higher priority in the swap phase.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Dynamic flip probability based on solution quality\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.5 + solution_quality * 0.5))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.8 - solution_quality * 0.3))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 159,
        "algorithm": "The algorithm selects a promising solution from the archive by balancing objective values and diversity, then applies an adaptive local search that probabilistically flips items based on their combined marginal contributions to both objectives, with higher probability for items already included (prioritizing high-value items) and lower for excluded items (focusing on potential additions). It then performs a Pareto-guided swap to improve the solution further by replacing low-value included items with high-value excluded ones while ensuring feasibility. The method combines probabilistic exploration with targeted exploitation to balance diversity and solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with balanced objective values and high diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.5 + objectives[:, 1] * 0.5 + normalized_crowding * 0.4\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions with different weights\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.6 + marginal2 * 0.4\n\n    # Adaptive flip probability with different parameters\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.2 + solution_quality * 0.4))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.6 - solution_quality * 0.2))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap with different prioritization\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by value-to-weight ratio (descending)\n        included_sorted = included[np.argsort(-(value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-10))]\n        # Sort excluded items by value-to-weight ratio (ascending)\n        excluded_sorted = excluded[np.argsort((value1_lst[excluded] + value2_lst[excluded]) / (weight_lst[excluded] + 1e-10))]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] * 0.9 or value2_lst[add_idx] > value2_lst[remove_idx] * 0.9):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8974815796616671,
            0.4831301271915436
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with balanced objective values and high diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.5 + objectives[:, 1] * 0.5 + normalized_crowding * 0.4\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions with different weights\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.6 + marginal2 * 0.4\n\n    # Adaptive flip probability with different parameters\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.2 + solution_quality * 0.4))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.6 - solution_quality * 0.2))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap with different prioritization\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by value-to-weight ratio (descending)\n        included_sorted = included[np.argsort(-(value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-10))]\n        # Sort excluded items by value-to-weight ratio (ascending)\n        excluded_sorted = excluded[np.argsort((value1_lst[excluded] + value2_lst[excluded]) / (weight_lst[excluded] + 1e-10))]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] * 0.9 or value2_lst[add_idx] > value2_lst[remove_idx] * 0.9):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 5 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThis heuristic algorithm first selects a promising solution from the archive by balancing objective values and diversity (crowding distance), then applies a dynamic local search that probabilistically flips items based on their marginal contributions to both objectives, with higher probability for less valuable items and lower probability for more valuable ones. Finally, it performs an enhanced Pareto-guided swap to improve solution quality by prioritizing high-margin items, ensuring feasibility throughout the process. The algorithm adapts search intensity based on solution quality and diversity, with included items given higher priority in the swap phase.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Dynamic flip probability based on solution quality\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.5 + solution_quality * 0.5))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.8 - solution_quality * 0.3))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm first selects a promising solution from the archive by combining objective values and crowding distance, then applies a hybrid local search operator that probabilistically flips items based on their marginal contributions in both objectives while ensuring feasibility, followed by a Pareto-guided swap to further improve both objectives. It prioritizes items with high marginal contributions, uses non-linear probabilities for flips, and adaptively adjusts the search based on solution quality and diversity. The method intelligently balances exploration and exploitation by favoring items with significant combined value-to-weight ratios in both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then applies a value-weighted perturbation strategy that probabilistically flips items based on their combined marginal contributions in both objectives, followed by a multi-objective guided insertion to add high-value items while ensuring feasibility. The perturbation prioritizes flipping items with high marginal contributions (included items have higher removal probability, excluded items have higher addition probability), while the insertion focuses on adding items that improve both objectives simultaneously.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided insertion\n    if len(excluded) > 0:\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        # Select top items to try adding\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                # Check if adding this item improves both objectives\n                if value1_lst[add_idx] > 0 and value2_lst[add_idx] > 0:\n                    new_solution[add_idx] = 1\n                    current_weight += weight_lst[add_idx]\n                    break\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by balancing high objective values and diversity (crowding distance), then generates a neighbor solution through a value-weighted perturbation strategy that flips items probabilistically based on their combined marginal contributions, while maintaining feasibility through adaptive capacity checks. It further refines the solution with a multi-objective guided swap operation that intelligently exchanges items between included and excluded sets to improve both objectives. The approach prioritizes items with higher value-to-weight ratios and ensures the neighbor solution remains feasible by carefully adjusting the knapsack capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Value-weighted perturbation strategy\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Select items to flip based on marginal contribution and current inclusion\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Probabilistic flip based on marginal contribution\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded])\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 160,
        "algorithm": "The algorithm selects a promising solution from the archive by balancing high objective values and moderate crowding distance, then applies a hybrid local search combining probabilistic flips (weighted by marginal contributions) and Pareto-guided swaps to generate a feasible neighbor solution, ensuring improvements in both objectives while respecting the weight constraint.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Probabilistic flip based on marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 1.5)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.8)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8591607762382655,
            0.43042343854904175
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Probabilistic flip based on marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 1.5)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.8)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 8 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding distance\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        # Combine objectives and crowding distance\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Adaptive Value-Weighted Perturbations\n    perturbation_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Higher probability for items with low marginal contribution\n    perturbation_prob[included] = 0.5 * (1 - np.tanh(combined_marginal[included] * 0.1))\n    perturbation_prob[excluded] = 0.5 * np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Aware Pareto-Optimal Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is feasible\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Perform swap based on objective dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Dynamic Weight Adjustment for Feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest combined marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then generates a neighbor through three phases: (1) a dynamic flip operator that probabilistically toggles items based on their combined value-to-weight ratios and inclusion status, (2) a Pareto-guided swap that prioritizes high-marginal-contribution items to improve both objectives, and (3) a lightweight repair that removes low-marginal-contribution items to restore feasibility. The method balances exploration and exploitation by focusing on items with significant combined marginal contributions while maintaining feasibility through continuous capacity checks and marginal-based removals.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 161,
        "algorithm": "The algorithm selects a high-quality solution from the archive by combining objective values, crowding distance, and marginal contributions, then generates a neighbor through three phases: a probabilistic flip operator based on combined value-to-weight ratios, a Pareto-guided swap prioritizing high-marginal-contribution items, and a lightweight repair that removes low-marginal-contribution items to maintain feasibility. The method balances exploration and exploitation by focusing on items with significant combined marginal contributions while ensuring feasibility through continuous capacity checks and marginal-based removals.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values, low crowding distance, and high marginal contributions\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-10)\n        marginal2 = value2_lst / (weight_lst + 1e-10)\n        combined_marginal = marginal1 + marginal2\n\n        # Combine objectives, crowding distance, and marginal contributions\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding + np.sum(combined_marginal * archive[0][0])\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions for the selected solution\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8632374992705341,
            0.34268060326576233
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values, low crowding distance, and high marginal contributions\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-10)\n        marginal2 = value2_lst / (weight_lst + 1e-10)\n        combined_marginal = marginal1 + marginal2\n\n        # Combine objectives, crowding distance, and marginal contributions\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding + np.sum(combined_marginal * archive[0][0])\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions for the selected solution\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using adaptive weighted scoring based on diversity, age, and weight utilization, then applies a hybrid local search with critical-item flips (top 5% value-to-weight ratio), objective-correlated swaps, and segment-based diversification, while ensuring feasibility through capacity checks and dynamic adjustments. It prioritizes high-value-to-weight items and balances optimization between objectives based on their correlation, with randomized segment flips and dynamic intensity adjustments for exploration. The solution is always repaired to ensure feasibility if capacity constraints are violated.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive weighted selection\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    age_weights = np.linspace(1.0, 0.5, len(archive))  # Younger solutions have higher weight\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    selection_scores = diversity[0] * diversity[1] * weight_utilization * age_weights\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.6:  # Strong correlation\n        if np.random.rand() < 0.7:  # 70% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_ratio[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_ratio[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 30% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_ratio[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_ratio[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_ratio > np.median(value1_ratio))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_ratio > np.median(value2_ratio))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Dynamic intensity adjustment\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.5:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic algorithm combines dominance-aware selection with crowding distance to choose diverse Pareto-optimal solutions, then applies a multi-phase local search that prioritizes critical items (top 12.5% by combined value-to-weight ratio) with higher refinement probabilities, followed by probabilistic perturbations of non-critical items with adaptive intensity based on recent improvement trends, while ensuring feasibility through a value-to-weight-based repair mechanism. The algorithm balances exploration and exploitation by dynamically adjusting perturbation probabilities and focusing refinement on promising items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining dominance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n\n    # Identify Pareto front solutions\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if len(pareto_front) > 1:\n        # Calculate diversity among Pareto solutions\n        pareto_obj = objectives[pareto_front]\n        normalized_obj = (pareto_obj - pareto_obj.min(axis=0)) / (pareto_obj.max(axis=0) - pareto_obj.min(axis=0) + 1e-10)\n        crowding_dist = np.zeros(len(pareto_front))\n\n        for m in range(2):\n            sorted_idx = np.argsort(normalized_obj[:, m])\n            crowding_dist[sorted_idx[0]] = np.inf\n            crowding_dist[sorted_idx[-1]] = np.inf\n            for i in range(1, len(pareto_front)-1):\n                crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n        selected_idx = pareto_front[np.argmax(crowding_dist)]\n    else:\n        selected_idx = pareto_front[0] if pareto_front else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Multi-phase local search with adaptive intensity\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = (value_to_weight1 + value_to_weight2) * (1 + base_solution)  # Prioritize included items\n\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//8):]  # Top 12.5% items\n\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.7:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Probabilistic perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Adaptive perturbation intensity\n        recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                              for i, j in zip(range(len(archive)), range(1, len(archive)))]\n\n        avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n        perturbation_prob = min(0.7, max(0.1, 0.3 + 0.4 * (1 - avg_improvement)))\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.6:  # Higher probability to remove non-critical items\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 155,
        "algorithm": "The algorithm combines multi-objective selection with adaptive segment-based perturbations, prioritizing solutions with high crowding distance and weight utilization, then applies dynamic segment flips and guided swaps focused on high-value-to-weight items while ensuring feasibility through incremental capacity checks and adaptive intensity adjustments based on recent improvement trends. It balances exploration and exploitation through context-aware segment diversification and prioritizes items with combined value-to-weight ratios in both selection and perturbation phases.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective selection combining crowding distance and weight utilization\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)\n    crowding_dist = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(normalized_obj[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    selection_scores = crowding_dist * weight_utilization\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Step 2: Adaptive segment-based perturbations\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = value_to_weight1 + value_to_weight2\n\n    # Phase 1: High-value segment identification\n    sorted_items = np.argsort(combined_value_to_weight)[::-1]\n    segment_size = max(3, n_items // 10)\n    segment_start = np.random.randint(0, n_items - segment_size)\n    high_value_segment = sorted_items[segment_start:segment_start+segment_size]\n\n    # Phase 2: Segment flip with adaptive intensity\n    recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                          for i, j in zip(range(len(archive)), range(1, len(archive)))]\n    avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n    flip_prob = min(0.8, max(0.2, 0.4 + 0.4 * (1 - avg_improvement)))\n\n    if np.random.rand() < flip_prob:\n        segment_mask = np.zeros(n_items, dtype=bool)\n        segment_mask[high_value_segment] = True\n        new_segment = 1 - new_solution[segment_mask]\n        temp_weight = current_weight - np.sum(weight_lst[segment_mask][new_solution[segment_mask] == 1]) + np.sum(weight_lst[segment_mask][new_segment == 1])\n\n        if temp_weight <= capacity:\n            new_solution[segment_mask] = new_segment\n            current_weight = temp_weight\n        else:\n            # Guided repair: remove lowest value-to-weight items in segment\n            excess_weight = temp_weight - capacity\n            removable_items = high_value_segment[new_segment == 1]\n            removable_ratios = combined_value_to_weight[removable_items]\n            sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n            for item in sorted_removable:\n                if excess_weight <= 0:\n                    break\n                new_solution[item] = 0\n                excess_weight -= weight_lst[item]\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Guided swaps for diversification\n    if np.random.rand() < 0.5:\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Remove item with lowest combined value-to-weight\n            remove_item = included_items[np.argmin(combined_value_to_weight[included_items])]\n            # Add item with highest value-to-weight that fits\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_value_to_weight[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n\n    # Final feasibility check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_value_to_weight[excess_items]\n        sorted_excess = excess_items[np.argsort(excess_ratios)]\n\n        for item in sorted_excess:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.5892132224622462,
            6.09550204873085
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective selection combining crowding distance and weight utilization\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)\n    crowding_dist = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(normalized_obj[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    selection_scores = crowding_dist * weight_utilization\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Step 2: Adaptive segment-based perturbations\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = value_to_weight1 + value_to_weight2\n\n    # Phase 1: High-value segment identification\n    sorted_items = np.argsort(combined_value_to_weight)[::-1]\n    segment_size = max(3, n_items // 10)\n    segment_start = np.random.randint(0, n_items - segment_size)\n    high_value_segment = sorted_items[segment_start:segment_start+segment_size]\n\n    # Phase 2: Segment flip with adaptive intensity\n    recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                          for i, j in zip(range(len(archive)), range(1, len(archive)))]\n    avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n    flip_prob = min(0.8, max(0.2, 0.4 + 0.4 * (1 - avg_improvement)))\n\n    if np.random.rand() < flip_prob:\n        segment_mask = np.zeros(n_items, dtype=bool)\n        segment_mask[high_value_segment] = True\n        new_segment = 1 - new_solution[segment_mask]\n        temp_weight = current_weight - np.sum(weight_lst[segment_mask][new_solution[segment_mask] == 1]) + np.sum(weight_lst[segment_mask][new_segment == 1])\n\n        if temp_weight <= capacity:\n            new_solution[segment_mask] = new_segment\n            current_weight = temp_weight\n        else:\n            # Guided repair: remove lowest value-to-weight items in segment\n            excess_weight = temp_weight - capacity\n            removable_items = high_value_segment[new_segment == 1]\n            removable_ratios = combined_value_to_weight[removable_items]\n            sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n            for item in sorted_removable:\n                if excess_weight <= 0:\n                    break\n                new_solution[item] = 0\n                excess_weight -= weight_lst[item]\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Guided swaps for diversification\n    if np.random.rand() < 0.5:\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Remove item with lowest combined value-to-weight\n            remove_item = included_items[np.argmin(combined_value_to_weight[included_items])]\n            # Add item with highest value-to-weight that fits\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_value_to_weight[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n\n    # Final feasibility check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_value_to_weight[excess_items]\n        sorted_excess = excess_items[np.argsort(excess_ratios)]\n\n        for item in sorted_excess:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines marginal contribution analysis with crowding distance to select a high-quality solution from the archive, then applies a three-phase local search: value-balanced perturbation (probabilistically flipping items based on weighted marginal contributions), diversity-guided swap (prioritizing high-marginal items for replacement), and contextual perturbation (random flips weighted by marginal contributions) while ensuring feasibility through capacity checks. The method balances exploration and exploitation by adaptively weighting objective-specific contributions and maintaining solution diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 162,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Adaptive item replacement based on Pareto dominance and marginal contributions\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate normalized marginal contributions for both objectives\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        # Identify items with highest marginal contributions in both objectives\n        top_marginal1 = np.argsort(-marginal1)[0]\n        top_marginal2 = np.argsort(-marginal2)[0]\n\n        # Perform Pareto-guided replacement\n        if top_marginal1 in included and top_marginal2 in excluded:\n            if (current_weight - weight_lst[top_marginal1] + weight_lst[top_marginal2]) <= capacity:\n                new_solution[top_marginal1] = 0\n                new_solution[top_marginal2] = 1\n                current_weight = current_weight - weight_lst[top_marginal1] + weight_lst[top_marginal2]\n\n    # Phase 2: Objective-specific perturbation\n    for obj in range(2):\n        if np.random.rand() < 0.4:  # 40% chance for each objective\n            if obj == 0:\n                # Perturb objective 1\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                # Perturb objective 2\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            # Select items to flip based on marginal contributions\n            flip_prob = np.zeros(len(weight_lst))\n            flip_prob[included] = marginal[included] * 0.7\n            flip_prob[excluded] = marginal[excluded] * 0.3 * (weight_lst[excluded] <= (capacity - current_weight))\n\n            flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n            for idx in np.where(flip_mask)[0]:\n                if new_solution[idx] == 1:\n                    new_weight = current_weight - weight_lst[idx]\n                    if new_weight >= 0:\n                        new_solution[idx] = 0\n                        current_weight = new_weight\n                else:\n                    new_weight = current_weight + weight_lst[idx]\n                    if new_weight <= capacity:\n                        new_solution[idx] = 1\n                        current_weight = new_weight\n\n    # Phase 3: Context-aware item addition\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        # Calculate combined marginal contributions\n        combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Find best item to add that fits remaining capacity\n        feasible_items = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_items) > 0:\n            best_item = feasible_items[np.argmax(combined_marginal[feasible_items])]\n            new_solution[best_item] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.32959314218314617,
            2.058982878923416
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Adaptive item replacement based on Pareto dominance and marginal contributions\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate normalized marginal contributions for both objectives\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        # Identify items with highest marginal contributions in both objectives\n        top_marginal1 = np.argsort(-marginal1)[0]\n        top_marginal2 = np.argsort(-marginal2)[0]\n\n        # Perform Pareto-guided replacement\n        if top_marginal1 in included and top_marginal2 in excluded:\n            if (current_weight - weight_lst[top_marginal1] + weight_lst[top_marginal2]) <= capacity:\n                new_solution[top_marginal1] = 0\n                new_solution[top_marginal2] = 1\n                current_weight = current_weight - weight_lst[top_marginal1] + weight_lst[top_marginal2]\n\n    # Phase 2: Objective-specific perturbation\n    for obj in range(2):\n        if np.random.rand() < 0.4:  # 40% chance for each objective\n            if obj == 0:\n                # Perturb objective 1\n                marginal = value1_lst / (weight_lst + 1e-6)\n            else:\n                # Perturb objective 2\n                marginal = value2_lst / (weight_lst + 1e-6)\n\n            # Select items to flip based on marginal contributions\n            flip_prob = np.zeros(len(weight_lst))\n            flip_prob[included] = marginal[included] * 0.7\n            flip_prob[excluded] = marginal[excluded] * 0.3 * (weight_lst[excluded] <= (capacity - current_weight))\n\n            flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n            for idx in np.where(flip_mask)[0]:\n                if new_solution[idx] == 1:\n                    new_weight = current_weight - weight_lst[idx]\n                    if new_weight >= 0:\n                        new_solution[idx] = 0\n                        current_weight = new_weight\n                else:\n                    new_weight = current_weight + weight_lst[idx]\n                    if new_weight <= capacity:\n                        new_solution[idx] = 1\n                        current_weight = new_weight\n\n    # Phase 3: Context-aware item addition\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        # Calculate combined marginal contributions\n        combined_marginal = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Find best item to add that fits remaining capacity\n        feasible_items = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_items) > 0:\n            best_item = feasible_items[np.argmax(combined_marginal[feasible_items])]\n            new_solution[best_item] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding distance\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        # Combine objectives and crowding distance\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Adaptive Value-Weighted Perturbations\n    perturbation_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Higher probability for items with low marginal contribution\n    perturbation_prob[included] = 0.5 * (1 - np.tanh(combined_marginal[included] * 0.1))\n    perturbation_prob[excluded] = 0.5 * np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Aware Pareto-Optimal Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is feasible\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Perform swap based on objective dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Dynamic Weight Adjustment for Feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest combined marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines marginal contribution analysis with crowding distance to select a high-quality solution from the archive, then applies a three-phase local search: value-balanced perturbation (probabilistically flipping items based on weighted marginal contributions), diversity-guided swap (prioritizing high-marginal items for replacement), and contextual perturbation (random flips weighted by marginal contributions) while ensuring feasibility through capacity checks. The method balances exploration and exploitation by adaptively weighting objective-specific contributions and maintaining solution diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 163,
        "algorithm": "The algorithm combines marginal contribution analysis with adaptive objective weighting to select a solution from the archive, then applies a three-phase local search: 1) probabilistic flips weighted by marginal contributions and objective-specific trade-offs, 2) Pareto-guided swaps prioritizing high-marginal items, and 3) dynamic capacity adjustment through iterative removal of low-contribution items, while maintaining feasibility. The method balances exploration and exploitation by adaptively weighting objectives and ensuring solution diversity through marginal contribution checks and Pareto dominance comparisons.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate objective-weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    weighted_marginal = (marginal1 * 0.7 + marginal2 * 0.3) * (1 + np.random.rand(len(weight_lst)) * 0.15)\n\n    # Phase 1: Adaptive Objective-Weighted Flips\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-weighted_marginal[included] * 0.6)\n    flip_prob[excluded] = np.exp(-weighted_marginal[excluded] * 0.4) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Dominance Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-weighted_marginal[included])]\n        excluded_sorted = excluded[np.argsort(weighted_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                # Check Pareto dominance\n                if (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] >= value2_lst[remove_idx] and value1_lst[add_idx] > value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Dynamic Capacity Adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest weighted marginal contribution\n        remove_idx = candidates[np.argmin(weighted_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7971185209090591,
            0.4324529469013214
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate objective-weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    weighted_marginal = (marginal1 * 0.7 + marginal2 * 0.3) * (1 + np.random.rand(len(weight_lst)) * 0.15)\n\n    # Phase 1: Adaptive Objective-Weighted Flips\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-weighted_marginal[included] * 0.6)\n    flip_prob[excluded] = np.exp(-weighted_marginal[excluded] * 0.4) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Dominance Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-weighted_marginal[included])]\n        excluded_sorted = excluded[np.argsort(weighted_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                # Check Pareto dominance\n                if (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] >= value2_lst[remove_idx] and value1_lst[add_idx] > value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Dynamic Capacity Adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest weighted marginal contribution\n        remove_idx = candidates[np.argmin(weighted_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines marginal contribution analysis with crowding distance to select a high-quality solution from the archive, then applies a three-phase local search: value-balanced perturbation (probabilistically flipping items based on weighted marginal contributions), diversity-guided swap (prioritizing high-marginal items for replacement), and contextual perturbation (random flips weighted by marginal contributions) while ensuring feasibility through capacity checks. The method balances exploration and exploitation by adaptively weighting objective-specific contributions and maintaining solution diversity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 164,
        "algorithm": "This heuristic algorithm selects a promising solution from the archive using a hybrid of dominance analysis and crowding distance, then applies a four-phase local search combining value-weighted random flips, objective-balanced swaps, marginal contribution-based perturbations, and a novel dynamic objective weighting adjustment phase to generate high-quality neighbor solutions while ensuring feasibility. The algorithm prioritizes items with high combined marginal contributions for both objectives, dynamically adjusts objective weights based on the current solution's structure, and uses probabilistic flips to explore the solution space efficiently.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Hybrid selection: combine dominance and crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Dominance-based selection\n        dominance_scores = np.zeros(len(archive))\n        for i in range(len(archive)):\n            dominated_by = 0\n            for j in range(len(archive)):\n                if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1]):\n                    dominated_by += 1\n            dominance_scores[i] = 1 / (1 + dominated_by)\n\n        # Crowding distance\n        sorted_indices1 = np.argsort(objectives[:, 0])\n        sorted_indices2 = np.argsort(objectives[:, 1])\n        crowding = np.zeros(len(archive))\n        for i in range(len(archive)):\n            if i == 0 or i == len(archive) - 1:\n                crowding[i] = float('inf')\n            else:\n                crowding[i] = (objectives[sorted_indices1[i+1], 0] - objectives[sorted_indices1[i-1], 0]) + \\\n                              (objectives[sorted_indices2[i+1], 1] - objectives[sorted_indices2[i-1], 1])\n\n        # Combine scores\n        scores = dominance_scores * (1 + crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Value-weighted random flips phase\n    flip_prob = combined_marginal * (1 - np.random.rand(len(weight_lst)) * 0.3)\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Objective-balanced swap phase\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual dominance phase\n    num_flips = max(1, int(0.15 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=combined_marginal/np.sum(combined_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Novel: Dynamic objective weighting adjustment\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0:\n        current_ratio = np.sum(value1_lst[included]) / (np.sum(value2_lst[included]) + 1e-10)\n        weight1 = 1 / (1 + current_ratio)\n        weight2 = 1 - weight1\n\n        # Recalculate marginal contributions with dynamic weights\n        dynamic_marginal = (weight1 * marginal1 + weight2 * marginal2) * (1 + np.random.rand(len(weight_lst)) * 0.15)\n\n        # Final perturbation with dynamic weights\n        final_flip_prob = dynamic_marginal * (1 - np.random.rand(len(weight_lst)) * 0.2)\n        final_flip_mask = np.random.rand(len(weight_lst)) < final_flip_prob\n        for idx in np.where(final_flip_mask)[0]:\n            if new_solution[idx] == 1:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n            else:\n                if weight_lst[idx] <= remaining_capacity:\n                    new_solution[idx] = 1\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.4441965486056035,
            0.8087809383869171
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Hybrid selection: combine dominance and crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Dominance-based selection\n        dominance_scores = np.zeros(len(archive))\n        for i in range(len(archive)):\n            dominated_by = 0\n            for j in range(len(archive)):\n                if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1]):\n                    dominated_by += 1\n            dominance_scores[i] = 1 / (1 + dominated_by)\n\n        # Crowding distance\n        sorted_indices1 = np.argsort(objectives[:, 0])\n        sorted_indices2 = np.argsort(objectives[:, 1])\n        crowding = np.zeros(len(archive))\n        for i in range(len(archive)):\n            if i == 0 or i == len(archive) - 1:\n                crowding[i] = float('inf')\n            else:\n                crowding[i] = (objectives[sorted_indices1[i+1], 0] - objectives[sorted_indices1[i-1], 0]) + \\\n                              (objectives[sorted_indices2[i+1], 1] - objectives[sorted_indices2[i-1], 1])\n\n        # Combine scores\n        scores = dominance_scores * (1 + crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Value-weighted random flips phase\n    flip_prob = combined_marginal * (1 - np.random.rand(len(weight_lst)) * 0.3)\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Objective-balanced swap phase\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual dominance phase\n    num_flips = max(1, int(0.15 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=combined_marginal/np.sum(combined_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Novel: Dynamic objective weighting adjustment\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0:\n        current_ratio = np.sum(value1_lst[included]) / (np.sum(value2_lst[included]) + 1e-10)\n        weight1 = 1 / (1 + current_ratio)\n        weight2 = 1 - weight1\n\n        # Recalculate marginal contributions with dynamic weights\n        dynamic_marginal = (weight1 * marginal1 + weight2 * marginal2) * (1 + np.random.rand(len(weight_lst)) * 0.15)\n\n        # Final perturbation with dynamic weights\n        final_flip_prob = dynamic_marginal * (1 - np.random.rand(len(weight_lst)) * 0.2)\n        final_flip_mask = np.random.rand(len(weight_lst)) < final_flip_prob\n        for idx in np.where(final_flip_mask)[0]:\n            if new_solution[idx] == 1:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n            else:\n                if weight_lst[idx] <= remaining_capacity:\n                    new_solution[idx] = 1\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic algorithm combines dominance-aware selection with crowding distance to choose diverse Pareto-optimal solutions, then applies a multi-phase local search that prioritizes critical items (top 12.5% by combined value-to-weight ratio) with higher refinement probabilities, followed by probabilistic perturbations of non-critical items with adaptive intensity based on recent improvement trends, while ensuring feasibility through a value-to-weight-based repair mechanism. The algorithm balances exploration and exploitation by dynamically adjusting perturbation probabilities and focusing refinement on promising items.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining dominance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n\n    # Identify Pareto front solutions\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if len(pareto_front) > 1:\n        # Calculate diversity among Pareto solutions\n        pareto_obj = objectives[pareto_front]\n        normalized_obj = (pareto_obj - pareto_obj.min(axis=0)) / (pareto_obj.max(axis=0) - pareto_obj.min(axis=0) + 1e-10)\n        crowding_dist = np.zeros(len(pareto_front))\n\n        for m in range(2):\n            sorted_idx = np.argsort(normalized_obj[:, m])\n            crowding_dist[sorted_idx[0]] = np.inf\n            crowding_dist[sorted_idx[-1]] = np.inf\n            for i in range(1, len(pareto_front)-1):\n                crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n        selected_idx = pareto_front[np.argmax(crowding_dist)]\n    else:\n        selected_idx = pareto_front[0] if pareto_front else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Multi-phase local search with adaptive intensity\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = (value_to_weight1 + value_to_weight2) * (1 + base_solution)  # Prioritize included items\n\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//8):]  # Top 12.5% items\n\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.7:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Probabilistic perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Adaptive perturbation intensity\n        recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                              for i, j in zip(range(len(archive)), range(1, len(archive)))]\n\n        avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n        perturbation_prob = min(0.7, max(0.1, 0.3 + 0.4 * (1 - avg_improvement)))\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.6:  # Higher probability to remove non-critical items\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 156,
        "algorithm": "The algorithm selects a promising solution from the archive by combining dominance ranking with structural diversity, then applies a three-stage local search that prioritizes high-marginal-contribution items for both objectives, followed by balanced reallocation and structure-preserving perturbations while ensuring feasibility through dynamic repair. The method intelligently balances objective-specific refinement, combined contribution analysis, and controlled randomness to explore high-quality neighbor solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Enhanced selection combining dominance and structural diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance ranks\n    dominance_ranks = np.zeros(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominance_ranks[i] += 1\n\n    # Calculate structural diversity metric\n    structural_diversity = np.zeros(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                structural_diversity[i] += np.sum(solutions[i] != solutions[j])\n\n    # Combine metrics for selection\n    combined_scores = (1 - dominance_ranks / len(archive)) + (structural_diversity / (len(archive) * len(weight_lst)))\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Three-stage local search with marginal contribution analysis\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Stage 1: Objective-specific refinement\n    for obj_idx in [0, 1]:\n        value_lst = value1_lst if obj_idx == 0 else value2_lst\n        marginal_contributions = (value_lst / weight_lst) * (1 + base_solution)\n\n        # Select top 20% items by marginal contribution\n        threshold = np.percentile(marginal_contributions, 80)\n        high_contrib_items = np.where(marginal_contributions >= threshold)[0]\n\n        for item in high_contrib_items:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1 and random.random() < 0.8:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Stage 2: Balanced reallocation\n    combined_contrib = (value1_lst + value2_lst) / weight_lst\n    sorted_items = np.argsort(combined_contrib)[::-1]\n\n    for item in sorted_items:\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            # Check if adding improves both objectives\n            if (np.sum(value1_lst[new_solution == 1] + value1_lst[item]) > np.sum(value1_lst[new_solution == 1]) and\n                np.sum(value2_lst[new_solution == 1] + value2_lst[item]) > np.sum(value2_lst[new_solution == 1])):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        elif new_solution[item] == 1 and random.random() < 0.3:\n            # Check if removal maintains both objectives\n            if (np.sum(value1_lst[new_solution == 1]) > np.sum(value1_lst[new_solution == 1]) - value1_lst[item] and\n                np.sum(value2_lst[new_solution == 1]) > np.sum(value2_lst[new_solution == 1]) - value2_lst[item]):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Stage 3: Structure-preserving perturbations\n    if random.random() < 0.5:\n        # Swap items between high and low contribution groups\n        high_contrib = np.where(combined_contrib >= np.median(combined_contrib))[0]\n        low_contrib = np.where(combined_contrib < np.median(combined_contrib))[0]\n\n        if len(high_contrib) > 0 and len(low_contrib) > 0:\n            h_item = np.random.choice(high_contrib)\n            l_item = np.random.choice(low_contrib)\n\n            if new_solution[h_item] == 1 and new_solution[l_item] == 0:\n                if current_weight - weight_lst[h_item] + weight_lst[l_item] <= capacity:\n                    new_solution[h_item], new_solution[l_item] = 0, 1\n\n    # Step 3: Dynamic feasibility repair\n    if current_weight > capacity:\n        excess = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        if len(removable_items) > 0:\n            # Remove items with lowest combined contribution first\n            contrib_ratios = combined_contrib[removable_items]\n            sorted_removable = removable_items[np.argsort(contrib_ratios)]\n\n            for item in sorted_removable:\n                if excess <= 0:\n                    break\n                new_solution[item] = 0\n                excess -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.5122057188108269,
            4.3021979331970215
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Enhanced selection combining dominance and structural diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance ranks\n    dominance_ranks = np.zeros(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominance_ranks[i] += 1\n\n    # Calculate structural diversity metric\n    structural_diversity = np.zeros(len(archive))\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                structural_diversity[i] += np.sum(solutions[i] != solutions[j])\n\n    # Combine metrics for selection\n    combined_scores = (1 - dominance_ranks / len(archive)) + (structural_diversity / (len(archive) * len(weight_lst)))\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = solutions[selected_idx].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Three-stage local search with marginal contribution analysis\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Stage 1: Objective-specific refinement\n    for obj_idx in [0, 1]:\n        value_lst = value1_lst if obj_idx == 0 else value2_lst\n        marginal_contributions = (value_lst / weight_lst) * (1 + base_solution)\n\n        # Select top 20% items by marginal contribution\n        threshold = np.percentile(marginal_contributions, 80)\n        high_contrib_items = np.where(marginal_contributions >= threshold)[0]\n\n        for item in high_contrib_items:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1 and random.random() < 0.8:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Stage 2: Balanced reallocation\n    combined_contrib = (value1_lst + value2_lst) / weight_lst\n    sorted_items = np.argsort(combined_contrib)[::-1]\n\n    for item in sorted_items:\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            # Check if adding improves both objectives\n            if (np.sum(value1_lst[new_solution == 1] + value1_lst[item]) > np.sum(value1_lst[new_solution == 1]) and\n                np.sum(value2_lst[new_solution == 1] + value2_lst[item]) > np.sum(value2_lst[new_solution == 1])):\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        elif new_solution[item] == 1 and random.random() < 0.3:\n            # Check if removal maintains both objectives\n            if (np.sum(value1_lst[new_solution == 1]) > np.sum(value1_lst[new_solution == 1]) - value1_lst[item] and\n                np.sum(value2_lst[new_solution == 1]) > np.sum(value2_lst[new_solution == 1]) - value2_lst[item]):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Stage 3: Structure-preserving perturbations\n    if random.random() < 0.5:\n        # Swap items between high and low contribution groups\n        high_contrib = np.where(combined_contrib >= np.median(combined_contrib))[0]\n        low_contrib = np.where(combined_contrib < np.median(combined_contrib))[0]\n\n        if len(high_contrib) > 0 and len(low_contrib) > 0:\n            h_item = np.random.choice(high_contrib)\n            l_item = np.random.choice(low_contrib)\n\n            if new_solution[h_item] == 1 and new_solution[l_item] == 0:\n                if current_weight - weight_lst[h_item] + weight_lst[l_item] <= capacity:\n                    new_solution[h_item], new_solution[l_item] = 0, 1\n\n    # Step 3: Dynamic feasibility repair\n    if current_weight > capacity:\n        excess = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        if len(removable_items) > 0:\n            # Remove items with lowest combined contribution first\n            contrib_ratios = combined_contrib[removable_items]\n            sorted_removable = removable_items[np.argsort(contrib_ratios)]\n\n            for item in sorted_removable:\n                if excess <= 0:\n                    break\n                new_solution[item] = 0\n                excess -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines marginal contribution analysis with crowding distance to select a high-quality solution from the archive, then applies a three-phase local search: value-balanced perturbation (probabilistically flipping items based on weighted marginal contributions), diversity-guided swap (prioritizing high-marginal items for replacement), and contextual perturbation (random flips weighted by marginal contributions) while ensuring feasibility through capacity checks. The method balances exploration and exploitation by adaptively weighting objective-specific contributions and maintaining solution diversity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 165,
        "algorithm": "The algorithm selects a high-potential solution from the archive by evaluating marginal contributions with adaptive objective weighting, then applies a four-phase local search: objective-weighted flips prioritizing high-marginal items, diversity-aware swaps between complementary items, capacity-aware random flips, and balance-guided refinements to improve objective balance while ensuring feasibility. The selection prioritizes solutions with strong marginal contributions, and the local search phases systematically explore the solution space while maintaining feasibility through capacity checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and adaptive objective weighting\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute adaptive weights based on objective diversity\n        obj_range1 = np.max(objectives[:, 0]) - np.min(objectives[:, 0])\n        obj_range2 = np.max(objectives[:, 1]) - np.min(objectives[:, 1])\n        total_range = obj_range1 + obj_range2 + 1e-10\n        weight1 = obj_range1 / total_range\n        weight2 = obj_range2 / total_range\n\n        # Calculate marginal contributions with adaptive weights\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = weight1 * marginal1 + weight2 * marginal2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate dynamically weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    dynamic_weight = 0.5 + 0.5 * np.random.rand()\n    weighted_marginal = (marginal1 * dynamic_weight + marginal2 * (1 - dynamic_weight)) * (1 + np.random.rand(len(weight_lst)) * 0.3)\n\n    # Objective-weighted perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-weighted_marginal[included] * 0.7)\n    flip_prob[excluded] = np.exp(-weighted_marginal[excluded] * 0.4) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-aware swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by weighted marginal, excluded by inverse weighted marginal\n        included_sorted = included[np.argsort(-weighted_marginal[included])]\n        excluded_sorted = excluded[np.argsort(weighted_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Capacity-aware perturbation phase\n    capacity_ratio = remaining_capacity / capacity\n    num_flips = max(1, int(0.15 * len(weight_lst) * capacity_ratio))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=weighted_marginal/np.sum(weighted_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Balance-guided refinement phase\n    current_val1 = np.sum(value1_lst[new_solution == 1])\n    current_val2 = np.sum(value2_lst[new_solution == 1])\n    balance_factor = abs(current_val1 - current_val2) / (current_val1 + current_val2 + 1e-10)\n\n    if balance_factor > 0.2:\n        # Adjust to improve balance\n        if current_val1 > current_val2:\n            # Try to increase value2\n            candidates = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n            if len(candidates) > 0:\n                best_candidate = candidates[np.argmax(value2_lst[candidates] / (weight_lst[candidates] + 1e-10))]\n                new_solution[best_candidate] = 1\n                remaining_capacity -= weight_lst[best_candidate]\n        else:\n            # Try to increase value1\n            candidates = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n            if len(candidates) > 0:\n                best_candidate = candidates[np.argmax(value1_lst[candidates] / (weight_lst[candidates] + 1e-10))]\n                new_solution[best_candidate] = 1\n                remaining_capacity -= weight_lst[best_candidate]\n\n    return new_solution\n\n",
        "score": [
            -0.7906108066362907,
            0.5899360179901123
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and adaptive objective weighting\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute adaptive weights based on objective diversity\n        obj_range1 = np.max(objectives[:, 0]) - np.min(objectives[:, 0])\n        obj_range2 = np.max(objectives[:, 1]) - np.min(objectives[:, 1])\n        total_range = obj_range1 + obj_range2 + 1e-10\n        weight1 = obj_range1 / total_range\n        weight2 = obj_range2 / total_range\n\n        # Calculate marginal contributions with adaptive weights\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = weight1 * marginal1 + weight2 * marginal2\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate dynamically weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    dynamic_weight = 0.5 + 0.5 * np.random.rand()\n    weighted_marginal = (marginal1 * dynamic_weight + marginal2 * (1 - dynamic_weight)) * (1 + np.random.rand(len(weight_lst)) * 0.3)\n\n    # Objective-weighted perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-weighted_marginal[included] * 0.7)\n    flip_prob[excluded] = np.exp(-weighted_marginal[excluded] * 0.4) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-aware swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by weighted marginal, excluded by inverse weighted marginal\n        included_sorted = included[np.argsort(-weighted_marginal[included])]\n        excluded_sorted = excluded[np.argsort(weighted_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Capacity-aware perturbation phase\n    capacity_ratio = remaining_capacity / capacity\n    num_flips = max(1, int(0.15 * len(weight_lst) * capacity_ratio))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=weighted_marginal/np.sum(weighted_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Balance-guided refinement phase\n    current_val1 = np.sum(value1_lst[new_solution == 1])\n    current_val2 = np.sum(value2_lst[new_solution == 1])\n    balance_factor = abs(current_val1 - current_val2) / (current_val1 + current_val2 + 1e-10)\n\n    if balance_factor > 0.2:\n        # Adjust to improve balance\n        if current_val1 > current_val2:\n            # Try to increase value2\n            candidates = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n            if len(candidates) > 0:\n                best_candidate = candidates[np.argmax(value2_lst[candidates] / (weight_lst[candidates] + 1e-10))]\n                new_solution[best_candidate] = 1\n                remaining_capacity -= weight_lst[best_candidate]\n        else:\n            # Try to increase value1\n            candidates = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n            if len(candidates) > 0:\n                best_candidate = candidates[np.argmax(value1_lst[candidates] / (weight_lst[candidates] + 1e-10))]\n                new_solution[best_candidate] = 1\n                remaining_capacity -= weight_lst[best_candidate]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic algorithm combines dominance-aware selection with crowding distance to choose diverse Pareto-optimal solutions, then applies a multi-phase local search that prioritizes critical items (top 12.5% by combined value-to-weight ratio) with higher refinement probabilities, followed by probabilistic perturbations of non-critical items with adaptive intensity based on recent improvement trends, while ensuring feasibility through a value-to-weight-based repair mechanism. The algorithm balances exploration and exploitation by dynamically adjusting perturbation probabilities and focusing refinement on promising items.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining dominance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n\n    # Identify Pareto front solutions\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if len(pareto_front) > 1:\n        # Calculate diversity among Pareto solutions\n        pareto_obj = objectives[pareto_front]\n        normalized_obj = (pareto_obj - pareto_obj.min(axis=0)) / (pareto_obj.max(axis=0) - pareto_obj.min(axis=0) + 1e-10)\n        crowding_dist = np.zeros(len(pareto_front))\n\n        for m in range(2):\n            sorted_idx = np.argsort(normalized_obj[:, m])\n            crowding_dist[sorted_idx[0]] = np.inf\n            crowding_dist[sorted_idx[-1]] = np.inf\n            for i in range(1, len(pareto_front)-1):\n                crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n        selected_idx = pareto_front[np.argmax(crowding_dist)]\n    else:\n        selected_idx = pareto_front[0] if pareto_front else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Multi-phase local search with adaptive intensity\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = (value_to_weight1 + value_to_weight2) * (1 + base_solution)  # Prioritize included items\n\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//8):]  # Top 12.5% items\n\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.7:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Probabilistic perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Adaptive perturbation intensity\n        recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                              for i, j in zip(range(len(archive)), range(1, len(archive)))]\n\n        avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n        perturbation_prob = min(0.7, max(0.1, 0.3 + 0.4 * (1 - avg_improvement)))\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.6:  # Higher probability to remove non-critical items\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 157,
        "algorithm": "The algorithm combines a hybrid selection metric (60% balance score, 40% crowding distance) to choose a base solution, then applies a marginal-contribution-driven local search with dynamic probabilities (higher for top items) to refine it, followed by probabilistic diversification (higher for less-balanced solutions) and a value-to-weight prioritized feasibility repair. The key innovations include adaptive probabilities in refinement and diversification, and prioritization based on combined marginal contributions for both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining balance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)\n\n    # Calculate balance score (closer to 1 is more balanced)\n    balance_scores = 1 - np.abs(normalized_obj[:, 0] - normalized_obj[:, 1])\n\n    # Calculate crowding distance\n    crowding_dist = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(normalized_obj[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n    # Combined selection metric\n    selection_metric = balance_scores * 0.6 + crowding_dist * 0.4\n    selected_idx = np.argmax(selection_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Marginal contribution-based local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * (1 + base_solution) + marginal2 * (1 + base_solution)\n\n    # Prioritize items with high marginal contributions\n    prioritized_items = np.argsort(combined_marginal)[::-1]\n\n    # Dynamic refinement probabilities\n    refinement_probs = np.linspace(0.9, 0.3, len(prioritized_items))\n\n    for i, item in enumerate(prioritized_items):\n        if random.random() < refinement_probs[i]:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1 and random.random() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Step 3: Probabilistic diversification\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Diversification probability based on solution balance\n        solution_balance = abs(normalized_obj[selected_idx, 0] - normalized_obj[selected_idx, 1])\n        div_prob = min(0.5, max(0.1, 0.3 + 0.4 * solution_balance))\n\n        for item in excluded_items:\n            if random.random() < div_prob and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Feasibility repair with value-to-weight prioritization\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest combined marginal contribution first\n        removable_marginals = combined_marginal[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_marginals)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.37437584121056466,
            0.5554092526435852
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining balance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)\n\n    # Calculate balance score (closer to 1 is more balanced)\n    balance_scores = 1 - np.abs(normalized_obj[:, 0] - normalized_obj[:, 1])\n\n    # Calculate crowding distance\n    crowding_dist = np.zeros(len(archive))\n    for m in range(2):\n        sorted_idx = np.argsort(normalized_obj[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n    # Combined selection metric\n    selection_metric = balance_scores * 0.6 + crowding_dist * 0.4\n    selected_idx = np.argmax(selection_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Marginal contribution-based local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * (1 + base_solution) + marginal2 * (1 + base_solution)\n\n    # Prioritize items with high marginal contributions\n    prioritized_items = np.argsort(combined_marginal)[::-1]\n\n    # Dynamic refinement probabilities\n    refinement_probs = np.linspace(0.9, 0.3, len(prioritized_items))\n\n    for i, item in enumerate(prioritized_items):\n        if random.random() < refinement_probs[i]:\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1 and random.random() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Step 3: Probabilistic diversification\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Diversification probability based on solution balance\n        solution_balance = abs(normalized_obj[selected_idx, 0] - normalized_obj[selected_idx, 1])\n        div_prob = min(0.5, max(0.1, 0.3 + 0.4 * solution_balance))\n\n        for item in excluded_items:\n            if random.random() < div_prob and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Step 4: Feasibility repair with value-to-weight prioritization\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest combined marginal contribution first\n        removable_marginals = combined_marginal[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_marginals)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 6 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines marginal contribution analysis with crowding distance to select a high-quality solution from the archive, then applies a three-phase local search: value-balanced perturbation (probabilistically flipping items based on weighted marginal contributions), diversity-guided swap (prioritizing high-marginal items for replacement), and contextual perturbation (random flips weighted by marginal contributions) while ensuring feasibility through capacity checks. The method balances exploration and exploitation by adaptively weighting objective-specific contributions and maintaining solution diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate value-balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Value-balanced perturbation phase\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Diversity-guided swap phase\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Contextual perturbation phase\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic algorithm first selects a promising solution from the archive by balancing objective values and diversity (crowding distance), then applies a dynamic local search that probabilistically flips items based on their marginal contributions to both objectives, with higher probability for less valuable items and lower probability for more valuable ones. Finally, it performs an enhanced Pareto-guided swap to improve solution quality by prioritizing high-margin items, ensuring feasibility throughout the process. The algorithm adapts search intensity based on solution quality and diversity, with included items given higher priority in the swap phase.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] * 0.6 + objectives[:, 1] * 0.4 - normalized_crowding * 0.3\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 * 0.7 + marginal2 * 0.3\n\n    # Dynamic flip probability based on solution quality\n    solution_quality = (objectives[selected_idx][0] + objectives[selected_idx][1]) / (np.sum(value1_lst) + np.sum(value2_lst))\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1] * (1.5 + solution_quality * 0.5))\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0] * (0.8 - solution_quality * 0.3))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Enhanced Pareto-guided swap\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first selects a promising solution from the archive by combining objective values and crowding distance, then applies a hybrid local search operator that probabilistically flips items based on their marginal contributions in both objectives while ensuring feasibility, followed by a Pareto-guided swap to further improve both objectives. It prioritizes items with high marginal contributions, uses non-linear probabilities for flips, and adaptively adjusts the search based on solution quality and diversity. The method intelligently balances exploration and exploitation by favoring items with significant combined value-to-weight ratios in both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then applies a value-weighted perturbation strategy that probabilistically flips items based on their combined marginal contributions in both objectives, followed by a multi-objective guided insertion to add high-value items while ensuring feasibility. The perturbation prioritizes flipping items with high marginal contributions (included items have higher removal probability, excluded items have higher addition probability), while the insertion focuses on adding items that improve both objectives simultaneously.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Probabilistic flip based on non-linear marginal contribution and inclusion status\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 2)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.5)\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided insertion\n    if len(excluded) > 0:\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        # Select top items to try adding\n        for i in range(min(3, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if (current_weight + weight_lst[add_idx]) <= capacity:\n                # Check if adding this item improves both objectives\n                if value1_lst[add_idx] > 0 and value2_lst[add_idx] > 0:\n                    new_solution[add_idx] = 1\n                    current_weight += weight_lst[add_idx]\n                    break\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by balancing high objective values and diversity (crowding distance), then generates a neighbor solution through a value-weighted perturbation strategy that flips items probabilistically based on their combined marginal contributions, while maintaining feasibility through adaptive capacity checks. It further refines the solution with a multi-objective guided swap operation that intelligently exchanges items between included and excluded sets to improve both objectives. The approach prioritizes items with higher value-to-weight ratios and ensures the neighbor solution remains feasible by carefully adjusting the knapsack capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Value-weighted perturbation strategy\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Select items to flip based on marginal contribution and current inclusion\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Probabilistic flip based on marginal contribution\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded])\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Multi-objective guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 166,
        "algorithm": "The algorithm selects a promising solution from the archive by balancing high marginal contributions (0.6*value1 + 0.4*value2) and moderate crowding distance, then applies a hybrid local search that includes probabilistic flips, Pareto-guided swaps, diversity-guided insertions, and contextual perturbations to generate feasible neighbor solutions while prioritizing balanced objective improvements. The method ensures feasibility through capacity checks and adaptively adjusts selection probabilities based on item inclusion status and marginal contributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Probabilistic flip based on balanced marginal and inclusion status\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Diversity-guided insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-balanced_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if weight_lst[add_idx] <= remaining_capacity:\n                new_solution[add_idx] = 1\n                remaining_capacity -= weight_lst[add_idx]\n                break\n\n    # Contextual perturbation\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9163465577483625,
            0.6627281904220581
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined marginal contributions and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with marginal contributions\n        marginal1 = np.sum(value1_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        marginal2 = np.sum(value2_lst[archive[0][0] == 1]) / (np.sum(weight_lst[archive[0][0] == 1]) + 1e-10)\n        scores = marginal1 + marginal2 - (crowding / np.max(crowding + 1e-10))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate balanced marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    balanced_marginal = (marginal1 * 0.6 + marginal2 * 0.4) * (1 + np.random.rand(len(weight_lst)) * 0.2)\n\n    # Probabilistic flip based on balanced marginal and inclusion status\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-balanced_marginal[included] * 0.8)\n    flip_prob[excluded] = np.exp(-balanced_marginal[excluded] * 0.3) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-balanced_marginal[included])]\n        excluded_sorted = excluded[np.argsort(balanced_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Diversity-guided insertion\n    if len(excluded) > 0:\n        excluded_sorted = excluded[np.argsort(-balanced_marginal[excluded])]\n        for i in range(min(2, len(excluded_sorted))):\n            add_idx = excluded_sorted[i]\n            if weight_lst[add_idx] <= remaining_capacity:\n                new_solution[add_idx] = 1\n                remaining_capacity -= weight_lst[add_idx]\n                break\n\n    # Contextual perturbation\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False, p=balanced_marginal/np.sum(balanced_marginal))\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe heuristic algorithm combines dominance-aware selection with crowding distance to choose diverse Pareto-optimal solutions, then applies a multi-phase local search that prioritizes critical items (top 12.5% by combined value-to-weight ratio) with higher refinement probabilities, followed by probabilistic perturbations of non-critical items with adaptive intensity based on recent improvement trends, while ensuring feasibility through a value-to-weight-based repair mechanism. The algorithm balances exploration and exploitation by dynamically adjusting perturbation probabilities and focusing refinement on promising items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection strategy combining dominance and diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n\n    # Identify Pareto front solutions\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if len(pareto_front) > 1:\n        # Calculate diversity among Pareto solutions\n        pareto_obj = objectives[pareto_front]\n        normalized_obj = (pareto_obj - pareto_obj.min(axis=0)) / (pareto_obj.max(axis=0) - pareto_obj.min(axis=0) + 1e-10)\n        crowding_dist = np.zeros(len(pareto_front))\n\n        for m in range(2):\n            sorted_idx = np.argsort(normalized_obj[:, m])\n            crowding_dist[sorted_idx[0]] = np.inf\n            crowding_dist[sorted_idx[-1]] = np.inf\n            for i in range(1, len(pareto_front)-1):\n                crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n        selected_idx = pareto_front[np.argmax(crowding_dist)]\n    else:\n        selected_idx = pareto_front[0] if pareto_front else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Multi-phase local search with adaptive intensity\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = (value_to_weight1 + value_to_weight2) * (1 + base_solution)  # Prioritize included items\n\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//8):]  # Top 12.5% items\n\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.7:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Probabilistic perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Adaptive perturbation intensity\n        recent_improvements = [abs(objectives[i, 0] - objectives[j, 0]) + abs(objectives[i, 1] - objectives[j, 1])\n                              for i, j in zip(range(len(archive)), range(1, len(archive)))]\n\n        avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):]) if len(recent_improvements) > 0 else 0\n        perturbation_prob = min(0.7, max(0.1, 0.3 + 0.4 * (1 - avg_improvement)))\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.6:  # Higher probability to remove non-critical items\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 3: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm selects a diverse solution from the archive using crowding distance to prioritize under-explored regions, then applies a hybrid local search that flips critical items (top 10% value-to-weight ratio) with adaptive random swaps for non-critical items, dynamically adjusting perturbation intensity based on recent improvements while ensuring feasibility through incremental weight checks. The method balances exploration and exploitation by prioritizing high-value items and probabilistically perturbing others, with critical items receiving higher attention than non-critical ones.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a diverse solution using crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)\n\n    # Calculate crowding distance\n    crowding_dist = np.zeros(len(archive))\n    for m in range(2):  # For each objective\n        sorted_idx = np.argsort(normalized_obj[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (normalized_obj[sorted_idx[i+1], m] - normalized_obj[sorted_idx[i-1], m])\n\n    # Select solution with highest crowding distance (most diverse)\n    selected_idx = np.argmax(crowding_dist)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate value-to-weight ratios and identify critical items\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = value_to_weight1 + value_to_weight2\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//10):]  # Top 10% items\n\n    # Step 3: Apply hybrid local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Flip critical items (with higher probability for non-included items)\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Step 4: Adaptive random swaps (more aggressive if recent improvements are small)\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Determine swap intensity based on recent improvement (simplified)\n        swap_prob = 0.5 if len(archive) < 5 else 0.3  # More exploration in early stages\n\n        for item in non_critical_items:\n            if random.random() < swap_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Ensure feasibility (in case of any errors)\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        np.random.shuffle(excess_items)\n        for item in excess_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive, applies a flip mutation to a random item, and then attempts to iteratively add or remove items to improve both objectives while ensuring feasibility. It prioritizes flipping a single item first, followed by greedy additions and removals of items that don't worsen either objective. The heuristic balances exploration (random selection and flipping) with exploitation (greedy improvements) to generate diverse, high-quality neighbors.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hybrid local search: Flip a randomly selected item and attempt to add/remove items to improve both objectives\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Randomly flip one item (basic local search)\n    flip_idx = np.random.choice(n_items)\n    new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n    # Ensure feasibility\n    if np.sum(weight_lst * new_solution) > capacity:\n        # If flipping violates capacity, undo the flip\n        new_solution[flip_idx] = base_solution[flip_idx]\n\n    # Step 2: Attempt to add items not in the solution to improve both objectives\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            # Temporarily add the item\n            temp_solution = new_solution.copy()\n            temp_solution[item] = 1\n            temp_weight = np.sum(weight_lst * temp_solution)\n\n            # Check if adding improves both objectives (approximate)\n            if temp_weight <= capacity:\n                # If no objective worsens, keep the change\n                new_solution = temp_solution\n                current_weight = temp_weight\n\n    # Step 3: Attempt to remove items to improve both objectives\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        temp_solution = new_solution.copy()\n        temp_solution[item] = 0\n        temp_weight = np.sum(weight_lst * temp_solution)\n\n        # Check if removing improves both objectives (approximate)\n        if temp_weight <= capacity:\n            # If no objective worsens, keep the change\n            new_solution = temp_solution\n            current_weight = temp_weight\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in your requested format:\n\n**Keywords**:\n- **Adaptive selection**: Age-weighted diversity to prioritize unexplored solutions.\n- **Hybrid perturbations**: Critical-item flips (top 5% value-to-weight) + guided random swaps.\n- **Incremental feasibility**: Capacity-aware swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement/stagnation.\n\n**Advice**:\n1. **Select**: Use age-weighted diversity to bias selection toward unexplored regions.\n2. **Perturb**: Flip critical items and perform guided swaps to balance exploitation/exploration.\n3. **Feasibility**: Ensure capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n**Avoid**:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical-item flips alone.\n- Fixed perturbation intensity.\n\n**Explanation**:\nThis heuristic balances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring feasibility incrementally. Dynamic intensity prevents premature convergence, while incremental checks avoid overhead. It escapes local optima by integrating context-aware strategies.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 158,
        "algorithm": "The algorithm selects a solution from the archive using age-weighted diversity to prioritize less explored regions, then applies a multi-phase local search that first refines critical items (top 5% by combined value-to-weight ratio) with higher probability flips, followed by adaptive probabilistic perturbations of non-critical items, while ensuring feasibility through incremental weight checks and value-to-weight-based repair. Critical items are given higher attention, and perturbation intensity is dynamically adjusted based on recent improvement trends to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Age-weighted diversity selection\n    objectives = np.array([obj for _, obj in archive])\n    ages = np.array([i for i in range(len(archive))])  # Age is inversely proportional to index\n    diversity_scores = np.zeros(len(archive))\n\n    for m in range(2):  # For each objective\n        sorted_idx = np.argsort(objectives[:, m])\n        for i in range(len(archive)):\n            if i == 0 or i == len(archive)-1:\n                diversity_scores[i] += np.inf  # Boundary solutions are most diverse\n            else:\n                diversity_scores[i] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m])\n\n    # Combine diversity and age (older solutions get higher weight)\n    combined_scores = diversity_scores * (1 + 1/(1 + ages))\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate value-to-weight ratios and identify critical items\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = value_to_weight1 + value_to_weight2\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//20):]  # Top 5% items\n\n    # Step 3: Multi-phase local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.8:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Adaptive perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Calculate perturbation intensity based on recent improvements\n        if len(archive) > 1:\n            recent_improvements = [np.sum(np.abs(objectives[i] - objectives[i-1])) for i in range(1, len(archive))]\n            avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):])\n            perturbation_prob = min(0.8, max(0.2, 0.5 + 0.3 * (1 - avg_improvement)))\n        else:\n            perturbation_prob = 0.5\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.7:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 4: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.45189533395889103,
            0.5567282438278198
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Age-weighted diversity selection\n    objectives = np.array([obj for _, obj in archive])\n    ages = np.array([i for i in range(len(archive))])  # Age is inversely proportional to index\n    diversity_scores = np.zeros(len(archive))\n\n    for m in range(2):  # For each objective\n        sorted_idx = np.argsort(objectives[:, m])\n        for i in range(len(archive)):\n            if i == 0 or i == len(archive)-1:\n                diversity_scores[i] += np.inf  # Boundary solutions are most diverse\n            else:\n                diversity_scores[i] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m])\n\n    # Combine diversity and age (older solutions get higher weight)\n    combined_scores = diversity_scores * (1 + 1/(1 + ages))\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate value-to-weight ratios and identify critical items\n    value_to_weight1 = value1_lst / weight_lst\n    value_to_weight2 = value2_lst / weight_lst\n    combined_value_to_weight = value_to_weight1 + value_to_weight2\n    critical_items = np.argsort(combined_value_to_weight)[-max(3, len(weight_lst)//20):]  # Top 5% items\n\n    # Step 3: Multi-phase local search\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Critical items refinement\n    for item in critical_items:\n        if new_solution[item] == 0:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n        else:\n            if random.random() < 0.8:  # Higher probability to remove critical items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Adaptive perturbation of non-critical items\n    non_critical_items = np.setdiff1d(np.arange(len(weight_lst)), critical_items)\n    if len(non_critical_items) > 0:\n        # Calculate perturbation intensity based on recent improvements\n        if len(archive) > 1:\n            recent_improvements = [np.sum(np.abs(objectives[i] - objectives[i-1])) for i in range(1, len(archive))]\n            avg_improvement = np.mean(recent_improvements[-min(5, len(recent_improvements)):])\n            perturbation_prob = min(0.8, max(0.2, 0.5 + 0.3 * (1 - avg_improvement)))\n        else:\n            perturbation_prob = 0.5\n\n        for item in non_critical_items:\n            if random.random() < perturbation_prob:\n                if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                elif new_solution[item] == 1 and random.random() < 0.7:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Step 4: Feasibility repair\n    if current_weight > capacity:\n        excess_weight = current_weight - capacity\n        removable_items = np.where(new_solution == 1)[0]\n\n        # Remove items with lowest value-to-weight ratio first\n        removable_ratios = combined_value_to_weight[removable_items]\n        sorted_removable = removable_items[np.argsort(removable_ratios)]\n\n        for item in sorted_removable:\n            if excess_weight <= 0:\n                break\n            new_solution[item] = 0\n            excess_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 8 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding distance\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        # Combine objectives and crowding distance\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Adaptive Value-Weighted Perturbations\n    perturbation_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Higher probability for items with low marginal contribution\n    perturbation_prob[included] = 0.5 * (1 - np.tanh(combined_marginal[included] * 0.1))\n    perturbation_prob[excluded] = 0.5 * np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Aware Pareto-Optimal Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is feasible\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Perform swap based on objective dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Dynamic Weight Adjustment for Feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest combined marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then generates a neighbor through three phases: (1) a dynamic flip operator that probabilistically toggles items based on their combined value-to-weight ratios and inclusion status, (2) a Pareto-guided swap that prioritizes high-marginal-contribution items to improve both objectives, and (3) a lightweight repair that removes low-marginal-contribution items to restore feasibility. The method balances exploration and exploitation by focusing on items with significant combined marginal contributions while maintaining feasibility through continuous capacity checks and marginal-based removals.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 167,
        "algorithm": "The algorithm combines a hybrid selection mechanism (prioritizing high-value, crowded, and high-marginal-contribution solutions) with a three-phase local search: probabilistic flips based on marginal contributions, Pareto-guided swaps, and adaptive random perturbations, all while ensuring feasibility. It emphasizes marginal contributions (value/weight ratios) for guiding item selection and uses dynamic probabilities to balance exploration/exploitation. The solution is refined through iterative improvements while maintaining capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: combine objectives, crowding distance, and marginal contributions\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-10)\n        marginal2 = value2_lst / (weight_lst + 1e-10)\n        combined_marginal = marginal1 + marginal2\n\n        # Combine scores\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n        for i, (sol, _) in enumerate(archive):\n            included = np.where(sol == 1)[0]\n            if len(included) > 0:\n                scores[i] += np.sum(combined_marginal[included]) * 0.5\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive Perturbation with Dynamic Threshold\n    num_perturbations = max(1, int(0.1 * len(weight_lst)))\n    perturbation_indices = np.random.choice(len(weight_lst), num_perturbations, replace=False)\n\n    for idx in perturbation_indices:\n        if new_solution[idx] == 1:\n            if np.random.rand() < 0.3 and (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if np.random.rand() < 0.7 and (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7746790713589394,
            0.6763870716094971
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: combine objectives, crowding distance, and marginal contributions\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n\n        # Calculate marginal contributions\n        marginal1 = value1_lst / (weight_lst + 1e-10)\n        marginal2 = value2_lst / (weight_lst + 1e-10)\n        combined_marginal = marginal1 + marginal2\n\n        # Combine scores\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n        for i, (sol, _) in enumerate(archive):\n            included = np.where(sol == 1)[0]\n            if len(included) > 0:\n                scores[i] += np.sum(combined_marginal[included]) * 0.5\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive Perturbation with Dynamic Threshold\n    num_perturbations = max(1, int(0.1 * len(weight_lst)))\n    perturbation_indices = np.random.choice(len(weight_lst), num_perturbations, replace=False)\n\n    for idx in perturbation_indices:\n        if new_solution[idx] == 1:\n            if np.random.rand() < 0.3 and (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if np.random.rand() < 0.7 and (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (prioritizing high combined normalized objective value), applies a random swap mutation, and then performs a weighted greedy local search to add/remove items, favoring objective 1 (60%) over objective 2 (40%) while ensuring feasibility. The weighted approach balances improvements across objectives, and the random shuffling ensures exploration. The structure combines mutation and greedy search to efficiently explore the neighborhood of the selected solution.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_score = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        score = (v1 + v2) / (np.sum(weight_lst * sol) + 1e-6)  # Normalized by weight\n        if score > max_score:\n            max_score = score\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Randomly swap two items (swap mutation)\n    if n_items >= 2:\n        swap_indices = np.random.choice(n_items, 2, replace=False)\n        new_solution[swap_indices[0]], new_solution[swap_indices[1]] = new_solution[swap_indices[1]], new_solution[swap_indices[0]]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Greedy addition based on weighted objective improvement\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            # Calculate weighted objective improvement\n            weight_improvement = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Greedy removal based on weighted objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        # Calculate weighted objective loss\n        weight_loss = 0.6 * value1_lst[item] + 0.4 * value2_lst[item]\n        new_solution[item] = 0\n        current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a younger solution from the archive, prioritizes critical high-value items for flipping with age-based probability, and performs guided probabilistic swaps to improve objective values while maintaining feasibility through incremental weight checks and a repair mechanism that removes low-value items first. It dynamically adjusts perturbation intensity based on solution age and combines value-to-weight ratios for intelligent item selection.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Age-based selection: prioritize younger solutions (lower index)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)  # Select from younger 25% or last if fewer than 4\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 20):]  # Top 5% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with probability inversely proportional to age\n    flip_prob = 1.0 / (selected_idx + 1)  # Younger solutions have higher flip probability\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.3 / (selected_idx + 1)  # Younger solutions have higher swap probability\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm prioritizes solutions with high variance in objective values, applies a random bit-flip mutation, and performs a dynamic weighted greedy search that alternates between objectives based on their current dominance, ensuring feasibility through adaptive weight adjustments. The selection of solutions is biased toward those with significant differences between the two objectives, while the local search dynamically adjusts weights to balance improvements in either objective, with occasional removals to maintain diversity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    max_variance = -1\n    selected_idx = 0\n    for i, (sol, (v1, v2)) in enumerate(archive):\n        variance = abs(v1 - v2)\n        if variance > max_variance:\n            max_variance = variance\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n_items = len(base_solution)\n\n    # Step 1: Random bit-flip mutation\n    flip_index = np.random.randint(n_items)\n    new_solution[flip_index] = 1 - new_solution[flip_index]\n\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Dynamic weighted greedy search\n    remaining_items = np.where(new_solution == 0)[0]\n    np.random.shuffle(remaining_items)\n\n    # Determine objective weights dynamically\n    total_v1 = np.sum(value1_lst * new_solution)\n    total_v2 = np.sum(value2_lst * new_solution)\n    w1 = 0.7 if total_v1 < total_v2 else 0.3\n    w2 = 1 - w1\n\n    for item in remaining_items:\n        if current_weight + weight_lst[item] <= capacity:\n            improvement = w1 * value1_lst[item] + w2 * value2_lst[item]\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Step 3: Dynamic weighted greedy removal\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n\n    for item in included_items:\n        loss = w1 * value1_lst[item] + w2 * value2_lst[item]\n        if np.random.rand() < 0.3:  # 30% chance to remove to maintain diversity\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a combined score of value dominance and weight utilization, then generates a neighbor by adaptively swapping high-value items with biased probabilities, probabilistically removing low-value items, and finally repairing feasibility by removing items with the smallest normalized marginal value-to-weight ratio. It prioritizes high-value items through value biases and ensures feasibility by guided repair, making it effective for bi-objective knapsack problems.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined value dominance and weight utilization\n    objectives = np.array([obj for _, obj in archive])\n    value_dominance = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    weight_utilization = np.array([np.sum(weight_lst[sol[0] == 1]) / capacity for sol in archive])\n    combined_scores = value_dominance[0] * value_dominance[1] * weight_utilization\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate adaptive value biases\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    value_biases = (value1_lst / (total_value1 + 1e-6) + value2_lst / (total_value2 + 1e-6)) / 2\n    value_biases = np.power(value_biases, 1.5)  # Stronger bias towards high-value items\n\n    # Adaptive value-biased swaps\n    swap_prob = 0.4 * (1 - selected_idx / len(archive))  # Higher probability for earlier solutions\n    for i in range(n_items):\n        if np.random.rand() < swap_prob * value_biases[i]:\n            if new_solution[i] == 0:\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n            else:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Probabilistic removal of low-value items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_ranks = np.argsort(value_biases[included_items])\n        for i in range(min(2, len(included_items))):  # Remove up to 2 lowest-value items\n            remove_idx = included_items[value_ranks[i]]\n            if np.random.rand() < 0.6:  # 60% chance to remove\n                new_solution[remove_idx] = 0\n                current_weight -= weight_lst[remove_idx]\n\n    # Guided repair mechanism\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with smallest normalized marginal value-to-weight ratio\n            marginal_ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n            normalized_ratios = marginal_ratios / np.max(marginal_ratios + 1e-6)\n            remove_idx = excess_items[np.argmin(normalized_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm adaptively selects a solution from the archive by prioritizing younger, more diverse solutions (based on objective trade-offs), then applies a hybrid local search that dynamically flips critical items (top 10% by combined value-to-weight ratio) with age-based probabilities and performs guided probabilistic swaps (removing low-value items and adding high-value ones) while ensuring feasibility through incremental weight checks and targeted removals. The method balances exploration (via probabilistic flips) and exploitation (via critical item focus) while maintaining solution quality across both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection: prioritize younger solutions with high objective diversity\n    selected_idx = min(int(len(archive) * 0.3), len(archive) - 1)  # Select from younger 30%\n    max_diversity = -1\n    for i in range(selected_idx, len(archive)):\n        sol, (v1, v2) = archive[i]\n        diversity = abs(v1 - v2) / (np.sum(weight_lst * sol) + 1e-6)\n        if diversity > max_diversity:\n            max_diversity = diversity\n            selected_idx = i\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate combined value-to-weight ratios\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, len(weight_lst) // 10):]  # Top 10% items\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip critical items with age-based probability\n    flip_prob = 0.5 / (selected_idx + 1)\n    for item in critical_items:\n        if np.random.rand() < flip_prob:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n            else:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Guided probabilistic swap: higher probability for high-value items\n    swap_prob = 0.4 / (selected_idx + 1)\n    if np.random.rand() < swap_prob and len(np.where(new_solution == 1)[0]) > 0:\n        in_items = np.where(new_solution == 1)[0]\n        out_items = np.where(new_solution == 0)[0]\n\n        if len(in_items) > 0 and len(out_items) > 0:\n            # Select swap-out item (lowest value-to-weight in current solution)\n            swap_out = in_items[np.argmin(combined_ratios[in_items])]\n            # Select swap-in item (highest value-to-weight not in solution)\n            swap_in = out_items[np.argmax(combined_ratios[out_items])]\n\n            potential_weight = current_weight - weight_lst[swap_out] + weight_lst[swap_in]\n            if potential_weight <= capacity:\n                new_solution[swap_out] = 0\n                new_solution[swap_in] = 1\n                current_weight = potential_weight\n\n    # Ensure feasibility: remove low-value items if needed\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        excess_ratios = combined_ratios[excess_items]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(excess_ratios)]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            excess_items = np.where(new_solution == 1)[0]\n            excess_ratios = combined_ratios[excess_items]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive solution selection (prioritizing diverse, high-improvement-potential solutions) with a hybrid local search that alternates between critical-item flips (top 10% value-to-weight ratio items), guided swaps (balanced marginal contributions), and dynamic intensity adjustments (increased randomness when stuck). It ensures feasibility through incremental capacity checks and value-to-weight ratio-based repairs, dynamically adjusting perturbation intensity to balance exploration and exploitation. The selection mechanism emphasizes diversity and improvement potential, while the local search prioritizes high-value items while maintaining feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on age-weighted diversity\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.std(objectives, axis=0)\n    improvement_potential = np.max(objectives, axis=0) - objectives\n    selection_scores = diversity[0] * diversity[1] * np.sum(improvement_potential, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Calculate value-to-weight ratios for both objectives\n    value1_ratio = value1_lst / weight_lst\n    value2_ratio = value2_lst / weight_lst\n    combined_ratio = value1_ratio + value2_ratio\n\n    # Identify critical items (top 10% combined ratio)\n    critical_items = np.argsort(combined_ratio)[-max(1, n_items // 10):]\n\n    # Hybrid perturbation: critical flips, guided swaps, and dynamic intensity\n    for _ in range(3):  # Three perturbation rounds\n        # Critical item flip with probability based on diversity\n        if len(critical_items) > 0 and np.random.rand() < 0.7:\n            item = np.random.choice(critical_items)\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n        # Guided swap: items with balanced marginal contributions\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Find item to remove (lowest marginal combined ratio)\n            remove_item = included_items[np.argmin(combined_ratio[included_items])]\n            # Find item to add (highest marginal combined ratio among feasible candidates)\n            feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n            if len(feasible_add) > 0:\n                add_item = feasible_add[np.argmax(combined_ratio[feasible_add])]\n                new_solution[remove_item] = 0\n                new_solution[add_item] = 1\n                current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n\n    # Dynamic intensity adjustment: increase randomness if stuck\n    if selected_idx == np.argmax(selection_scores) and np.random.rand() < 0.4:\n        # Random bit-flip mutation with capacity check\n        flip_index = np.random.randint(n_items)\n        if new_solution[flip_index] == 0 and current_weight + weight_lst[flip_index] <= capacity:\n            new_solution[flip_index] = 1\n        elif new_solution[flip_index] == 1:\n            new_solution[flip_index] = 0\n\n    # Incremental feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            # Remove item with lowest combined value-to-weight ratio\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThis heuristic combines adaptive solution selection with a multi-phase perturbation strategy that prioritizes critical items (top 10% value-to-weight ratio), objective-correlated swaps (guided by value correlation), and segment-based diversification, while strictly maintaining feasibility through weight checks and excess-item removal. The algorithm dynamically balances exploitation (focused flips and swaps) with exploration (random segment flips) and adjusts perturbation intensity based on objective correlation, ensuring both local optimization and global search capabilities.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or recent improvement\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives[i+1, 0] - objectives[i-1, 0]) + (objectives[i+1, 1] - objectives[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 10% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 10):]\n    for item in critical_items:\n        if np.random.rand() < 0.4:  # 40% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.5:  # Strong correlation\n        if np.random.rand() < 0.6:  # 60% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 40% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.median(value1_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.median(value2_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.3:  # 30% chance for segment flip\n        segment_length = np.random.randint(2, min(7, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### Keywords:\n- **Adaptive selection**: Crowding-distance + age-based diversity.\n- **Hybrid perturbations**: Critical flips (top 5% value-to-weight) + guided swaps.\n- **Incremental feasibility**: Capacity checks during swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement trends.\n\n### Advice:\n1. **Select**: Bias toward solutions with high crowding distance and younger age.\n2. **Perturb**: Flip critical items and swap high-value items with guided randomness.\n3. **Feasibility**: Enforce capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n### Avoid:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical flips alone.\n- Fixed perturbation intensity.\n\n### Explanation:\nBalances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring incremental feasibility. Dynamic intensity prevents premature convergence, while context-aware strategies escape local optima.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 159,
        "algorithm": "The algorithm implements an adaptive multi-phase heuristic that first selects a promising solution from the archive based on crowding distance or objective correlation, then applies critical-item flips, objective-correlated swaps, and adaptive segment flips to generate a neighbor while ensuring feasibility through incremental weight checks. It prioritizes high-value-to-weight items (critical items) and dynamically adjusts optimization focus based on objective correlation, with segment-based diversification to escape local optima. The algorithm balances exploration (random segment flips) and exploitation (targeted swaps) while strictly maintaining knapsack capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or strong objective correlation\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives_sorted = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives_sorted[i+1, 0] - objectives_sorted[i-1, 0]) + (objectives_sorted[i+1, 1] - objectives_sorted[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.7:  # Strong correlation\n        if np.random.rand() < 0.7:  # 70% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 30% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.percentile(value1_lst, 75))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.percentile(value2_lst, 75))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.6910069008655083,
            0.5712176561355591
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or strong objective correlation\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives_sorted = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives_sorted[i+1, 0] - objectives_sorted[i-1, 0]) + (objectives_sorted[i+1, 1] - objectives_sorted[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 5% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 20):]\n    for item in critical_items:\n        if np.random.rand() < 0.5:  # 50% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.7:  # Strong correlation\n        if np.random.rand() < 0.7:  # 70% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 30% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.percentile(value1_lst, 75))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.percentile(value2_lst, 75))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.4:  # 40% chance for segment flip\n        segment_length = np.random.randint(2, min(5, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic combines adaptive solution selection with a multi-phase perturbation strategy that prioritizes critical items (top 10% value-to-weight ratio), objective-correlated swaps (guided by value correlation), and segment-based diversification, while strictly maintaining feasibility through weight checks and excess-item removal. The algorithm dynamically balances exploitation (focused flips and swaps) with exploration (random segment flips) and adjusts perturbation intensity based on objective correlation, ensuring both local optimization and global search capabilities.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest crowding distance or recent improvement\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives[i+1, 0] - objectives[i-1, 0]) + (objectives[i+1, 1] - objectives[i-1, 1])\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Critical-item flips (top 10% value-to-weight)\n    combined_ratios = (value1_lst + value2_lst) / weight_lst\n    critical_items = np.argsort(combined_ratios)[-max(1, n_items // 10):]\n    for item in critical_items:\n        if np.random.rand() < 0.4:  # 40% flip probability for critical items\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 2: Objective-correlated swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.5:  # Strong correlation\n        if np.random.rand() < 0.6:  # 60% chance for value1 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:  # 40% chance for value2 optimization\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:  # Weak correlation - balanced optimization\n        if np.random.rand() < 0.5:\n            item = np.random.choice(np.where(value1_lst > np.median(value1_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n        else:\n            item = np.random.choice(np.where(value2_lst > np.median(value2_lst))[0])\n            if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n            elif new_solution[item] == 1:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Adaptive segment flips\n    if np.random.rand() < 0.3:  # 30% chance for segment flip\n        segment_length = np.random.randint(2, min(7, n_items))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            remove_item = excess_items[np.argmin((value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on objective diversity and solution age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = diversity[0] * diversity[1] / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1)\n\n    # Hybrid local search: guided additions and removals with probabilistic guidance\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    np.random.shuffle(candidates)\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Step 3: Critical item flip with probability based on diversity\n    critical_items = np.argsort(combined_ratio)[-max(1, len(weight_lst) // 10):]\n    if len(critical_items) > 0 and np.random.rand() < 0.7:\n        item = np.random.choice(critical_items)\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n        elif new_solution[item] == 1:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        combined_ratios = combined_ratio[excess_items]\n        remove_item = excess_items[np.argmin(combined_ratios)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### Keywords:\n- **Adaptive selection**: Crowding-distance + age-based diversity.\n- **Hybrid perturbations**: Critical flips (top 5% value-to-weight) + guided swaps.\n- **Incremental feasibility**: Capacity checks during swaps + excess-item removal.\n- **Dynamic intensity**: Adjust randomness based on improvement trends.\n\n### Advice:\n1. **Select**: Bias toward solutions with high crowding distance and younger age.\n2. **Perturb**: Flip critical items and swap high-value items with guided randomness.\n3. **Feasibility**: Enforce capacity constraints during swaps and repair incrementally.\n4. **Intensity**: Increase randomness if stuck, reduce if improving.\n\n### Avoid:\n- Static selection (e.g., pure randomness).\n- Post-perturbation feasibility checks.\n- Over-reliance on critical flips alone.\n- Fixed perturbation intensity.\n\n### Explanation:\nBalances exploration/exploitation via adaptive selection and hybrid perturbations, ensuring incremental feasibility. Dynamic intensity prevents premature convergence, while context-aware strategies escape local optima.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 160,
        "algorithm": "The algorithm combines adaptive solution selection (prioritizing high-crowding-distance solutions with correlated objectives) with a multi-phase local search strategy: (1) dynamic segment-based flips for diversification, (2) correlated value-driven swaps to exploit objective relationships, and (3) balanced value-to-weight ratio operations for feasibility maintenance. It intelligently balances exploration and exploitation while ensuring feasibility through incremental checks and excess-item removal.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on crowding distance and objective correlation\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives_sorted = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives_sorted[i+1, 0] - objectives_sorted[i-1, 0]) + (objectives_sorted[i+1, 1] - objectives_sorted[i-1, 1])\n        corr = np.corrcoef(objectives[:, 0], objectives[:, 1])[0, 1]\n        selection_scores = crowding * (1 + abs(corr))\n        selected_idx = sorted_indices[np.argmax(selection_scores)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Dynamic segment-based flips\n    if np.random.rand() < 0.4:\n        segment_length = np.random.randint(2, min(5, n_items // 2))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Phase 2: Correlated value-driven swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.3:\n        if np.random.rand() < 0.5:\n            # Value1-driven swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:\n            # Value2-driven swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:\n        # Balanced optimization\n        if np.random.rand() < 0.5:\n            # Add high combined value-to-weight item\n            candidates = np.where(new_solution == 0)[0]\n            if len(candidates) > 0:\n                combined_ratio = (value1_lst + value2_lst) / weight_lst\n                add_item = candidates[np.argmax(combined_ratio[candidates])]\n                if current_weight + weight_lst[add_item] <= capacity:\n                    new_solution[add_item] = 1\n                    current_weight += weight_lst[add_item]\n        else:\n            # Remove low combined value-to-weight item\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) > 0:\n                combined_ratio = (value1_lst + value2_lst) / weight_lst\n                remove_item = included_items[np.argmin(combined_ratio[included_items])]\n                new_solution[remove_item] = 0\n                current_weight -= weight_lst[remove_item]\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            combined_ratio = (value1_lst + value2_lst) / weight_lst\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.7805528389626107,
            0.42278245091438293
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on crowding distance and objective correlation\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 2:\n        sorted_indices = np.argsort(objectives[:, 0])\n        objectives_sorted = objectives[sorted_indices]\n        crowding = np.zeros(len(objectives))\n        crowding[0] = crowding[-1] = np.inf\n        for i in range(1, len(objectives)-1):\n            crowding[i] = (objectives_sorted[i+1, 0] - objectives_sorted[i-1, 0]) + (objectives_sorted[i+1, 1] - objectives_sorted[i-1, 1])\n        corr = np.corrcoef(objectives[:, 0], objectives[:, 1])[0, 1]\n        selection_scores = crowding * (1 + abs(corr))\n        selected_idx = sorted_indices[np.argmax(selection_scores)]\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    n_items = len(weight_lst)\n\n    # Phase 1: Dynamic segment-based flips\n    if np.random.rand() < 0.4:\n        segment_length = np.random.randint(2, min(5, n_items // 2))\n        start_idx = np.random.randint(0, n_items - segment_length)\n        segment = new_solution[start_idx:start_idx+segment_length]\n        new_segment = 1 - segment\n        temp_weight = current_weight - np.sum(weight_lst[start_idx:start_idx+segment_length][segment == 1]) + np.sum(weight_lst[start_idx:start_idx+segment_length][new_segment == 1])\n        if temp_weight <= capacity:\n            new_solution[start_idx:start_idx+segment_length] = new_segment\n            current_weight = temp_weight\n\n    # Phase 2: Correlated value-driven swaps\n    corr = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if corr > 0.3:\n        if np.random.rand() < 0.5:\n            # Value1-driven swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value1_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value1_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n        else:\n            # Value2-driven swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                remove_item = included_items[np.argmin(value2_lst[included_items] / weight_lst[included_items])]\n                feasible_add = excluded_items[weight_lst[excluded_items] <= (capacity - current_weight + weight_lst[remove_item])]\n                if len(feasible_add) > 0:\n                    add_item = feasible_add[np.argmax(value2_lst[feasible_add] / weight_lst[feasible_add])]\n                    new_solution[remove_item] = 0\n                    new_solution[add_item] = 1\n                    current_weight = current_weight - weight_lst[remove_item] + weight_lst[add_item]\n    else:\n        # Balanced optimization\n        if np.random.rand() < 0.5:\n            # Add high combined value-to-weight item\n            candidates = np.where(new_solution == 0)[0]\n            if len(candidates) > 0:\n                combined_ratio = (value1_lst + value2_lst) / weight_lst\n                add_item = candidates[np.argmax(combined_ratio[candidates])]\n                if current_weight + weight_lst[add_item] <= capacity:\n                    new_solution[add_item] = 1\n                    current_weight += weight_lst[add_item]\n        else:\n            # Remove low combined value-to-weight item\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) > 0:\n                combined_ratio = (value1_lst + value2_lst) / weight_lst\n                remove_item = included_items[np.argmin(combined_ratio[included_items])]\n                new_solution[remove_item] = 0\n                current_weight -= weight_lst[remove_item]\n\n    # Feasibility enforcement\n    if current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(excess_items) > 0:\n            combined_ratio = (value1_lst + value2_lst) / weight_lst\n            remove_item = excess_items[np.argmin(combined_ratio[excess_items])]\n            new_solution[remove_item] = 0\n            current_weight -= weight_lst[remove_item]\n            excess_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm combines adaptive selection with a multi-phase local search to generate high-quality neighbors for the BI-KP. It first selects promising solutions based on crowding distance and marginal contributions to both objectives, then applies probabilistic flips (weighted by marginal gains) and Pareto-guided swaps, while dynamically adjusting exploration-exploitation balance and ensuring feasibility through marginal checks and feasibility repair. The method prioritizes items with higher marginal contributions while maintaining diversity and solution quality through careful perturbation and targeted swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 168,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate dominance scores and crowding distances\n    objectives = np.array([obj for (sol, obj) in archive])\n    dominance_scores = np.zeros(len(archive))\n    crowding_distances = np.zeros(len(archive))\n\n    for i in range(len(archive)):\n        sol_i = archive[i][0]\n        weight_i = np.sum(weight_lst[sol_i == 1])\n        dominated = False\n        for j in range(len(archive)):\n            if i == j:\n                continue\n            sol_j = archive[j][0]\n            weight_j = np.sum(weight_lst[sol_j == 1])\n            if (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        dominance_scores[i] = 1 if not dominated else 0\n\n        # Compute crowding distance\n        if len(archive) > 1:\n            sorted_indices = np.argsort(objectives[:, 0])\n            rank = np.where(sorted_indices == i)[0][0]\n            if rank == 0 or rank == len(archive) - 1:\n                crowding_distances[i] = float('inf')\n            else:\n                crowding_distances[i] = (objectives[sorted_indices[rank+1], 0] - objectives[sorted_indices[rank-1], 0]) / (objectives[-1, 0] - objectives[0, 0] + 1e-10)\n\n    # Select solution with high dominance score and high crowding distance\n    scores = dominance_scores * (1 + crowding_distances)\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Objective-specific marginal contribution analysis\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 2: Dynamic perturbation based on objective trade-offs\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate objective trade-off ratios\n        value_ratio = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        tradeoff_ratio = value1_lst / (value2_lst + 1e-6)\n\n        # Identify items to potentially remove and add\n        remove_candidates = included[np.argsort(tradeoff_ratio[included])[:max(1, len(included)//4)]]\n        add_candidates = excluded[np.argsort(-value_ratio[excluded])[:max(1, len(excluded)//4)]]\n\n        for remove_idx in remove_candidates:\n            for add_idx in add_candidates:\n                if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                    if np.random.rand() < 0.7:  # Higher probability for trade-off beneficial swaps\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                        break\n\n    # Phase 3: Feasibility-preserving enhancement\n    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n    if remaining_capacity > 0:\n        # Add items with highest combined marginal gain\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = combined_marginal[feasible_excluded]\n            best_adds = feasible_excluded[np.argsort(-marginal_gains)[:min(2, len(feasible_excluded))]]\n            for add_idx in best_adds:\n                if np.random.rand() < 0.6:  # 60% chance to add high-marginal items\n                    new_solution[add_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.8672214203514818,
            2.1826327741146088
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate dominance scores and crowding distances\n    objectives = np.array([obj for (sol, obj) in archive])\n    dominance_scores = np.zeros(len(archive))\n    crowding_distances = np.zeros(len(archive))\n\n    for i in range(len(archive)):\n        sol_i = archive[i][0]\n        weight_i = np.sum(weight_lst[sol_i == 1])\n        dominated = False\n        for j in range(len(archive)):\n            if i == j:\n                continue\n            sol_j = archive[j][0]\n            weight_j = np.sum(weight_lst[sol_j == 1])\n            if (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        dominance_scores[i] = 1 if not dominated else 0\n\n        # Compute crowding distance\n        if len(archive) > 1:\n            sorted_indices = np.argsort(objectives[:, 0])\n            rank = np.where(sorted_indices == i)[0][0]\n            if rank == 0 or rank == len(archive) - 1:\n                crowding_distances[i] = float('inf')\n            else:\n                crowding_distances[i] = (objectives[sorted_indices[rank+1], 0] - objectives[sorted_indices[rank-1], 0]) / (objectives[-1, 0] - objectives[0, 0] + 1e-10)\n\n    # Select solution with high dominance score and high crowding distance\n    scores = dominance_scores * (1 + crowding_distances)\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Objective-specific marginal contribution analysis\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 2: Dynamic perturbation based on objective trade-offs\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate objective trade-off ratios\n        value_ratio = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        tradeoff_ratio = value1_lst / (value2_lst + 1e-6)\n\n        # Identify items to potentially remove and add\n        remove_candidates = included[np.argsort(tradeoff_ratio[included])[:max(1, len(included)//4)]]\n        add_candidates = excluded[np.argsort(-value_ratio[excluded])[:max(1, len(excluded)//4)]]\n\n        for remove_idx in remove_candidates:\n            for add_idx in add_candidates:\n                if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                    if np.random.rand() < 0.7:  # Higher probability for trade-off beneficial swaps\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                        break\n\n    # Phase 3: Feasibility-preserving enhancement\n    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n    if remaining_capacity > 0:\n        # Add items with highest combined marginal gain\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = combined_marginal[feasible_excluded]\n            best_adds = feasible_excluded[np.argsort(-marginal_gains)[:min(2, len(feasible_excluded))]]\n            for add_idx in best_adds:\n                if np.random.rand() < 0.6:  # 60% chance to add high-marginal items\n                    new_solution[add_idx] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm combines adaptive selection with a multi-phase local search to generate high-quality neighbors for the BI-KP. It first selects promising solutions based on crowding distance and marginal contributions to both objectives, then applies probabilistic flips (weighted by marginal gains) and Pareto-guided swaps, while dynamically adjusting exploration-exploitation balance and ensuring feasibility through marginal checks and feasibility repair. The method prioritizes items with higher marginal contributions while maintaining diversity and solution quality through careful perturbation and targeted swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 169,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Select solution with highest combined marginal contribution\n    selected_idx = np.argmax([np.sum(combined_marginal * sol) for (sol, _) in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip items based on marginal contribution and weight\n    flip_prob = np.zeros(len(weight_lst))\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            flip_prob[i] = 1 - np.tanh(combined_marginal[i] * 0.5)\n        else:\n            flip_prob[i] = np.tanh(combined_marginal[i] * 0.5)\n\n    # Apply flips with feasibility check\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Adaptive swapping: swap items between included and excluded based on marginal contribution\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.29517507317100167,
            0.95170858502388
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Select solution with highest combined marginal contribution\n    selected_idx = np.argmax([np.sum(combined_marginal * sol) for (sol, _) in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic perturbation: flip items based on marginal contribution and weight\n    flip_prob = np.zeros(len(weight_lst))\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            flip_prob[i] = 1 - np.tanh(combined_marginal[i] * 0.5)\n        else:\n            flip_prob[i] = np.tanh(combined_marginal[i] * 0.5)\n\n    # Apply flips with feasibility check\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Adaptive swapping: swap items between included and excluded based on marginal contribution\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm combines adaptive selection with a multi-phase local search to generate high-quality neighbors for the BI-KP. It first selects promising solutions based on crowding distance and marginal contributions to both objectives, then applies probabilistic flips (weighted by marginal gains) and Pareto-guided swaps, while dynamically adjusting exploration-exploitation balance and ensuring feasibility through marginal checks and feasibility repair. The method prioritizes items with higher marginal contributions while maintaining diversity and solution quality through careful perturbation and targeted swaps.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 170,
        "algorithm": "The algorithm combines marginal utility-based selection with a three-phase local search: dynamic probabilistic flips prioritize high-marginal items, Pareto-optimal swaps target balanced improvements, and feasibility repair ensures weight constraints. It dynamically weights item flips by normalized marginal gains and prioritizes swaps between high-utility included and low-utility excluded items, while maintaining feasibility through marginal-aware removal. The solution selection is guided by combined marginal utility scores, and probabilistic flips introduce controlled randomness to escape local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate normalized marginal contributions\n    total_weight = np.sum(weight_lst)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    norm_marginal = (marginal1 + marginal2) / (np.max(marginal1 + marginal2) + 1e-10)\n\n    # Select solution with highest combined marginal utility\n    utility_scores = [np.sum(norm_marginal * sol) for (sol, _) in archive]\n    selected_idx = np.argmax(utility_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Dynamic probabilistic flips\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 0.5 * (1 - np.exp(-norm_marginal[included] * 2))\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = 0.5 * np.exp(norm_marginal[excluded] * 2)\n\n    # Apply flips with probability\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-optimal swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort by marginal utility and weight ratio\n        utility_ratio = norm_marginal / (weight_lst + 1e-10)\n        included_sorted = included[np.argsort(-utility_ratio[included])]\n        excluded_sorted = excluded[np.argsort(utility_ratio[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is Pareto-improving\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair with marginal consideration\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with lowest marginal utility\n        remove_idx = candidates[np.argmin(norm_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.2542221500822676,
            0.6994553804397583
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate normalized marginal contributions\n    total_weight = np.sum(weight_lst)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    norm_marginal = (marginal1 + marginal2) / (np.max(marginal1 + marginal2) + 1e-10)\n\n    # Select solution with highest combined marginal utility\n    utility_scores = [np.sum(norm_marginal * sol) for (sol, _) in archive]\n    selected_idx = np.argmax(utility_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Dynamic probabilistic flips\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 0.5 * (1 - np.exp(-norm_marginal[included] * 2))\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = 0.5 * np.exp(norm_marginal[excluded] * 2)\n\n    # Apply flips with probability\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-optimal swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort by marginal utility and weight ratio\n        utility_ratio = norm_marginal / (weight_lst + 1e-10)\n        included_sorted = included[np.argsort(-utility_ratio[included])]\n        excluded_sorted = excluded[np.argsort(utility_ratio[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is Pareto-improving\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] >= value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]) or \\\n                   (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair with marginal consideration\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with lowest marginal utility\n        remove_idx = candidates[np.argmin(norm_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm combines adaptive selection with a multi-phase local search to generate high-quality neighbors for the BI-KP. It first selects promising solutions based on crowding distance and marginal contributions to both objectives, then applies probabilistic flips (weighted by marginal gains) and Pareto-guided swaps, while dynamically adjusting exploration-exploitation balance and ensuring feasibility through marginal checks and feasibility repair. The method prioritizes items with higher marginal contributions while maintaining diversity and solution quality through careful perturbation and targeted swaps.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 171,
        "algorithm": "This hybrid algorithm combines adaptive solution selection based on dominance scores and marginal contributions with a multi-phase local search that includes probabilistic flips, Pareto-optimal swaps, and feasibility-aware perturbations. It prioritizes solutions with high dominance scores and strong marginal contributions, then refines them through temperature-controlled flips, dominance-guided swaps, and diversification, while always maintaining feasibility. The algorithm balances exploration and exploitation by dynamically adjusting flip probabilities based on capacity utilization and item contributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate normalized marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    normalized_marginal = (marginal1 + marginal2) / (np.sum(marginal1) + np.sum(marginal2) + 1e-10)\n\n    # Compute dominance-based selection scores\n    dominance_scores = np.zeros(len(archive))\n    objectives = np.array([obj for (sol, obj) in archive])\n    for i in range(len(archive)):\n        dominates = 0\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] > objectives[j, 1]) or \\\n                   (objectives[i, 0] > objectives[j, 0] and objectives[i, 1] >= objectives[j, 1]):\n                    dominates += 1\n        dominance_scores[i] = dominates\n\n    # Select solution with highest dominance score and combined marginal contribution\n    combined_scores = dominance_scores + np.array([np.sum(normalized_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips with adaptive temperature\n    flip_temp = 0.5 + 0.5 * (1 - current_weight / capacity)\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(normalized_marginal[included] * flip_temp)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(normalized_marginal[excluded] * flip_temp)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dominance-guided swaps with objective balance\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by normalized marginal and objective balance\n        included_sorted = included[np.argsort(-normalized_marginal[included])]\n        excluded_sorted = excluded[np.argsort(normalized_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate objective balance improvement\n            obj1_improvement = value1_lst[add_idx] - value1_lst[remove_idx]\n            obj2_improvement = value2_lst[add_idx] - value2_lst[remove_idx]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (obj1_improvement > 0 or obj2_improvement > 0) and \\\n                   (obj1_improvement + obj2_improvement > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility-aware perturbation\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with least contribution to both objectives\n        remove_idx = candidates[np.argmin(normalized_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    # Phase 4: Solution diversification\n    if np.random.rand() < 0.3:\n        # Randomly flip a small number of items\n        num_flips = min(3, len(weight_lst) // 5)\n        flip_indices = np.random.choice(len(weight_lst), size=num_flips, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n            else:\n                if (current_weight + weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.8927852868658097,
            1.1379701495170593
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate normalized marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    normalized_marginal = (marginal1 + marginal2) / (np.sum(marginal1) + np.sum(marginal2) + 1e-10)\n\n    # Compute dominance-based selection scores\n    dominance_scores = np.zeros(len(archive))\n    objectives = np.array([obj for (sol, obj) in archive])\n    for i in range(len(archive)):\n        dominates = 0\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] > objectives[j, 1]) or \\\n                   (objectives[i, 0] > objectives[j, 0] and objectives[i, 1] >= objectives[j, 1]):\n                    dominates += 1\n        dominance_scores[i] = dominates\n\n    # Select solution with highest dominance score and combined marginal contribution\n    combined_scores = dominance_scores + np.array([np.sum(normalized_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips with adaptive temperature\n    flip_temp = 0.5 + 0.5 * (1 - current_weight / capacity)\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(normalized_marginal[included] * flip_temp)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(normalized_marginal[excluded] * flip_temp)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dominance-guided swaps with objective balance\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by normalized marginal and objective balance\n        included_sorted = included[np.argsort(-normalized_marginal[included])]\n        excluded_sorted = excluded[np.argsort(normalized_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate objective balance improvement\n            obj1_improvement = value1_lst[add_idx] - value1_lst[remove_idx]\n            obj2_improvement = value2_lst[add_idx] - value2_lst[remove_idx]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (obj1_improvement > 0 or obj2_improvement > 0) and \\\n                   (obj1_improvement + obj2_improvement > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility-aware perturbation\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with least contribution to both objectives\n        remove_idx = candidates[np.argmin(normalized_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    # Phase 4: Solution diversification\n    if np.random.rand() < 0.3:\n        # Randomly flip a small number of items\n        num_flips = min(3, len(weight_lst) // 5)\n        flip_indices = np.random.choice(len(weight_lst), size=num_flips, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n            else:\n                if (current_weight + weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 1\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThis algorithm combines adaptive selection with a multi-phase local search to generate high-quality neighbors for the BI-KP. It first selects promising solutions based on crowding distance and marginal contributions to both objectives, then applies probabilistic flips (weighted by marginal gains) and Pareto-guided swaps, while dynamically adjusting exploration-exploitation balance and ensuring feasibility through marginal checks and feasibility repair. The method prioritizes items with higher marginal contributions while maintaining diversity and solution quality through careful perturbation and targeted swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n\n    # Compute crowding distance for each solution\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    combined_marginal = marginal1 + marginal2\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic flips based on marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort items by marginal contributions\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if swap improves at least one objective\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or\n                    value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive objective weighting with a multi-phase local search strategy, prioritizing solutions with high diversity and marginal contribution while dynamically balancing exploration (perturbations) and exploitation (targeted swaps) to generate high-quality neighbor solutions while ensuring feasibility through marginal contribution-based adjustments. It uses dynamic objective weighting (alpha) to balance the two objectives, first randomly perturbs the solution based on marginal contributions, then performs targeted swaps between high-marginal items, and finally repairs infeasible solutions by removing low-marginal items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Enhanced selection strategy: Combine objective scores with diversity and marginal contribution\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute diversity scores (crowding distance)\n        diversity = np.zeros(len(archive))\n        for dim in range(2):\n            sorted_indices = np.argsort(objectives[:, dim])\n            diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n            for i in range(1, len(archive) - 1):\n                diversity[sorted_indices[i]] += objectives[sorted_indices[i+1], dim] - objectives[sorted_indices[i-1], dim]\n        # Normalize diversity and combine with objectives\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 0.5 * normalized_diversity)\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    alpha = np.random.uniform(0.3, 0.7)  # Dynamic objective weighting factor\n    combined_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    perturbation_prob = np.zeros(len(weight_lst))\n    if len(included) > 0:\n        # Remove items with low marginal contribution\n        remove_prob = 1 - np.tanh(combined_marginal[included] * 0.2)\n        perturbation_prob[included] = remove_prob * np.random.uniform(0.7, 1.0, len(included))\n\n    if len(excluded) > 0:\n        # Add items with high marginal contribution that fit capacity\n        add_prob = np.tanh(combined_marginal[excluded] * 0.2)\n        add_mask = (weight_lst[excluded] <= (capacity - current_weight))\n        perturbation_prob[excluded] = add_prob * add_mask * np.random.uniform(0.7, 1.0, len(excluded))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Specific Swaps with Marginal Gain Analysis\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        swap_count = min(2, len(included_sorted), len(excluded_sorted))\n        for i in range(swap_count):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Calculate potential gain\n            gain1 = value1_lst[add_idx] - value1_lst[remove_idx]\n            gain2 = value2_lst[add_idx] - value2_lst[remove_idx]\n            net_gain = alpha * gain1 + (1 - alpha) * gain2\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity and net_gain > 0:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Adaptive Removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove items with lowest marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and high diversity (crowding distance)\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objectives\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] + normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Adaptive Objective-Driven Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Objective-Driven Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    if np.random.rand() < 0.5:  # Focus on value1\n        perturbation_prob[included] = 1 - np.tanh(marginal1[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal1[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n    else:  # Focus on value2\n        perturbation_prob[included] = 1 - np.tanh(marginal2[included] * 0.1)\n        perturbation_prob[excluded] = np.tanh(marginal2[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Dynamic objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility Repair with Marginal Contribution\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 172,
        "algorithm": "The algorithm first selects a promising solution from the archive by prioritizing high crowding distance and combined marginal contributions, then applies a hybrid local search with three phases: probabilistic flips weighted by dynamic marginal gains, Pareto-guided swaps between high-marginal items, and feasibility repair by removing low-marginal items. The method dynamically balances exploration-exploitation by randomly weighting objectives during perturbations and swaps, ensuring simultaneous improvements in both objectives while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions and crowding distance\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic objective weighting\n    alpha = np.random.uniform(0.3, 0.7)\n    weighted_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Probabilistic flips based on dynamic marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(weighted_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(weighted_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-weighted_marginal[included])]\n        excluded_sorted = excluded[np.argsort(weighted_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weighted_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.3822345199860961,
            0.4254402220249176
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate marginal contributions and crowding distance\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    crowding = np.zeros(len(archive))\n    for dim in range(2):\n        sorted_idx = np.argsort(objectives[:, dim])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_idx[i]] += (objectives[sorted_idx[i+1], dim] - objectives[sorted_idx[i-1], dim])\n\n    # Select solution with highest crowding distance and combined marginal contribution\n    scores = crowding + np.array([np.sum(combined_marginal * sol) for (sol, obj) in archive])\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic objective weighting\n    alpha = np.random.uniform(0.3, 0.7)\n    weighted_marginal = alpha * marginal1 + (1 - alpha) * marginal2\n\n    # Phase 1: Probabilistic flips based on dynamic marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        flip_prob[included] = 1 - np.tanh(weighted_marginal[included] * 0.3)\n\n    if len(excluded) > 0:\n        flip_prob[excluded] = np.tanh(weighted_marginal[excluded] * 0.3)\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-weighted_marginal[included])]\n        excluded_sorted = excluded[np.argsort(weighted_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] > value1_lst[remove_idx] or value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weighted_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 8 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing high-objective-value solutions with low crowding distance, then applies a hybrid local search combining random item swaps and adaptive perturbations to explore neighbors while ensuring feasibility. It prioritizes solutions with better combined objective values and diversity (crowding distance) to guide selection, and uses a mix of item swaps and random flips to generate neighbors, with feasibility checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Hybrid local search: item swapping and adaptive perturbation\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Swap items between included and excluded with feasibility check\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Select a random item to remove\n        remove_idx = np.random.choice(included)\n        # Select a random item to add\n        add_idx = np.random.choice(excluded)\n\n        # Check feasibility\n        if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n            new_solution[remove_idx] = 0\n            new_solution[add_idx] = 1\n\n    # Adaptive perturbation: randomly flip a small number of items\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive based on a combination of objective values and crowding distance, then applies a three-phase local search: first reallocating items by removing low-value/high-weight items and adding high-value/low-weight items, followed by targeted flipping of high-value items, and finally adding items with the highest marginal gain while ensuring feasibility. The selection prioritizes solutions with balanced objectives and diversity, while the local search strategically improves solutions by focusing on value ratios, weight impacts, and marginal gains.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high combined objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = (objectives[:, 0] + objectives[:, 1]) * (1 + 1 / (1 + crowding))\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Intelligent reallocation based on value ratios and weight impacts\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate value ratios and weight impacts\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        included_ratios = value_ratios[included]\n        excluded_ratios = value_ratios[excluded]\n\n        # Find best candidate to remove and add\n        remove_candidate = included[np.argmin(included_ratios)]\n        add_candidate = excluded[np.argmax(excluded_ratios)]\n\n        if (current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]) <= capacity:\n            new_solution[remove_candidate] = 0\n            new_solution[add_candidate] = 1\n            current_weight = current_weight - weight_lst[remove_candidate] + weight_lst[add_candidate]\n\n    # Phase 2: Targeted perturbation in high-value regions\n    if len(included) > 0:\n        # Identify high-value regions\n        included_values = value1_lst[included] + value2_lst[included]\n        high_value_indices = included[np.argsort(included_values)[-max(1, len(included)//5):]]\n\n        # Flip some items in these regions\n        for idx in high_value_indices:\n            if np.random.rand() < 0.3:  # 30% chance to flip\n                if new_solution[idx] == 1 and (current_weight - weight_lst[idx]) <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    # Phase 3: Feasibility-preserving mutation with marginal gain\n    if len(excluded) > 0:\n        # Calculate marginal gains\n        remaining_capacity = capacity - current_weight\n        feasible_excluded = excluded[weight_lst[excluded] <= remaining_capacity]\n        if len(feasible_excluded) > 0:\n            marginal_gains = (value1_lst[feasible_excluded] + value2_lst[feasible_excluded]) / weight_lst[feasible_excluded]\n            best_add = feasible_excluded[np.argmax(marginal_gains)]\n\n            if np.random.rand() < 0.5:  # 50% chance to add best marginal gain item\n                new_solution[best_add] = 1\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel local search: Dynamic Value-Weight Ratio Guided Perturbation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive Objective-Specific Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(2, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Adaptive objective-specific swap\n                if np.random.rand() < 0.5:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                        current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Novel Adaptive Multi-Objective Perturbation with Dynamic Thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Dynamic Value-Weight Ratio Guided Perturbation\n    perturbation_prob = np.zeros(len(weight_lst))\n    perturbation_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.1)\n    perturbation_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - np.sum(weight_lst[new_solution == 1])))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n        else:\n            if np.sum(weight_lst[new_solution == 1]) + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n\n    # Phase 2: Objective-Specific Swaps with Adaptive Threshold\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (ascending)\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        # Adaptive threshold for swaps based on solution quality\n        swap_threshold = 0.3 if (objectives[selected_idx][0] + objectives[selected_idx][1]) > np.mean(objectives.sum(axis=1)) else 0.5\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if np.sum(weight_lst[new_solution == 1]) - weight_lst[remove_idx] + weight_lst[add_idx] <= capacity:\n                # Objective-specific swap with adaptive threshold\n                if np.random.rand() < swap_threshold:  # Focus on value1\n                    if value1_lst[add_idx] > value1_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n                else:  # Focus on value2\n                    if value2_lst[add_idx] > value2_lst[remove_idx]:\n                        new_solution[remove_idx] = 0\n                        new_solution[add_idx] = 1\n\n    # Phase 3: Lightweight Feasibility Repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (value1_lst[candidates] + value2_lst[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding distance\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        # Combine objectives and crowding distance\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Adaptive Value-Weighted Perturbations\n    perturbation_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Higher probability for items with low marginal contribution\n    perturbation_prob[included] = 0.5 * (1 - np.tanh(combined_marginal[included] * 0.1))\n    perturbation_prob[excluded] = 0.5 * np.tanh(combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    for idx in np.where(np.random.rand(len(weight_lst)) < perturbation_prob)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Objective-Aware Pareto-Optimal Swaps\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by descending marginal contribution\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by ascending marginal contribution\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            # Check if swap is feasible\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Perform swap based on objective dominance\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Dynamic Weight Adjustment for Feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        # Remove item with smallest combined marginal contribution\n        remove_idx = candidates[np.argmin(combined_marginal[candidates])]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid score combining objective values and crowding distance, then generates a neighbor through three phases: (1) a dynamic flip operator that probabilistically toggles items based on their combined value-to-weight ratios and inclusion status, (2) a Pareto-guided swap that prioritizes high-marginal-contribution items to improve both objectives, and (3) a lightweight repair that removes low-marginal-contribution items to restore feasibility. The method balances exploration and exploitation by focusing on items with significant combined marginal contributions while maintaining feasibility through continuous capacity checks and marginal-based removals.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high objective values and moderate crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize crowding and combine with objective values\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions (value-to-weight ratios)\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Dynamic Marginal Contribution-Based Probabilistic Flip\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.tanh(combined_marginal[included] * 0.2)\n    flip_prob[excluded] = np.tanh(combined_marginal[excluded] * 0.2) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-Guided Swap with Combined Marginal Contributions\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included items by combined marginal contribution (descending)\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        # Sort excluded items by combined marginal contribution (descending)\n        excluded_sorted = excluded[np.argsort(-combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                # Check if the swap improves both objectives\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] > value2_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Lightweight Feasibility Repair with Marginal Contributions\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 173,
        "algorithm": "The algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a four-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, adaptive perturbations to escape local optima, and lightweight feasibility repair. It emphasizes marginal contributions for item selection, Pareto dominance for swaps, and ensures feasibility through targeted repairs. The selection process balances objective values and diversity, while the local search phases systematically explore the solution space while maintaining constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 0.1)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbations\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 4: Lightweight feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7634326358023262,
            0.4250226318836212
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flips weighted by marginal contributions\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included] * 0.1)\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded] * 0.1) * (weight_lst[excluded] <= (capacity - current_weight))\n\n    flip_indices = np.where(np.random.rand(len(weight_lst)) < flip_prob)[0]\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swaps\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                if (value1_lst[add_idx] > value1_lst[remove_idx] and value2_lst[add_idx] >= value2_lst[remove_idx]) or \\\n                   (value2_lst[add_idx] > value2_lst[remove_idx] and value1_lst[add_idx] >= value1_lst[remove_idx]):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbations\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 4: Lightweight feasibility repair\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess = np.sum(weight_lst[new_solution == 1]) - capacity\n        candidates = np.where(new_solution == 1)[0]\n        if len(candidates) == 0:\n            break\n        remove_idx = candidates[np.argmin(weight_lst[candidates] / (combined_marginal[candidates] + 1e-10))]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with higher combined objective values and lower weights, then applies a hybrid local search that intelligently swaps items between objectives to balance improvements in both value dimensions while ensuring feasibility. It first tries beneficial swaps between included and excluded items, then adds new items if possible, and finally removes the least valuable item if no improvements are found. The selection criterion balances both objectives and weight, while the local search prioritizes simultaneous improvements in both value dimensions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (higher objectives, lower weight)\n    selected_idx = np.argmax([obj[0] + obj[1] - 0.1 * np.sum(weight_lst * sol) for sol, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: balance improvements in both objectives\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = archive[selected_idx][1][0]\n    current_value2 = archive[selected_idx][1][1]\n\n    # Identify items to potentially swap or reallocate\n    zero_indices = np.where(base_solution == 0)[0]\n    one_indices = np.where(base_solution == 1)[0]\n\n    # Try to improve both objectives by swapping items\n    for i in one_indices:\n        for j in zero_indices:\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                delta_value1 = value1_lst[j] - value1_lst[i]\n                delta_value2 = value2_lst[j] - value2_lst[i]\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    return new_solution\n\n    # If no beneficial swap, try to add a new item if possible\n    for j in zero_indices:\n        if current_weight + weight_lst[j] <= capacity:\n            new_solution[j] = 1\n            return new_solution\n\n    # If no improvement, try to remove the least valuable item\n    if len(one_indices) > 0:\n        least_value_idx = one_indices[np.argmin(value1_lst[one_indices] + value2_lst[one_indices])]\n        new_solution[least_value_idx] = 0\n        return new_solution\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive using a combined score of objective values and crowding distance, then generates a neighbor through a hybrid local search that combines probabilistic flips (weighted by marginal contributions), Pareto-guided swaps (for simultaneous improvements in both objectives), and adaptive perturbations (small random flips) to ensure feasibility and further exploration. The marginal contributions prioritize items with higher combined value-to-weight ratios, while the Pareto-guided swaps specifically target swaps that improve both objectives simultaneously.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps + adaptive perturbations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flip weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation (small random flips)\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 174,
        "algorithm": "This algorithm selects a promising solution from the archive by balancing objective values and diversity, then generates a neighbor through a hybrid local search combining probabilistic flips (weighted by marginal contribution ratios) and Pareto-guided swaps (targeting complementary items), ensuring feasibility while adaptively perturbing the solution for exploration. The selection prioritizes solutions with high combined objectives and diversity, while the local search intelligently balances exploitation (via marginal-ratio-weighted flips) and exploration (via Pareto-guided swaps and adaptive perturbations).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        diversity = np.zeros(len(archive))\n        diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            diversity[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_diversity\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contribution ratios\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    ratio_marginal = marginal1 / (marginal2 + 1e-10)\n\n    # Hybrid local search: probabilistic flips weighted by marginal ratios + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flip weighted by marginal ratios\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-ratio_marginal[included])\n    flip_prob[excluded] = np.exp(-ratio_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap for complementary marginal contributions\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-ratio_marginal[included])]\n        excluded_sorted = excluded[np.argsort(ratio_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation (small random flips)\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7278697707354206,
            0.43923839926719666
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and diversity\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        diversity = np.zeros(len(archive))\n        diversity[sorted_indices[0]] = diversity[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            diversity[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_diversity = (diversity - np.min(diversity)) / (np.max(diversity) - np.min(diversity) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_diversity\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contribution ratios\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    ratio_marginal = marginal1 / (marginal2 + 1e-10)\n\n    # Hybrid local search: probabilistic flips weighted by marginal ratios + Pareto-guided swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flip weighted by marginal ratios\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-ratio_marginal[included])\n    flip_prob[excluded] = np.exp(-ratio_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap for complementary marginal contributions\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-ratio_marginal[included])]\n        excluded_sorted = excluded[np.argsort(ratio_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation (small random flips)\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive by prioritizing both objective values and crowding distance, then generates a neighbor through a three-phase process: probabilistic flips weighted by marginal contributions, Pareto-guided swaps between included and excluded items, and adaptive perturbations to escape local optima while maintaining feasibility. The method balances exploration (via marginal contributions and random flips) with exploitation (via Pareto-guided swaps) to efficiently navigate the solution space.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high objective values and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        # Compute crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        # Normalize and combine with objective values\n        scores = objectives[:, 0] + objectives[:, 1] + crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip with marginal weighting\n    flip_prob = np.zeros(len(weight_lst))\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= remaining_capacity)\n\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            new_solution[idx] = 0\n            remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (weight_lst[add_idx] - weight_lst[remove_idx]) <= remaining_capacity:\n                new_solution[remove_idx] = 0\n                new_solution[add_idx] = 1\n                remaining_capacity += weight_lst[remove_idx] - weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 0\n                remaining_capacity += weight_lst[idx]\n        else:\n            if weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-quality solution from the archive using a combined score of objective values and crowding distance, then generates a neighbor through a hybrid local search that combines probabilistic flips (weighted by marginal contributions), Pareto-guided swaps (for simultaneous improvements in both objectives), and adaptive perturbations (small random flips) to ensure feasibility and further exploration. The marginal contributions prioritize items with higher combined value-to-weight ratios, while the Pareto-guided swaps specifically target swaps that improve both objectives simultaneously.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high combined objectives and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps + adaptive perturbations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 1: Probabilistic flip weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[included] = 1 - np.exp(-combined_marginal[included])\n    flip_prob[excluded] = np.exp(-combined_marginal[excluded]) * (weight_lst[excluded] <= (capacity - current_weight))\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap\n    if len(included) > 0 and len(excluded) > 0:\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(3, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation (small random flips)\n    num_flips = max(1, int(0.05 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined, ultra-concise response in 4 bullet points (<100 words):\n\n- **Keywords**: Multi-objective trade-offs, adaptive local search, marginal contribution, probabilistic flips, Pareto-guided swaps.\n- **Advice**: Hybridize probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps to balance exploration and exploitation. Use crowding distance to prioritize diverse solutions.\n- **Avoid**: Greedy single-objective swaps, static local search (e.g., 2-opt), and ignoring weight constraints.\n- **Explanation**: Focus on simultaneous improvements in both objectives while adaptively perturbing solutions to escape local optima, ensuring feasibility via marginal contribution checks.\n\n**Code Implementation** (Python-like pseudocode):\n```python\ndef select_neighbor(archive, weights, capacity):\n    # Select solution with high marginal contributions or low crowding distance\n    selected = max(archive, key=lambda x: x['marginal_contribution'] + x['crowding_distance'])\n\n    # Hybrid local search: probabilistic flips + Pareto-guided swaps\n    neighbor = selected.copy()\n    for i in range(len(neighbor)):\n        if random.random() < 0.5:  # Probabilistic flip\n            neighbor[i] = 1 - neighbor[i]\n            if sum(neighbor * weights) > capacity:  # Feasibility check\n                neighbor[i] = 1 - neighbor[i]  # Revert if infeasible\n\n    # Pareto-guided swap (if neighbor dominates or is non-dominated)\n    for i, j in combinations(range(len(neighbor)), 2):\n        if neighbor[i] != neighbor[j]:\n            swap = neighbor.copy()\n            swap[i], swap[j] = swap[j], swap[i]\n            if sum(swap * weights) <= capacity and dominates(swap, selected):\n                neighbor = swap\n                break\n\n    return neighbor\n```\n\nThis approach ensures high-quality, feasible neighbors while balancing exploration and exploitation. The hybrid operator combines probabilistic flips (weighted by marginal contributions) with Pareto-guided swaps, ensuring simultaneous improvements in both objectives. Crowding distance prioritizes diverse solutions, and feasibility checks are enforced at every step.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 175,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high marginal contributions and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1])\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0]) * (weight_lst[new_solution == 0] <= (capacity - current_weight))\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap with objective-aware selection\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation with objective-aware selection\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9035063426805255,
            0.5088266730308533
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with high marginal contributions and low crowding distance\n    objectives = np.array([obj for (sol, obj) in archive])\n    if len(archive) > 1:\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive) - 1):\n            crowding[sorted_indices[i]] = objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]\n        normalized_crowding = (crowding - np.min(crowding)) / (np.max(crowding) - np.min(crowding) + 1e-10)\n        scores = objectives[:, 0] + objectives[:, 1] - normalized_crowding\n    else:\n        scores = np.array([1.0])\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-10)\n    marginal2 = value2_lst / (weight_lst + 1e-10)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic flip weighted by marginal contributions\n    flip_prob = np.zeros(len(weight_lst))\n    flip_prob[new_solution == 1] = 1 - np.exp(-combined_marginal[new_solution == 1])\n    flip_prob[new_solution == 0] = np.exp(-combined_marginal[new_solution == 0]) * (weight_lst[new_solution == 0] <= (capacity - current_weight))\n    flip_mask = np.random.rand(len(weight_lst)) < flip_prob\n\n    for idx in np.where(flip_mask)[0]:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Pareto-guided swap with objective-aware selection\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Sort included by descending marginal, excluded by ascending marginal\n        included_sorted = included[np.argsort(-combined_marginal[included])]\n        excluded_sorted = excluded[np.argsort(combined_marginal[excluded])]\n\n        for i in range(min(5, len(included_sorted), len(excluded_sorted))):\n            remove_idx = included_sorted[i]\n            add_idx = excluded_sorted[i]\n\n            if (current_weight - weight_lst[remove_idx] + weight_lst[add_idx]) <= capacity:\n                delta_value1 = value1_lst[add_idx] - value1_lst[remove_idx]\n                delta_value2 = value2_lst[add_idx] - value2_lst[remove_idx]\n\n                if (delta_value1 > 0 and delta_value2 > 0) or (delta_value1 * delta_value2 > 0):\n                    new_solution[remove_idx] = 0\n                    new_solution[add_idx] = 1\n                    current_weight = current_weight - weight_lst[remove_idx] + weight_lst[add_idx]\n\n    # Phase 3: Adaptive perturbation with objective-aware selection\n    num_flips = max(1, int(0.1 * len(weight_lst)))\n    flip_indices = np.random.choice(len(weight_lst), num_flips, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if (current_weight - weight_lst[idx]) <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if (current_weight + weight_lst[idx]) <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Adaptive selection based on objective diversity and solution age\n    objectives = np.array([obj for _, obj in archive])\n    diversity = np.max(objectives, axis=0) - np.min(objectives, axis=0)\n    ages = np.arange(len(archive))  # Younger solutions have lower indices\n    selection_scores = diversity[0] * diversity[1] / (ages + 1)\n    selection_probs = selection_scores / np.sum(selection_scores)\n    selected_idx = np.random.choice(len(archive), p=selection_probs)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic perturbation intensity based on solution's position in Pareto front\n    front_pos = np.sum(np.all(objectives >= objectives[selected_idx], axis=1))\n    perturbation_intensity = 1.0 / (front_pos + 1)\n\n    # Hybrid local search: guided additions and removals with probabilistic guidance\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 1: Add items with high combined value-to-weight ratio\n    candidates = np.where(new_solution == 0)[0]\n    np.random.shuffle(candidates)\n    for item in candidates:\n        if np.random.rand() < perturbation_intensity and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            current_value1 += value1_lst[item]\n            current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined value-to-weight ratio\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n    for item in included_items:\n        if np.random.rand() < perturbation_intensity:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            current_value1 -= value1_lst[item]\n            current_value2 -= value2_lst[item]\n\n    # Step 3: Critical item flip with probability based on diversity\n    critical_items = np.argsort(combined_ratio)[-max(1, len(weight_lst) // 10):]\n    if len(critical_items) > 0 and np.random.rand() < 0.7:\n        item = np.random.choice(critical_items)\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n        elif new_solution[item] == 1:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Ensure feasibility: remove items with smallest combined value-to-weight ratio\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        combined_ratios = combined_ratio[excess_items]\n        remove_item = excess_items[np.argmin(combined_ratios)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 161,
        "algorithm": "The algorithm selects a solution from the archive using crowding distance to prioritize less crowded regions of the Pareto front, then applies a guided evolutionary mutation that dynamically adjusts perturbation intensity based on objective correlations and solution position, combining value-to-weight ratios with critical item flips to generate high-quality neighbors while ensuring feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances to identify solutions in less crowded regions\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for i in range(2):  # For each objective\n        sorted_indices = np.argsort(objectives[:, i])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for j in range(1, len(archive)-1):\n            if objectives[sorted_indices[-1], i] == objectives[sorted_indices[0], i]:\n                crowding_distances[sorted_indices[j]] += 0\n            else:\n                crowding_distances[sorted_indices[j]] += (objectives[sorted_indices[j+1], i] - objectives[sorted_indices[j-1], i]) / (objectives[sorted_indices[-1], i] - objectives[sorted_indices[0], i])\n\n    # Select solution with highest crowding distance (less crowded region)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate objective correlations and diversity scores\n    objective_correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    diversity_scores = np.std(objectives, axis=0)\n    mutation_rate = 0.3 + 0.4 * (1 - objective_correlation)  # Higher mutation when objectives are less correlated\n\n    # Calculate combined value-to-weight ratios with diversity weighting\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = (ratio1 * diversity_scores[0] + ratio2 * diversity_scores[1]) / np.sum(diversity_scores)\n\n    # Dynamic perturbation based on solution's position in objective space\n    normalized_objectives = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0))\n    solution_position = normalized_objectives[selected_idx]\n    perturbation_intensity = 0.5 * (1 - np.min(solution_position))  # More perturbation for solutions in less optimal regions\n\n    # Guided evolutionary mutation\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 1: Add items with high combined ratio and low weight\n    candidates = np.where(new_solution == 0)[0]\n    np.random.shuffle(candidates)\n    for item in candidates:\n        if np.random.rand() < mutation_rate and current_weight + weight_lst[item] <= capacity:\n            if np.random.rand() < perturbation_intensity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                current_value1 += value1_lst[item]\n                current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined ratio and high weight\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n    for item in included_items:\n        if np.random.rand() < mutation_rate * 0.7:\n            if np.random.rand() < perturbation_intensity:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                current_value1 -= value1_lst[item]\n                current_value2 -= value2_lst[item]\n\n    # Step 3: Critical item flip with adaptive probability\n    critical_items = np.argsort(combined_ratio)[-max(1, len(weight_lst) // 8):]\n    if len(critical_items) > 0 and np.random.rand() < 0.6 * (1 - objective_correlation):\n        item = np.random.choice(critical_items)\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n        elif new_solution[item] == 1:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Ensure feasibility\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        combined_ratios = combined_ratio[excess_items]\n        remove_item = excess_items[np.argmin(combined_ratios)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "score": [
            -0.5548091930900465,
            4.165278643369675
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate crowding distances to identify solutions in less crowded regions\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for i in range(2):  # For each objective\n        sorted_indices = np.argsort(objectives[:, i])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for j in range(1, len(archive)-1):\n            if objectives[sorted_indices[-1], i] == objectives[sorted_indices[0], i]:\n                crowding_distances[sorted_indices[j]] += 0\n            else:\n                crowding_distances[sorted_indices[j]] += (objectives[sorted_indices[j+1], i] - objectives[sorted_indices[j-1], i]) / (objectives[sorted_indices[-1], i] - objectives[sorted_indices[0], i])\n\n    # Select solution with highest crowding distance (less crowded region)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate objective correlations and diversity scores\n    objective_correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    diversity_scores = np.std(objectives, axis=0)\n    mutation_rate = 0.3 + 0.4 * (1 - objective_correlation)  # Higher mutation when objectives are less correlated\n\n    # Calculate combined value-to-weight ratios with diversity weighting\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = (ratio1 * diversity_scores[0] + ratio2 * diversity_scores[1]) / np.sum(diversity_scores)\n\n    # Dynamic perturbation based on solution's position in objective space\n    normalized_objectives = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0))\n    solution_position = normalized_objectives[selected_idx]\n    perturbation_intensity = 0.5 * (1 - np.min(solution_position))  # More perturbation for solutions in less optimal regions\n\n    # Guided evolutionary mutation\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 1: Add items with high combined ratio and low weight\n    candidates = np.where(new_solution == 0)[0]\n    np.random.shuffle(candidates)\n    for item in candidates:\n        if np.random.rand() < mutation_rate and current_weight + weight_lst[item] <= capacity:\n            if np.random.rand() < perturbation_intensity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                current_value1 += value1_lst[item]\n                current_value2 += value2_lst[item]\n\n    # Step 2: Remove items with low combined ratio and high weight\n    included_items = np.where(new_solution == 1)[0]\n    np.random.shuffle(included_items)\n    for item in included_items:\n        if np.random.rand() < mutation_rate * 0.7:\n            if np.random.rand() < perturbation_intensity:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                current_value1 -= value1_lst[item]\n                current_value2 -= value2_lst[item]\n\n    # Step 3: Critical item flip with adaptive probability\n    critical_items = np.argsort(combined_ratio)[-max(1, len(weight_lst) // 8):]\n    if len(critical_items) > 0 and np.random.rand() < 0.6 * (1 - objective_correlation):\n        item = np.random.choice(critical_items)\n        if new_solution[item] == 0 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n        elif new_solution[item] == 1:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    # Ensure feasibility\n    while current_weight > capacity and len(np.where(new_solution == 1)[0]) > 0:\n        excess_items = np.where(new_solution == 1)[0]\n        combined_ratios = combined_ratio[excess_items]\n        remove_item = excess_items[np.argmin(combined_ratios)]\n        new_solution[remove_item] = 0\n        current_weight -= weight_lst[remove_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    }
]