[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 1,
        "algorithm": "The algorithm selects a random solution from the archive and applies a hybrid local search combining item swapping and probabilistic flipping, ensuring feasibility by reverting infeasible moves. It prioritizes adding items (30% chance) and removing items (20% chance) while checking weight constraints, with swaps only occurring between already included items. The method balances exploration (random selection) and exploitation (targeted flips) to generate diverse feasible neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5540389014644479,
            6.753941833972931
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 1,
        "algorithm": "The algorithm selects a random solution from the archive and applies a hybrid local search combining item swapping and probabilistic flipping, ensuring feasibility by reverting infeasible moves. It prioritizes adding items (30% chance) and removing items (20% chance) while checking weight constraints, with swaps only occurring between already included items. The method balances exploration (random selection) and exploitation (targeted flips) to generate diverse feasible neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5540389014644479,
            6.753941833972931
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 1,
        "algorithm": "The algorithm selects a random solution from the archive and applies a hybrid local search combining item swapping and probabilistic flipping, ensuring feasibility by reverting infeasible moves. It prioritizes adding items (30% chance) and removing items (20% chance) while checking weight constraints, with swaps only occurring between already included items. The method balances exploration (random selection) and exploitation (targeted flips) to generate diverse feasible neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5540389014644479,
            6.753941833972931
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 2,
        "algorithm": "The algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\n",
        "score": [
            -0.36325323951985933,
            1.8801483511924744
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 2,
        "algorithm": "The algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\n",
        "score": [
            -0.36325323951985933,
            1.8801483511924744
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 3,
        "algorithm": "The algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.3908341285465272,
            0.2811393439769745
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 3,
        "algorithm": "The algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.3908341285465272,
            0.2811393439769745
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 4,
        "algorithm": "The heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.6520115066392751,
            3.8804451525211334
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 4,
        "algorithm": "The heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.6520115066392751,
            3.8804451525211334
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 5,
        "algorithm": "The algorithm selects a promising solution from the top 20% of the archive (prioritizing higher combined objective values) and applies a hybrid local search combining item swaps and path-relinking to generate a neighbor solution, ensuring feasibility by checking weight constraints. It first swaps an item from the current solution with one not included, then greedily adds additional items if capacity allows. The selection prioritizes solutions with higher potential for improvement, while the local search balances exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (e.g., top 20% by objective values)\n    archive_sorted = sorted(archive, key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    top_solutions = archive_sorted[:max(1, len(archive) // 5)]\n    selected = random.choice(top_solutions)\n    base_solution = selected[0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = selected[1][0]\n    current_value2 = selected[1][1]\n\n    # Hybrid local search: item swap with path-relinking\n    new_solution = base_solution.copy()\n    candidates = np.where(new_solution == 1)[0]\n    non_candidates = np.where(new_solution == 0)[0]\n\n    if len(candidates) > 0 and len(non_candidates) > 0:\n        # Swap one item from candidates with one from non-candidates\n        swap_out = random.choice(candidates)\n        swap_in = random.choice(non_candidates)\n\n        if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n            new_solution[swap_out] = 0\n            new_solution[swap_in] = 1\n\n        # Path-relinking: try to include additional items if possible\n        remaining_weight = capacity - np.sum(weight_lst * new_solution)\n        for item in non_candidates:\n            if new_solution[item] == 0 and weight_lst[item] <= remaining_weight:\n                new_solution[item] = 1\n                remaining_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.303579395532158,
            2.4485089480876923
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (e.g., top 20% by objective values)\n    archive_sorted = sorted(archive, key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    top_solutions = archive_sorted[:max(1, len(archive) // 5)]\n    selected = random.choice(top_solutions)\n    base_solution = selected[0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = selected[1][0]\n    current_value2 = selected[1][1]\n\n    # Hybrid local search: item swap with path-relinking\n    new_solution = base_solution.copy()\n    candidates = np.where(new_solution == 1)[0]\n    non_candidates = np.where(new_solution == 0)[0]\n\n    if len(candidates) > 0 and len(non_candidates) > 0:\n        # Swap one item from candidates with one from non-candidates\n        swap_out = random.choice(candidates)\n        swap_in = random.choice(non_candidates)\n\n        if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n            new_solution[swap_out] = 0\n            new_solution[swap_in] = 1\n\n        # Path-relinking: try to include additional items if possible\n        remaining_weight = capacity - np.sum(weight_lst * new_solution)\n        for item in non_candidates:\n            if new_solution[item] == 0 and weight_lst[item] <= remaining_weight:\n                new_solution[item] = 1\n                remaining_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            \n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 6,
        "algorithm": "The algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n",
        "score": [
            -0.40184509587363354,
            0.9591511785984039
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            \n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 7,
        "algorithm": "The algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9382821717578097,
            0.46551522612571716
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            \n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 7,
        "algorithm": "The algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9382821717578097,
            0.46551522612571716
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 8,
        "algorithm": "The algorithm selects a solution with the lowest dominance count (most diverse) from the archive and applies a hybrid local search combining probabilistic swaps and value-weighted replacements to generate a neighbor solution while ensuring feasibility. It prioritizes items with higher combined value-to-weight ratios and performs targeted swaps or replacements to improve both objectives, with a final feasibility check to remove excess items if necessary.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate dominance counts and select a solution with high diversity\n    dominance_counts = []\n    for i, (sol, _) in enumerate(archive):\n        count = sum(1 for (_, obj) in archive if obj[0] > archive[i][1][0] and obj[1] > archive[i][1][1])\n        dominance_counts.append(count)\n\n    # Select solution with lowest dominance count (most diverse)\n    selected_idx = np.argmin(dominance_counts)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Probabilistic swaps based on value-weight ratio\n    for i in range(n_items):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(n_items)\n            if i != j and new_solution[i] != new_solution[j]:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += delta_weight\n\n    # Value-weighted replacements\n    for i in range(n_items):\n        if new_solution[i] == 1 and np.random.random() < 0.5:  # 50% chance to consider replacement\n            # Calculate value-to-weight ratios\n            ratios = (value1_lst + value2_lst) / weight_lst\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                               (new_solution == 0) &\n                               (ratios > ratios[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(ratios[candidates])]  # Select best candidate\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest value-to-weight ratio\n        ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n        i = excess_items[np.argmin(ratios)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.44655856702936997,
            1.5639890134334564
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate dominance counts and select a solution with high diversity\n    dominance_counts = []\n    for i, (sol, _) in enumerate(archive):\n        count = sum(1 for (_, obj) in archive if obj[0] > archive[i][1][0] and obj[1] > archive[i][1][1])\n        dominance_counts.append(count)\n\n    # Select solution with lowest dominance count (most diverse)\n    selected_idx = np.argmin(dominance_counts)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Probabilistic swaps based on value-weight ratio\n    for i in range(n_items):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(n_items)\n            if i != j and new_solution[i] != new_solution[j]:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += delta_weight\n\n    # Value-weighted replacements\n    for i in range(n_items):\n        if new_solution[i] == 1 and np.random.random() < 0.5:  # 50% chance to consider replacement\n            # Calculate value-to-weight ratios\n            ratios = (value1_lst + value2_lst) / weight_lst\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                               (new_solution == 0) &\n                               (ratios > ratios[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(ratios[candidates])]  # Select best candidate\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest value-to-weight ratio\n        ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n        i = excess_items[np.argmin(ratios)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 9,
        "algorithm": "This algorithm selects a promising solution from the archive based on its proximity to the Pareto front, then applies a hybrid local search combining probabilistic item swaps and a novel value-ratio-based replacement strategy to generate neighbors while ensuring feasibility. It prioritizes swaps and replacements that improve both objectives, with special attention to items with high value ratios, and finally enforces feasibility by removing low-value items if necessary. The selection of the base solution is biased toward solutions that are close to the Pareto front, while the local search focuses on improving both objectives through intelligent item swaps and replacements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution based on dominance in objective space\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1 = np.max(objectives[:, 0])\n    max_obj2 = np.max(objectives[:, 1])\n\n    # Prioritize solutions that are close to the Pareto front\n    dominance_scores = np.abs(objectives[:, 0] - max_obj1) + np.abs(objectives[:, 1] - max_obj2)\n    selected_idx = np.argmin(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Probabilistic swaps based on value ratios\n    for _ in range(min(5, n_items // 3)):\n        i = np.random.choice(np.where(new_solution == 1)[0])\n        j = np.random.choice(np.where(new_solution == 0)[0])\n\n        # Calculate value ratios for both objectives\n        ratio1 = value1_lst[j] / (value1_lst[i] + 1e-10)\n        ratio2 = value2_lst[j] / (value2_lst[i] + 1e-10)\n\n        # Higher probability for swaps that improve both objectives\n        if (ratio1 > 0.8 or ratio2 > 0.8) and (weight_lst[j] <= weight_lst[i] or current_weight + weight_lst[j] - weight_lst[i] <= capacity):\n            new_solution[i], new_solution[j] = 0, 1\n            current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 2: Value-ratio based replacement\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Find items to replace with better value ratios\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > 0.9 * value1_lst[i]) | (value2_lst > 0.9 * value2_lst[i])))[0]\n\n            if len(candidates) > 0:\n                # Select candidate with highest combined value ratio\n                ratios = (value1_lst[candidates] / value1_lst[i] + value2_lst[candidates] / value2_lst[i])\n                j = candidates[np.argmax(ratios)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined value ratio\n        ratios = value1_lst[excess_items] + value2_lst[excess_items]\n        i = excess_items[np.argmin(ratios)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5934214454646424,
            2.5533646941184998
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution based on dominance in objective space\n    objectives = np.array([obj for _, obj in archive])\n    max_obj1 = np.max(objectives[:, 0])\n    max_obj2 = np.max(objectives[:, 1])\n\n    # Prioritize solutions that are close to the Pareto front\n    dominance_scores = np.abs(objectives[:, 0] - max_obj1) + np.abs(objectives[:, 1] - max_obj2)\n    selected_idx = np.argmin(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Probabilistic swaps based on value ratios\n    for _ in range(min(5, n_items // 3)):\n        i = np.random.choice(np.where(new_solution == 1)[0])\n        j = np.random.choice(np.where(new_solution == 0)[0])\n\n        # Calculate value ratios for both objectives\n        ratio1 = value1_lst[j] / (value1_lst[i] + 1e-10)\n        ratio2 = value2_lst[j] / (value2_lst[i] + 1e-10)\n\n        # Higher probability for swaps that improve both objectives\n        if (ratio1 > 0.8 or ratio2 > 0.8) and (weight_lst[j] <= weight_lst[i] or current_weight + weight_lst[j] - weight_lst[i] <= capacity):\n            new_solution[i], new_solution[j] = 0, 1\n            current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 2: Value-ratio based replacement\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Find items to replace with better value ratios\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > 0.9 * value1_lst[i]) | (value2_lst > 0.9 * value2_lst[i])))[0]\n\n            if len(candidates) > 0:\n                # Select candidate with highest combined value ratio\n                ratios = (value1_lst[candidates] / value1_lst[i] + value2_lst[candidates] / value2_lst[i])\n                j = candidates[np.argmax(ratios)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined value ratio\n        ratios = value1_lst[excess_items] + value2_lst[excess_items]\n        i = excess_items[np.argmin(ratios)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### Ineffective Self-Reflection\n- **Keywords**: Over-optimization, rigid structure, lack of adaptability, neglecting trade-offs.\n- **Advice**: Avoid over-engineering; prioritize simplicity and flexibility in local search.\n- **Avoid**: Falling into local optima by rigidly following one strategy; ensure diversity in operations.\n- **Explanation**: Excessive complexity can hinder performance; balance exploitation and exploration dynamically.\n\n### Effective Self-Reflection\n- **Keywords**: Hybridization, marginal impact, adaptive selection, feasibility-aware.\n- **Advice**: Combine marginal impact analysis with adaptive local search (e.g., swap/replace items based on Pareto dominance).\n- **Avoid**: Static thresholds (e.g., top 20%); use dynamic criteria (e.g., solutions near the frontier).\n- **Explanation**: Dynamic criteria ensure solutions are both feasible and promising, avoiding premature convergence.\n\n**Key Idea**: Design a heuristic that intelligently selects neighbors by combining marginal impact with adaptive local search, ensuring feasibility and high-quality solutions.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 10,
        "algorithm": "The algorithm intelligently selects a solution from the archive, prioritizes items with high combined marginal gain for both objectives, performs targeted flips and replacements to improve Pareto-dominance, and ensures feasibility through dynamic weight checks. It balances exploration (via random selection and candidate ranking) with exploitation (via marginal gain analysis and Pareto-dominance checks), while strictly maintaining capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligently select a solution with high marginal impact\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Dynamic marginal impact analysis\n    included = new_solution == 1\n    excluded = new_solution == 0\n\n    # Calculate marginal gains for excluded items (potential additions)\n    marginal_gain1 = value1_lst[excluded] / weight_lst[excluded]\n    marginal_gain2 = value2_lst[excluded] / weight_lst[excluded]\n\n    # Rank items by combined marginal gain (Pareto-dominance approximation)\n    combined_gain = marginal_gain1 + marginal_gain2\n    top_candidates = np.argsort(combined_gain)[-min(5, len(combined_gain)):]\n\n    # Step 2: Adaptive local search - targeted flips and replacements\n    for i in top_candidates:\n        if current_weight + weight_lst[i] <= capacity:\n            new_solution[i] = 1\n            current_weight += weight_lst[i]\n\n    # Step 3: Dynamic Pareto-dominance check for included items\n    for i in np.where(included)[0]:\n        # Find items that would improve both objectives when swapped\n        candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                            (excluded) &\n                            ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n\n        if len(candidates) > 0:\n            j = np.random.choice(candidates)\n            new_solution[i], new_solution[j] = 0, 1\n            current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (final check)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7989523428841754,
            1.974008470773697
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligently select a solution with high marginal impact\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Dynamic marginal impact analysis\n    included = new_solution == 1\n    excluded = new_solution == 0\n\n    # Calculate marginal gains for excluded items (potential additions)\n    marginal_gain1 = value1_lst[excluded] / weight_lst[excluded]\n    marginal_gain2 = value2_lst[excluded] / weight_lst[excluded]\n\n    # Rank items by combined marginal gain (Pareto-dominance approximation)\n    combined_gain = marginal_gain1 + marginal_gain2\n    top_candidates = np.argsort(combined_gain)[-min(5, len(combined_gain)):]\n\n    # Step 2: Adaptive local search - targeted flips and replacements\n    for i in top_candidates:\n        if current_weight + weight_lst[i] <= capacity:\n            new_solution[i] = 1\n            current_weight += weight_lst[i]\n\n    # Step 3: Dynamic Pareto-dominance check for included items\n    for i in np.where(included)[0]:\n        # Find items that would improve both objectives when swapped\n        candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                            (excluded) &\n                            ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n\n        if len(candidates) > 0:\n            j = np.random.choice(candidates)\n            new_solution[i], new_solution[j] = 0, 1\n            current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (final check)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution with the lowest dominance count (most diverse) from the archive and applies a hybrid local search combining probabilistic swaps and value-weighted replacements to generate a neighbor solution while ensuring feasibility. It prioritizes items with higher combined value-to-weight ratios and performs targeted swaps or replacements to improve both objectives, with a final feasibility check to remove excess items if necessary.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate dominance counts and select a solution with high diversity\n    dominance_counts = []\n    for i, (sol, _) in enumerate(archive):\n        count = sum(1 for (_, obj) in archive if obj[0] > archive[i][1][0] and obj[1] > archive[i][1][1])\n        dominance_counts.append(count)\n\n    # Select solution with lowest dominance count (most diverse)\n    selected_idx = np.argmin(dominance_counts)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Probabilistic swaps based on value-weight ratio\n    for i in range(n_items):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(n_items)\n            if i != j and new_solution[i] != new_solution[j]:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += delta_weight\n\n    # Value-weighted replacements\n    for i in range(n_items):\n        if new_solution[i] == 1 and np.random.random() < 0.5:  # 50% chance to consider replacement\n            # Calculate value-to-weight ratios\n            ratios = (value1_lst + value2_lst) / weight_lst\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                               (new_solution == 0) &\n                               (ratios > ratios[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(ratios[candidates])]  # Select best candidate\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest value-to-weight ratio\n        ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n        i = excess_items[np.argmin(ratios)]\n        new_solution[i] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### Ineffective Self-Reflection\n- **Keywords**: Over-optimization, rigid structure, lack of adaptability, neglecting trade-offs.\n- **Advice**: Avoid over-engineering; prioritize simplicity and flexibility in local search.\n- **Avoid**: Falling into local optima by rigidly following one strategy; ensure diversity in operations.\n- **Explanation**: Excessive complexity can hinder performance; balance exploitation and exploration dynamically.\n\n### Effective Self-Reflection\n- **Keywords**: Hybridization, marginal impact, adaptive selection, feasibility-aware.\n- **Advice**: Combine marginal impact analysis with adaptive local search (e.g., swap/replace items based on Pareto dominance).\n- **Avoid**: Static thresholds (e.g., top 20%); use dynamic criteria (e.g., solutions near the frontier).\n- **Explanation**: Dynamic criteria ensure solutions are both feasible and promising, avoiding premature convergence.\n\n**Key Idea**: Design a heuristic that intelligently selects neighbors by combining marginal impact with adaptive local search, ensuring feasibility and high-quality solutions.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 11,
        "algorithm": "The algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5841007254867978,
            1.7514790296554565
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### Ineffective Self-Reflection\n- **Keywords**: Over-optimization, rigid structure, lack of adaptability, neglecting trade-offs.\n- **Advice**: Avoid over-engineering; prioritize simplicity and flexibility in local search.\n- **Avoid**: Falling into local optima by rigidly following one strategy; ensure diversity in operations.\n- **Explanation**: Excessive complexity can hinder performance; balance exploitation and exploration dynamically.\n\n### Effective Self-Reflection\n- **Keywords**: Hybridization, marginal impact, adaptive selection, feasibility-aware.\n- **Advice**: Combine marginal impact analysis with adaptive local search (e.g., swap/replace items based on Pareto dominance).\n- **Avoid**: Static thresholds (e.g., top 20%); use dynamic criteria (e.g., solutions near the frontier).\n- **Explanation**: Dynamic criteria ensure solutions are both feasible and promising, avoiding premature convergence.\n\n**Key Idea**: Design a heuristic that intelligently selects neighbors by combining marginal impact with adaptive local search, ensuring feasibility and high-quality solutions.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 12,
        "algorithm": "The algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9093676276444078,
            0.6341427564620972
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 13,
        "algorithm": "The algorithm selects the highest-scoring solution from the archive (based on normalized sum of both objectives) and generates a neighbor by flipping a weighted subset of top-impact items, ensuring feasibility through marginal impact calculations and weighted random selection. It prioritizes items with high combined marginal impact while balancing exploration (random subset selection) and exploitation (top-impact filtering). The solution remains feasible by dynamically adjusting flips based on remaining capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    normalized_scores = []\n    for sol, obj in archive:\n        norm1 = obj[0] / (value1_lst.sum() + 1e-6)\n        norm2 = obj[1] / (value2_lst.sum() + 1e-6)\n        normalized_scores.append(norm1 + norm2)\n    selected_idx = np.argmax(normalized_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact for both objectives\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Combine marginal impacts with weights and normalize\n    combined_impact = (marginal_impact1 + marginal_impact2) * (1 - weight_lst / (capacity + 1e-6))\n\n    # Select top 20% items with highest combined impact\n    num_items = len(weight_lst)\n    top_items = np.argsort(combined_impact)[-max(1, num_items // 5):]\n\n    # Randomly select a subset of top items to flip with weighted probability\n    if len(top_items) > 0:\n        weights = combined_impact[top_items] / (combined_impact[top_items].sum() + 1e-6)\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False, p=weights)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.5235000682259698,
            7.025820434093475
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    normalized_scores = []\n    for sol, obj in archive:\n        norm1 = obj[0] / (value1_lst.sum() + 1e-6)\n        norm2 = obj[1] / (value2_lst.sum() + 1e-6)\n        normalized_scores.append(norm1 + norm2)\n    selected_idx = np.argmax(normalized_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact for both objectives\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Combine marginal impacts with weights and normalize\n    combined_impact = (marginal_impact1 + marginal_impact2) * (1 - weight_lst / (capacity + 1e-6))\n\n    # Select top 20% items with highest combined impact\n    num_items = len(weight_lst)\n    top_items = np.argsort(combined_impact)[-max(1, num_items // 5):]\n\n    # Randomly select a subset of top items to flip with weighted probability\n    if len(top_items) > 0:\n        weights = combined_impact[top_items] / (combined_impact[top_items].sum() + 1e-6)\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False, p=weights)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 14,
        "algorithm": "The algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.45860529653600796,
            0.24088504910469055
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 14,
        "algorithm": "The algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.45860529653600796,
            0.24088504910469055
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined **ineffective self-reflection** with clear actionable points:\n\n- **Keywords**: Over-reliance on fixed thresholds, static exploration, neglecting trade-off dynamics, rigidity in operations.\n- **Advice**: Adopt dynamic thresholds (e.g., percentile-based marginal impact) and hybridize operations (e.g., add/swap/replace) to balance exploration/exploitation.\n- **Avoid**: Hardcoding item selection criteria (e.g., \"top 20%\") without adapting to solution diversity or instance characteristics.\n- **Explanation**: Static strategies risk premature convergence or inefficiency. Dynamic adaptation ensures feasible, high-quality neighbors while scaling to problem complexity.\n\n**Key Idea**: Use adaptive marginal impact and hybrid operations to avoid over-optimization and ensure robust performance.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 15,
        "algorithm": "The algorithm selects a promising solution from the archive (top 30% by combined objective value) and applies a hybrid local search combining probabilistic additions of high-impact items, targeted swaps between included and excluded items, and adaptive removals of low-impact items to ensure feasibility while balancing exploration and exploitation. It prioritizes items with high marginal impact (top percentile) in both objectives and dynamically adjusts the solution to maintain capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 30% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[min(len(archive) // 3, len(archive) - 1)][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impact (percentile-based)\n    included = base_solution == 1\n    marginal_impact1 = value1_lst - value1_lst[included].sum()\n    marginal_impact2 = value2_lst - value2_lst[included].sum()\n    top_percentile = min(30, len(weight_lst) // 5)  # Dynamic threshold\n\n    top_items1 = np.argsort(marginal_impact1)[-top_percentile:]\n    top_items2 = np.argsort(marginal_impact2)[-top_percentile:]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Hybrid local search: Phase 1 - Probabilistic additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0) & np.isin(np.arange(len(weight_lst)), top_items))[0]\n    if len(candidates) > 0:\n        for idx in np.random.permutation(candidates):\n            if np.random.rand() < 0.5:  # 50% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2 - Targeted swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if i not in top_items:\n            continue\n        for j in excluded_items:\n            if j not in top_items:\n                continue\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3 - Adaptive removals\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_item = included_items[np.argmin(marginal_combined)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9131241836479801,
            3.4764381051063538
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 30% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[min(len(archive) // 3, len(archive) - 1)][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impact (percentile-based)\n    included = base_solution == 1\n    marginal_impact1 = value1_lst - value1_lst[included].sum()\n    marginal_impact2 = value2_lst - value2_lst[included].sum()\n    top_percentile = min(30, len(weight_lst) // 5)  # Dynamic threshold\n\n    top_items1 = np.argsort(marginal_impact1)[-top_percentile:]\n    top_items2 = np.argsort(marginal_impact2)[-top_percentile:]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Hybrid local search: Phase 1 - Probabilistic additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0) & np.isin(np.arange(len(weight_lst)), top_items))[0]\n    if len(candidates) > 0:\n        for idx in np.random.permutation(candidates):\n            if np.random.rand() < 0.5:  # 50% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2 - Targeted swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if i not in top_items:\n            continue\n        for j in excluded_items:\n            if j not in top_items:\n                continue\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3 - Adaptive removals\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_item = included_items[np.argmin(marginal_combined)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **ineffective self-reflection** with clear actionable points:\n\n- **Keywords**: Over-reliance on fixed thresholds, static exploration, neglecting trade-off dynamics, rigidity in operations.\n- **Advice**: Adopt dynamic thresholds (e.g., percentile-based marginal impact) and hybridize operations (e.g., add/swap/replace) to balance exploration/exploitation.\n- **Avoid**: Hardcoding item selection criteria (e.g., \"top 20%\") without adapting to solution diversity or instance characteristics.\n- **Explanation**: Static strategies risk premature convergence or inefficiency. Dynamic adaptation ensures feasible, high-quality neighbors while scaling to problem complexity.\n\n**Key Idea**: Use adaptive marginal impact and hybrid operations to avoid over-optimization and ensure robust performance.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 16,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a hybrid local search strategy that prioritizes high-impact items by first removing low-performing items below a 75th percentile ratio threshold, then strategically inserting top 30% high-margin items from excluded options, and finally performing probabilistic item replacements to rebalance the solution while maintaining feasibility through continuous weight tracking and dynamic adjustments. The method balances exploration (via random selection and insertion) with exploitation (targeted removal and replacement) while ensuring capacity constraints are respected through final feasibility checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Calculate dynamic thresholds for marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        # Calculate value ratios for included items\n        included_ratios = (value1_lst[included_items] + 1e-6) / (value2_lst[included_items] + 1e-6)\n        ratio_threshold = np.percentile(included_ratios, 75)  # Top 25% ratio threshold\n\n        # Phase 1: Remove low-impact items below threshold\n        for i in included_items:\n            if (value1_lst[i] + 1e-6) / (value2_lst[i] + 1e-6) < ratio_threshold:\n                if np.random.rand() < 0.4:  # 40% chance to remove\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 2: Insert high-impact items from excluded\n    if len(excluded_items) > 0:\n        # Calculate marginal improvements for excluded items\n        marginal_weights = weight_lst[excluded_items]\n        marginal_value1 = value1_lst[excluded_items]\n        marginal_value2 = value2_lst[excluded_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = excluded_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                if np.random.rand() < 0.6:  # 60% chance to insert\n                    new_solution[item] = 1\n                    current_weight += marginal_weights[idx]\n\n    # Phase 3: Probabilistic replacement for rebalancing\n    if current_weight < capacity:\n        # Find items to potentially replace\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Calculate replacement potential\n            for i in included_items:\n                for j in excluded_items:\n                    if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                        # Calculate potential improvement\n                        delta_value1 = value1_lst[j] - value1_lst[i]\n                        delta_value2 = value2_lst[j] - value2_lst[i]\n\n                        # Accept if both objectives improve or one improves significantly\n                        if (delta_value1 > 0 and delta_value2 > 0) or \\\n                           (delta_value1 > 0 and delta_value2 >= -0.1 * value2_lst[i]) or \\\n                           (delta_value2 > 0 and delta_value1 >= -0.1 * value1_lst[i]):\n                            if np.random.rand() < 0.5:  # 50% chance to replace\n                                new_solution[i], new_solution[j] = 0, 1\n                                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                                break\n\n    # Final feasibility check\n    if np.sum(weight_lst * new_solution) > capacity:\n        # Remove items with lowest value ratio until feasible\n        while np.sum(weight_lst * new_solution) > capacity:\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) == 0:\n                break\n            value_ratios = (value1_lst[included_items] + 1e-6) / (value2_lst[included_items] + 1e-6)\n            worst_item = included_items[np.argmin(value_ratios)]\n            new_solution[worst_item] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8781872264842637,
            3.1987808644771576
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Calculate dynamic thresholds for marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        # Calculate value ratios for included items\n        included_ratios = (value1_lst[included_items] + 1e-6) / (value2_lst[included_items] + 1e-6)\n        ratio_threshold = np.percentile(included_ratios, 75)  # Top 25% ratio threshold\n\n        # Phase 1: Remove low-impact items below threshold\n        for i in included_items:\n            if (value1_lst[i] + 1e-6) / (value2_lst[i] + 1e-6) < ratio_threshold:\n                if np.random.rand() < 0.4:  # 40% chance to remove\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 2: Insert high-impact items from excluded\n    if len(excluded_items) > 0:\n        # Calculate marginal improvements for excluded items\n        marginal_weights = weight_lst[excluded_items]\n        marginal_value1 = value1_lst[excluded_items]\n        marginal_value2 = value2_lst[excluded_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = excluded_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                if np.random.rand() < 0.6:  # 60% chance to insert\n                    new_solution[item] = 1\n                    current_weight += marginal_weights[idx]\n\n    # Phase 3: Probabilistic replacement for rebalancing\n    if current_weight < capacity:\n        # Find items to potentially replace\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Calculate replacement potential\n            for i in included_items:\n                for j in excluded_items:\n                    if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                        # Calculate potential improvement\n                        delta_value1 = value1_lst[j] - value1_lst[i]\n                        delta_value2 = value2_lst[j] - value2_lst[i]\n\n                        # Accept if both objectives improve or one improves significantly\n                        if (delta_value1 > 0 and delta_value2 > 0) or \\\n                           (delta_value1 > 0 and delta_value2 >= -0.1 * value2_lst[i]) or \\\n                           (delta_value2 > 0 and delta_value1 >= -0.1 * value1_lst[i]):\n                            if np.random.rand() < 0.5:  # 50% chance to replace\n                                new_solution[i], new_solution[j] = 0, 1\n                                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                                break\n\n    # Final feasibility check\n    if np.sum(weight_lst * new_solution) > capacity:\n        # Remove items with lowest value ratio until feasible\n        while np.sum(weight_lst * new_solution) > capacity:\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) == 0:\n                break\n            value_ratios = (value1_lst[included_items] + 1e-6) / (value2_lst[included_items] + 1e-6)\n            worst_item = included_items[np.argmin(value_ratios)]\n            new_solution[worst_item] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution with the lowest dominance count (most diverse) from the archive and applies a hybrid local search combining probabilistic swaps and value-weighted replacements to generate a neighbor solution while ensuring feasibility. It prioritizes items with higher combined value-to-weight ratios and performs targeted swaps or replacements to improve both objectives, with a final feasibility check to remove excess items if necessary.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate dominance counts and select a solution with high diversity\n    dominance_counts = []\n    for i, (sol, _) in enumerate(archive):\n        count = sum(1 for (_, obj) in archive if obj[0] > archive[i][1][0] and obj[1] > archive[i][1][1])\n        dominance_counts.append(count)\n\n    # Select solution with lowest dominance count (most diverse)\n    selected_idx = np.argmin(dominance_counts)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Probabilistic swaps based on value-weight ratio\n    for i in range(n_items):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(n_items)\n            if i != j and new_solution[i] != new_solution[j]:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += delta_weight\n\n    # Value-weighted replacements\n    for i in range(n_items):\n        if new_solution[i] == 1 and np.random.random() < 0.5:  # 50% chance to consider replacement\n            # Calculate value-to-weight ratios\n            ratios = (value1_lst + value2_lst) / weight_lst\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                               (new_solution == 0) &\n                               (ratios > ratios[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(ratios[candidates])]  # Select best candidate\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest value-to-weight ratio\n        ratios = (value1_lst[excess_items] + value2_lst[excess_items]) / weight_lst[excess_items]\n        i = excess_items[np.argmin(ratios)]\n        new_solution[i] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **ineffective self-reflection** with clear actionable points:\n\n- **Keywords**: Over-reliance on fixed thresholds, static exploration, neglecting trade-off dynamics, rigidity in operations.\n- **Advice**: Adopt dynamic thresholds (e.g., percentile-based marginal impact) and hybridize operations (e.g., add/swap/replace) to balance exploration/exploitation.\n- **Avoid**: Hardcoding item selection criteria (e.g., \"top 20%\") without adapting to solution diversity or instance characteristics.\n- **Explanation**: Static strategies risk premature convergence or inefficiency. Dynamic adaptation ensures feasible, high-quality neighbors while scaling to problem complexity.\n\n**Key Idea**: Use adaptive marginal impact and hybrid operations to avoid over-optimization and ensure robust performance.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 17,
        "algorithm": "The algorithm selects a promising solution from the archive based on marginal potential scores (prioritizing items with high value-weight ratios for both objectives), then applies a three-phase hybrid local search: probabilistic additions of high-marginal items, targeted swaps improving both objectives, and dynamic rebalancing by removing low-performing items below a 30th-percentile threshold to ensure feasibility. The method intelligently balances exploration and exploitation by dynamically adjusting thresholds and hybridizing operations to maintain solution quality across objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest marginal potential\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        obj1, obj2 = obj\n\n        # Calculate marginal impact for included items\n        included_marginal = (value1_lst[included] / weight_lst[included], value2_lst[included] / weight_lst[included])\n        # Calculate potential marginal impact for excluded items\n        excluded_marginal = (value1_lst[excluded] / weight_lst[excluded], value2_lst[excluded] / weight_lst[excluded])\n\n        # Score based on both objectives' potential improvement\n        score = np.mean(included_marginal[0]) + np.mean(included_marginal[1]) + 0.5 * np.mean(excluded_marginal[0]) + 0.5 * np.mean(excluded_marginal[1])\n        marginal_scores.append(score)\n\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic additions based on normalized marginal impact\n    remaining_capacity = capacity - current_weight\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized marginal impact scores\n        marginal_impact = (value1_lst[excluded_items] / weight_lst[excluded_items] + value2_lst[excluded_items] / weight_lst[excluded_items]) / 2\n        marginal_impact /= np.max(marginal_impact) if np.max(marginal_impact) > 0 else 1\n\n        # Add items with probability proportional to their normalized marginal impact\n        for i in excluded_items:\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < marginal_impact[i] * 0.8:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                remaining_capacity -= weight_lst[i]\n\n    # Phase 2: Targeted swaps improving both objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Calculate objective improvements\n                obj1_improve = value1_lst[j] > value1_lst[i]\n                obj2_improve = value2_lst[j] > value2_lst[i]\n\n                # Accept swap if it improves both objectives or maintains feasibility\n                if (obj1_improve and obj2_improve) or (obj1_improve and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or (obj2_improve and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate value-weight ratios and find 30th percentile threshold\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        threshold = np.percentile(ratios, 30) if len(ratios) > 1 else np.min(ratios)\n\n        # Remove items below threshold\n        candidates = included_items[ratios <= threshold]\n        if len(candidates) > 0:\n            worst_item = candidates[np.argmin(ratios[ratios <= threshold])]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8309101407237802,
            1.9195991456508636
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest marginal potential\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        obj1, obj2 = obj\n\n        # Calculate marginal impact for included items\n        included_marginal = (value1_lst[included] / weight_lst[included], value2_lst[included] / weight_lst[included])\n        # Calculate potential marginal impact for excluded items\n        excluded_marginal = (value1_lst[excluded] / weight_lst[excluded], value2_lst[excluded] / weight_lst[excluded])\n\n        # Score based on both objectives' potential improvement\n        score = np.mean(included_marginal[0]) + np.mean(included_marginal[1]) + 0.5 * np.mean(excluded_marginal[0]) + 0.5 * np.mean(excluded_marginal[1])\n        marginal_scores.append(score)\n\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Probabilistic additions based on normalized marginal impact\n    remaining_capacity = capacity - current_weight\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(excluded_items) > 0:\n        # Calculate normalized marginal impact scores\n        marginal_impact = (value1_lst[excluded_items] / weight_lst[excluded_items] + value2_lst[excluded_items] / weight_lst[excluded_items]) / 2\n        marginal_impact /= np.max(marginal_impact) if np.max(marginal_impact) > 0 else 1\n\n        # Add items with probability proportional to their normalized marginal impact\n        for i in excluded_items:\n            if weight_lst[i] <= remaining_capacity and np.random.rand() < marginal_impact[i] * 0.8:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                remaining_capacity -= weight_lst[i]\n\n    # Phase 2: Targeted swaps improving both objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Calculate objective improvements\n                obj1_improve = value1_lst[j] > value1_lst[i]\n                obj2_improve = value2_lst[j] > value2_lst[i]\n\n                # Accept swap if it improves both objectives or maintains feasibility\n                if (obj1_improve and obj2_improve) or (obj1_improve and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or (obj2_improve and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate value-weight ratios and find 30th percentile threshold\n        ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        threshold = np.percentile(ratios, 30) if len(ratios) > 1 else np.min(ratios)\n\n        # Remove items below threshold\n        candidates = included_items[ratios <= threshold]\n        if len(candidates) > 0:\n            worst_item = candidates[np.argmin(ratios[ratios <= threshold])]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 18,
        "algorithm": "The algorithm employs a multi-phase approach that first selects the most promising solution from the archive (prioritizing those with higher combined normalized utility scores), then adaptively adds high-utility items while respecting capacity constraints, followed by objective-aware swaps that prioritize items improving both objectives, and finally ensures feasibility through a weighted removal strategy that balances both objectives and weight. The dynamic selection probabilities and dominance-checking mechanism are key to its effectiveness.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive solution selection and initialization\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate normalized utility scores\n    norm_value1 = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-6)\n    norm_value2 = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-6)\n    utility_scores = (norm_value1 + norm_value2) / (weight_lst + 1e-6)\n\n    # Phase 2: Dynamic item addition with adaptive probabilities\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        probs = utility_scores[candidates]\n        probs = probs / np.sum(probs)\n        selected = np.random.choice(candidates, size=min(3, len(candidates)), p=probs, replace=False)\n\n        for idx in selected:\n            if np.random.rand() < 0.8:  # 80% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Objective-aware swaps with dominance checking\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Novel dominance check\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                if (delta1 > 0 and delta2 > 0) or \\\n                   (delta1 > 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity)) or \\\n                   (delta2 > 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 4: Capacity-aware pruning with weighted removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate removal weights based on both objectives and weight\n        removal_weights = (value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6)\n        worst_item = included[np.argmin(removal_weights)]\n\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7899513916588247,
            0.9302513301372528
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive solution selection and initialization\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate normalized utility scores\n    norm_value1 = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-6)\n    norm_value2 = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-6)\n    utility_scores = (norm_value1 + norm_value2) / (weight_lst + 1e-6)\n\n    # Phase 2: Dynamic item addition with adaptive probabilities\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        probs = utility_scores[candidates]\n        probs = probs / np.sum(probs)\n        selected = np.random.choice(candidates, size=min(3, len(candidates)), p=probs, replace=False)\n\n        for idx in selected:\n            if np.random.rand() < 0.8:  # 80% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Objective-aware swaps with dominance checking\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Novel dominance check\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                if (delta1 > 0 and delta2 > 0) or \\\n                   (delta1 > 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity)) or \\\n                   (delta2 > 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 4: Capacity-aware pruning with weighted removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate removal weights based on both objectives and weight\n        removal_weights = (value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6)\n        worst_item = included[np.argmin(removal_weights)]\n\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 19,
        "algorithm": "The algorithm selects a high-potential solution from the archive, then performs a three-phase local search: 1) probabilistically adds high-dominance items, 2) swaps items to improve dominance while maintaining feasibility, and 3) removes low-dominance items to ensure capacity constraints are satisfied. Dominance is prioritized over individual objective values, with items evaluated by their combined value-to-weight ratio, and swaps are only performed if they improve the product of the two objective values.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high dominance potential\n    archive.sort(key=lambda x: (x[1][0] * x[1][1], sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate dominance scores for each item\n    dominance_scores = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(dominance_scores)\n\n    # Phase 1: Probabilistic dominance-based additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Dominance-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves dominance\n                if (value1_lst[j] * value2_lst[j] > value1_lst[i] * value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest dominance score first\n        worst_item = included_items[np.argmin(dominance_scores[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8100387846760317,
            1.3364790380001068
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high dominance potential\n    archive.sort(key=lambda x: (x[1][0] * x[1][1], sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate dominance scores for each item\n    dominance_scores = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(dominance_scores)\n\n    # Phase 1: Probabilistic dominance-based additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Dominance-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves dominance\n                if (value1_lst[j] * value2_lst[j] > value1_lst[i] * value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest dominance score first\n        worst_item = included_items[np.argmin(dominance_scores[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **ineffective self-reflection** with clear actionable points:\n\n- **Keywords**: Over-reliance on fixed thresholds, static exploration, neglecting trade-off dynamics, rigidity in operations.\n- **Advice**: Adopt dynamic thresholds (e.g., percentile-based marginal impact) and hybridize operations (e.g., add/swap/replace) to balance exploration/exploitation.\n- **Avoid**: Hardcoding item selection criteria (e.g., \"top 20%\") without adapting to solution diversity or instance characteristics.\n- **Explanation**: Static strategies risk premature convergence or inefficiency. Dynamic adaptation ensures feasible, high-quality neighbors while scaling to problem complexity.\n\n**Key Idea**: Use adaptive marginal impact and hybrid operations to avoid over-optimization and ensure robust performance.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 20,
        "algorithm": "The algorithm first selects a solution from the archive with high potential for improvement, then applies a hybrid local search that combines probabilistic item additions (prioritizing high-value items), targeted swaps (ensuring multi-objective improvements), and adaptive rebalancing (removing low-value items to maintain feasibility). The exploration intensity is dynamically adjusted based on the solution's current weight, and the search prioritizes items with favorable value ratios while ensuring both objectives are improved.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate dynamic value ratios and prioritize items\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions with dynamic intensity\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Adjust exploration intensity based on solution quality\n        exploration_prob = min(0.9, 0.5 + (current_weight / capacity) * 0.4)\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < exploration_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps with multi-objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check for multi-objective improvement\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive rebalancing with dynamic removal criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest value ratio or least contribution to objectives\n        removal_criteria = value_ratios[included_items] * (1 - (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst) + np.sum(value2_lst)))\n        worst_item = included_items[np.argmin(removal_criteria)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8370011111640884,
            0.5209901034832001
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate dynamic value ratios and prioritize items\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions with dynamic intensity\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Adjust exploration intensity based on solution quality\n        exploration_prob = min(0.9, 0.5 + (current_weight / capacity) * 0.4)\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < exploration_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps with multi-objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check for multi-objective improvement\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive rebalancing with dynamic removal criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest value ratio or least contribution to objectives\n        removal_criteria = value_ratios[included_items] * (1 - (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst) + np.sum(value2_lst)))\n        worst_item = included_items[np.argmin(removal_criteria)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 21,
        "algorithm": "The algorithm selects a promising solution from the archive (prioritizing high-value items) and applies a hybrid local search combining dynamic marginal impact analysis, hybrid operations (add/swap/replace), and percentile-based thresholds (top 30%) to balance exploration/exploitation while ensuring feasibility. It dynamically evaluates item impacts, adds high-value items, performs beneficial swaps, replaces low-value items, and enforces feasibility through final checks. The approach intelligently prioritizes items based on marginal gains for both objectives while maintaining capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (sum(x[1]), -np.sum(weight_lst[x[0] == 1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    included = base_solution == 1\n    excluded = base_solution == 0\n\n    marginal1 = value1_lst - np.sum(value1_lst[included])\n    marginal2 = value2_lst - np.sum(value2_lst[included])\n\n    # Dynamic thresholds (top 30% for each objective)\n    threshold1 = np.percentile(marginal1, 70)\n    threshold2 = np.percentile(marginal2, 70)\n\n    # Phase 1: Hybrid operations (add/swap/replace)\n    remaining_capacity = capacity - current_weight\n\n    # Add high-impact items (both objectives)\n    add_candidates = np.where((marginal1 > threshold1) & (marginal2 > threshold2) & (weight_lst <= remaining_capacity) & excluded)[0]\n    if len(add_candidates) > 0:\n        selected = np.random.choice(add_candidates, size=min(3, len(add_candidates)), replace=False)\n        new_solution[selected] = 1\n        current_weight += np.sum(weight_lst[selected])\n        remaining_capacity -= np.sum(weight_lst[selected])\n\n    # Swap operations (improve both objectives)\n    included_items = np.where(included)[0]\n    excluded_items = np.where(~included)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (marginal1[j] > marginal1[i]) and (marginal2[j] > marginal2[i]) and \\\n               (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Replace low-impact items with high-impact ones\n    if current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        for i in included_items:\n            if marginal1[i] < threshold1 and marginal2[i] < threshold2:\n                replace_candidates = np.where((marginal1 > threshold1) & (marginal2 > threshold2) &\n                                            (weight_lst <= remaining_capacity + weight_lst[i]) & excluded)[0]\n                if len(replace_candidates) > 0:\n                    j = np.random.choice(replace_candidates)\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Final feasibility check\n    if current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(marginal1[included_items] + marginal2[included_items])]\n        while current_weight > capacity and len(sorted_items) > 0:\n            item = sorted_items[0]\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            sorted_items = sorted_items[1:]\n\n    return new_solution\n\n",
        "score": [
            -0.7950984578459055,
            2.114767760038376
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (sum(x[1]), -np.sum(weight_lst[x[0] == 1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    included = base_solution == 1\n    excluded = base_solution == 0\n\n    marginal1 = value1_lst - np.sum(value1_lst[included])\n    marginal2 = value2_lst - np.sum(value2_lst[included])\n\n    # Dynamic thresholds (top 30% for each objective)\n    threshold1 = np.percentile(marginal1, 70)\n    threshold2 = np.percentile(marginal2, 70)\n\n    # Phase 1: Hybrid operations (add/swap/replace)\n    remaining_capacity = capacity - current_weight\n\n    # Add high-impact items (both objectives)\n    add_candidates = np.where((marginal1 > threshold1) & (marginal2 > threshold2) & (weight_lst <= remaining_capacity) & excluded)[0]\n    if len(add_candidates) > 0:\n        selected = np.random.choice(add_candidates, size=min(3, len(add_candidates)), replace=False)\n        new_solution[selected] = 1\n        current_weight += np.sum(weight_lst[selected])\n        remaining_capacity -= np.sum(weight_lst[selected])\n\n    # Swap operations (improve both objectives)\n    included_items = np.where(included)[0]\n    excluded_items = np.where(~included)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (marginal1[j] > marginal1[i]) and (marginal2[j] > marginal2[i]) and \\\n               (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Replace low-impact items with high-impact ones\n    if current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        for i in included_items:\n            if marginal1[i] < threshold1 and marginal2[i] < threshold2:\n                replace_candidates = np.where((marginal1 > threshold1) & (marginal2 > threshold2) &\n                                            (weight_lst <= remaining_capacity + weight_lst[i]) & excluded)[0]\n                if len(replace_candidates) > 0:\n                    j = np.random.choice(replace_candidates)\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Final feasibility check\n    if current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(marginal1[included_items] + marginal2[included_items])]\n        while current_weight > capacity and len(sorted_items) > 0:\n            item = sorted_items[0]\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            sorted_items = sorted_items[1:]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 22,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a multi-phase hybrid local search that first removes low-impact items (below the 30th percentile), then inserts high-value items (top 25% by combined normalized scores), and finally performs probabilistic swaps to balance objectives while ensuring feasibility. It prioritizes items with complementary improvements across both objectives and dynamically adjusts selections based on normalized marginal contributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Dynamic threshold-based item removal\n    items_in = np.where(new_solution == 1)[0]\n    if len(items_in) > 0:\n        # Calculate marginal impact for each item\n        marginal_impact1 = value1_lst[items_in]\n        marginal_impact2 = value2_lst[items_in]\n        combined_impact = marginal_impact1 + marginal_impact2\n\n        # Remove items below the 30th percentile of combined impact\n        threshold = np.percentile(combined_impact, 30)\n        remove_mask = combined_impact < threshold\n        for i in items_in[remove_mask]:\n            if current_weight - weight_lst[i] >= 0:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Phase 2: Targeted insertion of high-value items with complementary improvements\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvements and weights\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 25% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//4):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Probabilistic swaps to balance objectives\n    items_in = np.where(new_solution == 1)[0]\n    items_out = np.where(new_solution == 0)[0]\n\n    if len(items_in) > 0 and len(items_out) > 0:\n        # Calculate current objective values\n        current_value1 = np.sum(value1_lst * new_solution)\n        current_value2 = np.sum(value2_lst * new_solution)\n\n        # Perform probabilistic swaps\n        for _ in range(5):  # Limited iterations\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = current_value1 - value1_lst[i] + value1_lst[j]\n                new_value2 = current_value2 - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves or with small probability\n                if (new_value1 > current_value1 or new_value2 > current_value2) or np.random.rand() < 0.1:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n                    current_value1, current_value2 = new_value1, new_value2\n\n    return new_solution\n\n",
        "score": [
            -0.3531182653838381,
            0.6104568243026733
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Dynamic threshold-based item removal\n    items_in = np.where(new_solution == 1)[0]\n    if len(items_in) > 0:\n        # Calculate marginal impact for each item\n        marginal_impact1 = value1_lst[items_in]\n        marginal_impact2 = value2_lst[items_in]\n        combined_impact = marginal_impact1 + marginal_impact2\n\n        # Remove items below the 30th percentile of combined impact\n        threshold = np.percentile(combined_impact, 30)\n        remove_mask = combined_impact < threshold\n        for i in items_in[remove_mask]:\n            if current_weight - weight_lst[i] >= 0:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Phase 2: Targeted insertion of high-value items with complementary improvements\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvements and weights\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 25% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//4):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Probabilistic swaps to balance objectives\n    items_in = np.where(new_solution == 1)[0]\n    items_out = np.where(new_solution == 0)[0]\n\n    if len(items_in) > 0 and len(items_out) > 0:\n        # Calculate current objective values\n        current_value1 = np.sum(value1_lst * new_solution)\n        current_value2 = np.sum(value2_lst * new_solution)\n\n        # Perform probabilistic swaps\n        for _ in range(5):  # Limited iterations\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = current_value1 - value1_lst[i] + value1_lst[j]\n                new_value2 = current_value2 - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves or with small probability\n                if (new_value1 > current_value1 or new_value2 > current_value2) or np.random.rand() < 0.1:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n                    current_value1, current_value2 = new_value1, new_value2\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 23,
        "algorithm": "The algorithm selects the highest-potential solution from the archive and applies a three-phase hybrid local search: first probabilistically removes low-impact items, then inserts high-impact items (top 25% by combined marginal value), and finally performs limited balanced swaps between items to improve both objectives while ensuring feasibility. It prioritizes items with significant marginal contributions and dynamically adjusts thresholds to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic removal of low-impact items\n    items_in = np.where(new_solution == 1)[0]\n    for i in items_in:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Calculate marginal impact for remaining items\n    marginal_impact1 = value1_lst - value1_lst[new_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[new_solution == 1].sum()\n\n    # Phase 2: Targeted insertion of high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Dynamic threshold: top 25% by combined marginal impact\n        combined_impact = marginal_impact1 + marginal_impact2\n        top_threshold = np.percentile(combined_impact[available_items], 75)\n        top_items = available_items[combined_impact[available_items] >= top_threshold]\n\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 3: Balanced swaps considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items with high marginal impact for swap\n            candidate_in = items_in[np.random.choice(len(items_in), min(3, len(items_in)), replace=False)]\n            candidate_out = items_out[np.random.choice(len(items_out), min(3, len(items_out)), replace=False)]\n\n            for i in candidate_in:\n                for j in candidate_out:\n                    delta_weight = weight_lst[j] - weight_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        # Check if at least one objective improves\n                        if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight += delta_weight\n                            break\n\n    return new_solution\n\n",
        "score": [
            -0.480205777568841,
            0.7474227249622345
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic removal of low-impact items\n    items_in = np.where(new_solution == 1)[0]\n    for i in items_in:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Calculate marginal impact for remaining items\n    marginal_impact1 = value1_lst - value1_lst[new_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[new_solution == 1].sum()\n\n    # Phase 2: Targeted insertion of high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Dynamic threshold: top 25% by combined marginal impact\n        combined_impact = marginal_impact1 + marginal_impact2\n        top_threshold = np.percentile(combined_impact[available_items], 75)\n        top_items = available_items[combined_impact[available_items] >= top_threshold]\n\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 3: Balanced swaps considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items with high marginal impact for swap\n            candidate_in = items_in[np.random.choice(len(items_in), min(3, len(items_in)), replace=False)]\n            candidate_out = items_out[np.random.choice(len(items_out), min(3, len(items_out)), replace=False)]\n\n            for i in candidate_in:\n                for j in candidate_out:\n                    delta_weight = weight_lst[j] - weight_lst[i]\n                    if current_weight + delta_weight <= capacity:\n                        # Check if at least one objective improves\n                        if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight += delta_weight\n                            break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 24,
        "algorithm": "The algorithm selects the highest-sum-objective solution from the archive and applies a four-phase local search: first removing low-contribution items, then intelligently inserting high-value items using crowding-distance-based selection, performing value-aware balanced swaps, and finally applying limited simulated annealing to escape local optima while maintaining feasibility. The approach prioritizes balanced objective improvements and leverages crowding distances to explore diverse regions of the solution space, with simulated annealing providing controlled exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with highest sum of objectives\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Remove low-contribution items\n    items_in = np.where(new_solution == 1)[0]\n    if len(items_in) > 0:\n        # Calculate contribution scores\n        contrib1 = value1_lst[items_in] / weight_lst[items_in]\n        contrib2 = value2_lst[items_in] / weight_lst[items_in]\n        combined_contrib = contrib1 + contrib2\n\n        # Remove bottom 20% lowest contributors\n        cutoff = np.percentile(combined_contrib, 20)\n        to_remove = items_in[combined_contrib <= cutoff]\n        new_solution[to_remove] = 0\n        current_weight -= np.sum(weight_lst[to_remove])\n\n    # Phase 2: Crowding-distance-based insertion\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate crowding distances\n        v1 = value1_lst[available_items]\n        v2 = value2_lst[available_items]\n        w = weight_lst[available_items]\n\n        # Normalize values\n        norm_v1 = (v1 - np.min(v1)) / (np.max(v1) - np.min(v1) + 1e-8)\n        norm_v2 = (v2 - np.min(v2)) / (np.max(v2) - np.min(v2) + 1e-8)\n\n        # Calculate crowding distances\n        crowding = np.zeros(len(available_items))\n        for i in range(len(available_items)):\n            distances = np.sqrt((norm_v1 - norm_v1[i])**2 + (norm_v2 - norm_v2[i])**2)\n            crowding[i] = np.sum(np.sort(distances)[:3])  # Average of 3 closest neighbors\n\n        # Select top 30% highest crowding distances\n        top_indices = np.argsort(crowding)[-max(1, len(crowding)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + w[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += w[idx]\n\n    # Phase 3: Value-aware balanced swap\n    for _ in range(5):\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Calculate value ratios\n            ratios_in = (value1_lst[items_in] / value2_lst[items_in] + 1e-8)\n            ratios_out = (value1_lst[items_out] / value2_lst[items_out] + 1e-8)\n\n            # Select items with most balanced ratios\n            i = items_in[np.argmin(np.abs(ratios_in - np.mean(ratios_out)))]\n            j = items_out[np.argmin(np.abs(ratios_out - np.mean(ratios_in)))]\n\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                # Calculate potential improvement\n                old_value1 = sum(value1_lst * new_solution)\n                old_value2 = sum(value2_lst * new_solution)\n                new_value1 = old_value1 - value1_lst[i] + value1_lst[j]\n                new_value2 = old_value2 - value2_lst[i] + value2_lst[j]\n\n                # Accept if both objectives improve or at least one improves significantly\n                if (new_value1 > old_value1 and new_value2 > old_value2) or \\\n                   (new_value1 > 1.05 * old_value1 or new_value2 > 1.05 * old_value2):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    # Phase 4: Limited simulated annealing\n    temperature = 10.0\n    for _ in range(3):\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n            delta_weight = weight_lst[j] - weight_lst[i]\n\n            if current_weight + delta_weight <= capacity:\n                old_value1 = sum(value1_lst * new_solution)\n                old_value2 = sum(value2_lst * new_solution)\n                new_value1 = old_value1 - value1_lst[i] + value1_lst[j]\n                new_value2 = old_value2 - value2_lst[i] + value2_lst[j]\n\n                # Acceptance probability\n                delta1 = new_value1 - old_value1\n                delta2 = new_value2 - old_value2\n                prob = min(1, np.exp((delta1 + delta2) / temperature))\n\n                if np.random.rand() < prob:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n        temperature *= 0.7  # Cool down\n\n    return new_solution\n\n",
        "score": [
            -0.26286243936764453,
            5.614683896303177
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select solution with highest sum of objectives\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Remove low-contribution items\n    items_in = np.where(new_solution == 1)[0]\n    if len(items_in) > 0:\n        # Calculate contribution scores\n        contrib1 = value1_lst[items_in] / weight_lst[items_in]\n        contrib2 = value2_lst[items_in] / weight_lst[items_in]\n        combined_contrib = contrib1 + contrib2\n\n        # Remove bottom 20% lowest contributors\n        cutoff = np.percentile(combined_contrib, 20)\n        to_remove = items_in[combined_contrib <= cutoff]\n        new_solution[to_remove] = 0\n        current_weight -= np.sum(weight_lst[to_remove])\n\n    # Phase 2: Crowding-distance-based insertion\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate crowding distances\n        v1 = value1_lst[available_items]\n        v2 = value2_lst[available_items]\n        w = weight_lst[available_items]\n\n        # Normalize values\n        norm_v1 = (v1 - np.min(v1)) / (np.max(v1) - np.min(v1) + 1e-8)\n        norm_v2 = (v2 - np.min(v2)) / (np.max(v2) - np.min(v2) + 1e-8)\n\n        # Calculate crowding distances\n        crowding = np.zeros(len(available_items))\n        for i in range(len(available_items)):\n            distances = np.sqrt((norm_v1 - norm_v1[i])**2 + (norm_v2 - norm_v2[i])**2)\n            crowding[i] = np.sum(np.sort(distances)[:3])  # Average of 3 closest neighbors\n\n        # Select top 30% highest crowding distances\n        top_indices = np.argsort(crowding)[-max(1, len(crowding)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + w[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += w[idx]\n\n    # Phase 3: Value-aware balanced swap\n    for _ in range(5):\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Calculate value ratios\n            ratios_in = (value1_lst[items_in] / value2_lst[items_in] + 1e-8)\n            ratios_out = (value1_lst[items_out] / value2_lst[items_out] + 1e-8)\n\n            # Select items with most balanced ratios\n            i = items_in[np.argmin(np.abs(ratios_in - np.mean(ratios_out)))]\n            j = items_out[np.argmin(np.abs(ratios_out - np.mean(ratios_in)))]\n\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                # Calculate potential improvement\n                old_value1 = sum(value1_lst * new_solution)\n                old_value2 = sum(value2_lst * new_solution)\n                new_value1 = old_value1 - value1_lst[i] + value1_lst[j]\n                new_value2 = old_value2 - value2_lst[i] + value2_lst[j]\n\n                # Accept if both objectives improve or at least one improves significantly\n                if (new_value1 > old_value1 and new_value2 > old_value2) or \\\n                   (new_value1 > 1.05 * old_value1 or new_value2 > 1.05 * old_value2):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    # Phase 4: Limited simulated annealing\n    temperature = 10.0\n    for _ in range(3):\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n            delta_weight = weight_lst[j] - weight_lst[i]\n\n            if current_weight + delta_weight <= capacity:\n                old_value1 = sum(value1_lst * new_solution)\n                old_value2 = sum(value2_lst * new_solution)\n                new_value1 = old_value1 - value1_lst[i] + value1_lst[j]\n                new_value2 = old_value2 - value2_lst[i] + value2_lst[j]\n\n                # Acceptance probability\n                delta1 = new_value1 - old_value1\n                delta2 = new_value2 - old_value2\n                prob = min(1, np.exp((delta1 + delta2) / temperature))\n\n                if np.random.rand() < prob:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n        temperature *= 0.7  # Cool down\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 25,
        "algorithm": "This algorithm selects the highest-potential solution from the archive (based on combined objective values) and applies a three-phase local search: first probabilistically removing low-utility items, then greedily inserting high-utility items with a weighted objective balance (70% value1, 30% value2), and finally performing targeted swaps to improve both objectives while ensuring feasibility. The method prioritizes value1 over value2 in utility calculations (0.6 vs. 0.4 in Phase 1, 0.7 vs. 0.3 in Phase 2) and balances both objectives equally (0.5 each) in the final swap phase.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Remove low-utility items probabilistically\n    for i in np.where(new_solution == 1)[0]:\n        utility1 = value1_lst[i] / weight_lst[i]\n        utility2 = value2_lst[i] / weight_lst[i]\n        combined_utility = 0.6 * utility1 + 0.4 * utility2\n        if np.random.rand() < 0.2 * (1 - combined_utility):  # Higher chance to remove lower utility items\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Insert high-utility items based on weighted objectives\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted utility for each item\n        utility1 = value1_lst[available_items] / weight_lst[available_items]\n        utility2 = value2_lst[available_items] / weight_lst[available_items]\n        weighted_utility = 0.7 * utility1 + 0.3 * utility2\n\n        # Select top 40% items by weighted utility\n        top_indices = np.argsort(weighted_utility)[-max(1, len(weighted_utility)//2):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 3: Targeted swaps to improve both objectives\n    for _ in range(8):  # More iterations for better exploration\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items with highest and lowest utility\n            in_utilities = (0.5 * value1_lst[items_in] + 0.5 * value2_lst[items_in]) / weight_lst[items_in]\n            out_utilities = (0.5 * value1_lst[items_out] + 0.5 * value2_lst[items_out]) / weight_lst[items_out]\n\n            i = items_in[np.argmin(in_utilities)]  # Remove worst utility item\n            j = items_out[np.argmax(out_utilities)]  # Add best utility item\n\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if both objectives improve or at least one does significantly\n                if (new_value1 > sum(value1_lst * new_solution) and new_value2 > sum(value2_lst * new_solution)) or \\\n                   (new_value1 > 1.05 * sum(value1_lst * new_solution) or new_value2 > 1.05 * sum(value2_lst * new_solution)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n",
        "score": [
            -0.31978505473147706,
            10.235911935567856
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Remove low-utility items probabilistically\n    for i in np.where(new_solution == 1)[0]:\n        utility1 = value1_lst[i] / weight_lst[i]\n        utility2 = value2_lst[i] / weight_lst[i]\n        combined_utility = 0.6 * utility1 + 0.4 * utility2\n        if np.random.rand() < 0.2 * (1 - combined_utility):  # Higher chance to remove lower utility items\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Insert high-utility items based on weighted objectives\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted utility for each item\n        utility1 = value1_lst[available_items] / weight_lst[available_items]\n        utility2 = value2_lst[available_items] / weight_lst[available_items]\n        weighted_utility = 0.7 * utility1 + 0.3 * utility2\n\n        # Select top 40% items by weighted utility\n        top_indices = np.argsort(weighted_utility)[-max(1, len(weighted_utility)//2):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 3: Targeted swaps to improve both objectives\n    for _ in range(8):  # More iterations for better exploration\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items with highest and lowest utility\n            in_utilities = (0.5 * value1_lst[items_in] + 0.5 * value2_lst[items_in]) / weight_lst[items_in]\n            out_utilities = (0.5 * value1_lst[items_out] + 0.5 * value2_lst[items_out]) / weight_lst[items_out]\n\n            i = items_in[np.argmin(in_utilities)]  # Remove worst utility item\n            j = items_out[np.argmax(out_utilities)]  # Add best utility item\n\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if both objectives improve or at least one does significantly\n                if (new_value1 > sum(value1_lst * new_solution) and new_value2 > sum(value2_lst * new_solution)) or \\\n                   (new_value1 > 1.05 * sum(value1_lst * new_solution) or new_value2 > 1.05 * sum(value2_lst * new_solution)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects the highest-potential solution from the archive, then applies a three-phase local search: first removing items probabilistically to free capacity, next inserting high-value items based on normalized combined objective scores, and finally performing limited swaps to balance objectives while ensuring feasibility. The method prioritizes items with better combined normalized values and maintains feasibility through strict weight checks at each step.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select a solution with high potential (highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic item removal to create capacity\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.3:  # 30% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Value-driven insertion of high-potential items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 30% items by combined score\n        top_indices = np.argsort(combined_score)[-max(1, len(combined_score)//3):]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment considering both objectives\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > sum(value1_lst * new_solution) or new_value2 > sum(value2_lst * new_solution):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 26,
        "algorithm": "The algorithm intelligently selects a promising solution from the archive (prioritizing high-value, less-explored solutions) and applies a four-phase hybrid local search: (1) probabilistically removes low-marginal-impact items to free capacity, (2) inserts high-marginal-impact items using adaptive 90th-percentile thresholds, (3) performs limited swaps to balance objectives while ensuring feasibility, and (4) refines the solution by replacing items with higher potential marginal impact based on dynamic thresholds. The method prioritizes items that improve both objectives, using normalized scores and strict weight checks to maintain feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # Prioritize solutions with high objective values and low exploration frequency\n    archive.sort(key=lambda x: (sum(x[1]), -np.sum(archive.count(x[0]))), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic removal of low-marginal-impact items\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Targeted insertion of high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 10% items by combined score\n        threshold = np.percentile(combined_score, 90)\n        top_indices = np.where(combined_score >= threshold)[0]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment through limited swaps\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap based on marginal improvement\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if both objectives improve or at least one does significantly\n                if (new_value1 > sum(value1_lst * new_solution) and new_value2 > sum(value2_lst * new_solution)) or \\\n                   (new_value1 > sum(value1_lst * new_solution) * 1.05 or new_value2 > sum(value2_lst * new_solution) * 1.05):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    # Phase 4: Final adjustment with dynamic thresholds\n    items_in = np.where(new_solution == 1)[0]\n    if len(items_in) > 0:\n        # Calculate current solution's objective values\n        current_value1 = sum(value1_lst * new_solution)\n        current_value2 = sum(value2_lst * new_solution)\n\n        # Find items that could improve both objectives\n        for i in items_in:\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                 (new_solution == 0) &\n                                 ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined normalized score\n                candidate_scores = []\n                for j in candidates:\n                    norm_score1 = (value1_lst[j] - value1_lst[i]) / (current_value1 + 1e-8)\n                    norm_score2 = (value2_lst[j] - value2_lst[i]) / (current_value2 + 1e-8)\n                    candidate_scores.append(norm_score1 + norm_score2)\n\n                if candidate_scores:\n                    best_candidate = candidates[np.argmax(candidate_scores)]\n                    new_solution[i], new_solution[best_candidate] = 0, 1\n                    current_weight += weight_lst[best_candidate] - weight_lst[i]\n\n    return new_solution\n\n",
        "score": [
            -0.44662036794409715,
            6.148115903139114
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # Prioritize solutions with high objective values and low exploration frequency\n    archive.sort(key=lambda x: (sum(x[1]), -np.sum(archive.count(x[0]))), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Phase 1: Probabilistic removal of low-marginal-impact items\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Phase 2: Targeted insertion of high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate marginal improvement for each item\n        marginal_value1 = value1_lst[available_items]\n        marginal_value2 = value2_lst[available_items]\n        marginal_weights = weight_lst[available_items]\n\n        # Normalize and combine marginal improvements\n        norm_value1 = (marginal_value1 - np.min(marginal_value1)) / (np.max(marginal_value1) - np.min(marginal_value1) + 1e-8)\n        norm_value2 = (marginal_value2 - np.min(marginal_value2)) / (np.max(marginal_value2) - np.min(marginal_value2) + 1e-8)\n        combined_score = norm_value1 + norm_value2\n\n        # Select top 10% items by combined score\n        threshold = np.percentile(combined_score, 90)\n        top_indices = np.where(combined_score >= threshold)[0]\n        for idx in top_indices:\n            item = available_items[idx]\n            if current_weight + marginal_weights[idx] <= capacity:\n                new_solution[item] = 1\n                current_weight += marginal_weights[idx]\n\n    # Phase 3: Balanced adjustment through limited swaps\n    for _ in range(5):  # Limited iterations\n        items_in = np.where(new_solution == 1)[0]\n        items_out = np.where(new_solution == 0)[0]\n\n        if len(items_in) > 0 and len(items_out) > 0:\n            # Select items to swap based on marginal improvement\n            i = np.random.choice(items_in)\n            j = np.random.choice(items_out)\n\n            # Calculate potential improvement\n            delta_weight = weight_lst[j] - weight_lst[i]\n            if current_weight + delta_weight <= capacity:\n                new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n                new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n\n                # Accept if both objectives improve or at least one does significantly\n                if (new_value1 > sum(value1_lst * new_solution) and new_value2 > sum(value2_lst * new_solution)) or \\\n                   (new_value1 > sum(value1_lst * new_solution) * 1.05 or new_value2 > sum(value2_lst * new_solution) * 1.05):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += delta_weight\n\n    # Phase 4: Final adjustment with dynamic thresholds\n    items_in = np.where(new_solution == 1)[0]\n    if len(items_in) > 0:\n        # Calculate current solution's objective values\n        current_value1 = sum(value1_lst * new_solution)\n        current_value2 = sum(value2_lst * new_solution)\n\n        # Find items that could improve both objectives\n        for i in items_in:\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                 (new_solution == 0) &\n                                 ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined normalized score\n                candidate_scores = []\n                for j in candidates:\n                    norm_score1 = (value1_lst[j] - value1_lst[i]) / (current_value1 + 1e-8)\n                    norm_score2 = (value2_lst[j] - value2_lst[i]) / (current_value2 + 1e-8)\n                    candidate_scores.append(norm_score1 + norm_score2)\n\n                if candidate_scores:\n                    best_candidate = candidates[np.argmax(candidate_scores)]\n                    new_solution[i], new_solution[best_candidate] = 0, 1\n                    current_weight += weight_lst[best_candidate] - weight_lst[i]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 27,
        "algorithm": "The algorithm combines adaptive item selection with hybrid local search operations, prioritizing high-value items (70th percentile) while dynamically balancing exploration and exploitation through phases that add, replace, and rebalance items while ensuring feasibility through weight constraints. It starts with the best solution from the archive, then sequentially adds promising items, replaces low-value items with better alternatives, and finally performs weight-sensitive rebalancing to maintain feasibility. The heuristic emphasizes both objective values through percentile-based thresholds and value ratios, ensuring diverse and high-quality neighbor solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (max(x[1][0], 1) + max(x[1][1], 1)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic thresholds (70th percentile)\n    value1_threshold = np.percentile(value1_lst, 70)\n    value2_threshold = np.percentile(value2_lst, 70)\n    weight_threshold = np.percentile(weight_lst, 70)\n\n    # Phase 1: Add high-value items (hybrid of both objectives)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) &\n                         (new_solution == 0) &\n                         ((value1_lst >= value1_threshold) | (value2_lst >= value2_threshold)))[0]\n\n    if len(candidates) > 0:\n        for idx in candidates:\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2: Replace low-value items with high-value items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if (value1_lst[i] < value1_threshold and value2_lst[i] < value2_threshold) or \\\n           weight_lst[i] > weight_threshold:\n            for j in excluded_items:\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                       (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                       (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest combined value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8219155731355212,
            0.846238374710083
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (max(x[1][0], 1) + max(x[1][1], 1)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic thresholds (70th percentile)\n    value1_threshold = np.percentile(value1_lst, 70)\n    value2_threshold = np.percentile(value2_lst, 70)\n    weight_threshold = np.percentile(weight_lst, 70)\n\n    # Phase 1: Add high-value items (hybrid of both objectives)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) &\n                         (new_solution == 0) &\n                         ((value1_lst >= value1_threshold) | (value2_lst >= value2_threshold)))[0]\n\n    if len(candidates) > 0:\n        for idx in candidates:\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2: Replace low-value items with high-value items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if (value1_lst[i] < value1_threshold and value2_lst[i] < value2_threshold) or \\\n           weight_lst[i] > weight_threshold:\n            for j in excluded_items:\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                       (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                       (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest combined value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm first selects a solution from the archive with high potential for improvement, then applies a hybrid local search that combines probabilistic item additions (prioritizing high-value items), targeted swaps (ensuring multi-objective improvements), and adaptive rebalancing (removing low-value items to maintain feasibility). The exploration intensity is dynamically adjusted based on the solution's current weight, and the search prioritizes items with favorable value ratios while ensuring both objectives are improved.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate dynamic value ratios and prioritize items\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions with dynamic intensity\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Adjust exploration intensity based on solution quality\n        exploration_prob = min(0.9, 0.5 + (current_weight / capacity) * 0.4)\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < exploration_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps with multi-objective improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check for multi-objective improvement\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive rebalancing with dynamic removal criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest value ratio or least contribution to objectives\n        removal_criteria = value_ratios[included_items] * (1 - (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst) + np.sum(value2_lst)))\n        worst_item = included_items[np.argmin(removal_criteria)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 28,
        "algorithm": "This algorithm selects a promising solution from an archive based on weight utilization and objective dominance, then applies a three-phase hybrid local search: Phase 1 adds high-impact items using dynamic marginal thresholds, Phase 2 performs targeted swaps between high-impact items from both objectives, and Phase 3 adaptively removes low-contribution items when capacity is exceeded, using percentile-based criteria to balance both objectives while ensuring feasibility. The selection prioritizes solutions with higher weight utilization and better objective product scores, while the local search dynamically adjusts thresholds to explore the solution space more effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution based on weight utilization and objective dominance\n    utilization = [np.sum(weight_lst[s[0] == 1]) / capacity for s in archive]\n    dominance = [s[1][0] * s[1][1] for s in archive]\n    scores = [u * d for u, d in zip(utilization, dominance)]\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Dynamic marginal impact thresholding\n    marginal1 = value1_lst - np.sum(value1_lst[base_solution == 1])\n    marginal2 = value2_lst - np.sum(value2_lst[base_solution == 1])\n\n    # Calculate dynamic thresholds based on percentiles\n    p1_threshold = np.percentile(marginal1, 70)\n    p2_threshold = np.percentile(marginal2, 70)\n\n    # Identify candidate items\n    candidates1 = np.where((marginal1 > p1_threshold) & (new_solution == 0) & (weight_lst <= capacity - current_weight))[0]\n    candidates2 = np.where((marginal2 > p2_threshold) & (new_solution == 0) & (weight_lst <= capacity - current_weight))[0]\n    candidates = np.union1d(candidates1, candidates2)\n\n    # Add high-impact items\n    for idx in candidates:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Targeted swaps between objectives\n    included1 = np.where((new_solution == 1) & (marginal1 > p1_threshold))[0]\n    included2 = np.where((new_solution == 1) & (marginal2 > p2_threshold))[0]\n\n    for i in included1:\n        for j in included2:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive replacement with percentile-based criteria\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate replacement criteria based on marginal impact percentiles\n        p1 = np.percentile(marginal1[included], 30)\n        p2 = np.percentile(marginal2[included], 30)\n        criteria = (marginal1[included] <= p1) | (marginal2[included] <= p2)\n\n        if np.any(criteria):\n            worst_items = included[criteria]\n            remove_idx = worst_items[np.argmin(weight_lst[worst_items])]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n        else:\n            # Remove least valuable item if no candidates meet criteria\n            remove_idx = included[np.argmin(marginal1[included] + marginal2[included])]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9203671744323288,
            0.809229165315628
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution based on weight utilization and objective dominance\n    utilization = [np.sum(weight_lst[s[0] == 1]) / capacity for s in archive]\n    dominance = [s[1][0] * s[1][1] for s in archive]\n    scores = [u * d for u, d in zip(utilization, dominance)]\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Dynamic marginal impact thresholding\n    marginal1 = value1_lst - np.sum(value1_lst[base_solution == 1])\n    marginal2 = value2_lst - np.sum(value2_lst[base_solution == 1])\n\n    # Calculate dynamic thresholds based on percentiles\n    p1_threshold = np.percentile(marginal1, 70)\n    p2_threshold = np.percentile(marginal2, 70)\n\n    # Identify candidate items\n    candidates1 = np.where((marginal1 > p1_threshold) & (new_solution == 0) & (weight_lst <= capacity - current_weight))[0]\n    candidates2 = np.where((marginal2 > p2_threshold) & (new_solution == 0) & (weight_lst <= capacity - current_weight))[0]\n    candidates = np.union1d(candidates1, candidates2)\n\n    # Add high-impact items\n    for idx in candidates:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Targeted swaps between objectives\n    included1 = np.where((new_solution == 1) & (marginal1 > p1_threshold))[0]\n    included2 = np.where((new_solution == 1) & (marginal2 > p2_threshold))[0]\n\n    for i in included1:\n        for j in included2:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive replacement with percentile-based criteria\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate replacement criteria based on marginal impact percentiles\n        p1 = np.percentile(marginal1[included], 30)\n        p2 = np.percentile(marginal2[included], 30)\n        criteria = (marginal1[included] <= p1) | (marginal2[included] <= p2)\n\n        if np.any(criteria):\n            worst_items = included[criteria]\n            remove_idx = worst_items[np.argmin(weight_lst[worst_items])]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n        else:\n            # Remove least valuable item if no candidates meet criteria\n            remove_idx = included[np.argmin(marginal1[included] + marginal2[included])]\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 30% by combined objective value) and applies a hybrid local search combining probabilistic additions of high-impact items, targeted swaps between included and excluded items, and adaptive removals of low-impact items to ensure feasibility while balancing exploration and exploitation. It prioritizes items with high marginal impact (top percentile) in both objectives and dynamically adjusts the solution to maintain capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 30% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[min(len(archive) // 3, len(archive) - 1)][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impact (percentile-based)\n    included = base_solution == 1\n    marginal_impact1 = value1_lst - value1_lst[included].sum()\n    marginal_impact2 = value2_lst - value2_lst[included].sum()\n    top_percentile = min(30, len(weight_lst) // 5)  # Dynamic threshold\n\n    top_items1 = np.argsort(marginal_impact1)[-top_percentile:]\n    top_items2 = np.argsort(marginal_impact2)[-top_percentile:]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Hybrid local search: Phase 1 - Probabilistic additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0) & np.isin(np.arange(len(weight_lst)), top_items))[0]\n    if len(candidates) > 0:\n        for idx in np.random.permutation(candidates):\n            if np.random.rand() < 0.5:  # 50% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2 - Targeted swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if i not in top_items:\n            continue\n        for j in excluded_items:\n            if j not in top_items:\n                continue\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3 - Adaptive removals\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_item = included_items[np.argmin(marginal_combined)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 29,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a three-phase local search strategy: it first selects high-impact items using dynamic percentile thresholds, probabilistically flips them to explore the solution space while maintaining feasibility, then performs greedy replacements of low-impact items with high-impact alternatives, and finally ensures feasibility through targeted removals. The approach prioritizes items with high combined marginal impact in both objectives while dynamically adjusting the solution to balance the bi-objective trade-off and capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 20% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[min(len(archive) // 5, len(archive) - 1)][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact (percentile-based)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n    top_percentile = min(25, len(weight_lst) // 4)  # Dynamic threshold\n\n    top_items1 = np.argsort(marginal_impact1)[-top_percentile:]\n    top_items2 = np.argsort(marginal_impact2)[-top_percentile:]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Phase 1: Probabilistic flips of high-impact items\n    for idx in top_items:\n        if np.random.rand() < 0.3:  # 30% chance to flip\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Greedy replacement of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Find items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_items = included_items[np.argsort(marginal_combined)[:min(3, len(included_items))]]\n\n        for worst in worst_items:\n            # Find best replacement item (highest marginal impact, not already included)\n            candidates = np.where((weight_lst <= current_weight - weight_lst[worst] + capacity) &\n                                (new_solution == 0) &\n                                np.isin(np.arange(len(weight_lst)), top_items))[0]\n            if len(candidates) > 0:\n                best_candidate = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[worst] = 0\n                new_solution[best_candidate] = 1\n                current_weight = current_weight - weight_lst[worst] + weight_lst[best_candidate]\n\n    # Phase 3: Final feasibility check and adjustments\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_item = included_items[np.argmin(marginal_combined)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.4449410662760863,
            0.5290350914001465
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 20% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[min(len(archive) // 5, len(archive) - 1)][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact (percentile-based)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n    top_percentile = min(25, len(weight_lst) // 4)  # Dynamic threshold\n\n    top_items1 = np.argsort(marginal_impact1)[-top_percentile:]\n    top_items2 = np.argsort(marginal_impact2)[-top_percentile:]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Phase 1: Probabilistic flips of high-impact items\n    for idx in top_items:\n        if np.random.rand() < 0.3:  # 30% chance to flip\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Greedy replacement of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Find items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_items = included_items[np.argsort(marginal_combined)[:min(3, len(included_items))]]\n\n        for worst in worst_items:\n            # Find best replacement item (highest marginal impact, not already included)\n            candidates = np.where((weight_lst <= current_weight - weight_lst[worst] + capacity) &\n                                (new_solution == 0) &\n                                np.isin(np.arange(len(weight_lst)), top_items))[0]\n            if len(candidates) > 0:\n                best_candidate = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[worst] = 0\n                new_solution[best_candidate] = 1\n                current_weight = current_weight - weight_lst[worst] + weight_lst[best_candidate]\n\n    # Phase 3: Final feasibility check and adjustments\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        marginal_combined = value1_lst[included_items] + value2_lst[included_items]\n        worst_item = included_items[np.argmin(marginal_combined)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 30,
        "algorithm": "The algorithm selects a promising solution from the archive based on a fitness score combining both objectives and solution density, then generates a neighbor by flipping items with probabilities weighted by their marginal impacts on the objectives, dynamically adjusted by the solution's density, while ensuring feasibility through capacity checks. The key design ideas include prioritizing items with high marginal contributions to either objective (weighted by current density) and intelligently selecting flips to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    fitness_scores = []\n    for sol, obj in archive:\n        density = np.sum(sol) / len(sol)\n        fitness = (obj[0] + obj[1]) * (1 - density)  # Balance between total value and solution density\n        fitness_scores.append(fitness)\n\n    # Select solution with highest fitness\n    selected_idx = np.argmax(fitness_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Normalize impacts\n    norm_impact1 = (marginal_impact1 - np.mean(marginal_impact1)) / (np.std(marginal_impact1) + 1e-8)\n    norm_impact2 = (marginal_impact2 - np.mean(marginal_impact2)) / (np.std(marginal_impact2) + 1e-8)\n\n    # Combine impacts with weights based on current solution density\n    density = np.sum(base_solution) / len(base_solution)\n    combined_impact = (1 - density) * norm_impact1 + density * norm_impact2\n\n    # Select items to flip with probability proportional to their combined impact\n    flip_probs = np.exp(combined_impact) / np.sum(np.exp(combined_impact))\n    flip_indices = np.random.choice(len(weight_lst), size=min(5, len(weight_lst)), p=flip_probs, replace=False)\n\n    # Apply flips while maintaining feasibility\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                total_weight -= weight_lst[idx]\n        else:\n            if total_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.3111615780230508,
            0.4342575967311859
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    fitness_scores = []\n    for sol, obj in archive:\n        density = np.sum(sol) / len(sol)\n        fitness = (obj[0] + obj[1]) * (1 - density)  # Balance between total value and solution density\n        fitness_scores.append(fitness)\n\n    # Select solution with highest fitness\n    selected_idx = np.argmax(fitness_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Normalize impacts\n    norm_impact1 = (marginal_impact1 - np.mean(marginal_impact1)) / (np.std(marginal_impact1) + 1e-8)\n    norm_impact2 = (marginal_impact2 - np.mean(marginal_impact2)) / (np.std(marginal_impact2) + 1e-8)\n\n    # Combine impacts with weights based on current solution density\n    density = np.sum(base_solution) / len(base_solution)\n    combined_impact = (1 - density) * norm_impact1 + density * norm_impact2\n\n    # Select items to flip with probability proportional to their combined impact\n    flip_probs = np.exp(combined_impact) / np.sum(np.exp(combined_impact))\n    flip_indices = np.random.choice(len(weight_lst), size=min(5, len(weight_lst)), p=flip_probs, replace=False)\n\n    # Apply flips while maintaining feasibility\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if total_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                total_weight -= weight_lst[idx]\n        else:\n            if total_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 31,
        "algorithm": "The algorithm selects a solution from the archive with the highest combined objective value, then performs targeted swaps between high-impact (top 20% for each objective) and low-impact (bottom 20%) items to balance improvements while ensuring feasibility, with up to 3 swaps per iteration. High-impact items are prioritized for removal, and low-impact items for addition, leveraging marginal impact calculations to guide the search.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined marginal impact\n    base_solution = max(archive, key=lambda x: sum(x[1]))[0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - current_value1\n    marginal_impact2 = value2_lst - current_value2\n\n    # Identify high-impact items (top 20% for each objective)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    high_impact_items = np.union1d(top_items1, top_items2)\n\n    # Identify low-impact items (bottom 20% for each objective)\n    low_items1 = np.argsort(marginal_impact1)[:max(1, num_items // 5)]\n    low_items2 = np.argsort(marginal_impact2)[:max(1, num_items // 5)]\n    low_impact_items = np.union1d(low_items1, low_items2)\n\n    # Perform targeted swaps between high and low impact items\n    for _ in range(3):  # Perform up to 3 swaps\n        if len(high_impact_items) > 0 and len(low_impact_items) > 0:\n            # Select a high-impact item to remove\n            remove_idx = np.random.choice(high_impact_items)\n            if new_solution[remove_idx] == 1:\n                if total_weight - weight_lst[remove_idx] <= capacity:\n                    new_solution[remove_idx] = 0\n                    total_weight -= weight_lst[remove_idx]\n\n            # Select a low-impact item to add\n            add_idx = np.random.choice(low_impact_items)\n            if new_solution[add_idx] == 0:\n                if total_weight + weight_lst[add_idx] <= capacity:\n                    new_solution[add_idx] = 1\n                    total_weight += weight_lst[add_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6461403397145022,
            1.026967704296112
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined marginal impact\n    base_solution = max(archive, key=lambda x: sum(x[1]))[0].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - current_value1\n    marginal_impact2 = value2_lst - current_value2\n\n    # Identify high-impact items (top 20% for each objective)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    high_impact_items = np.union1d(top_items1, top_items2)\n\n    # Identify low-impact items (bottom 20% for each objective)\n    low_items1 = np.argsort(marginal_impact1)[:max(1, num_items // 5)]\n    low_items2 = np.argsort(marginal_impact2)[:max(1, num_items // 5)]\n    low_impact_items = np.union1d(low_items1, low_items2)\n\n    # Perform targeted swaps between high and low impact items\n    for _ in range(3):  # Perform up to 3 swaps\n        if len(high_impact_items) > 0 and len(low_impact_items) > 0:\n            # Select a high-impact item to remove\n            remove_idx = np.random.choice(high_impact_items)\n            if new_solution[remove_idx] == 1:\n                if total_weight - weight_lst[remove_idx] <= capacity:\n                    new_solution[remove_idx] = 0\n                    total_weight -= weight_lst[remove_idx]\n\n            # Select a low-impact item to add\n            add_idx = np.random.choice(low_impact_items)\n            if new_solution[add_idx] == 0:\n                if total_weight + weight_lst[add_idx] <= capacity:\n                    new_solution[add_idx] = 1\n                    total_weight += weight_lst[add_idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 32,
        "algorithm": "The algorithm selects a random solution from the archive and generates a neighbor by flipping a dynamic subset of high-impact items (top 25% by marginal contribution to either objective), using a hybrid local search that includes add, swap, and replace operations, prioritizing items that improve both objectives while ensuring feasibility. It dynamically adjusts the number of flips (1-5) and includes occasional swaps to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, max(1, len(top_items) // 2))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n        if np.random.rand() < 0.3:\n            swap_indices = np.random.choice(top_items, size=min(2, len(top_items)), replace=False)\n            for idx in swap_indices:\n                if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n                elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.40979307830754713,
            0.24711576104164124
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, max(1, len(top_items) // 2))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n        if np.random.rand() < 0.3:\n            swap_indices = np.random.choice(top_items, size=min(2, len(top_items)), replace=False)\n            for idx in swap_indices:\n                if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n                elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined **effective self-reflection** with actionable insights:\n\n- **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 33,
        "algorithm": "The heuristic algorithm intelligently selects a solution from the archive based on marginal impact potential (prioritizing solutions with high combined value-to-weight ratios), then applies a hybrid local search that dynamically adds high-impact items, swaps low-impact items with high-impact ones, and replaces items while ensuring feasibility. The algorithm prioritizes top 25% items by marginal impact and balances exploration/exploitation through sequential operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest marginal impact potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6) + x[1][1] / (x[1][0] + 1e-6)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n\n    # Dynamic thresholds (top 25% by marginal impact)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, len(weight_lst) // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, len(weight_lst) // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Hybrid local search operations\n    operations = ['add', 'swap', 'replace']\n    for op in operations:\n        if op == 'add':\n            # Add high-impact items not in solution\n            candidates = np.where((weight_lst <= (capacity - current_weight)) & (new_solution == 0))[0]\n            candidates = np.intersect1d(candidates, top_items)\n            if len(candidates) > 0:\n                add_idx = np.random.choice(candidates)\n                new_solution[add_idx] = 1\n                current_weight += weight_lst[add_idx]\n\n        elif op == 'swap':\n            # Swap low-impact items with high-impact items\n            included = np.where(new_solution == 1)[0]\n            excluded = np.where(new_solution == 0)[0]\n            excluded = np.intersect1d(excluded, top_items)\n\n            if len(included) > 0 and len(excluded) > 0:\n                low_impact = np.argsort(marginal_impact1[included] + marginal_impact2[included])[0]\n                for high_idx in excluded:\n                    if weight_lst[high_idx] <= capacity - current_weight + weight_lst[included[low_impact]]:\n                        new_solution[included[low_impact]] = 0\n                        new_solution[high_idx] = 1\n                        current_weight = current_weight - weight_lst[included[low_impact]] + weight_lst[high_idx]\n                        break\n\n        elif op == 'replace':\n            # Replace low-impact items with high-impact items\n            included = np.where(new_solution == 1)[0]\n            excluded = np.where(new_solution == 0)[0]\n            excluded = np.intersect1d(excluded, top_items)\n\n            if len(included) > 0 and len(excluded) > 0:\n                low_impact = np.argsort(marginal_impact1[included] + marginal_impact2[included])[0]\n                for high_idx in excluded:\n                    if weight_lst[high_idx] <= capacity - current_weight + weight_lst[included[low_impact]]:\n                        new_solution[included[low_impact]] = 0\n                        new_solution[high_idx] = 1\n                        current_weight = current_weight - weight_lst[included[low_impact]] + weight_lst[high_idx]\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.7699804102195866,
            2.2092077136039734
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest marginal impact potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6) + x[1][1] / (x[1][0] + 1e-6)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n\n    # Dynamic thresholds (top 25% by marginal impact)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, len(weight_lst) // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, len(weight_lst) // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Hybrid local search operations\n    operations = ['add', 'swap', 'replace']\n    for op in operations:\n        if op == 'add':\n            # Add high-impact items not in solution\n            candidates = np.where((weight_lst <= (capacity - current_weight)) & (new_solution == 0))[0]\n            candidates = np.intersect1d(candidates, top_items)\n            if len(candidates) > 0:\n                add_idx = np.random.choice(candidates)\n                new_solution[add_idx] = 1\n                current_weight += weight_lst[add_idx]\n\n        elif op == 'swap':\n            # Swap low-impact items with high-impact items\n            included = np.where(new_solution == 1)[0]\n            excluded = np.where(new_solution == 0)[0]\n            excluded = np.intersect1d(excluded, top_items)\n\n            if len(included) > 0 and len(excluded) > 0:\n                low_impact = np.argsort(marginal_impact1[included] + marginal_impact2[included])[0]\n                for high_idx in excluded:\n                    if weight_lst[high_idx] <= capacity - current_weight + weight_lst[included[low_impact]]:\n                        new_solution[included[low_impact]] = 0\n                        new_solution[high_idx] = 1\n                        current_weight = current_weight - weight_lst[included[low_impact]] + weight_lst[high_idx]\n                        break\n\n        elif op == 'replace':\n            # Replace low-impact items with high-impact items\n            included = np.where(new_solution == 1)[0]\n            excluded = np.where(new_solution == 0)[0]\n            excluded = np.intersect1d(excluded, top_items)\n\n            if len(included) > 0 and len(excluded) > 0:\n                low_impact = np.argsort(marginal_impact1[included] + marginal_impact2[included])[0]\n                for high_idx in excluded:\n                    if weight_lst[high_idx] <= capacity - current_weight + weight_lst[included[low_impact]]:\n                        new_solution[included[low_impact]] = 0\n                        new_solution[high_idx] = 1\n                        current_weight = current_weight - weight_lst[included[low_impact]] + weight_lst[high_idx]\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **effective self-reflection** with actionable insights:\n\n- **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 34,
        "algorithm": "The algorithm selects a high-potential solution from the archive using a combined objective and ratio-based metric, then applies a three-phase hybrid local search: (1) probabilistically adds items with high marginal impact while respecting capacity, (2) performs targeted swaps to improve Pareto dominance, and (3) dynamically rebalances by removing low-impact items while preserving high-value ones to maintain feasibility. The approach prioritizes both objectives through marginal impact calculations and balances exploration/exploitation with randomness and dynamic thresholds.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8741939652873877,
            0.7856585383415222
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a dynamic subset of high-impact items (top 25% by marginal contribution to either objective), using a hybrid local search that includes add, swap, and replace operations, prioritizing items that improve both objectives while ensuring feasibility. It dynamically adjusts the number of flips (1-5) and includes occasional swaps to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, max(1, len(top_items) // 2))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n        if np.random.rand() < 0.3:\n            swap_indices = np.random.choice(top_items, size=min(2, len(top_items)), replace=False)\n            for idx in swap_indices:\n                if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n                elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **effective self-reflection** with actionable insights:\n\n- **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 35,
        "algorithm": "The algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8979598934609019,
            1.824746549129486
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 36,
        "algorithm": "This algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8714989902267396,
            3.2112058997154236
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 37,
        "algorithm": "The algorithm selects a high-potential solution from the archive, prioritizes items with strong individual impacts (alternating between objectives), performs adaptive swaps to improve single-objective values, and rebalances by removing low-impact items while ensuring feasibility through capacity checks. It uses a weighted objective score to guide selection and employs a phased approach (selection, swapping, rebalancing) to explore the solution space efficiently. The method balances exploration and exploitation by focusing on impactful items and adaptively adjusting the solution.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8698622795054695,
            0.6550902426242828
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined **effective self-reflection** with actionable insights:\n\n- **Keywords**: Adaptive marginal impact, hybrid operations, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid local search (e.g., add/swap/replace items) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 38,
        "algorithm": "The algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8172125621391388,
            0.5021446347236633
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 39,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9322289850854732,
            0.6346070170402527
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 40,
        "algorithm": "The algorithm employs a three-phase approach to generate a neighbor solution: first, it probabilistically adds items from high-potential clusters identified via k-means clustering, then performs dynamic slack-based swaps prioritizing marginal impact, and finally rebalances the solution by removing Pareto-dominated items to ensure feasibility. It intelligently selects promising solutions from the archive and balances both objectives through cluster-based addition, dynamic swaps, and multi-objective rebalancing.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 3, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Cluster-based probabilistic addition\n    n_clusters = min(5, len(sorted_indices) // 2)\n    if n_clusters > 1:\n        from sklearn.cluster import KMeans\n        features = np.column_stack([weight_lst, value1_lst, value2_lst])\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(features)\n        cluster_labels = kmeans.labels_\n\n        # Calculate cluster potential (average marginal impact)\n        cluster_potential = np.zeros(n_clusters)\n        for i in range(n_clusters):\n            cluster_items = np.where(cluster_labels == i)[0]\n            cluster_potential[i] = np.mean(marginal_impact[cluster_items])\n\n        # Add items from high-potential clusters with probability\n        remaining_capacity = capacity - current_weight\n        candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n        if len(candidates) > 0:\n            for idx in candidates:\n                cluster = cluster_labels[idx]\n                add_prob = min(0.9, 0.3 + 0.6 * (cluster_potential[cluster] / np.max(cluster_potential)))\n                if np.random.rand() < add_prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Dynamic slack-based swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            weight_diff = weight_lst[j] - weight_lst[i]\n            slack = capacity - (current_weight + weight_diff)\n            if slack >= 0 and (marginal_impact[j] > marginal_impact[i] or\n                              (value1_lst[j] > value1_lst[i] and slack >= 0) or\n                              (value2_lst[j] > value2_lst[i] and slack >= 0)):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_diff\n                break\n\n    # Phase 3: Multi-objective rebalancing with Pareto dominance\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Identify non-dominated items to keep\n        values = np.column_stack([value1_lst[included_items], value2_lst[included_items]])\n        is_dominated = np.zeros(len(included_items), dtype=bool)\n\n        for i in range(len(included_items)):\n            for j in range(len(included_items)):\n                if i != j and (values[j, 0] >= values[i, 0] and values[j, 1] >= values[i, 1]):\n                    if values[j, 0] > values[i, 0] or values[j, 1] > values[i, 1]:\n                        is_dominated[i] = True\n                        break\n\n        if np.any(~is_dominated):\n            dominated_items = included_items[is_dominated]\n            worst_item = dominated_items[np.argmin(marginal_impact[dominated_items])]\n        else:\n            worst_item = included_items[np.argmin(marginal_impact[included_items])]\n\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8534718172506067,
            4.172479033470154
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 3, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Cluster-based probabilistic addition\n    n_clusters = min(5, len(sorted_indices) // 2)\n    if n_clusters > 1:\n        from sklearn.cluster import KMeans\n        features = np.column_stack([weight_lst, value1_lst, value2_lst])\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(features)\n        cluster_labels = kmeans.labels_\n\n        # Calculate cluster potential (average marginal impact)\n        cluster_potential = np.zeros(n_clusters)\n        for i in range(n_clusters):\n            cluster_items = np.where(cluster_labels == i)[0]\n            cluster_potential[i] = np.mean(marginal_impact[cluster_items])\n\n        # Add items from high-potential clusters with probability\n        remaining_capacity = capacity - current_weight\n        candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n        if len(candidates) > 0:\n            for idx in candidates:\n                cluster = cluster_labels[idx]\n                add_prob = min(0.9, 0.3 + 0.6 * (cluster_potential[cluster] / np.max(cluster_potential)))\n                if np.random.rand() < add_prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Dynamic slack-based swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            weight_diff = weight_lst[j] - weight_lst[i]\n            slack = capacity - (current_weight + weight_diff)\n            if slack >= 0 and (marginal_impact[j] > marginal_impact[i] or\n                              (value1_lst[j] > value1_lst[i] and slack >= 0) or\n                              (value2_lst[j] > value2_lst[i] and slack >= 0)):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_diff\n                break\n\n    # Phase 3: Multi-objective rebalancing with Pareto dominance\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Identify non-dominated items to keep\n        values = np.column_stack([value1_lst[included_items], value2_lst[included_items]])\n        is_dominated = np.zeros(len(included_items), dtype=bool)\n\n        for i in range(len(included_items)):\n            for j in range(len(included_items)):\n                if i != j and (values[j, 0] >= values[i, 0] and values[j, 1] >= values[i, 1]):\n                    if values[j, 0] > values[i, 0] or values[j, 1] > values[i, 1]:\n                        is_dominated[i] = True\n                        break\n\n        if np.any(~is_dominated):\n            dominated_items = included_items[is_dominated]\n            worst_item = dominated_items[np.argmin(marginal_impact[dominated_items])]\n        else:\n            worst_item = included_items[np.argmin(marginal_impact[included_items])]\n\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 41,
        "algorithm": "The algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items (based on combined marginal impact) through three phases: adding top 15% of high-impact excluded items, performing targeted swaps between included and excluded items that improve at least one objective while respecting capacity, and dynamically removing low-impact included items when capacity is exceeded. It intelligently selects solutions from the top 15% of the archive for improvement, using adaptive thresholds and dynamic probabilities to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8683496923227703,
            0.5731351971626282
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 42,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -1.011331337201361,
            1.7290266454219818
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 42,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -1.011331337201361,
            1.7290266454219818
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 43,
        "algorithm": "The algorithm selects the highest-scoring solution from the archive (based on combined objective values), then enhances it through three phases: adding high-impact items (60% chance for top 10% marginal items), performing capacity-aware swaps where excluded items strictly dominate included ones in both objectives, and dynamically rebalancing by removing the worst-performing items when capacity is exceeded. The algorithm prioritizes high-marginal-value items and ensures feasibility through strict capacity checks and targeted swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 10% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 10)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap strictly improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove the worst-performing item\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7744648496093236,
            1.9541884064674377
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select the solution with the highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 10% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 10)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap strictly improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove the worst-performing item\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, feasibility-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 44,
        "algorithm": "The algorithm selects a promising solution from the archive (top 20% by combined marginal impact) and generates a neighbor through four phases: 1) adding high-impact items (top 20%) with 70% probability, 2) performing capacity-aware swaps of high-impact items, 3) dynamically removing low-marginal items if capacity is exceeded, and 4) optionally flipping high-impact items for diversity. It prioritizes marginal impact, ensures feasibility, and balances exploitation/exploration through probabilistic selection. The algorithm is structured to iteratively refine solutions while maintaining diversity and feasibility constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Optional random flip of high-impact items for diversity\n    if np.random.rand() < 0.3:  # 30% chance to perform this phase\n        top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n        if len(top_impact_items) > 0:\n            num_to_flip = min(2, len(top_impact_items))  # Flip up to 2 items\n            flip_indices = np.random.choice(top_impact_items, size=num_to_flip, replace=False)\n            for idx in flip_indices:\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7497767617170195,
            0.49938327074050903
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Optional random flip of high-impact items for diversity\n    if np.random.rand() < 0.3:  # 30% chance to perform this phase\n        top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n        if len(top_impact_items) > 0:\n            num_to_flip = min(2, len(top_impact_items))  # Flip up to 2 items\n            flip_indices = np.random.choice(top_impact_items, size=num_to_flip, replace=False)\n            for idx in flip_indices:\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 2 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n**Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n**Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n**Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 45,
        "algorithm": "The algorithm adaptively selects a high-potential solution from the archive, prioritizes items with high marginal impacts (70th percentile thresholds), and applies a three-phase hybrid local search: adding high-impact items, strategically swapping items for better trade-offs (weighted by 0.7 for value1 and 0.3 for value2), and removing low-impact items, while ensuring feasibility through dynamic weight adjustments and final feasibility checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high Pareto potential\n    archive.sort(key=lambda x: (x[1][0], x[1][1]), reverse=True)\n    selected_idx = np.random.choice(min(3, len(archive)), p=[0.6, 0.3, 0.1] if len(archive) >= 3 else [1.0])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    included = base_solution == 1\n    marginal1 = value1_lst - np.sum(value1_lst[included])\n    marginal2 = value2_lst - np.sum(value2_lst[included])\n\n    # Dynamic thresholds (70th percentile)\n    threshold1 = np.percentile(marginal1, 70)\n    threshold2 = np.percentile(marginal2, 70)\n\n    # Hybrid local search\n    for _ in range(3):  # Multiple passes for better convergence\n        # Phase 1: Add high-impact items\n        candidates = np.where((marginal1 > threshold1) | (marginal2 > threshold2))[0]\n        candidates = candidates[new_solution[candidates] == 0]\n        candidates = candidates[weight_lst[candidates] <= (capacity - current_weight)]\n\n        if len(candidates) > 0:\n            selected = np.random.choice(candidates, size=min(2, len(candidates)), replace=False)\n            new_solution[selected] = 1\n            current_weight += np.sum(weight_lst[selected])\n\n        # Phase 2: Swap items for better trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        for i in included_items:\n            for j in excluded_items:\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if weight_diff <= (capacity - current_weight):\n                    # Weighted improvement check\n                    improvement = (marginal1[j] * 0.7 + marginal2[j] * 0.3) - (marginal1[i] * 0.7 + marginal2[i] * 0.3)\n                    if improvement > 0:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight += weight_diff\n                        break\n\n        # Phase 3: Remove low-impact items\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            low_impact = np.where((marginal1[included_items] < threshold1) & (marginal2[included_items] < threshold2))[0]\n            if len(low_impact) > 0:\n                to_remove = included_items[low_impact]\n                new_solution[to_remove] = 0\n                current_weight -= np.sum(weight_lst[to_remove])\n\n    # Final feasibility check\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal1[included_items] + marginal2[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9009514313912396,
            4.112062603235245
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high Pareto potential\n    archive.sort(key=lambda x: (x[1][0], x[1][1]), reverse=True)\n    selected_idx = np.random.choice(min(3, len(archive)), p=[0.6, 0.3, 0.1] if len(archive) >= 3 else [1.0])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    included = base_solution == 1\n    marginal1 = value1_lst - np.sum(value1_lst[included])\n    marginal2 = value2_lst - np.sum(value2_lst[included])\n\n    # Dynamic thresholds (70th percentile)\n    threshold1 = np.percentile(marginal1, 70)\n    threshold2 = np.percentile(marginal2, 70)\n\n    # Hybrid local search\n    for _ in range(3):  # Multiple passes for better convergence\n        # Phase 1: Add high-impact items\n        candidates = np.where((marginal1 > threshold1) | (marginal2 > threshold2))[0]\n        candidates = candidates[new_solution[candidates] == 0]\n        candidates = candidates[weight_lst[candidates] <= (capacity - current_weight)]\n\n        if len(candidates) > 0:\n            selected = np.random.choice(candidates, size=min(2, len(candidates)), replace=False)\n            new_solution[selected] = 1\n            current_weight += np.sum(weight_lst[selected])\n\n        # Phase 2: Swap items for better trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        for i in included_items:\n            for j in excluded_items:\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if weight_diff <= (capacity - current_weight):\n                    # Weighted improvement check\n                    improvement = (marginal1[j] * 0.7 + marginal2[j] * 0.3) - (marginal1[i] * 0.7 + marginal2[i] * 0.3)\n                    if improvement > 0:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight += weight_diff\n                        break\n\n        # Phase 3: Remove low-impact items\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            low_impact = np.where((marginal1[included_items] < threshold1) & (marginal2[included_items] < threshold2))[0]\n            if len(low_impact) > 0:\n                to_remove = included_items[low_impact]\n                new_solution[to_remove] = 0\n                current_weight -= np.sum(weight_lst[to_remove])\n\n    # Final feasibility check\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal1[included_items] + marginal2[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n**Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n**Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n**Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 46,
        "algorithm": "The algorithm employs a three-phase approach: first selecting a Pareto-optimal base solution, then performing cluster-aware swaps to improve both objectives, and finally ensuring feasibility through dynamic utility-based removal of low-value items. It prioritizes items with high dominance metrics in their respective objectives and uses clustering to guide targeted improvements, while maintaining feasibility through weight-sensitive replacement with adaptive thresholds.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-aware selection with clustering\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate objective-specific dominance metrics\n    dominance1 = value1_lst / (weight_lst + 1e-6)\n    dominance2 = value2_lst / (weight_lst + 1e-6)\n\n    # Cluster items by their dominance characteristics\n    from sklearn.cluster import KMeans\n    X = np.column_stack([dominance1, dominance2])\n    kmeans = KMeans(n_clusters=min(3, len(X)), random_state=42).fit(X)\n    cluster_labels = kmeans.labels_\n\n    # Phase 2: Targeted swaps using cluster information\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        cluster_i = cluster_labels[i]\n        # Find items in the same cluster that could improve both objectives\n        potential_swaps = [j for j in excluded_items if\n                          (cluster_labels[j] == cluster_i) and\n                          (weight_lst[j] <= capacity - current_weight + weight_lst[i]) and\n                          (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])]\n\n        if potential_swaps:\n            j = np.random.choice(potential_swaps)\n            new_solution[i], new_solution[j] = 0, 1\n            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n            break\n\n    # Phase 3: Weight-sensitive replacement with dynamic utility\n    utility = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    dynamic_threshold = np.percentile(utility[new_solution == 1], 30)\n\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items below dynamic utility threshold first\n        removal_candidates = [i for i in included_items if utility[i] < dynamic_threshold]\n        if not removal_candidates:\n            removal_candidates = included_items[np.argsort(utility[included_items])[:2]]\n\n        for i in removal_candidates:\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n            if current_weight <= capacity:\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.3183421998093418,
            10.619145333766937
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-aware selection with clustering\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate objective-specific dominance metrics\n    dominance1 = value1_lst / (weight_lst + 1e-6)\n    dominance2 = value2_lst / (weight_lst + 1e-6)\n\n    # Cluster items by their dominance characteristics\n    from sklearn.cluster import KMeans\n    X = np.column_stack([dominance1, dominance2])\n    kmeans = KMeans(n_clusters=min(3, len(X)), random_state=42).fit(X)\n    cluster_labels = kmeans.labels_\n\n    # Phase 2: Targeted swaps using cluster information\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        cluster_i = cluster_labels[i]\n        # Find items in the same cluster that could improve both objectives\n        potential_swaps = [j for j in excluded_items if\n                          (cluster_labels[j] == cluster_i) and\n                          (weight_lst[j] <= capacity - current_weight + weight_lst[i]) and\n                          (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])]\n\n        if potential_swaps:\n            j = np.random.choice(potential_swaps)\n            new_solution[i], new_solution[j] = 0, 1\n            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n            break\n\n    # Phase 3: Weight-sensitive replacement with dynamic utility\n    utility = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    dynamic_threshold = np.percentile(utility[new_solution == 1], 30)\n\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items below dynamic utility threshold first\n        removal_candidates = [i for i in included_items if utility[i] < dynamic_threshold]\n        if not removal_candidates:\n            removal_candidates = included_items[np.argsort(utility[included_items])[:2]]\n\n        for i in removal_candidates:\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n            if current_weight <= capacity:\n                break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n**Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n**Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n**Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 47,
        "algorithm": "The algorithm combines a Pareto-aware selection phase with a multi-criteria swap operation and adaptive capacity management, prioritizing high-marginal-impact items while ensuring feasibility. It dynamically balances exploration (adding promising items) and exploitation (removing low-impact items) using percentile-based thresholds, with a strong focus on both objectives through impact dominance criteria. The method innovatively integrates dynamic thresholding and hybrid operations to optimize both objectives while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-aware selection with adaptive diversity\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact with adaptive weights\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 2: Multi-criteria swap with impact dominance\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Impact dominance criterion: better in both objectives or better in one with significant improvement\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or\n                    (value1_lst[j] > value1_lst[i] and (value1_lst[j] - value1_lst[i]) > 0.2 * value1_lst[i]) or\n                    (value2_lst[j] > value2_lst[i] and (value2_lst[j] - value2_lst[i]) > 0.2 * value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity-aware replacement with adaptive thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate removal impact\n        removal_impact = marginal_impact[included_items]\n        threshold = np.percentile(removal_impact, 30)  # More aggressive removal\n\n        # Remove items below threshold or with highest weight if no threshold items\n        candidates = included_items[removal_impact <= threshold]\n        if len(candidates) == 0:\n            candidates = included_items[np.argsort(weight_lst[included_items])[-1:]]  # Heaviest item\n\n        worst_item = candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Final phase: Add high-impact items with dynamic capacity check\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 30% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 3)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.8:  # Higher chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.5600570837663172,
            2.246350586414337
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-aware selection with adaptive diversity\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact with adaptive weights\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 2: Multi-criteria swap with impact dominance\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Impact dominance criterion: better in both objectives or better in one with significant improvement\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or\n                    (value1_lst[j] > value1_lst[i] and (value1_lst[j] - value1_lst[i]) > 0.2 * value1_lst[i]) or\n                    (value2_lst[j] > value2_lst[i] and (value2_lst[j] - value2_lst[i]) > 0.2 * value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity-aware replacement with adaptive thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate removal impact\n        removal_impact = marginal_impact[included_items]\n        threshold = np.percentile(removal_impact, 30)  # More aggressive removal\n\n        # Remove items below threshold or with highest weight if no threshold items\n        candidates = included_items[removal_impact <= threshold]\n        if len(candidates) == 0:\n            candidates = included_items[np.argsort(weight_lst[included_items])[-1:]]  # Heaviest item\n\n        worst_item = candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Final phase: Add high-impact items with dynamic capacity check\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 30% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 3)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.8:  # Higher chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 48,
        "algorithm": "The algorithm selects a promising non-dominated solution from the archive, prioritizes high-marginal-impact items for addition, performs targeted swaps between included and excluded items to improve both objectives, and ensures feasibility by removing low-value items if capacity is exceeded. It balances exploration (via probabilistic addition) and exploitation (via targeted swaps) while dynamically adjusting to the current solution's weight constraints. The marginal impact analysis and adaptive thresholds guide the search toward high-quality neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (weighted sum of objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with normalization\n    marginal_impact1 = value1_lst / (weight_lst + 1e-8)  # Avoid division by zero\n    marginal_impact2 = value2_lst / (weight_lst + 1e-8)\n    marginal_impact1 *= (1 - base_solution)\n    marginal_impact2 *= (1 - base_solution)\n\n    # Adaptive thresholds (80th percentile)\n    threshold1 = np.percentile(marginal_impact1, 80)\n    threshold2 = np.percentile(marginal_impact2, 80)\n\n    # Phase 1: Probabilistic addition of high-value items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact for either objective\n        high_impact_items = candidates[(marginal_impact1[candidates] > threshold1) |\n                                      (marginal_impact2[candidates] > threshold2)]\n\n        if len(high_impact_items) > 0:\n            # Select top 30% of high impact items\n            top_items = high_impact_items[np.argsort(-marginal_impact1[high_impact_items] -\n                                                   marginal_impact2[high_impact_items])[:max(1, len(high_impact_items)//3)]]\n\n            for idx in top_items:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > 1.2 * marginal_impact1[i] or\n                 marginal_impact2[j] > 1.2 * marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal with objective consideration\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest combined marginal impact\n        removal_scores = (marginal_impact1[included_items] + marginal_impact2[included_items]) / weight_lst[included_items]\n        worst_item = included_items[np.argmin(removal_scores)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8197028528493364,
            0.6654719114303589
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (weighted sum of objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with normalization\n    marginal_impact1 = value1_lst / (weight_lst + 1e-8)  # Avoid division by zero\n    marginal_impact2 = value2_lst / (weight_lst + 1e-8)\n    marginal_impact1 *= (1 - base_solution)\n    marginal_impact2 *= (1 - base_solution)\n\n    # Adaptive thresholds (80th percentile)\n    threshold1 = np.percentile(marginal_impact1, 80)\n    threshold2 = np.percentile(marginal_impact2, 80)\n\n    # Phase 1: Probabilistic addition of high-value items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact for either objective\n        high_impact_items = candidates[(marginal_impact1[candidates] > threshold1) |\n                                      (marginal_impact2[candidates] > threshold2)]\n\n        if len(high_impact_items) > 0:\n            # Select top 30% of high impact items\n            top_items = high_impact_items[np.argsort(-marginal_impact1[high_impact_items] -\n                                                   marginal_impact2[high_impact_items])[:max(1, len(high_impact_items)//3)]]\n\n            for idx in top_items:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > 1.2 * marginal_impact1[i] or\n                 marginal_impact2[j] > 1.2 * marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal with objective consideration\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest combined marginal impact\n        removal_scores = (marginal_impact1[included_items] + marginal_impact2[included_items]) / weight_lst[included_items]\n        worst_item = included_items[np.argmin(removal_scores)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 49,
        "algorithm": "The algorithm selects the most promising solution from the archive using a normalized combined fitness metric of both objectives, then applies a three-phase hybrid local search: Phase 1 greedily adds high-value items, Phase 2 performs weight-balanced swaps between included and excluded items, and Phase 3 removes items to restore feasibility, prioritizing marginal gains and weight efficiency while maintaining diversity through probabilistic selection.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized combined fitness\n    archive.sort(key=lambda x: (x[1][0] / np.max(value1_lst) + x[1][1] / np.max(value2_lst)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal gains with weight normalization\n    marginal_gain1 = value1_lst * (1 - base_solution) / weight_lst\n    marginal_gain2 = value2_lst * (1 - base_solution) / weight_lst\n\n    # Dynamic thresholds (90th percentile)\n    threshold_gain1 = np.percentile(marginal_gain1, 90)\n    threshold_gain2 = np.percentile(marginal_gain2, 90)\n\n    # Phase 1: Greedy addition with gain ratio\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select items with high gain ratio\n        high_gain_ratio = candidates[np.maximum(marginal_gain1[candidates], marginal_gain2[candidates]) > np.maximum(threshold_gain1, threshold_gain2)]\n\n        for idx in high_gain_ratio:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_gain1[j] > marginal_gain1[i] * 0.9 or marginal_gain2[j] > marginal_gain2[i] * 0.9)):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort((value1_lst + value2_lst)[included_items] / weight_lst[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.816500988352376,
            0.944715827703476
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized combined fitness\n    archive.sort(key=lambda x: (x[1][0] / np.max(value1_lst) + x[1][1] / np.max(value2_lst)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal gains with weight normalization\n    marginal_gain1 = value1_lst * (1 - base_solution) / weight_lst\n    marginal_gain2 = value2_lst * (1 - base_solution) / weight_lst\n\n    # Dynamic thresholds (90th percentile)\n    threshold_gain1 = np.percentile(marginal_gain1, 90)\n    threshold_gain2 = np.percentile(marginal_gain2, 90)\n\n    # Phase 1: Greedy addition with gain ratio\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select items with high gain ratio\n        high_gain_ratio = candidates[np.maximum(marginal_gain1[candidates], marginal_gain2[candidates]) > np.maximum(threshold_gain1, threshold_gain2)]\n\n        for idx in high_gain_ratio:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_gain1[j] > marginal_gain1[i] * 0.9 or marginal_gain2[j] > marginal_gain2[i] * 0.9)):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort((value1_lst + value2_lst)[included_items] / weight_lst[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n**Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n**Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n**Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 50,
        "algorithm": "The algorithm selects a high-potential solution from the archive, prioritizing those with strong combined objective values, and then performs a three-phase local search: Phase 1 adds high-impact items using adaptive marginal impact analysis, Phase 2 conducts targeted swaps between included and excluded items with dynamic thresholds to improve both objectives, and Phase 3 removes low-impact items if the solution exceeds capacity. The approach ensures feasibility while balancing both objectives through hybrid operations and dynamic thresholds.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (combining both objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts (combining both objectives)\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n    combined_threshold = np.percentile(combined_impact, 75)\n\n    # Phase 1: Hybrid add operations with combined impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high combined marginal impact\n        high_impact_items = candidates[combined_impact[candidates] > combined_threshold]\n\n        for idx in high_impact_items:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (combined_impact[j] > combined_impact[i] or\n                 (marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]))):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n                break\n\n    # Phase 3: Weight-sensitive rebalancing with combined impact\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact first\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8105757666918838,
            0.7309177219867706
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (combining both objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts (combining both objectives)\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n    combined_threshold = np.percentile(combined_impact, 75)\n\n    # Phase 1: Hybrid add operations with combined impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high combined marginal impact\n        high_impact_items = candidates[combined_impact[candidates] > combined_threshold]\n\n        for idx in high_impact_items:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (combined_impact[j] > combined_impact[i] or\n                 (marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]))):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n                break\n\n    # Phase 3: Weight-sensitive rebalancing with combined impact\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact first\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design with actionable steps:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 51,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high combined marginal impact potential\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    sorted_indices = np.argsort(combined_marginal)[::-1]\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)  # Dynamic threshold\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                if np.random.rand() < 0.8:  # Higher probability for strong candidates\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Targeted swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx] / (np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx] / (np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9399430074619346,
            1.8024620115756989
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high combined marginal impact potential\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    sorted_indices = np.argsort(combined_marginal)[::-1]\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)  # Dynamic threshold\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                if np.random.rand() < 0.8:  # Higher probability for strong candidates\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Targeted swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx] / (np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx] / (np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design with actionable steps:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 52,
        "algorithm": "The algorithm implements a multi-phase approach for the BI-KP, first selecting high-impact solutions from the archive (top 30% by combined marginal value), then applying a hybrid local search with probabilistic swaps, conditional replacements, and capacity-aware additions guided by dynamic thresholds (70th percentile of current item impacts), while strictly enforcing feasibility through continuous weight validation. It prioritizes solutions with high combined marginal impact and employs adaptive thresholds to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive Pareto-aware selection with impact-weighted prioritization\n    combined_marginal = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        in_sol = sol == 1\n        marginal1 = np.sum(value1_lst[in_sol])\n        marginal2 = np.sum(value2_lst[in_sol])\n        combined_marginal[i] = marginal1 + marginal2\n\n    # Select top 30% by combined marginal impact\n    threshold = np.percentile(combined_marginal, 70)\n    candidate_indices = [i for i, score in enumerate(combined_marginal) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 2: Impact-weighted hybrid local search\n    # Calculate dynamic impact thresholds\n    in_sol = new_solution == 1\n    out_sol = new_solution == 0\n\n    # Calculate marginal contributions for both objectives\n    marginal1_in = value1_lst[in_sol]\n    marginal2_in = value2_lst[in_sol]\n    marginal1_out = value1_lst[out_sol]\n    marginal2_out = value2_lst[out_sol]\n\n    # Dynamic thresholds (70th percentile of current solution's items)\n    if len(marginal1_in) > 0:\n        threshold1_in = np.percentile(marginal1_in, 70)\n        threshold2_in = np.percentile(marginal2_in, 70)\n    else:\n        threshold1_in, threshold2_in = 0, 0\n\n    if len(marginal1_out) > 0:\n        threshold1_out = np.percentile(marginal1_out, 70)\n        threshold2_out = np.percentile(marginal2_out, 70)\n    else:\n        threshold1_out, threshold2_out = 0, 0\n\n    # Operation 1: Probabilistic swaps of high-impact items\n    for i in np.where(in_sol)[0]:\n        for j in np.where(out_sol)[0]:\n            if np.random.random() < 0.3:  # 30% chance for swap attempt\n                delta_weight = weight_lst[j] - weight_lst[i]\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if at least one objective improves\n                    if (value1_lst[j] > threshold1_out and value2_lst[j] > threshold2_out) or \\\n                       (value1_lst[i] > threshold1_in and value2_lst[i] > threshold2_in):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n                        break\n\n    # Operation 2: Conditional replacement of low-impact items\n    for i in np.where(in_sol)[0]:\n        if (value1_lst[i] < threshold1_in or value2_lst[i] < threshold2_in) and np.random.random() < 0.4:\n            # Find high-impact candidates to replace with\n            candidates = np.where((out_sol) &\n                                ((value1_lst > threshold1_out) | (value2_lst > threshold2_out)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select best candidate by combined marginal impact\n                combined_marginal = value1_lst[candidates] + value2_lst[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Operation 3: Capacity-aware addition of high-impact items\n    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n    if remaining_capacity > 0:\n        candidates = np.where((out_sol) &\n                            ((value1_lst > threshold1_out) | (value2_lst > threshold2_out)) &\n                            (weight_lst <= remaining_capacity))[0]\n        if len(candidates) > 0:\n            # Add top 3 items by combined marginal impact\n            combined_marginal = value1_lst[candidates] + value2_lst[candidates]\n            top_candidates = candidates[np.argsort(combined_marginal)[-min(3, len(candidates)):]]\n            for j in top_candidates:\n                new_solution[j] = 1\n\n    # Phase 3: Final feasibility and dominance check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = value1_lst[excess_items] + value2_lst[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    # Ensure the new solution dominates or is at least as good as the base solution\n    base_obj = archive[selected_idx][1]\n    new_obj = (np.sum(value1_lst[new_solution == 1]), np.sum(value2_lst[new_solution == 1]))\n    if not (new_obj[0] >= base_obj[0] and new_obj[1] >= base_obj[1] and (new_obj[0] > base_obj[0] or new_obj[1] > base_obj[1])):\n        # If not better, perform one final improvement step\n        candidates = np.where((out_sol) &\n                            ((value1_lst > threshold1_out) | (value2_lst > threshold2_out)) &\n                            (weight_lst <= capacity - np.sum(weight_lst[new_solution == 1])))[0]\n        if len(candidates) > 0:\n            j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n            new_solution[j] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.5029954166627215,
            5.502338081598282
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive Pareto-aware selection with impact-weighted prioritization\n    combined_marginal = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        in_sol = sol == 1\n        marginal1 = np.sum(value1_lst[in_sol])\n        marginal2 = np.sum(value2_lst[in_sol])\n        combined_marginal[i] = marginal1 + marginal2\n\n    # Select top 30% by combined marginal impact\n    threshold = np.percentile(combined_marginal, 70)\n    candidate_indices = [i for i, score in enumerate(combined_marginal) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 2: Impact-weighted hybrid local search\n    # Calculate dynamic impact thresholds\n    in_sol = new_solution == 1\n    out_sol = new_solution == 0\n\n    # Calculate marginal contributions for both objectives\n    marginal1_in = value1_lst[in_sol]\n    marginal2_in = value2_lst[in_sol]\n    marginal1_out = value1_lst[out_sol]\n    marginal2_out = value2_lst[out_sol]\n\n    # Dynamic thresholds (70th percentile of current solution's items)\n    if len(marginal1_in) > 0:\n        threshold1_in = np.percentile(marginal1_in, 70)\n        threshold2_in = np.percentile(marginal2_in, 70)\n    else:\n        threshold1_in, threshold2_in = 0, 0\n\n    if len(marginal1_out) > 0:\n        threshold1_out = np.percentile(marginal1_out, 70)\n        threshold2_out = np.percentile(marginal2_out, 70)\n    else:\n        threshold1_out, threshold2_out = 0, 0\n\n    # Operation 1: Probabilistic swaps of high-impact items\n    for i in np.where(in_sol)[0]:\n        for j in np.where(out_sol)[0]:\n            if np.random.random() < 0.3:  # 30% chance for swap attempt\n                delta_weight = weight_lst[j] - weight_lst[i]\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if at least one objective improves\n                    if (value1_lst[j] > threshold1_out and value2_lst[j] > threshold2_out) or \\\n                       (value1_lst[i] > threshold1_in and value2_lst[i] > threshold2_in):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n                        break\n\n    # Operation 2: Conditional replacement of low-impact items\n    for i in np.where(in_sol)[0]:\n        if (value1_lst[i] < threshold1_in or value2_lst[i] < threshold2_in) and np.random.random() < 0.4:\n            # Find high-impact candidates to replace with\n            candidates = np.where((out_sol) &\n                                ((value1_lst > threshold1_out) | (value2_lst > threshold2_out)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select best candidate by combined marginal impact\n                combined_marginal = value1_lst[candidates] + value2_lst[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Operation 3: Capacity-aware addition of high-impact items\n    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n    if remaining_capacity > 0:\n        candidates = np.where((out_sol) &\n                            ((value1_lst > threshold1_out) | (value2_lst > threshold2_out)) &\n                            (weight_lst <= remaining_capacity))[0]\n        if len(candidates) > 0:\n            # Add top 3 items by combined marginal impact\n            combined_marginal = value1_lst[candidates] + value2_lst[candidates]\n            top_candidates = candidates[np.argsort(combined_marginal)[-min(3, len(candidates)):]]\n            for j in top_candidates:\n                new_solution[j] = 1\n\n    # Phase 3: Final feasibility and dominance check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = value1_lst[excess_items] + value2_lst[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    # Ensure the new solution dominates or is at least as good as the base solution\n    base_obj = archive[selected_idx][1]\n    new_obj = (np.sum(value1_lst[new_solution == 1]), np.sum(value2_lst[new_solution == 1]))\n    if not (new_obj[0] >= base_obj[0] and new_obj[1] >= base_obj[1] and (new_obj[0] > base_obj[0] or new_obj[1] > base_obj[1])):\n        # If not better, perform one final improvement step\n        candidates = np.where((out_sol) &\n                            ((value1_lst > threshold1_out) | (value2_lst > threshold2_out)) &\n                            (weight_lst <= capacity - np.sum(weight_lst[new_solution == 1])))[0]\n        if len(candidates) > 0:\n            j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n            new_solution[j] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design with actionable steps:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 53,
        "algorithm": "The algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.6787151048152297,
            0.9140625596046448
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 54,
        "algorithm": "This algorithm selects a solution from the least crowded cluster of non-dominated solutions, applies an objective-aware perturbation that probabilistically flips items based on their marginal contribution to both objectives while ensuring feasibility, and performs targeted replacements of low-contribution items with high-contribution candidates from the same cluster. It prioritizes items with higher marginal impact and aligns the solution with the cluster's objective space, balancing exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Extract objective values\n    objectives = np.array([obj for (_, obj) in archive])\n\n    # Cluster solutions using k-means (k=3)\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=min(3, len(archive)), random_state=42).fit(objectives)\n    cluster_sizes = np.bincount(kmeans.labels_)\n    least_crowded_cluster = np.argmin(cluster_sizes)\n\n    # Select from least crowded cluster\n    candidate_indices = [i for i, label in enumerate(kmeans.labels_) if label == least_crowded_cluster]\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal contributions\n    total_value1 = np.sum(value1_lst[base_solution == 1])\n    total_value2 = np.sum(value2_lst[base_solution == 1])\n    marginal1 = (value1_lst - total_value1) / (total_value1 + 1e-6)\n    marginal2 = (value2_lst - total_value2) / (total_value2 + 1e-6)\n\n    # Objective-aware perturbation\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for flip\n            if new_solution[i] == 1:\n                # Probability of removal based on marginal impact\n                prob = 0.5 * (1 - min(marginal1[i], marginal2[i]))\n                if np.random.random() < prob:\n                    if current_weight - weight_lst[i] <= capacity:\n                        new_solution[i] = 0\n                        current_weight -= weight_lst[i]\n            else:\n                # Probability of addition based on marginal impact\n                prob = 0.5 * (max(marginal1[i], marginal2[i]) + 0.5)\n                if np.random.random() < prob:\n                    if current_weight + weight_lst[i] <= capacity:\n                        new_solution[i] = 1\n                        current_weight += weight_lst[i]\n\n    # Targeted replacement from same cluster\n    cluster_center = kmeans.cluster_centers_[least_crowded_cluster]\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate closest to cluster center\n                candidate_values = np.array([(value1_lst[j], value2_lst[j]) for j in candidates])\n                distances = np.linalg.norm(candidate_values - cluster_center, axis=1)\n                j = candidates[np.argmin(distances)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined normalized marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7558906541284062,
            4.563586562871933
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Extract objective values\n    objectives = np.array([obj for (_, obj) in archive])\n\n    # Cluster solutions using k-means (k=3)\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=min(3, len(archive)), random_state=42).fit(objectives)\n    cluster_sizes = np.bincount(kmeans.labels_)\n    least_crowded_cluster = np.argmin(cluster_sizes)\n\n    # Select from least crowded cluster\n    candidate_indices = [i for i, label in enumerate(kmeans.labels_) if label == least_crowded_cluster]\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal contributions\n    total_value1 = np.sum(value1_lst[base_solution == 1])\n    total_value2 = np.sum(value2_lst[base_solution == 1])\n    marginal1 = (value1_lst - total_value1) / (total_value1 + 1e-6)\n    marginal2 = (value2_lst - total_value2) / (total_value2 + 1e-6)\n\n    # Objective-aware perturbation\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for flip\n            if new_solution[i] == 1:\n                # Probability of removal based on marginal impact\n                prob = 0.5 * (1 - min(marginal1[i], marginal2[i]))\n                if np.random.random() < prob:\n                    if current_weight - weight_lst[i] <= capacity:\n                        new_solution[i] = 0\n                        current_weight -= weight_lst[i]\n            else:\n                # Probability of addition based on marginal impact\n                prob = 0.5 * (max(marginal1[i], marginal2[i]) + 0.5)\n                if np.random.random() < prob:\n                    if current_weight + weight_lst[i] <= capacity:\n                        new_solution[i] = 1\n                        current_weight += weight_lst[i]\n\n    # Targeted replacement from same cluster\n    cluster_center = kmeans.cluster_centers_[least_crowded_cluster]\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate closest to cluster center\n                candidate_values = np.array([(value1_lst[j], value2_lst[j]) for j in candidates])\n                distances = np.linalg.norm(candidate_values - cluster_center, axis=1)\n                j = candidates[np.argmin(distances)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined normalized marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 55,
        "algorithm": "The algorithm selects high-performing solutions from the top 50% of the archive (based on combined objective scores) and applies a hybrid local search strategy: it first probabilistically inserts/removes items to explore the solution space, then performs targeted swaps between items with complementary marginal impacts (positive for one objective, negative for the other) to balance trade-offs, while always ensuring feasibility through dynamic weight adjustments and candidate filtering. Marginal contributions for both objectives guide the search, with items contributing negatively to either objective being prioritized for removal, and swaps are only performed if they improve at least one objective.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.4403713406693853,
            1.8680947124958038
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design with actionable steps:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 56,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.4060125350045944,
            1.6354747712612152
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 57,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values and diversity, then generates a neighbor through three phases: adaptive addition of high-marginal-impact items, capacity-constrained hybrid swaps/replacements, and dynamic threshold-based rebalancing to maintain feasibility while optimizing both objectives. It prioritizes items with strong combined marginal impacts and performs targeted swaps to improve complementary objectives, while ensuring feasibility through iterative removal of low-utility items. The selection criterion favors solutions with higher combined objective scores and greater diversity, while the local search operations focus on marginal improvements and capacity-constrained transformations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9103505739019655,
            2.007381856441498
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal_impact1 = value1_lst * (1 - base_solution)\n    marginal_impact2 = value2_lst * (1 - base_solution)\n\n    # Dynamic thresholds (75th percentile)\n    threshold1 = np.percentile(marginal_impact1, 75)\n    threshold2 = np.percentile(marginal_impact2, 75)\n\n    # Phase 1: Hybrid add/swap operations\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add items with high marginal impact\n        high_impact1 = candidates[marginal_impact1[candidates] > threshold1]\n        high_impact2 = candidates[marginal_impact2[candidates] > threshold2]\n\n        for idx in np.concatenate([high_impact1, high_impact2]):\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with feasibility check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if (weight_lst[j] <= capacity - current_weight + weight_lst[i] and\n                (marginal_impact1[j] > marginal_impact1[i] or marginal_impact2[j] > marginal_impact2[i])):\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                break\n\n    # Phase 3: Weight-sensitive removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(marginal_impact1[included_items] + marginal_impact2[included_items])]\n        worst_item = removal_candidates[0]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive using a combined objective and ratio-based metric, then applies a three-phase hybrid local search: (1) probabilistically adds items with high marginal impact while respecting capacity, (2) performs targeted swaps to improve Pareto dominance, and (3) dynamically rebalances by removing low-impact items while preserving high-value ones to maintain feasibility. The approach prioritizes both objectives through marginal impact calculations and balances exploration/exploitation with randomness and dynamic thresholds.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 58,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a three-phase hybrid local search: it first probabilistically adds high-ratio items, then performs ratio-guided Pareto-improving swaps, and finally removes low-value-to-weight items to maintain feasibility, all while dynamically balancing exploration and exploitation through probabilistic thresholds and dominance metrics. The selection prioritizes solutions with high combined objective diversity, and the search operations are guided by normalized marginal impact ratios to optimize both objectives simultaneously.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective diversity\n    archive.sort(key=lambda x: (x[1][0] * (1 + 0.1 * np.random.rand()) + x[1][1] * (1 + 0.1 * np.random.rand())) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact ratios\n    marginal_ratio1 = value1_lst / (weight_lst + 1e-6)\n    marginal_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = (marginal_ratio1 + marginal_ratio2) / 2\n\n    # Phase 1: Probabilistic additions with ratio-based selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        ratio_threshold = np.percentile(combined_ratio[candidates], 60)\n        for idx in candidates:\n            if combined_ratio[idx] > ratio_threshold and np.random.rand() < 0.4:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Ratio-guided swaps with Pareto dominance check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement in ratio space\n                if (marginal_ratio1[j] > marginal_ratio1[i] and marginal_ratio2[j] >= marginal_ratio2[i]) or \\\n                   (marginal_ratio1[j] >= marginal_ratio1[i] and marginal_ratio2[j] > marginal_ratio2[i]) or \\\n                   (np.random.rand() < 0.2 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Value-to-weight dominance-based removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest value-to-weight dominance score\n        dominance_score = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_item = included_items[np.argmin(dominance_score * (1 + 0.3 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7891303128745395,
            0.844609260559082
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective diversity\n    archive.sort(key=lambda x: (x[1][0] * (1 + 0.1 * np.random.rand()) + x[1][1] * (1 + 0.1 * np.random.rand())) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact ratios\n    marginal_ratio1 = value1_lst / (weight_lst + 1e-6)\n    marginal_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = (marginal_ratio1 + marginal_ratio2) / 2\n\n    # Phase 1: Probabilistic additions with ratio-based selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        ratio_threshold = np.percentile(combined_ratio[candidates], 60)\n        for idx in candidates:\n            if combined_ratio[idx] > ratio_threshold and np.random.rand() < 0.4:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Ratio-guided swaps with Pareto dominance check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement in ratio space\n                if (marginal_ratio1[j] > marginal_ratio1[i] and marginal_ratio2[j] >= marginal_ratio2[i]) or \\\n                   (marginal_ratio1[j] >= marginal_ratio1[i] and marginal_ratio2[j] > marginal_ratio2[i]) or \\\n                   (np.random.rand() < 0.2 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Value-to-weight dominance-based removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest value-to-weight dominance score\n        dominance_score = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_item = included_items[np.argmin(dominance_score * (1 + 0.3 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high combined marginal impact potential\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    sorted_indices = np.argsort(combined_marginal)[::-1]\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)  # Dynamic threshold\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                if np.random.rand() < 0.8:  # Higher probability for strong candidates\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Targeted swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx] / (np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx] / (np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive using a combined objective and ratio-based metric, then applies a three-phase hybrid local search: (1) probabilistically adds items with high marginal impact while respecting capacity, (2) performs targeted swaps to improve Pareto dominance, and (3) dynamically rebalances by removing low-impact items while preserving high-value ones to maintain feasibility. The approach prioritizes both objectives through marginal impact calculations and balances exploration/exploitation with randomness and dynamic thresholds.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 59,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a three-phase hybrid local search: Phase 1 adds high-impact items probabilistically, Phase 2 performs targeted swaps for complementary improvements, and Phase 3 dynamically rebalances the solution by removing low-impact items while prioritizing weighted objectives. The selection of the base solution is guided by a hybrid metric balancing combined objective values and their ratios, and the entire process includes probabilistic exploration to avoid local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on hybrid metric\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) * (1 + 0.3 * (x[1][0] / (x[1][1] + 1e-6))), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts with objective weighting\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (0.6 * marginal1 + 0.4 * marginal2)  # Weighted combination\n    sorted_indices = np.argsort(combined_marginal)[::-1]\n\n    # Phase 1: Probabilistic addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 60)  # Lower threshold for exploration\n        for idx in sorted_indices:\n            if idx in candidates and combined_marginal[idx] >= threshold and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements with probabilistic acceptance\n                if ((marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i]) or\n                    (marginal1[j] >= marginal1[i] and marginal2[j] > marginal2[i])) and np.random.rand() < 0.5:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal with weighted objectives\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - 0.5 * (value1_lst[idx] / (np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx] / (np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8812616319885691,
            3.1532897651195526
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on hybrid metric\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) * (1 + 0.3 * (x[1][0] / (x[1][1] + 1e-6))), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts with objective weighting\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (0.6 * marginal1 + 0.4 * marginal2)  # Weighted combination\n    sorted_indices = np.argsort(combined_marginal)[::-1]\n\n    # Phase 1: Probabilistic addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 60)  # Lower threshold for exploration\n        for idx in sorted_indices:\n            if idx in candidates and combined_marginal[idx] >= threshold and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements with probabilistic acceptance\n                if ((marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i]) or\n                    (marginal1[j] >= marginal1[i] and marginal2[j] > marginal2[i])) and np.random.rand() < 0.5:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal with weighted objectives\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - 0.5 * (value1_lst[idx] / (np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx] / (np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a high-potential solution from the archive using a combined objective and ratio-based metric, then applies a three-phase hybrid local search: (1) probabilistically adds items with high marginal impact while respecting capacity, (2) performs targeted swaps to improve Pareto dominance, and (3) dynamically rebalances by removing low-impact items while preserving high-value ones to maintain feasibility. The approach prioritizes both objectives through marginal impact calculations and balances exploration/exploitation with randomness and dynamic thresholds.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 60,
        "algorithm": "The algorithm selects a promising solution from the archive using a crowding-distance-based metric that balances both objectives, then applies a four-phase hybrid local search: (1) probabilistically adds high-utility items while respecting capacity, (2) swaps items using a novel dominance-aware metric to improve both objectives, (3) dynamically removes low-utility items to restore feasibility, and (4) adjusts capacity by adding marginal items. The approach prioritizes items with higher combined utility (value1 + value2) normalized by weight, ensuring both objectives are improved while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Novel selection metric combining crowding distance and objective ratios\n    def crowding_metric(sol):\n        obj1, obj2 = sol[1]\n        crowding = (obj1 + obj2) / (obj1 / (obj2 + 1e-6) + 1)\n        return crowding * (1 + 0.1 * np.random.rand())\n\n    archive.sort(key=crowding_metric, reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Probabilistic addition with novel utility function\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Novel utility function combining both objectives and weight efficiency\n        utility = (value1_lst + value2_lst) * (1 + 0.2 * np.random.rand()) / (weight_lst + 1e-6)\n        threshold = np.percentile(utility[candidates], 60)  # Top 40% utility\n\n        for idx in np.argsort(utility)[::-1]:\n            if idx in candidates and utility[idx] >= threshold and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swap with novel dominance-aware metric\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            # Novel swap metric combining both objectives and weight efficiency\n            swap_metric = (value1_lst[j] - value1_lst[i] + value2_lst[j] - value2_lst[i]) / (weight_lst[j] - weight_lst[i] + 1e-6)\n            if swap_metric > 0 and weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n                break\n\n    # Phase 3: Dynamic removal with novel utility preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Novel removal metric combining utility and randomness\n        removal_metric = (value1_lst + value2_lst) / (weight_lst + 1e-6) * (1 - 0.3 * np.random.rand())\n        worst_item = included_items[np.argmin(removal_metric[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Novel capacity adjustment\n    if current_weight < capacity:\n        # Add items with highest marginal utility until capacity is reached\n        remaining_capacity = capacity - current_weight\n        candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n        if len(candidates) > 0:\n            utility = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            for idx in np.argsort(utility)[::-1]:\n                if idx in candidates:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    if remaining_capacity <= 0:\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.5768744645873509,
            5.9133521020412445
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Novel selection metric combining crowding distance and objective ratios\n    def crowding_metric(sol):\n        obj1, obj2 = sol[1]\n        crowding = (obj1 + obj2) / (obj1 / (obj2 + 1e-6) + 1)\n        return crowding * (1 + 0.1 * np.random.rand())\n\n    archive.sort(key=crowding_metric, reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Probabilistic addition with novel utility function\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Novel utility function combining both objectives and weight efficiency\n        utility = (value1_lst + value2_lst) * (1 + 0.2 * np.random.rand()) / (weight_lst + 1e-6)\n        threshold = np.percentile(utility[candidates], 60)  # Top 40% utility\n\n        for idx in np.argsort(utility)[::-1]:\n            if idx in candidates and utility[idx] >= threshold and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swap with novel dominance-aware metric\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            # Novel swap metric combining both objectives and weight efficiency\n            swap_metric = (value1_lst[j] - value1_lst[i] + value2_lst[j] - value2_lst[i]) / (weight_lst[j] - weight_lst[i] + 1e-6)\n            if swap_metric > 0 and weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n                break\n\n    # Phase 3: Dynamic removal with novel utility preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Novel removal metric combining utility and randomness\n        removal_metric = (value1_lst + value2_lst) / (weight_lst + 1e-6) * (1 - 0.3 * np.random.rand())\n        worst_item = included_items[np.argmin(removal_metric[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Novel capacity adjustment\n    if current_weight < capacity:\n        # Add items with highest marginal utility until capacity is reached\n        remaining_capacity = capacity - current_weight\n        candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n        if len(candidates) > 0:\n            utility = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n            for idx in np.argsort(utility)[::-1]:\n                if idx in candidates:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    if remaining_capacity <= 0:\n                        break\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a high-potential solution from the archive using a combined objective and ratio-based metric, then applies a three-phase hybrid local search: (1) probabilistically adds items with high marginal impact while respecting capacity, (2) performs targeted swaps to improve Pareto dominance, and (3) dynamically rebalances by removing low-impact items while preserving high-value ones to maintain feasibility. The approach prioritizes both objectives through marginal impact calculations and balances exploration/exploitation with randomness and dynamic thresholds.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 61,
        "algorithm": "The algorithm selects a promising solution from the archive based on a weighted combination of objectives and their ratios, then applies a four-phase hybrid local search: greedily adding high-impact items, performing multi-objective swaps, removing low-impact items with dynamic thresholds, and finally rebalancing by reinserting high-impact items. Value1 is prioritized (0.6 weight) over value2 (0.4 weight), with marginal impact calculated as a weighted average of both objectives divided by weight. The method ensures feasibility by dynamically adjusting capacity and using probabilistic acceptance for swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on weighted objectives and ratios\n    archive.sort(key=lambda x: (0.6 * x[1][0] + 0.4 * x[1][1]) / (0.7 * x[1][0] / (x[1][1] + 1e-6) + 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives with different weights\n    marginal_impact = (0.5 * value1_lst + 0.5 * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Greedy item additions with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 60)  # Top 40% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Multi-objective swaps with probabilistic acceptance\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.4 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Diversity-preserving removal with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact but with higher probability for lower impact items\n        removal_prob = marginal_impact[included_items] / (np.sum(marginal_impact[included_items]) + 1e-6)\n        worst_item = np.random.choice(included_items, p=removal_prob)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Final rebalancing by selectively reinserting high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 80)  # Top 20% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.7468112292783999,
            2.193416953086853
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on weighted objectives and ratios\n    archive.sort(key=lambda x: (0.6 * x[1][0] + 0.4 * x[1][1]) / (0.7 * x[1][0] / (x[1][1] + 1e-6) + 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives with different weights\n    marginal_impact = (0.5 * value1_lst + 0.5 * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Greedy item additions with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 60)  # Top 40% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Multi-objective swaps with probabilistic acceptance\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.4 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Diversity-preserving removal with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact but with higher probability for lower impact items\n        removal_prob = marginal_impact[included_items] / (np.sum(marginal_impact[included_items]) + 1e-6)\n        worst_item = np.random.choice(included_items, p=removal_prob)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Final rebalancing by selectively reinserting high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 80)  # Top 20% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive using a combined objective and ratio-based metric, then applies a three-phase hybrid local search: (1) probabilistically adds items with high marginal impact while respecting capacity, (2) performs targeted swaps to improve Pareto dominance, and (3) dynamically rebalances by removing low-impact items while preserving high-value ones to maintain feasibility. The approach prioritizes both objectives through marginal impact calculations and balances exploration/exploitation with randomness and dynamic thresholds.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 62,
        "algorithm": "The algorithm selects a high-potential solution from the archive based on a combined objective and ratio metric, then applies a four-phase hybrid local search: (1) probabilistically adds high-marginal-impact items, (2) performs targeted swaps to improve Pareto dominance, (3) dynamically rebalances by removing low-impact items, and (4) adaptively flips high-impact items. It prioritizes items with better marginal impact (considering both objectives) and ensures feasibility through dynamic capacity checks, balancing exploration and exploitation with randomness.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Adaptive flips of high-impact items\n    if np.random.rand() < 0.4:  # 40% chance to perform adaptive flips\n        top_items = sorted_indices[:max(1, len(sorted_indices) // 5)]  # Top 20% items\n        for idx in top_items:\n            if np.random.rand() < 0.5:  # 50% chance to flip each top item\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8082586878728926,
            0.8441665768623352
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential based on combined objectives and ratios\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (x[1][0] / (x[1][1] + 1e-6) + 1), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impact considering both objectives\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Probabilistic item additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(marginal_impact[candidates], 70)  # Top 30% marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and marginal_impact[idx] >= threshold and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid swap operations targeting Pareto improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (np.random.rand() < 0.3 and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity preservation\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact but preserve high-value items\n        worst_item = included_items[np.argmin(marginal_impact[included_items] * (1 + 0.2 * np.random.rand()))]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Adaptive flips of high-impact items\n    if np.random.rand() < 0.4:  # 40% chance to perform adaptive flips\n        top_items = sorted_indices[:max(1, len(sorted_indices) // 5)]  # Top 20% items\n        for idx in top_items:\n            if np.random.rand() < 0.5:  # 50% chance to flip each top item\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 63,
        "algorithm": "The algorithm selects a base solution from the archive (prioritizing higher-value1 and lower-value2), then applies a three-phase local search: probabilistic addition of high-marginal-impact items, capacity-aware swaps for complementary improvements, and dynamic removal of low-utility items to maintain feasibility. It balances exploration (adaptive thresholds) and exploitation (utility-based removal), dynamically adjusting weights based on current solution quality and capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection\n    archive.sort(key=lambda x: (x[1][0], -x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*25)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.861261648178678,
            1.6979512572288513
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection\n    archive.sort(key=lambda x: (x[1][0], -x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*25)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact) and generates a neighbor through four phases: 1) adding high-impact items (top 20%) with 70% probability, 2) performing capacity-aware swaps of high-impact items, 3) dynamically removing low-marginal items if capacity is exceeded, and 4) optionally flipping high-impact items for diversity. It prioritizes marginal impact, ensures feasibility, and balances exploitation/exploration through probabilistic selection. The algorithm is structured to iteratively refine solutions while maintaining diversity and feasibility constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Optional random flip of high-impact items for diversity\n    if np.random.rand() < 0.3:  # 30% chance to perform this phase\n        top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n        if len(top_impact_items) > 0:\n            num_to_flip = min(2, len(top_impact_items))  # Flip up to 2 items\n            flip_indices = np.random.choice(top_impact_items, size=num_to_flip, replace=False)\n            for idx in flip_indices:\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 64,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a tiered hybrid local search strategy, prioritizing high-impact items for addition, probabilistic swaps for improvement, and dynamic replacement to maintain feasibility. It selects solutions near the Pareto frontier and applies a three-tiered approach: (1) adding high-impact items not in the solution, (2) swapping items based on marginal improvement, and (3) removing low-impact items if over capacity. The method balances exploration and exploitation using dynamic thresholds and probabilistic operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution near the Pareto frontier (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Tier 1: Targeted additions of high-impact items not in solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Probabilistic swaps based on marginal impact improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement\n                improvement1 = (value1_lst[j] - value1_lst[i]) / (weight_lst[j] - weight_lst[i] + 1e-6)\n                improvement2 = (value2_lst[j] - value2_lst[i]) / (weight_lst[j] - weight_lst[i] + 1e-6)\n                if (improvement1 > 0 or improvement2 > 0) and np.random.rand() < 0.6:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Tier 3: Dynamic replacement if still over capacity\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% combined marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 20)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8755861620428236,
            0.8767749667167664
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution near the Pareto frontier (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Tier 1: Targeted additions of high-impact items not in solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Probabilistic swaps based on marginal impact improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement\n                improvement1 = (value1_lst[j] - value1_lst[i]) / (weight_lst[j] - weight_lst[i] + 1e-6)\n                improvement2 = (value2_lst[j] - value2_lst[i]) / (weight_lst[j] - weight_lst[i] + 1e-6)\n                if (improvement1 > 0 or improvement2 > 0) and np.random.rand() < 0.6:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Tier 3: Dynamic replacement if still over capacity\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% combined marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 20)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 65,
        "algorithm": "The algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-tiered local search: (1) probabilistically adds high-impact items (top 30% marginal impact) while respecting capacity constraints, (2) performs targeted swaps between included and excluded items based on dynamic dominance checks, and (3) removes low-impact items below a 25th percentile threshold to restore feasibility. The marginal impact is calculated as the weighted sum of both objectives divided by weight, ensuring a balanced approach to multi-objective optimization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Tier 1: Probabilistic addition of high-impact items (top 30%)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 3)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.6:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Tier 2: Targeted swaps based on dynamic dominance thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Dynamic dominance check with probabilistic acceptance\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i])) and np.random.rand() < 0.7:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Tier 3: Capacity-constrained replacement with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items below 25th percentile of marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 25)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7787371332626123,
            3.4352008998394012
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Tier 1: Probabilistic addition of high-impact items (top 30%)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 3)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.6:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Tier 2: Targeted swaps based on dynamic dominance thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Dynamic dominance check with probabilistic acceptance\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i])) and np.random.rand() < 0.7:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Tier 3: Capacity-constrained replacement with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items below 25th percentile of marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 25)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 66,
        "algorithm": "The algorithm selects a high-potential solution from the archive (top 25% by combined objective values) and applies a three-stage hybrid local search: first adding high-impact items probabilistically, then performing capacity-constrained swaps between included and excluded items based on improvement potential, and finally removing the least impactful items if capacity is exceeded. The selection prioritizes items with high normalized marginal impact for both objectives, and swaps are weighted by their potential improvement while respecting capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Weighted addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices[:max(1, len(sorted_indices) // 4)]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Weighted probability based on combined impact\n            prob = min(1.0, combined_impact[idx] / combined_impact.max())\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps with capacity check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate improvement potential\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                weight_diff = weight_lst[j] - weight_lst[i]\n\n                # Probabilistic swap based on improvement and capacity\n                if (delta1 > 0 or delta2 > 0) and (weight_diff <= remaining_capacity or weight_diff <= 0):\n                    prob = min(1.0, (delta1 + delta2) / (abs(weight_diff) + 1e-6))\n                    if np.random.rand() < prob:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight += weight_diff\n                        remaining_capacity = capacity - current_weight\n                        break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove item with lowest combined impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8041723306514293,
            2.285117506980896
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Weighted addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices[:max(1, len(sorted_indices) // 4)]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Weighted probability based on combined impact\n            prob = min(1.0, combined_impact[idx] / combined_impact.max())\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps with capacity check\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate improvement potential\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                weight_diff = weight_lst[j] - weight_lst[i]\n\n                # Probabilistic swap based on improvement and capacity\n                if (delta1 > 0 or delta2 > 0) and (weight_diff <= remaining_capacity or weight_diff <= 0):\n                    prob = min(1.0, (delta1 + delta2) / (abs(weight_diff) + 1e-6))\n                    if np.random.rand() < prob:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight += weight_diff\n                        remaining_capacity = capacity - current_weight\n                        break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove item with lowest combined impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 67,
        "algorithm": "The algorithm selects the highest-weighted solution from the archive (using 70% value1 and 30% value2), then performs a two-phase local search: first greedily adding top 30% high-ratio items (weighted 60% value1 and 40% value2), followed by probabilistic swaps of low-ratio included items for high-ratio excluded ones, while ensuring feasibility through capacity-aware removals of worst-performing items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest weighted objective (0.7 value1 + 0.3 value2)\n    archive.sort(key=lambda x: (0.7 * x[1][0] + 0.3 * x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact ratios\n    marginal_ratio1 = value1_lst / (weight_lst + 1e-6)\n    marginal_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = 0.6 * marginal_ratio1 + 0.4 * marginal_ratio2\n    sorted_indices = np.argsort(combined_ratio)[::-1]\n\n    # Phase 1: Greedy additions of top 30% high-ratio items\n    top_items = sorted_indices[:max(1, len(sorted_indices) // 3)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps based on marginal improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            improvement = (combined_ratio[j] - combined_ratio[i]) * (weight_lst[j] - weight_lst[i])\n            if improvement > 0 and np.random.rand() < 0.7:  # 70% chance for beneficial swaps\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n                break\n\n    # Phase 3: Capacity adjustment if needed\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_ratio[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8259863548254308,
            2.4613572359085083
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest weighted objective (0.7 value1 + 0.3 value2)\n    archive.sort(key=lambda x: (0.7 * x[1][0] + 0.3 * x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact ratios\n    marginal_ratio1 = value1_lst / (weight_lst + 1e-6)\n    marginal_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = 0.6 * marginal_ratio1 + 0.4 * marginal_ratio2\n    sorted_indices = np.argsort(combined_ratio)[::-1]\n\n    # Phase 1: Greedy additions of top 30% high-ratio items\n    top_items = sorted_indices[:max(1, len(sorted_indices) // 3)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps based on marginal improvement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            improvement = (combined_ratio[j] - combined_ratio[i]) * (weight_lst[j] - weight_lst[i])\n            if improvement > 0 and np.random.rand() < 0.7:  # 70% chance for beneficial swaps\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n                break\n\n    # Phase 3: Capacity adjustment if needed\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_ratio[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: **Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.**\n- **Advice**: **Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.**\n- **Avoid**: **Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.**\n- **Explanation**: **Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.**\n\n**Step-by-Step Heuristic Design**:\n1. **Select Neighbor**: Intelligently choose a solution from the archive using adaptive marginal impact (e.g., prioritize solutions near the Pareto frontier).\n2. **Hybrid Local Search**: Apply a mix of operations (add, swap, replace) based on dynamic thresholds (e.g., swap items with high marginal impact).\n3. **Feasibility Check**: Ensure all operations respect capacity constraints (e.g., reject swaps that exceed capacity).\n4. **Neighbor Validation**: Accept the neighbor if it improves at least one objective or dominates the current solution.\n\nThis approach ensures high-quality, feasible neighbors while avoiding over-optimization. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 68,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.952293818245308,
            1.7788688838481903
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the concise response:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 69,
        "algorithm": "The algorithm selects a high-potential solution from the archive based on combined marginal impact, then applies three adaptive phases: (1) probabilistically adding high-utility items with dynamic thresholds, (2) performing Pareto-aware swaps between complementary items, and (3) removing low-utility items while considering capacity constraints and trade-off analysis. The selection prioritizes solutions with strong combined marginal value, and the local search emphasizes both objective improvements and capacity feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined marginal impact\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-constrained removal with trade-off analysis\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate trade-off scores for removal\n        tradeoff_scores = []\n        for idx in included:\n            # Score considers both marginal impact and objective trade-offs\n            score = (combined_marginal[idx] *\n                    (1 - np.abs((value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) -\n                              (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))))\n            tradeoff_scores.append((idx, score))\n\n        tradeoff_scores.sort(key=lambda x: x[1])\n        worst_idx = tradeoff_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.899587906233434,
            2.7773032188415527
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined marginal impact\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-constrained removal with trade-off analysis\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate trade-off scores for removal\n        tradeoff_scores = []\n        for idx in included:\n            # Score considers both marginal impact and objective trade-offs\n            score = (combined_marginal[idx] *\n                    (1 - np.abs((value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) -\n                              (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))))\n            tradeoff_scores.append((idx, score))\n\n        tradeoff_scores.sort(key=lambda x: x[1])\n        worst_idx = tradeoff_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the concise response:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 70,
        "algorithm": "The heuristic algorithm prioritizes solutions near the Pareto frontier (top 1/3 of the archive) and employs a three-phase local search: (1) greedily adding high-impact items (top 30% marginal value), (2) adaptively swapping items with complementary marginal gains, and (3) capacity-preserving removals of low-impact items, while dynamically adjusting selection thresholds based on Pareto dominance and marginal value. The algorithm ensures feasibility through probabilistic capacity checks and balances exploration of both objectives by combining marginal impacts.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8969385342832971,
            2.7375846803188324
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the concise response:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 71,
        "algorithm": "The algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase hybrid local search: Phase 1 prioritizes adding high-value items (above 75th percentile), Phase 2 performs marginal-impact-aware swaps between included and excluded items, and Phase 3 dynamically removes low-value items (below 25th percentile) if capacity is exceeded. It balances exploration (via percentile thresholds) and exploitation (via normalized value ratios) while ensuring feasibility through capacity-aware adjustments.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined normalized value ratios\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Dynamic thresholds for marginal impact\n    p75 = np.percentile(combined_norm, 75)\n    p25 = np.percentile(combined_norm, 25)\n\n    # Phase 1: Hybrid operation selection\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Add items with high combined normalized value\n            if combined_norm[idx] > p75:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Marginal impact-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate marginal impact for both objectives\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with marginal impact priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value below 25th percentile\n        candidates = included[combined_norm[included] < p25]\n        if len(candidates) > 0:\n            worst_item = candidates[np.argmin(combined_norm[candidates])]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7750900062133148,
            3.983751654624939
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined normalized value ratios\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Dynamic thresholds for marginal impact\n    p75 = np.percentile(combined_norm, 75)\n    p25 = np.percentile(combined_norm, 25)\n\n    # Phase 1: Hybrid operation selection\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Add items with high combined normalized value\n            if combined_norm[idx] > p75:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Marginal impact-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate marginal impact for both objectives\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with marginal impact priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value below 25th percentile\n        candidates = included[combined_norm[included] < p25]\n        if len(candidates) > 0:\n            worst_item = candidates[np.argmin(combined_norm[candidates])]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 72,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid metric combining both objectives and solution diversity, then applies a four-phase local search: greedy insertion, objective-aware swaps, dynamic rebalancing, and targeted diversification, ensuring feasibility while prioritizing high-value items based on a hybrid value-weight ratio. The hybrid metric (weighted by \u03b1=0.7 for value1) guides all phases, with adaptive thresholds in swaps and diversification steps to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: balance between objective dominance and diversity\n    archive.sort(key=lambda x: (x[1][0] + x[1][1] + 0.1 * np.sum(x[0])), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate hybrid value metrics for both objectives\n    alpha = 0.7  # Weight for value1 in hybrid metric\n    hybrid_metric = alpha * value1_lst + (1 - alpha) * value2_lst\n    value_weight_ratio = hybrid_metric / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(value_weight_ratio)[::-1]\n\n    # Phase 1: Greedy insertion with hybrid metric\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps with adaptive thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate normalized improvement\n                norm_delta1 = (value1_lst[j] - value1_lst[i]) / (value1_lst[i] + 1e-6)\n                norm_delta2 = (value2_lst[j] - value2_lst[i]) / (value2_lst[i] + 1e-6)\n                # Adaptive acceptance criteria\n                if (norm_delta1 > 0.05 and norm_delta2 > 0.05) or (norm_delta1 > 0.15 and norm_delta2 > 0.02) or (norm_delta1 > 0.02 and norm_delta2 > 0.15):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with hybrid metric priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest hybrid metric\n        worst_item = included[np.argmin(hybrid_metric[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Targeted diversification\n    if np.random.rand() < 0.3:  # 30% chance to diversify\n        excluded = np.where(new_solution == 0)[0]\n        if len(excluded) > 0:\n            # Add a random item from the top 20% of hybrid metric\n            top_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n            candidates = [item for item in top_items if item in excluded and weight_lst[item] <= remaining_capacity]\n            if candidates:\n                new_item = np.random.choice(candidates)\n                new_solution[new_item] = 1\n                current_weight += weight_lst[new_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8782096946832624,
            5.395380288362503
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: balance between objective dominance and diversity\n    archive.sort(key=lambda x: (x[1][0] + x[1][1] + 0.1 * np.sum(x[0])), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate hybrid value metrics for both objectives\n    alpha = 0.7  # Weight for value1 in hybrid metric\n    hybrid_metric = alpha * value1_lst + (1 - alpha) * value2_lst\n    value_weight_ratio = hybrid_metric / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(value_weight_ratio)[::-1]\n\n    # Phase 1: Greedy insertion with hybrid metric\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps with adaptive thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate normalized improvement\n                norm_delta1 = (value1_lst[j] - value1_lst[i]) / (value1_lst[i] + 1e-6)\n                norm_delta2 = (value2_lst[j] - value2_lst[i]) / (value2_lst[i] + 1e-6)\n                # Adaptive acceptance criteria\n                if (norm_delta1 > 0.05 and norm_delta2 > 0.05) or (norm_delta1 > 0.15 and norm_delta2 > 0.02) or (norm_delta1 > 0.02 and norm_delta2 > 0.15):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with hybrid metric priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest hybrid metric\n        worst_item = included[np.argmin(hybrid_metric[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Targeted diversification\n    if np.random.rand() < 0.3:  # 30% chance to diversify\n        excluded = np.where(new_solution == 0)[0]\n        if len(excluded) > 0:\n            # Add a random item from the top 20% of hybrid metric\n            top_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n            candidates = [item for item in top_items if item in excluded and weight_lst[item] <= remaining_capacity]\n            if candidates:\n                new_item = np.random.choice(candidates)\n                new_solution[new_item] = 1\n                current_weight += weight_lst[new_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 73,
        "algorithm": "This algorithm selects a high-value solution from the archive and applies a four-phase local search: first greedily adds items prioritizing the underrepresented objective, then swaps items to improve Pareto dominance, next removes low-value items to maintain feasibility, and finally balances values between objectives through targeted swaps. The selection prioritizes items with high combined normalized values (70% objective 1, 30% objective 2), and the balancing phase specifically targets swaps that reduce the difference between objective values.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with adaptive objective weighting\n    archive.sort(key=lambda x: (0.6 * x[1][0] + 0.4 * x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive value-weight ratios\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (0.7 * norm1 + 0.3 * norm2)\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Objective-specific greedy additions\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Decide which objective to prioritize based on current balance\n            if np.sum(value1_lst[new_solution == 1]) < np.sum(value2_lst[new_solution == 1]):\n                if norm1[idx] > 0.9 * np.mean(norm1[new_solution == 1]):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n            else:\n                if norm2[idx] > 0.9 * np.mean(norm2[new_solution == 1]):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-dominance aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check Pareto dominance\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value, considering current objective balance\n        if np.sum(value1_lst[included]) < np.sum(value2_lst[included]):\n            worst_item = included[np.argmin(norm1[included])]\n        else:\n            worst_item = included[np.argmin(norm2[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Value-balancing operator\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if balancing both objectives would help\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] < value2_lst[i] and\n                    np.abs(np.sum(value1_lst[included]) - np.sum(value2_lst[included])) >\n                    np.abs((np.sum(value1_lst[included]) - value1_lst[i] + value1_lst[j]) -\n                           (np.sum(value2_lst[included]) - value2_lst[i] + value2_lst[j]))):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.8876409451061757,
            11.067071855068207
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with adaptive objective weighting\n    archive.sort(key=lambda x: (0.6 * x[1][0] + 0.4 * x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive value-weight ratios\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (0.7 * norm1 + 0.3 * norm2)\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Objective-specific greedy additions\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Decide which objective to prioritize based on current balance\n            if np.sum(value1_lst[new_solution == 1]) < np.sum(value2_lst[new_solution == 1]):\n                if norm1[idx] > 0.9 * np.mean(norm1[new_solution == 1]):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n            else:\n                if norm2[idx] > 0.9 * np.mean(norm2[new_solution == 1]):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-dominance aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check Pareto dominance\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value, considering current objective balance\n        if np.sum(value1_lst[included]) < np.sum(value2_lst[included]):\n            worst_item = included[np.argmin(norm1[included])]\n        else:\n            worst_item = included[np.argmin(norm2[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Value-balancing operator\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if balancing both objectives would help\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] < value2_lst[i] and\n                    np.abs(np.sum(value1_lst[included]) - np.sum(value2_lst[included])) >\n                    np.abs((np.sum(value1_lst[included]) - value1_lst[i] + value1_lst[j]) -\n                           (np.sum(value2_lst[included]) - value2_lst[i] + value2_lst[j]))):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThis algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the concise response:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 74,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (combining both objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item (normalized by weight)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item addition based on marginal impact and adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 30% items with highest marginal impact\n        top_items = sorted_indices[:max(1, len(sorted_indices) // 3)]\n        for idx in top_items:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid local search with Pareto-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n                # Or if one objective improves significantly while the other doesn't worsen too much\n                elif ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= 0.8 * value2_lst[i]) or\n                      (value2_lst[j] > value2_lst[i] and value1_lst[j] >= 0.8 * value1_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items in the bottom 20% of marginal impact\n        bottom_percentile = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= bottom_percentile]\n        if len(worst_items) > 0:\n            worst_item = np.random.choice(worst_items)\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n        else:\n            # If no items in bottom percentile, remove the worst one\n            worst_item = included_items[np.argmin(marginal_impact[included_items])]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Randomized high-impact flips for exploration\n    if len(included_items) > 0 and np.random.rand() < 0.3:  # 30% chance to flip\n        # Select top 15% items to potentially flip\n        top_flip_items = sorted_indices[:max(1, len(sorted_indices) // 6)]\n        flip_candidates = [i for i in top_flip_items if new_solution[i] == 1]\n        if len(flip_candidates) > 0:\n            flip_item = np.random.choice(flip_candidates)\n            new_solution[flip_item] = 0\n            current_weight -= weight_lst[flip_item]\n            # Try to add a high-impact excluded item\n            candidates = np.where((weight_lst <= remaining_capacity) &\n                                 (new_solution == 0) &\n                                 (np.isin(np.arange(len(weight_lst)), top_flip_items)))[0]\n            if len(candidates) > 0:\n                add_item = np.random.choice(candidates)\n                new_solution[add_item] = 1\n                current_weight += weight_lst[add_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8058395893663843,
            1.3271143734455109
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (combining both objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item (normalized by weight)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item addition based on marginal impact and adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 30% items with highest marginal impact\n        top_items = sorted_indices[:max(1, len(sorted_indices) // 3)]\n        for idx in top_items:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid local search with Pareto-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n                # Or if one objective improves significantly while the other doesn't worsen too much\n                elif ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= 0.8 * value2_lst[i]) or\n                      (value2_lst[j] > value2_lst[i] and value1_lst[j] >= 0.8 * value1_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items in the bottom 20% of marginal impact\n        bottom_percentile = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= bottom_percentile]\n        if len(worst_items) > 0:\n            worst_item = np.random.choice(worst_items)\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n        else:\n            # If no items in bottom percentile, remove the worst one\n            worst_item = included_items[np.argmin(marginal_impact[included_items])]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Randomized high-impact flips for exploration\n    if len(included_items) > 0 and np.random.rand() < 0.3:  # 30% chance to flip\n        # Select top 15% items to potentially flip\n        top_flip_items = sorted_indices[:max(1, len(sorted_indices) // 6)]\n        flip_candidates = [i for i in top_flip_items if new_solution[i] == 1]\n        if len(flip_candidates) > 0:\n            flip_item = np.random.choice(flip_candidates)\n            new_solution[flip_item] = 0\n            current_weight -= weight_lst[flip_item]\n            # Try to add a high-impact excluded item\n            candidates = np.where((weight_lst <= remaining_capacity) &\n                                 (new_solution == 0) &\n                                 (np.isin(np.arange(len(weight_lst)), top_flip_items)))[0]\n            if len(candidates) > 0:\n                add_item = np.random.choice(candidates)\n                new_solution[add_item] = 1\n                current_weight += weight_lst[add_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design distilled into 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 75,
        "algorithm": "The algorithm selects a diverse solution from the archive by combining objective values and diversity scores, then performs a three-phase hybrid local search: probabilistic addition of high-impact items, capacity-aware swaps targeting Pareto improvements, and dynamic rebalancing with utility-based removal to maintain feasibility while optimizing both objectives. It prioritizes items with high combined marginal impact for addition and swaps, while removing the least utility items when capacity is exceeded.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps targeting Pareto improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvements\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8712054113377715,
            2.601467788219452
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps targeting Pareto improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvements\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact) and generates a neighbor through four phases: 1) adding high-impact items (top 20%) with 70% probability, 2) performing capacity-aware swaps of high-impact items, 3) dynamically removing low-marginal items if capacity is exceeded, and 4) optionally flipping high-impact items for diversity. It prioritizes marginal impact, ensures feasibility, and balances exploitation/exploration through probabilistic selection. The algorithm is structured to iteratively refine solutions while maintaining diversity and feasibility constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Optional random flip of high-impact items for diversity\n    if np.random.rand() < 0.3:  # 30% chance to perform this phase\n        top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n        if len(top_impact_items) > 0:\n            num_to_flip = min(2, len(top_impact_items))  # Flip up to 2 items\n            flip_indices = np.random.choice(top_impact_items, size=num_to_flip, replace=False)\n            for idx in flip_indices:\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with strong individual impacts (alternating between objectives), performs adaptive swaps to improve single-objective values, and rebalances by removing low-impact items while ensuring feasibility through capacity checks. It uses a weighted objective score to guide selection and employs a phased approach (selection, swapping, rebalancing) to explore the solution space efficiently. The method balances exploration and exploitation by focusing on impactful items and adaptively adjusting the solution.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design distilled into 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 76,
        "algorithm": "The algorithm selects a promising solution from the archive (top 20% by a weighted objective score) and applies a three-phase local search: (1) adds high-impact items (top 20% by Pareto-aware scores) while ensuring feasibility, (2) performs targeted swaps between included and excluded items to improve Pareto dominance, and (3) removes low-impact items (below the 30th percentile) to rebalance the solution dynamically. It prioritizes items with higher combined impact on both objectives, uses probabilistic selection for diversity, and maintains feasibility through adaptive capacity checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined weighted score)\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate Pareto-aware impact scores\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    pareto_score = (impact1 + impact2) * (1 + np.random.rand(len(impact1)) * 0.2)  # Add noise for diversity\n    sorted_indices = np.argsort(pareto_score)[::-1]\n\n    # Phase 1: Selective addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add top 20% of items with highest Pareto score\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value2_lst[j] > value2_lst[i] and value1_lst[j] >= value1_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items below 30th percentile of Pareto score\n        remove_threshold = np.percentile(pareto_score[included_items], 30)\n        worst_items = included_items[pareto_score[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8917724935989965,
            3.143312633037567
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined weighted score)\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate Pareto-aware impact scores\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    pareto_score = (impact1 + impact2) * (1 + np.random.rand(len(impact1)) * 0.2)  # Add noise for diversity\n    sorted_indices = np.argsort(pareto_score)[::-1]\n\n    # Phase 1: Selective addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Add top 20% of items with highest Pareto score\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value2_lst[j] > value2_lst[i] and value1_lst[j] >= value1_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items below 30th percentile of Pareto score\n        remove_threshold = np.percentile(pareto_score[included_items], 30)\n        worst_items = included_items[pareto_score[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a dynamic subset of high-impact items (top 25% by marginal contribution to either objective), using a hybrid local search that includes add, swap, and replace operations, prioritizing items that improve both objectives while ensuring feasibility. It dynamically adjusts the number of flips (1-5) and includes occasional swaps to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, max(1, len(top_items) // 2))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n        if np.random.rand() < 0.3:\n            swap_indices = np.random.choice(top_items, size=min(2, len(top_items)), replace=False)\n            for idx in swap_indices:\n                if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n                elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with strong individual impacts (alternating between objectives), performs adaptive swaps to improve single-objective values, and rebalances by removing low-impact items while ensuring feasibility through capacity checks. It uses a weighted objective score to guide selection and employs a phased approach (selection, swapping, rebalancing) to explore the solution space efficiently. The method balances exploration and exploitation by focusing on impactful items and adaptively adjusting the solution.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design distilled into 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 77,
        "algorithm": "The algorithm combines adaptive marginal impact analysis with a tiered selection mechanism to generate high-quality neighbors by prioritizing high-impact items (75th percentile and above) for addition, followed by strategic swaps with mid-tier items (50th-75th percentile) and probabilistic adjustments to maintain feasibility. It alternates between objective-specific and combined impact assessments while ensuring capacity constraints through rigorous feasibility checks. The approach balances exploration (via probabilistic operations) and exploitation (via tiered prioritization) to navigate the solution space efficiently.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement using a weighted score\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined impact for Pareto-efficient selection\n    combined_impact = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Tiered selection mechanism\n    tier1_threshold = np.percentile(combined_impact, 75)\n    tier2_threshold = np.percentile(combined_impact, 50)\n    tier1_items = np.where(combined_impact >= tier1_threshold)[0]\n    tier2_items = np.where((combined_impact >= tier2_threshold) & (combined_impact < tier1_threshold))[0]\n\n    # Phase 1: Add high-impact items from tier 1\n    remaining_capacity = capacity - current_weight\n    for idx in tier1_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Swap with tier 2 items if beneficial\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        for j in tier2_items:\n            if new_solution[j] == 0 and weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] * 0.9 and value2_lst[j] > value2_lst[i] * 0.9) or \\\n                   (np.random.rand() < 0.4):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Remove low-impact items if over capacity\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic add/remove based on marginal impact\n    for idx in np.random.permutation(len(weight_lst)):\n        if np.random.rand() < 0.2:\n            if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n            elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity and combined_impact[idx] >= tier2_threshold:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.268237123764745,
            2.3759185671806335
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement using a weighted score\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined impact for Pareto-efficient selection\n    combined_impact = (value1_lst * 0.7 + value2_lst * 0.3) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Tiered selection mechanism\n    tier1_threshold = np.percentile(combined_impact, 75)\n    tier2_threshold = np.percentile(combined_impact, 50)\n    tier1_items = np.where(combined_impact >= tier1_threshold)[0]\n    tier2_items = np.where((combined_impact >= tier2_threshold) & (combined_impact < tier1_threshold))[0]\n\n    # Phase 1: Add high-impact items from tier 1\n    remaining_capacity = capacity - current_weight\n    for idx in tier1_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Swap with tier 2 items if beneficial\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        for j in tier2_items:\n            if new_solution[j] == 0 and weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] * 0.9 and value2_lst[j] > value2_lst[i] * 0.9) or \\\n                   (np.random.rand() < 0.4):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Remove low-impact items if over capacity\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic add/remove based on marginal impact\n    for idx in np.random.permutation(len(weight_lst)):\n        if np.random.rand() < 0.2:\n            if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n            elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity and combined_impact[idx] >= tier2_threshold:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a high-potential solution from the archive, prioritizes items with strong individual impacts (alternating between objectives), performs adaptive swaps to improve single-objective values, and rebalances by removing low-impact items while ensuring feasibility through capacity checks. It uses a weighted objective score to guide selection and employs a phased approach (selection, swapping, rebalancing) to explore the solution space efficiently. The method balances exploration and exploitation by focusing on impactful items and adaptively adjusting the solution.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 78,
        "algorithm": "The algorithm selects a high-potential solution from the archive using a dynamic objective weighting scheme, then applies a three-phase local search: first adding high-priority items, then performing targeted swaps between included and excluded items, and finally removing low-impact items to ensure feasibility while maximizing both objectives. The dynamic weighting prioritizes objectives based on their relative contributions, and the hybrid replacement strategy evaluates swaps based on both individual and combined impacts, with adaptive capacity checks to maintain feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective score with dynamic weighting\n    total_value1 = sum(x[1][0] for x in archive)\n    total_value2 = sum(x[1][1] for x in archive)\n    weight1 = total_value2 / (total_value1 + total_value2) if total_value1 + total_value2 > 0 else 0.5\n    weight2 = 1 - weight1\n    archive.sort(key=lambda x: (x[1][0] * weight1 + x[1][1] * weight2), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic item priorities\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    priority_scores = (value1_lst * weight1 + value2_lst * weight2) * (1 + np.log(value_ratio + 1))\n    sorted_indices = np.argsort(priority_scores)[::-1]\n\n    # Phase 1: Dynamic item addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid item replacement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                improvement_score = delta1 * weight1 + delta2 * weight2\n\n                if improvement_score > 0 and np.random.rand() < 0.5:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive capacity adjustment\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest weighted impact\n        weighted_impact = (value1_lst * weight1 + value2_lst * weight2) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(weighted_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8890229948295996,
            2.0007921159267426
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective score with dynamic weighting\n    total_value1 = sum(x[1][0] for x in archive)\n    total_value2 = sum(x[1][1] for x in archive)\n    weight1 = total_value2 / (total_value1 + total_value2) if total_value1 + total_value2 > 0 else 0.5\n    weight2 = 1 - weight1\n    archive.sort(key=lambda x: (x[1][0] * weight1 + x[1][1] * weight2), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic item priorities\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    priority_scores = (value1_lst * weight1 + value2_lst * weight2) * (1 + np.log(value_ratio + 1))\n    sorted_indices = np.argsort(priority_scores)[::-1]\n\n    # Phase 1: Dynamic item addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid item replacement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                improvement_score = delta1 * weight1 + delta2 * weight2\n\n                if improvement_score > 0 and np.random.rand() < 0.5:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive capacity adjustment\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest weighted impact\n        weighted_impact = (value1_lst * weight1 + value2_lst * weight2) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(weighted_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a high-potential solution from the archive, prioritizes items with strong individual impacts (alternating between objectives), performs adaptive swaps to improve single-objective values, and rebalances by removing low-impact items while ensuring feasibility through capacity checks. It uses a weighted objective score to guide selection and employs a phased approach (selection, swapping, rebalancing) to explore the solution space efficiently. The method balances exploration and exploitation by focusing on impactful items and adaptively adjusting the solution.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 79,
        "algorithm": "This algorithm dynamically selects a solution from the archive based on adaptive objective weighting that prioritizes objectives with higher diversity in the archive, then applies a hybrid local search combining ratio-based item addition and capacity-aware diversification with selective item removal and replacement. The method ensures feasibility by removing worst-performing items when necessary, with a focus on maintaining high combined value ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic objective weighting based on archive diversity\n    obj1_vals = np.array([s[1][0] for s in archive])\n    obj2_vals = np.array([s[1][1] for s in archive])\n    obj1_std = np.std(obj1_vals)\n    obj2_std = np.std(obj2_vals)\n    weight1 = obj1_std / (obj1_std + obj2_std + 1e-6)\n    weight2 = obj2_std / (obj1_std + obj2_std + 1e-6)\n\n    # Select solution with adaptive objective score\n    archive.sort(key=lambda x: (x[1][0] * weight1 + x[1][1] * weight2), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive value ratios\n    value_ratio1 = value1_lst / (weight_lst + 1e-6)\n    value_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = (value_ratio1 + value_ratio2) / 2\n\n    # Hybrid local search\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    # Phase 1: Ratio-based item selection with diversification\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_ratio)[::-1]\n        for i in range(min(5, len(sorted_indices))):\n            idx = sorted_indices[i]\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Capacity-aware diversification\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Select items for potential removal based on low marginal impact\n        marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        sorted_included = included_items[np.argsort(marginal_impact[included_items])]\n\n        for i in range(min(3, len(sorted_included))):\n            item_to_remove = sorted_included[i]\n            # Find suitable replacement\n            potential_replacements = [j for j in excluded_items if weight_lst[j] <= remaining_capacity + weight_lst[item_to_remove]]\n            if potential_replacements:\n                replacement = potential_replacements[np.argmax(combined_ratio[potential_replacements])]\n                if combined_ratio[replacement] > marginal_impact[item_to_remove] * 1.2:\n                    new_solution[item_to_remove] = 0\n                    new_solution[replacement] = 1\n                    current_weight = current_weight - weight_lst[item_to_remove] + weight_lst[replacement]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Ensure feasibility\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_ratio[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.3632047906177738,
            9.979150414466858
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic objective weighting based on archive diversity\n    obj1_vals = np.array([s[1][0] for s in archive])\n    obj2_vals = np.array([s[1][1] for s in archive])\n    obj1_std = np.std(obj1_vals)\n    obj2_std = np.std(obj2_vals)\n    weight1 = obj1_std / (obj1_std + obj2_std + 1e-6)\n    weight2 = obj2_std / (obj1_std + obj2_std + 1e-6)\n\n    # Select solution with adaptive objective score\n    archive.sort(key=lambda x: (x[1][0] * weight1 + x[1][1] * weight2), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive value ratios\n    value_ratio1 = value1_lst / (weight_lst + 1e-6)\n    value_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = (value_ratio1 + value_ratio2) / 2\n\n    # Hybrid local search\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    # Phase 1: Ratio-based item selection with diversification\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_ratio)[::-1]\n        for i in range(min(5, len(sorted_indices))):\n            idx = sorted_indices[i]\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Capacity-aware diversification\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Select items for potential removal based on low marginal impact\n        marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        sorted_included = included_items[np.argsort(marginal_impact[included_items])]\n\n        for i in range(min(3, len(sorted_included))):\n            item_to_remove = sorted_included[i]\n            # Find suitable replacement\n            potential_replacements = [j for j in excluded_items if weight_lst[j] <= remaining_capacity + weight_lst[item_to_remove]]\n            if potential_replacements:\n                replacement = potential_replacements[np.argmax(combined_ratio[potential_replacements])]\n                if combined_ratio[replacement] > marginal_impact[item_to_remove] * 1.2:\n                    new_solution[item_to_remove] = 0\n                    new_solution[replacement] = 1\n                    current_weight = current_weight - weight_lst[item_to_remove] + weight_lst[replacement]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Ensure feasibility\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_ratio[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with strong individual impacts (alternating between objectives), performs adaptive swaps to improve single-objective values, and rebalances by removing low-impact items while ensuring feasibility through capacity checks. It uses a weighted objective score to guide selection and employs a phased approach (selection, swapping, rebalancing) to explore the solution space efficiently. The method balances exploration and exploitation by focusing on impactful items and adaptively adjusting the solution.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] * 0.7 + x[1][1] * 0.3), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate individual objective impacts\n    impact1 = value1_lst / (weight_lst + 1e-6)\n    impact2 = value2_lst / (weight_lst + 1e-6)\n    sorted_indices1 = np.argsort(impact1)[::-1]\n    sorted_indices2 = np.argsort(impact2)[::-1]\n\n    # Phase 1: Objective-specific item selection\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Alternate between objectives\n        for i in range(len(sorted_indices1)):\n            if i % 2 == 0:\n                idx = sorted_indices1[i]\n            else:\n                idx = sorted_indices2[i % len(sorted_indices2)]\n\n            if idx in candidates and np.random.rand() < 0.5:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive single-objective swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for single-objective improvement\n                if (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.7) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact\n        combined_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design distilled into 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 80,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution near the Pareto frontier using a dynamic threshold\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Dynamic item selection based on marginal impact (top 20%)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    top_items = np.argsort(combined_impact)[-max(1, len(combined_impact) // 5):]\n\n    if len(candidates) > 0:\n        for idx in top_items:\n            if idx in candidates and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid local search (add/swap/replace) with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Prioritize items that improve both objectives\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.4) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.4):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact (bottom 30%)\n        worst_items = np.argsort(combined_impact[included_items])[:max(1, len(included_items) // 3)]\n        worst_item = included_items[worst_items[0]]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7209892243229155,
            1.0090019404888153
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution near the Pareto frontier using a dynamic threshold\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Dynamic item selection based on marginal impact (top 20%)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    top_items = np.argsort(combined_impact)[-max(1, len(combined_impact) // 5):]\n\n    if len(candidates) > 0:\n        for idx in top_items:\n            if idx in candidates and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Hybrid local search (add/swap/replace) with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Prioritize items that improve both objectives\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and np.random.rand() < 0.4) or \\\n                   (value2_lst[j] > value2_lst[i] and np.random.rand() < 0.4):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-adaptive rebalancing with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined impact (bottom 30%)\n        worst_items = np.argsort(combined_impact[included_items])[:max(1, len(included_items) // 3)]\n        worst_item = included_items[worst_items[0]]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the distilled response in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 81,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution near the Pareto frontier using dominance-aware selection\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Add high-impact items with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 90)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Hybrid swaps for complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with weighted utility removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8861905599613658,
            1.350620448589325
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution near the Pareto frontier using dominance-aware selection\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Add high-impact items with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 90)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Hybrid swaps for complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with weighted utility removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the distilled response in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 82,
        "algorithm": "The algorithm selects a promising solution from the archive based on normalized marginal impact scores for both objectives, then applies a three-tiered local search: first probabilistically adding high-impact items, then making targeted swaps between complementary items, and finally replacing low-impact items with higher-value candidates, while maintaining feasibility through continuous capacity checks and a final adjustment step. The selection prioritizes solutions with the highest combined marginal improvement potential, and the search tiers progressively refine the solution with decreasing probability thresholds to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9211463958209968,
            1.9493713676929474
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the distilled response in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 83,
        "algorithm": "The algorithm implements a three-phase hybrid local search that prioritizes solutions near the Pareto frontier, using combined marginal impact analysis to probabilistically add high-value items, perform targeted swaps between items, and rebalance solutions by replacing low-impact items with higher-impact candidates while maintaining feasibility. It emphasizes solutions with lower dominance scores (top 30%) and dynamically adjusts selection probabilities (50%, 40%, and 20%) to balance exploration and exploitation across objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.5:  # 50% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.4:  # 40% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.872891424234374,
            0.9477324485778809
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.5:  # 50% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.4:  # 40% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 84,
        "algorithm": "This algorithm employs a multi-phase adaptive local search that prioritizes solutions with high Pareto dominance and diversity, using a novel \"impact divergence\" strategy to guide item additions, swaps, and replacements while dynamically balancing objective weights. It intelligently selects promising solutions from the archive and applies a three-phase heuristic that first adds items with complementary objective impacts, then performs weighted swaps, and finally rebalances the solution while maintaining feasibility. The approach balances exploration and exploitation through probabilistic acceptance criteria and adaptive weighting schemes.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions with high Pareto dominance and diversity\n    dominance_scores = []\n    crowding_distances = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n\n        # Calculate crowding distance\n        sorted_obj1 = sorted(archive, key=lambda x: x[1][0])\n        sorted_obj2 = sorted(archive, key=lambda x: x[1][1])\n        idx1 = next(j for j, (s, _) in enumerate(sorted_obj1) if np.array_equal(s, sol))\n        idx2 = next(j for j, (s, _) in enumerate(sorted_obj2) if np.array_equal(s, sol))\n\n        dist1 = (sorted_obj1[(idx1+1)%len(archive)][1][0] - sorted_obj1[(idx1-1)%len(archive)][1][0]) if len(archive) > 1 else float('inf')\n        dist2 = (sorted_obj2[(idx2+1)%len(archive)][1][1] - sorted_obj2[(idx2-1)%len(archive)][1][1]) if len(archive) > 1 else float('inf')\n        crowding_distances.append(dist1 + dist2)\n\n    # Normalize scores\n    norm_dominance = np.array(dominance_scores) / max(dominance_scores) if max(dominance_scores) > 0 else np.zeros_like(dominance_scores)\n    norm_crowding = np.array(crowding_distances) / max(crowding_distances) if max(crowding_distances) > 0 else np.zeros_like(crowding_distances)\n\n    # Combine scores with adaptive weighting\n    alpha = 0.6 if np.random.rand() < 0.7 else 0.4  # Higher dominance weight 70% of the time\n    combined_scores = alpha * norm_dominance + (1 - alpha) * norm_crowding\n\n    # Select top 30% of solutions\n    threshold = np.percentile(combined_scores, 70)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate impact divergence metric\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    impact_divergence = np.zeros(len(weight_lst))\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            impact_divergence[i] = -np.abs(value1_lst[i] - value2_lst[i])  # Penalize items with similar impact on both objectives\n        else:\n            impact_divergence[i] = np.abs(value1_lst[i] - value2_lst[i])  # Prefer items with complementary impact\n\n    # Phase 1: Add items with high impact divergence\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(impact_divergence)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Dynamic objective weighting for swaps\n    obj_weight1 = 0.6 if np.random.rand() < 0.7 else 0.4\n    obj_weight2 = 1 - obj_weight1\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                weighted_delta = obj_weight1 * delta1 + obj_weight2 * delta2\n\n                if weighted_delta > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance using impact divergence\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (impact_divergence > impact_divergence[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(impact_divergence[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove item with lowest combined value\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8630374169179738,
            4.742278009653091
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions with high Pareto dominance and diversity\n    dominance_scores = []\n    crowding_distances = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n\n        # Calculate crowding distance\n        sorted_obj1 = sorted(archive, key=lambda x: x[1][0])\n        sorted_obj2 = sorted(archive, key=lambda x: x[1][1])\n        idx1 = next(j for j, (s, _) in enumerate(sorted_obj1) if np.array_equal(s, sol))\n        idx2 = next(j for j, (s, _) in enumerate(sorted_obj2) if np.array_equal(s, sol))\n\n        dist1 = (sorted_obj1[(idx1+1)%len(archive)][1][0] - sorted_obj1[(idx1-1)%len(archive)][1][0]) if len(archive) > 1 else float('inf')\n        dist2 = (sorted_obj2[(idx2+1)%len(archive)][1][1] - sorted_obj2[(idx2-1)%len(archive)][1][1]) if len(archive) > 1 else float('inf')\n        crowding_distances.append(dist1 + dist2)\n\n    # Normalize scores\n    norm_dominance = np.array(dominance_scores) / max(dominance_scores) if max(dominance_scores) > 0 else np.zeros_like(dominance_scores)\n    norm_crowding = np.array(crowding_distances) / max(crowding_distances) if max(crowding_distances) > 0 else np.zeros_like(crowding_distances)\n\n    # Combine scores with adaptive weighting\n    alpha = 0.6 if np.random.rand() < 0.7 else 0.4  # Higher dominance weight 70% of the time\n    combined_scores = alpha * norm_dominance + (1 - alpha) * norm_crowding\n\n    # Select top 30% of solutions\n    threshold = np.percentile(combined_scores, 70)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate impact divergence metric\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    impact_divergence = np.zeros(len(weight_lst))\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            impact_divergence[i] = -np.abs(value1_lst[i] - value2_lst[i])  # Penalize items with similar impact on both objectives\n        else:\n            impact_divergence[i] = np.abs(value1_lst[i] - value2_lst[i])  # Prefer items with complementary impact\n\n    # Phase 1: Add items with high impact divergence\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(impact_divergence)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Dynamic objective weighting for swaps\n    obj_weight1 = 0.6 if np.random.rand() < 0.7 else 0.4\n    obj_weight2 = 1 - obj_weight1\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                weighted_delta = obj_weight1 * delta1 + obj_weight2 * delta2\n\n                if weighted_delta > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance using impact divergence\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (impact_divergence > impact_divergence[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(impact_divergence[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove item with lowest combined value\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 85,
        "algorithm": "The algorithm employs a four-phase local search strategy on Pareto-aware solutions, prioritizing items with balanced marginal impact (60% value1, 40% value2) while dynamically adjusting selections through probabilistic additions, objective-balanced swaps, adaptive rebalancing, and frontier-pushing operations to explore underutilized regions of the Pareto frontier while maintaining feasibility through continuous weight-constrained adjustments. The solution selection focuses on less-dominated solutions (bottom 30%) to encourage exploration, with final feasibility enforced by minimally impactful item removals.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions with high potential for improvement (bottom 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 70)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    norm_marginal1 = marginal1 / np.max(np.abs(marginal1))\n    norm_marginal2 = marginal2 / np.max(np.abs(marginal2))\n    combined_marginal = 0.6 * norm_marginal1 + 0.4 * norm_marginal2\n\n    # Phase 1: Probabilistic addition with objective-aware thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates:\n                prob = min(0.8, 0.4 + 0.4 * combined_marginal[idx])\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Objective-balanced swaps with dynamic thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                if (delta1 > 0 and delta2 > 0) or (delta1 + delta2 > 0 and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive rebalancing with frontier-aware replacements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.4:\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Phase 4: Frontier-pushing phase (add/remove items near Pareto frontier)\n    if np.random.rand() < 0.3:\n        frontier_items = np.where((value1_lst > np.percentile(value1_lst, 75)) &\n                                 (value2_lst > np.percentile(value2_lst, 75)))[0]\n        for idx in frontier_items:\n            if new_solution[idx] == 0 and weight_lst[idx] <= capacity - current_weight:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1 and np.random.rand() < 0.2:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Final feasibility check with minimal impact removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8211966184794326,
            1.5129778981208801
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions with high potential for improvement (bottom 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 70)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    norm_marginal1 = marginal1 / np.max(np.abs(marginal1))\n    norm_marginal2 = marginal2 / np.max(np.abs(marginal2))\n    combined_marginal = 0.6 * norm_marginal1 + 0.4 * norm_marginal2\n\n    # Phase 1: Probabilistic addition with objective-aware thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates:\n                prob = min(0.8, 0.4 + 0.4 * combined_marginal[idx])\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Objective-balanced swaps with dynamic thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                if (delta1 > 0 and delta2 > 0) or (delta1 + delta2 > 0 and np.random.rand() < 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Adaptive rebalancing with frontier-aware replacements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.4:\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Phase 4: Frontier-pushing phase (add/remove items near Pareto frontier)\n    if np.random.rand() < 0.3:\n        frontier_items = np.where((value1_lst > np.percentile(value1_lst, 75)) &\n                                 (value2_lst > np.percentile(value2_lst, 75)))[0]\n        for idx in frontier_items:\n            if new_solution[idx] == 0 and weight_lst[idx] <= capacity - current_weight:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n            elif new_solution[idx] == 1 and np.random.rand() < 0.2:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Final feasibility check with minimal impact removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the distilled response in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 86,
        "algorithm": "The algorithm selects promising solutions from the archive (top 20% by dominance) and applies a three-phase hybrid local search: Phase 1 adds high-impact items (80% chance) with combined marginal impact, Phase 2 performs targeted swaps between included and excluded items based on marginal impact differences, and Phase 3 replaces low-impact items (60% chance) with high-impact candidates while ensuring feasibility through continuous weight checks. The method prioritizes items that improve both objectives and balances exploration with exploitation through adaptive thresholds.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.8:  # 80% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.6:  # 60% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8459707451701707,
            0.9693370163440704
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.8:  # 80% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.6:  # 60% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 87,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest Pareto frontier coverage\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) * (1 + 0.5 * np.random.rand(len(marginal1)))\n\n    # Phase 1: Dynamic threshold-based addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 90)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            if np.random.rand() < 0.8 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Hybrid swap operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Pareto-improving swap\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value2_lst[j] > value2_lst[i] and value1_lst[j] >= value1_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Remove least Pareto-contributing items\n        pareto_scores = []\n        for idx in included:\n            score = (value1_lst[idx] + value2_lst[idx]) / (weight_lst[idx] + 1e-6)\n            pareto_scores.append((idx, score))\n\n        pareto_scores.sort(key=lambda x: x[1])\n        worst_idx = pareto_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8570461300033256,
            3.2488121688365936
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest Pareto frontier coverage\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) * (1 + 0.5 * np.random.rand(len(marginal1)))\n\n    # Phase 1: Dynamic threshold-based addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 90)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            if np.random.rand() < 0.8 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 2: Hybrid swap operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Pareto-improving swap\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value2_lst[j] > value2_lst[i] and value1_lst[j] >= value1_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity-aware removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Remove least Pareto-contributing items\n        pareto_scores = []\n        for idx in included:\n            score = (value1_lst[idx] + value2_lst[idx]) / (weight_lst[idx] + 1e-6)\n            pareto_scores.append((idx, score))\n\n        pareto_scores.sort(key=lambda x: x[1])\n        worst_idx = pareto_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects high-performing solutions from the top 50% of the archive (based on combined objective scores) and applies a hybrid local search strategy: it first probabilistically inserts/removes items to explore the solution space, then performs targeted swaps between items with complementary marginal impacts (positive for one objective, negative for the other) to balance trade-offs, while always ensuring feasibility through dynamic weight adjustments and candidate filtering. Marginal contributions for both objectives guide the search, with items contributing negatively to either objective being prioritized for removal, and swaps are only performed if they improve at least one objective.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 88,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution from the top 20% of the archive based on combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 80)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    total_value1 = np.sum(value1_lst[base_solution == 1])\n    total_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Dynamic threshold for item selection based on percentile\n    value1_threshold = np.percentile(value1_lst, 75)\n    value2_threshold = np.percentile(value2_lst, 75)\n\n    # Phase 1: Adaptive item replacement based on marginal impact and dynamic thresholds\n    for i in range(len(weight_lst)):\n        if base_solution[i] == 1 and marginal1[i] < 0 and marginal2[i] < 0:\n            # Replace low-impact items with high-value items\n            candidates = np.where((base_solution == 0) &\n                                ((value1_lst >= value1_threshold) | (value2_lst >= value2_threshold)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Phase 2: Probabilistic addition of high-value items\n    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n    candidates = np.where((base_solution == 0) &\n                         ((value1_lst >= value1_threshold) | (value2_lst >= value2_threshold)) &\n                         (weight_lst <= remaining_capacity))[0]\n\n    if len(candidates) > 0:\n        for idx in np.random.permutation(candidates):\n            if np.random.rand() < 0.6 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Feasibility adjustment with Pareto-aware removal\n    while current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n        current_weight -= weight_lst[i]\n\n    return new_solution\n\n",
        "score": [
            -0.4326257697724507,
            2.5002341270446777
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution from the top 20% of the archive based on combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 80)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    total_value1 = np.sum(value1_lst[base_solution == 1])\n    total_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Dynamic threshold for item selection based on percentile\n    value1_threshold = np.percentile(value1_lst, 75)\n    value2_threshold = np.percentile(value2_lst, 75)\n\n    # Phase 1: Adaptive item replacement based on marginal impact and dynamic thresholds\n    for i in range(len(weight_lst)):\n        if base_solution[i] == 1 and marginal1[i] < 0 and marginal2[i] < 0:\n            # Replace low-impact items with high-value items\n            candidates = np.where((base_solution == 0) &\n                                ((value1_lst >= value1_threshold) | (value2_lst >= value2_threshold)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Phase 2: Probabilistic addition of high-value items\n    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n    candidates = np.where((base_solution == 0) &\n                         ((value1_lst >= value1_threshold) | (value2_lst >= value2_threshold)) &\n                         (weight_lst <= remaining_capacity))[0]\n\n    if len(candidates) > 0:\n        for idx in np.random.permutation(candidates):\n            if np.random.rand() < 0.6 and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Feasibility adjustment with Pareto-aware removal\n    while current_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n        current_weight -= weight_lst[i]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a dynamic subset of high-impact items (top 25% by marginal contribution to either objective), using a hybrid local search that includes add, swap, and replace operations, prioritizing items that improve both objectives while ensuring feasibility. It dynamically adjusts the number of flips (1-5) and includes occasional swaps to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, max(1, len(top_items) // 2))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n        if np.random.rand() < 0.3:\n            swap_indices = np.random.choice(top_items, size=min(2, len(top_items)), replace=False)\n            for idx in swap_indices:\n                if new_solution[idx] == 1 and total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n                elif new_solution[idx] == 0 and total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects high-performing solutions from the top 50% of the archive (based on combined objective scores) and applies a hybrid local search strategy: it first probabilistically inserts/removes items to explore the solution space, then performs targeted swaps between items with complementary marginal impacts (positive for one objective, negative for the other) to balance trade-offs, while always ensuring feasibility through dynamic weight adjustments and candidate filtering. Marginal contributions for both objectives guide the search, with items contributing negatively to either objective being prioritized for removal, and swaps are only performed if they improve at least one objective.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 89,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution based on Pareto frontier proximity and combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    pareto_frontier = []\n    for i, (solution, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_frontier.append(i)\n\n    if pareto_frontier:\n        selected_idx = np.random.choice(pareto_frontier)\n    else:\n        threshold = np.percentile(combined_scores, 75)\n        candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n        selected_idx = np.random.choice(candidate_indices) if candidate_indices else np.random.randint(0, len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive thresholds for item selection\n    threshold1 = np.percentile(value1_lst, 75)\n    threshold2 = np.percentile(value2_lst, 75)\n\n    # Hybrid local search with probabilistic operations\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted item replacement based on complementary objectives\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((value1_lst[i] < threshold1) | (value2_lst[i] < threshold2)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined value\n        combined_value = value1_lst[excess_items] + value2_lst[excess_items]\n        i = excess_items[np.argmin(combined_value)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.45235119509574456,
            1.5051530301570892
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution based on Pareto frontier proximity and combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    pareto_frontier = []\n    for i, (solution, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_frontier.append(i)\n\n    if pareto_frontier:\n        selected_idx = np.random.choice(pareto_frontier)\n    else:\n        threshold = np.percentile(combined_scores, 75)\n        candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n        selected_idx = np.random.choice(candidate_indices) if candidate_indices else np.random.randint(0, len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive thresholds for item selection\n    threshold1 = np.percentile(value1_lst, 75)\n    threshold2 = np.percentile(value2_lst, 75)\n\n    # Hybrid local search with probabilistic operations\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted item replacement based on complementary objectives\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((value1_lst[i] < threshold1) | (value2_lst[i] < threshold2)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                    new_solution[i] = 0\n                    new_solution[j] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined value\n        combined_value = value1_lst[excess_items] + value2_lst[excess_items]\n        i = excess_items[np.argmin(combined_value)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects high-performing solutions from the top 50% of the archive (based on combined objective scores) and applies a hybrid local search strategy: it first probabilistically inserts/removes items to explore the solution space, then performs targeted swaps between items with complementary marginal impacts (positive for one objective, negative for the other) to balance trade-offs, while always ensuring feasibility through dynamic weight adjustments and candidate filtering. Marginal contributions for both objectives guide the search, with items contributing negatively to either objective being prioritized for removal, and swaps are only performed if they improve at least one objective.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 90,
        "algorithm": "The algorithm selects top-performing solutions from the archive (top 25% by Pareto dominance and normalized scores) and applies a hybrid local search strategy: it first reallocates items based on dynamic marginal utility ratios (prioritizing high-ratio items for removal and low-ratio items for addition) and then performs complementary swaps between items with high trade-off potential, while ensuring feasibility through adaptive capacity adjustments and probabilistic acceptance of beneficial moves. The method balances exploration (random selection and probabilistic operations) with exploitation (utility-based prioritization and trade-off analysis), aiming to improve solution quality across both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 25% by Pareto dominance and normalized scores\n    objectives = np.array([obj for (_, obj) in archive])\n    normalized = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n    scores = np.sum(normalized, axis=1)\n    threshold = np.percentile(scores, 75)\n    candidate_indices = [i for i, score in enumerate(scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal utility ratios\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    utility_ratio = np.abs(marginal1) / (np.abs(marginal2) + 1e-8)\n\n    # Dynamic item reallocation based on utility ratios\n    for i in np.argsort(utility_ratio)[-min(5, len(utility_ratio)):]:\n        if np.random.random() < 0.6:  # 60% chance to reallocate\n            if base_solution[i] == 1 and np.random.random() < 0.7:  # 70% chance to remove high-ratio items\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.3:  # 30% chance to add low-ratio items\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Complementary swaps between items with high trade-off potential\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.5:  # 50% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                # Accept swap if it improves at least one objective or is beneficial in trade-off\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0) or \\\n                   (np.random.random() < 0.3 * (1 - utility_ratio[i] * utility_ratio[j])):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.31457213119376576,
            4.7606721222400665
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 25% by Pareto dominance and normalized scores\n    objectives = np.array([obj for (_, obj) in archive])\n    normalized = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n    scores = np.sum(normalized, axis=1)\n    threshold = np.percentile(scores, 75)\n    candidate_indices = [i for i, score in enumerate(scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal utility ratios\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    utility_ratio = np.abs(marginal1) / (np.abs(marginal2) + 1e-8)\n\n    # Dynamic item reallocation based on utility ratios\n    for i in np.argsort(utility_ratio)[-min(5, len(utility_ratio)):]:\n        if np.random.random() < 0.6:  # 60% chance to reallocate\n            if base_solution[i] == 1 and np.random.random() < 0.7:  # 70% chance to remove high-ratio items\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.3:  # 30% chance to add low-ratio items\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Complementary swaps between items with high trade-off potential\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.5:  # 50% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                # Accept swap if it improves at least one objective or is beneficial in trade-off\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0) or \\\n                   (np.random.random() < 0.3 * (1 - utility_ratio[i] * utility_ratio[j])):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects high-performing solutions from the top 50% of the archive (based on combined objective scores) and applies a hybrid local search strategy: it first probabilistically inserts/removes items to explore the solution space, then performs targeted swaps between items with complementary marginal impacts (positive for one objective, negative for the other) to balance trade-offs, while always ensuring feasibility through dynamic weight adjustments and candidate filtering. Marginal contributions for both objectives guide the search, with items contributing negatively to either objective being prioritized for removal, and swaps are only performed if they improve at least one objective.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 91,
        "algorithm": "The algorithm selects promising solutions from the top 30% of the archive by combined objective scores, then applies a hybrid local search with 60% probability for insertion/removal operations and 30% for targeted swaps between items with complementary marginal impacts, while ensuring feasibility through dynamic weight adjustments by removing items with the least combined marginal impact. The critical design ideas prioritize high-scoring solutions, balance exploration (random operations) with exploitation (targeted swaps), and maintain feasibility through marginal impact-based adjustments.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 30% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 70)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal with 60% probability\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.6:  # 60% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.7:  # 70% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.3:  # 30% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps with 30% probability\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5602024153683222,
            1.4936294853687286
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 30% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 70)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal with 60% probability\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.6:  # 60% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.7:  # 70% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.3:  # 30% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps with 30% probability\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects high-performing solutions from the top 50% of the archive (based on combined objective scores) and applies a hybrid local search strategy: it first probabilistically inserts/removes items to explore the solution space, then performs targeted swaps between items with complementary marginal impacts (positive for one objective, negative for the other) to balance trade-offs, while always ensuring feasibility through dynamic weight adjustments and candidate filtering. Marginal contributions for both objectives guide the search, with items contributing negatively to either objective being prioritized for removal, and swaps are only performed if they improve at least one objective.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 50% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 50)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal based on adaptive thresholds\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.6:  # 60% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.4:  # 40% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 92,
        "algorithm": "The heuristic selects top-performing solutions from the archive, then applies a hybrid local search that probabilistically modifies high-impact items, performs targeted swaps between complementary items, and replaces low-impact items with high-value alternatives\u2014all while ensuring feasibility through continuous weight checks. The algorithm prioritizes Pareto-aware selection and multi-stage operations to balance exploration and exploitation, focusing on marginal contributions and dynamic adjustments to improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 40% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 60)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal of high-impact items (top 25%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    for i in top_items:\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.7:  # 70% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.3:  # 30% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.5:  # 50% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += weight_lst[j] - weight_lst[i]\n\n    # Replace low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5130110357978296,
            2.420459896326065
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions from top 40% by combined objective scores\n    combined_scores = [obj[0] + obj[1] for (_, obj) in archive]\n    threshold = np.percentile(combined_scores, 60)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic insertion-removal of high-impact items (top 25%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    for i in top_items:\n        if np.random.random() < 0.5:  # 50% chance for operation\n            if base_solution[i] == 1 and np.random.random() < 0.7:  # 70% chance to remove\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n            elif base_solution[i] == 0 and np.random.random() < 0.3:  # 30% chance to insert\n                if current_weight + weight_lst[i] <= capacity:\n                    new_solution[i] = 1\n                    current_weight += weight_lst[i]\n\n    # Targeted swaps between items with complementary marginal impacts\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.5:  # 50% chance to consider swap\n            candidates = np.where((new_solution == 1) &\n                                ((marginal1[i] * marginal1 < 0) | (marginal2[i] * marginal2 < 0)) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += weight_lst[j] - weight_lst[i]\n\n    # Replace low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 93,
        "algorithm": "The algorithm selects a solution from the archive using a dynamic Pareto frontier-aware criterion (combining distance to ideal point and diversity) and applies a hybrid local search with adaptive marginal impact analysis and capacity-aware operations, including item additions, swaps, and dynamic rebalancing to ensure feasibility while balancing both objectives. The selection prioritizes solutions with strong marginal impact and Pareto frontier proximity, while the local search adaptively adjusts thresholds and probabilities based on current capacity and marginal contributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto frontier distance and diversity\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate distance to ideal point (simple Pareto frontier measure)\n        ideal1 = max(o[1][0] for o in archive)\n        ideal2 = max(o[1][1] for o in archive)\n        distance = np.sqrt((ideal1 - obj[0])**2 + (ideal2 - obj[1])**2)\n        # Calculate diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Combined score\n        score = distance * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2])\n    selected = archive_with_scores[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution's position on Pareto frontier\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*40)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact and Pareto distance\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements (one objective improves while the other doesn't worsen too much)\n                if (marginal1[j] > marginal1[i] * 0.9 and marginal2[j] > marginal2[i] * 0.9) or \\\n                   (marginal1[j] > marginal1[i] * 0.7 and marginal2[j] > marginal2[i] * 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with Pareto-aware removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate Pareto-aware utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and distance to Pareto frontier\n            current_obj = (np.sum(value1_lst[included]), np.sum(value2_lst[included]))\n            if len(included) > 1:\n                without_item = included[included != idx]\n                new_obj = (np.sum(value1_lst[without_item]), np.sum(value2_lst[without_item]))\n                # Calculate how much this item contributes to the Pareto frontier\n                distance_improvement = np.sqrt((current_obj[0] - new_obj[0])**2 +\n                                              (current_obj[1] - new_obj[1])**2)\n            else:\n                distance_improvement = 1.0\n\n            utility = combined_marginal[idx] * distance_improvement\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9300407129673764,
            2.2409281730651855
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto frontier distance and diversity\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate distance to ideal point (simple Pareto frontier measure)\n        ideal1 = max(o[1][0] for o in archive)\n        ideal2 = max(o[1][1] for o in archive)\n        distance = np.sqrt((ideal1 - obj[0])**2 + (ideal2 - obj[1])**2)\n        # Calculate diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Combined score\n        score = distance * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2])\n    selected = archive_with_scores[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution's position on Pareto frontier\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*40)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact and Pareto distance\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements (one objective improves while the other doesn't worsen too much)\n                if (marginal1[j] > marginal1[i] * 0.9 and marginal2[j] > marginal2[i] * 0.9) or \\\n                   (marginal1[j] > marginal1[i] * 0.7 and marginal2[j] > marginal2[i] * 0.7):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with Pareto-aware removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate Pareto-aware utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and distance to Pareto frontier\n            current_obj = (np.sum(value1_lst[included]), np.sum(value2_lst[included]))\n            if len(included) > 1:\n                without_item = included[included != idx]\n                new_obj = (np.sum(value1_lst[without_item]), np.sum(value2_lst[without_item]))\n                # Calculate how much this item contributes to the Pareto frontier\n                distance_improvement = np.sqrt((current_obj[0] - new_obj[0])**2 +\n                                              (current_obj[1] - new_obj[1])**2)\n            else:\n                distance_improvement = 1.0\n\n            utility = combined_marginal[idx] * distance_improvement\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 5 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\nNo. 3 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\nNo. 4 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining item swapping and probabilistic flipping, ensuring feasibility by reverting infeasible moves. It prioritizes adding items (30% chance) and removing items (20% chance) while checking weight constraints, with swaps only occurring between already included items. The method balances exploration (random selection) and exploitation (targeted flips) to generate diverse feasible neighbors.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\nNo. 5 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the top 20% of the archive (prioritizing higher combined objective values) and applies a hybrid local search combining item swaps and path-relinking to generate a neighbor solution, ensuring feasibility by checking weight constraints. It first swaps an item from the current solution with one not included, then greedily adds additional items if capacity allows. The selection prioritizes solutions with higher potential for improvement, while the local search balances exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (e.g., top 20% by objective values)\n    archive_sorted = sorted(archive, key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    top_solutions = archive_sorted[:max(1, len(archive) // 5)]\n    selected = random.choice(top_solutions)\n    base_solution = selected[0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = selected[1][0]\n    current_value2 = selected[1][1]\n\n    # Hybrid local search: item swap with path-relinking\n    new_solution = base_solution.copy()\n    candidates = np.where(new_solution == 1)[0]\n    non_candidates = np.where(new_solution == 0)[0]\n\n    if len(candidates) > 0 and len(non_candidates) > 0:\n        # Swap one item from candidates with one from non-candidates\n        swap_out = random.choice(candidates)\n        swap_in = random.choice(non_candidates)\n\n        if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n            new_solution[swap_out] = 0\n            new_solution[swap_in] = 1\n\n        # Path-relinking: try to include additional items if possible\n        remaining_weight = capacity - np.sum(weight_lst * new_solution)\n        for item in non_candidates:\n            if new_solution[item] == 0 and weight_lst[item] <= remaining_weight:\n                new_solution[item] = 1\n                remaining_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 94,
        "algorithm": "The algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8648943240272873,
            0.508335530757904
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 95,
        "algorithm": "The algorithm selects a promising solution from the archive by balancing Pareto dominance and diversity, then applies a hybrid local search that prioritizes high-impact items (based on adaptive marginal ratios) through strategic flips, probabilistic swaps, and targeted replacements, while ensuring feasibility through a final adjustment phase that removes least impactful items if needed. The algorithm dynamically adapts its focus between objectives by combining ratios for both value dimensions and uses probabilistic operations to explore the solution space efficiently.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high Pareto dominance and diversity\n    dominance_scores = []\n    diversity_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        diversity = sum(np.linalg.norm(np.array(obj) - np.array(other_obj)) for (_, other_obj) in archive)\n        dominance_scores.append(dominated)\n        diversity_scores.append(diversity)\n\n    combined_scores = np.array(dominance_scores) + 0.5 * np.array(diversity_scores)\n    threshold = np.percentile(combined_scores, 30)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate adaptive marginal impact ratios\n    in_items = base_solution == 1\n    out_items = base_solution == 0\n\n    ratio1 = np.where(in_items, -value1_lst / (weight_lst + 1e-6), value1_lst / (weight_lst + 1e-6))\n    ratio2 = np.where(in_items, -value2_lst / (weight_lst + 1e-6), value2_lst / (weight_lst + 1e-6))\n\n    # Step 3: Dynamic threshold selection for high-impact items\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(ratio1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(ratio2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(4, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined ratio\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (ratio1[i] + ratio1[j] > 0) or (ratio2[i] + ratio2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:\n            candidates = np.where((new_solution == 0) &\n                                ((ratio1 > ratio1[i]) | (ratio2 > ratio2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_ratio = ratio1[candidates] + ratio2[candidates]\n                j = candidates[np.argmax(combined_ratio)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_ratio = ratio1[excess_items] + ratio2[excess_items]\n        i = excess_items[np.argmin(combined_ratio)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.43824571846505356,
            2.5906517803668976
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high Pareto dominance and diversity\n    dominance_scores = []\n    diversity_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        diversity = sum(np.linalg.norm(np.array(obj) - np.array(other_obj)) for (_, other_obj) in archive)\n        dominance_scores.append(dominated)\n        diversity_scores.append(diversity)\n\n    combined_scores = np.array(dominance_scores) + 0.5 * np.array(diversity_scores)\n    threshold = np.percentile(combined_scores, 30)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate adaptive marginal impact ratios\n    in_items = base_solution == 1\n    out_items = base_solution == 0\n\n    ratio1 = np.where(in_items, -value1_lst / (weight_lst + 1e-6), value1_lst / (weight_lst + 1e-6))\n    ratio2 = np.where(in_items, -value2_lst / (weight_lst + 1e-6), value2_lst / (weight_lst + 1e-6))\n\n    # Step 3: Dynamic threshold selection for high-impact items\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(ratio1)[-max(1, num_items // 4):]\n    top_items2 = np.argsort(ratio2)[-max(1, num_items // 4):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(4, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined ratio\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (ratio1[i] + ratio1[j] > 0) or (ratio2[i] + ratio2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:\n            candidates = np.where((new_solution == 0) &\n                                ((ratio1 > ratio1[i]) | (ratio2 > ratio2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_ratio = ratio1[candidates] + ratio2[candidates]\n                j = candidates[np.argmax(combined_ratio)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_ratio = ratio1[excess_items] + ratio2[excess_items]\n        i = excess_items[np.argmin(combined_ratio)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 96,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate normalized marginal impact for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n\n    # Step 3: Dynamic threshold-based selection of items to modify\n    threshold = np.percentile(combined_marginal, 70)  # Select top 30% items\n    candidate_indices = np.where(combined_marginal >= threshold)[0]\n\n    # Step 4: Hybrid operation - probabilistic add/remove/swap\n    for idx in candidate_indices:\n        if np.random.random() < 0.4:  # 40% chance to modify\n            if new_solution[idx] == 1:\n                # Remove with probability based on marginal impact\n                remove_prob = 0.3 + 0.7 * (1 - combined_marginal[idx]/np.max(combined_marginal))\n                if np.random.random() < remove_prob and current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                # Add with probability based on marginal impact\n                add_prob = 0.3 + 0.7 * (combined_marginal[idx]/np.max(combined_marginal))\n                if np.random.random() < add_prob and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 5: Targeted swaps between high and low marginal items\n    high_marginal = np.where(combined_marginal >= threshold)[0]\n    low_marginal = np.where(combined_marginal < threshold)[0]\n\n    for i in high_marginal:\n        if new_solution[i] == 1:\n            for j in low_marginal:\n                if new_solution[j] == 0 and weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                    # Swap if it improves both objectives\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                       (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                       (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        i = excess_items[np.argmin(combined_marginal[excess_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.6034697791309518,
            2.300081342458725
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate normalized marginal impact for both objectives\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = marginal1 + marginal2\n\n    # Step 3: Dynamic threshold-based selection of items to modify\n    threshold = np.percentile(combined_marginal, 70)  # Select top 30% items\n    candidate_indices = np.where(combined_marginal >= threshold)[0]\n\n    # Step 4: Hybrid operation - probabilistic add/remove/swap\n    for idx in candidate_indices:\n        if np.random.random() < 0.4:  # 40% chance to modify\n            if new_solution[idx] == 1:\n                # Remove with probability based on marginal impact\n                remove_prob = 0.3 + 0.7 * (1 - combined_marginal[idx]/np.max(combined_marginal))\n                if np.random.random() < remove_prob and current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                # Add with probability based on marginal impact\n                add_prob = 0.3 + 0.7 * (combined_marginal[idx]/np.max(combined_marginal))\n                if np.random.random() < add_prob and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 5: Targeted swaps between high and low marginal items\n    high_marginal = np.where(combined_marginal >= threshold)[0]\n    low_marginal = np.where(combined_marginal < threshold)[0]\n\n    for i in high_marginal:\n        if new_solution[i] == 1:\n            for j in low_marginal:\n                if new_solution[j] == 0 and weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                    # Swap if it improves both objectives\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                       (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                       (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        i = excess_items[np.argmin(combined_marginal[excess_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 97,
        "algorithm": "The algorithm enhances local search by first selecting promising solutions from the archive using a combination of dominance and crowding distance metrics, then applying adaptive item flipping, probabilistic swaps, and targeted replacements based on dynamic marginal impact thresholds while ensuring feasibility through a final adjustment phase. The selection prioritizes solutions near the Pareto frontier, while the local search emphasizes high-margin items and diversity preservation, with probabilistic operations guided by adaptive temperature and diversity considerations. The algorithm balances exploration and exploitation by dynamically adjusting thresholds and weights based on the current solution's characteristics.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution with adaptive selection pressure\n    objectives = np.array([obj for (_, obj) in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Calculate crowding distances for each objective\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx])\n\n    # Select from top 30% by combined dominance and crowding\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n\n    combined_scores = np.array(dominance_scores) - crowding_distances\n    threshold = np.percentile(combined_scores, 30)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate adaptive marginal contributions\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Step 3: Dynamic threshold-based flipping\n    flipping_threshold = np.percentile(np.abs(combined_marginal), 75)\n    flip_candidates = np.where(np.abs(combined_marginal) >= flipping_threshold)[0]\n\n    if len(flip_candidates) > 0:\n        num_to_flip = min(3, len(flip_candidates))\n        flip_indices = np.random.choice(flip_candidates, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps with adaptive temperature\n    temperature = 1.0 - (current_weight / capacity) ** 2\n    for i in range(len(weight_lst)):\n        if np.random.random() < temperature:\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (combined_marginal[i] + combined_marginal[j]) > 0:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements with diversity consideration\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.25:\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility adjustment with diversity preservation\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with smallest combined marginal impact and least similar to others\n        similarity = np.zeros(len(excess_items))\n        for k, i in enumerate(excess_items):\n            similarity[k] = np.sum(np.abs(combined_marginal[excess_items] - combined_marginal[i]))\n        i = excess_items[np.argmin(combined_marginal[excess_items] * similarity)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.46004754158408095,
            2.02852526307106
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution with adaptive selection pressure\n    objectives = np.array([obj for (_, obj) in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Calculate crowding distances for each objective\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx])\n\n    # Select from top 30% by combined dominance and crowding\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n\n    combined_scores = np.array(dominance_scores) - crowding_distances\n    threshold = np.percentile(combined_scores, 30)\n    candidate_indices = [i for i, score in enumerate(combined_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate adaptive marginal contributions\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Step 3: Dynamic threshold-based flipping\n    flipping_threshold = np.percentile(np.abs(combined_marginal), 75)\n    flip_candidates = np.where(np.abs(combined_marginal) >= flipping_threshold)[0]\n\n    if len(flip_candidates) > 0:\n        num_to_flip = min(3, len(flip_candidates))\n        flip_indices = np.random.choice(flip_candidates, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps with adaptive temperature\n    temperature = 1.0 - (current_weight / capacity) ** 2\n    for i in range(len(weight_lst)):\n        if np.random.random() < temperature:\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (combined_marginal[i] + combined_marginal[j]) > 0:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements with diversity consideration\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.25:\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility adjustment with diversity preservation\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with smallest combined marginal impact and least similar to others\n        similarity = np.zeros(len(excess_items))\n        for k, i in enumerate(excess_items):\n            similarity[k] = np.sum(np.abs(combined_marginal[excess_items] - combined_marginal[i]))\n        i = excess_items[np.argmin(combined_marginal[excess_items] * similarity)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 98,
        "algorithm": "The algorithm dynamically selects high-potential solutions from the archive based on hypervolume contribution, then applies a hybrid local search combining adaptive item flipping, probabilistic swaps, and targeted replacements to explore the Pareto frontier while ensuring feasibility. It prioritizes items with high combined normalized marginal impact and uses adaptive probabilities to balance exploration and exploitation, with special attention to maintaining capacity constraints through final feasibility checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high hypervolume contribution (top 30%)\n    hypervolumes = []\n    for sol, obj in archive:\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        hypervolumes.append(-dominated)  # Higher hypervolume means fewer dominators\n\n    threshold = np.percentile(hypervolumes, 70)\n    candidate_indices = [i for i, hv in enumerate(hypervolumes) if hv >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate normalized marginal contributions\n    total_value1 = np.sum(value1_lst[base_solution == 1])\n    total_value2 = np.sum(value2_lst[base_solution == 1])\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-8)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-8)\n\n    marginal1 = np.where(base_solution == 1, -norm_value1, norm_value1)\n    marginal2 = np.where(base_solution == 1, -norm_value2, norm_value2)\n\n    # Step 3: Adaptive flipping based on combined normalized impact\n    combined_marginal = marginal1 + marginal2\n    top_items = np.argsort(combined_marginal)[-max(1, len(weight_lst) // 4):]\n\n    if len(top_items) > 0:\n        num_to_flip = min(4, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps with adaptive probability\n    for i in range(len(weight_lst)):\n        swap_prob = 0.4 * (1 - (current_weight / capacity))  # Higher probability when less full\n        if np.random.random() < swap_prob:\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements with adaptive selection\n    for i in np.where(new_solution == 1)[0]:\n        replace_prob = 0.3 * (value1_lst[i] + value2_lst[i]) / (total_value1 + total_value2 + 1e-8)\n        if np.random.random() < replace_prob:\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check with adaptive removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        removal_prob = 1.0 / len(excess_items)\n        if np.random.random() < removal_prob:\n            i = np.random.choice(excess_items)\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.3400623515583736,
            9.40419253706932
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high hypervolume contribution (top 30%)\n    hypervolumes = []\n    for sol, obj in archive:\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        hypervolumes.append(-dominated)  # Higher hypervolume means fewer dominators\n\n    threshold = np.percentile(hypervolumes, 70)\n    candidate_indices = [i for i, hv in enumerate(hypervolumes) if hv >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate normalized marginal contributions\n    total_value1 = np.sum(value1_lst[base_solution == 1])\n    total_value2 = np.sum(value2_lst[base_solution == 1])\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-8)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-8)\n\n    marginal1 = np.where(base_solution == 1, -norm_value1, norm_value1)\n    marginal2 = np.where(base_solution == 1, -norm_value2, norm_value2)\n\n    # Step 3: Adaptive flipping based on combined normalized impact\n    combined_marginal = marginal1 + marginal2\n    top_items = np.argsort(combined_marginal)[-max(1, len(weight_lst) // 4):]\n\n    if len(top_items) > 0:\n        num_to_flip = min(4, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps with adaptive probability\n    for i in range(len(weight_lst)):\n        swap_prob = 0.4 * (1 - (current_weight / capacity))  # Higher probability when less full\n        if np.random.random() < swap_prob:\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements with adaptive selection\n    for i in np.where(new_solution == 1)[0]:\n        replace_prob = 0.3 * (value1_lst[i] + value2_lst[i]) / (total_value1 + total_value2 + 1e-8)\n        if np.random.random() < replace_prob:\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check with adaptive removal\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        removal_prob = 1.0 / len(excess_items)\n        if np.random.random() < removal_prob:\n            i = np.random.choice(excess_items)\n            new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 20% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 20)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Flip high-impact items (top 20% by marginal impact)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Step 4: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Step 5: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 6: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 99,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions and identify high-impact items\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Identify top 20% items for each objective\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Phase 1: Flip high-impact items (top 20% by combined marginal impact)\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Phase 3: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 4: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.38411484530001716,
            1.5359329283237457
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Calculate marginal contributions and identify high-impact items\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Identify top 20% items for each objective\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Step 3: Hybrid local search with dynamic thresholds\n    # Phase 1: Flip high-impact items (top 20% by combined marginal impact)\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.3:  # 30% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Phase 3: Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.2:  # 20% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Step 4: Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 100,
        "algorithm": "This algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) a dynamic threshold-based addition of high-marginal-impact items, (2) capacity-aware swaps that prioritize complementary improvements between objectives, and (3) utility-based rebalancing to ensure feasibility through removal of least beneficial items. The selection prioritizes solutions near the Pareto frontier, and the local search adaptively balances marginal impacts while maintaining feasibility through weighted utility scores.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection: prioritize solutions near the Pareto frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1], x[1][0] / (x[1][1] + 1e-6)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Dynamic threshold-based addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*20)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Utility-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility balances marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8243016299905697,
            1.7706184685230255
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection: prioritize solutions near the Pareto frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1], x[1][0] / (x[1][1] + 1e-6)), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Dynamic threshold-based addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*20)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Utility-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility balances marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 101,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.38787421829303936,
            0.578872799873352
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 102,
        "algorithm": "The algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8016001442167747,
            0.7411961555480957
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 103,
        "algorithm": "The algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8936728047060143,
            0.8628978431224823
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 104,
        "algorithm": "The algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8585810284176447,
            0.4377875030040741
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 105,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion of objective values and diversity, then applies a three-phase local search: (1) probabilistically adds high-impact items with adaptive thresholds, (2) performs marginal-impact-aware swaps between included and excluded items, and (3) dynamically removes low-utility items while ensuring feasibility. The selection prioritizes solutions with higher combined objective values and greater diversity, while the local search emphasizes items with high combined marginal impact, balancing both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Marginal-impact-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.822225291147064,
            1.401720553636551
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Marginal-impact-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (marginal2[j] > marginal2[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 106,
        "algorithm": "The algorithm combines Pareto frontier-aware selection with a three-phase hybrid local search: Phase 1 adaptively adds high-utility items considering crowding distance, Phase 2 removes low-impact items deterministically, and Phase 3 probabilistically swaps items to improve complementary objectives, while always ensuring feasibility. It prioritizes items with balanced marginal utility and trade-off potential, using dynamic thresholds and adaptive probabilities to guide exploration/exploitation. The algorithm emphasizes both objective values equally in utility calculations and focuses on trade-off items in swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto frontier-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Identify Pareto frontier\n        is_dominated = np.zeros(len(objectives), dtype=bool)\n        for i in range(len(objectives)):\n            for j in range(len(objectives)):\n                if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and not (objectives[j][0] == objectives[i][0] and objectives[j][1] == objectives[i][1]):\n                    is_dominated[i] = True\n                    break\n        frontier_indices = np.where(~is_dominated)[0]\n        if len(frontier_indices) > 0:\n            selected_idx = np.random.choice(frontier_indices)\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Adaptive addition based on combined utility and crowding\n    remaining_capacity = capacity - current_weight\n    available_items = np.where(new_solution == 0)[0]\n\n    if len(available_items) > 0:\n        # Calculate combined utility and crowding\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_utility = (marginal1 + marginal2) / 2\n\n        # Calculate crowding for available items\n        crowding = np.zeros(len(available_items))\n        for i in range(2):\n            sorted_idx = np.argsort([value1_lst[item] if i == 0 else value2_lst[item] for item in available_items])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(available_items)-1):\n                if value1_lst[available_items[sorted_idx[-1]]] != value1_lst[available_items[sorted_idx[0]]] if i == 0 else value2_lst[available_items[sorted_idx[-1]]] != value2_lst[available_items[sorted_idx[0]]]:\n                    crowding[sorted_idx[j]] += (value1_lst[available_items[sorted_idx[j+1]]] - value1_lst[available_items[sorted_idx[j-1]]]) / \\\n                                             (value1_lst[available_items[sorted_idx[-1]]] - value1_lst[available_items[sorted_idx[0]]]) if i == 0 else \\\n                                             (value2_lst[available_items[sorted_idx[j+1]]] - value2_lst[available_items[sorted_idx[j-1]]]) / \\\n                                             (value2_lst[available_items[sorted_idx[-1]]] - value2_lst[available_items[sorted_idx[0]]])\n\n        # Score items by utility * crowding\n        item_scores = combined_utility * crowding\n        threshold = np.percentile(item_scores, 70)  # Dynamic threshold\n        strong_candidates = available_items[item_scores >= threshold]\n\n        if len(strong_candidates) > 0:\n            for item in strong_candidates:\n                if weight_lst[item] <= remaining_capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    remaining_capacity -= weight_lst[item]\n                    break  # Add one item per phase\n\n    # Phase 2: Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact scores\n        impact_scores = (value1_lst[included_items] / (np.sum(value1_lst[included_items]) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.sum(value2_lst[included_items]) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.3  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Probabilistic swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential swaps\n        for i in included_items:\n            for j in excluded_items:\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    # Check for complementary improvements\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] < value2_lst[i]) or \\\n                       (value1_lst[j] < value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                        # Swap with probability based on improvement\n                        prob = min(0.7, (abs(value1_lst[j] - value1_lst[i]) + abs(value2_lst[j] - value2_lst[i])) / (value1_lst[i] + value2_lst[i] + 1e-6))\n                        if np.random.rand() < prob:\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    return new_solution\n\n",
        "score": [
            -0.8094332655185635,
            1.9628313481807709
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto frontier-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Identify Pareto frontier\n        is_dominated = np.zeros(len(objectives), dtype=bool)\n        for i in range(len(objectives)):\n            for j in range(len(objectives)):\n                if i != j and (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and not (objectives[j][0] == objectives[i][0] and objectives[j][1] == objectives[i][1]):\n                    is_dominated[i] = True\n                    break\n        frontier_indices = np.where(~is_dominated)[0]\n        if len(frontier_indices) > 0:\n            selected_idx = np.random.choice(frontier_indices)\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Adaptive addition based on combined utility and crowding\n    remaining_capacity = capacity - current_weight\n    available_items = np.where(new_solution == 0)[0]\n\n    if len(available_items) > 0:\n        # Calculate combined utility and crowding\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_utility = (marginal1 + marginal2) / 2\n\n        # Calculate crowding for available items\n        crowding = np.zeros(len(available_items))\n        for i in range(2):\n            sorted_idx = np.argsort([value1_lst[item] if i == 0 else value2_lst[item] for item in available_items])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(available_items)-1):\n                if value1_lst[available_items[sorted_idx[-1]]] != value1_lst[available_items[sorted_idx[0]]] if i == 0 else value2_lst[available_items[sorted_idx[-1]]] != value2_lst[available_items[sorted_idx[0]]]:\n                    crowding[sorted_idx[j]] += (value1_lst[available_items[sorted_idx[j+1]]] - value1_lst[available_items[sorted_idx[j-1]]]) / \\\n                                             (value1_lst[available_items[sorted_idx[-1]]] - value1_lst[available_items[sorted_idx[0]]]) if i == 0 else \\\n                                             (value2_lst[available_items[sorted_idx[j+1]]] - value2_lst[available_items[sorted_idx[j-1]]]) / \\\n                                             (value2_lst[available_items[sorted_idx[-1]]] - value2_lst[available_items[sorted_idx[0]]])\n\n        # Score items by utility * crowding\n        item_scores = combined_utility * crowding\n        threshold = np.percentile(item_scores, 70)  # Dynamic threshold\n        strong_candidates = available_items[item_scores >= threshold]\n\n        if len(strong_candidates) > 0:\n            for item in strong_candidates:\n                if weight_lst[item] <= remaining_capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    remaining_capacity -= weight_lst[item]\n                    break  # Add one item per phase\n\n    # Phase 2: Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact scores\n        impact_scores = (value1_lst[included_items] / (np.sum(value1_lst[included_items]) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.sum(value2_lst[included_items]) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.3  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Probabilistic swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate potential swaps\n        for i in included_items:\n            for j in excluded_items:\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    # Check for complementary improvements\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] < value2_lst[i]) or \\\n                       (value1_lst[j] < value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                        # Swap with probability based on improvement\n                        prob = min(0.7, (abs(value1_lst[j] - value1_lst[i]) + abs(value2_lst[j] - value2_lst[i])) / (value1_lst[i] + value2_lst[i] + 1e-6))\n                        if np.random.rand() < prob:\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            remaining_capacity = capacity - current_weight\n                            break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 107,
        "algorithm": "The algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7804210783472116,
            0.3466379642486572
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 108,
        "algorithm": "The algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.9642388599604512,
            0.5729503333568573
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 109,
        "algorithm": "This algorithm selects a solution from the least crowded region in the objective space to explore underrepresented trade-offs, then applies a hybrid local search combining multi-objective greedy removal (prioritizing items with low combined marginal value) with probabilistic insertion of high-impact items (based on adaptive thresholds). The method ensures feasibility by checking weight constraints during both removal and insertion phases, and uses crowding distance to guide exploration toward less crowded regions of the Pareto front.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2 = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value (ascending)\n        sorted_items = included_items[np.argsort(combined_marginal)]\n\n        # Remove items with lowest combined marginal value\n        for item in sorted_items:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            # Check if solution is still feasible\n            if current_weight <= capacity:\n                break  # Remove only one item per iteration\n\n    # Probabilistic insertion of high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive thresholds based on current solution's objectives\n        current_value1 = np.sum(value1_lst[new_solution == 1])\n        current_value2 = np.sum(value2_lst[new_solution == 1])\n        threshold1 = current_value1 * 0.8  # 80% of current value1\n        threshold2 = current_value2 * 0.8  # 80% of current value2\n\n        # Filter items that meet both thresholds\n        eligible_items = [item for item in available_items\n                         if value1_lst[item] >= threshold1 and\n                         value2_lst[item] >= threshold2 and\n                         current_weight + weight_lst[item] <= capacity]\n\n        if eligible_items:\n            # Randomly select one item to add\n            selected_item = np.random.choice(eligible_items)\n            new_solution[selected_item] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7792387438254766,
            0.40261930227279663
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2 = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value (ascending)\n        sorted_items = included_items[np.argsort(combined_marginal)]\n\n        # Remove items with lowest combined marginal value\n        for item in sorted_items:\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n            # Check if solution is still feasible\n            if current_weight <= capacity:\n                break  # Remove only one item per iteration\n\n    # Probabilistic insertion of high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive thresholds based on current solution's objectives\n        current_value1 = np.sum(value1_lst[new_solution == 1])\n        current_value2 = np.sum(value2_lst[new_solution == 1])\n        threshold1 = current_value1 * 0.8  # 80% of current value1\n        threshold2 = current_value2 * 0.8  # 80% of current value2\n\n        # Filter items that meet both thresholds\n        eligible_items = [item for item in available_items\n                         if value1_lst[item] >= threshold1 and\n                         value2_lst[item] >= threshold2 and\n                         current_weight + weight_lst[item] <= capacity]\n\n        if eligible_items:\n            # Randomly select one item to add\n            selected_item = np.random.choice(eligible_items)\n            new_solution[selected_item] = 1\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 110,
        "algorithm": "The algorithm selects a promising solution from the archive using a Pareto-aware strategy that combines crowding distance and frontier proximity, then applies a hybrid local search that dynamically adapts between multi-objective greedy insertion, probabilistic removal of low-impact items, and targeted swapping of items based on their adaptive marginal impact scores, while strictly enforcing capacity constraints. The selection prioritizes solutions with high crowding distance and frontier proximity, while the local search uses dynamic thresholds (higher for insertion, lower for removal) that adjust based on the solution's position in the objective space to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined crowding distance and frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.max(objectives, axis=0)\n        frontier_dist = np.sqrt(np.sum((objectives - ideal_point)**2, axis=1))\n\n        # Combined score (crowding + frontier proximity)\n        combined_score = crowding + (1 / (frontier_dist + 1e-6))\n        selected_idx = np.argmax(combined_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive operations\n    available_items = np.where(base_solution == 0)[0]\n    included_items = np.where(base_solution == 1)[0]\n\n    # Calculate adaptive marginal impact scores\n    if len(available_items) > 0:\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Dynamic threshold for insertion (higher for solutions closer to frontier)\n        frontier_dist = np.sqrt(np.sum((objectives[selected_idx] - np.max(objectives, axis=0))**2))\n        insertion_threshold = 0.7 + 0.3 * (1 - min(frontier_dist / (np.max(frontier_dist) + 1e-6), 1))\n\n        # Try to add top items above threshold\n        for item in sorted_items:\n            if combined_marginal[np.where(sorted_items == item)[0][0]] >= insertion_threshold and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Calculate impact scores for included items\n    if len(included_items) > 0:\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Dynamic threshold for removal (lower for solutions closer to frontier)\n        removal_threshold = 0.3 - 0.2 * (1 - min(frontier_dist / (np.max(frontier_dist) + 1e-6), 1))\n\n        # Probabilistic removal with dynamic threshold\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < removal_threshold and np.random.rand() < (1 - impact_scores[i]) * 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Targeted swapping for items with high potential\n    if len(available_items) > 0 and len(included_items) > 0:\n        # Find items to potentially swap (low impact in current solution but high potential in available items)\n        swap_candidates = []\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < removal_threshold * 0.5:\n                swap_candidates.append(item)\n\n        if swap_candidates:\n            # Find best available item to swap with\n            best_swap = None\n            best_gain = -np.inf\n            for item_out in swap_candidates:\n                for item_in in available_items:\n                    if current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity:\n                        # Calculate potential gain\n                        gain = (value1_lst[item_in] - value1_lst[item_out]) + (value2_lst[item_in] - value2_lst[item_out])\n                        if gain > best_gain:\n                            best_gain = gain\n                            best_swap = (item_out, item_in)\n\n            if best_swap and best_gain > 0:\n                item_out, item_in = best_swap\n                new_solution[item_out] = 0\n                new_solution[item_in] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7386270335782507,
            7.754333406686783
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined crowding distance and frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.max(objectives, axis=0)\n        frontier_dist = np.sqrt(np.sum((objectives - ideal_point)**2, axis=1))\n\n        # Combined score (crowding + frontier proximity)\n        combined_score = crowding + (1 / (frontier_dist + 1e-6))\n        selected_idx = np.argmax(combined_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive operations\n    available_items = np.where(base_solution == 0)[0]\n    included_items = np.where(base_solution == 1)[0]\n\n    # Calculate adaptive marginal impact scores\n    if len(available_items) > 0:\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Dynamic threshold for insertion (higher for solutions closer to frontier)\n        frontier_dist = np.sqrt(np.sum((objectives[selected_idx] - np.max(objectives, axis=0))**2))\n        insertion_threshold = 0.7 + 0.3 * (1 - min(frontier_dist / (np.max(frontier_dist) + 1e-6), 1))\n\n        # Try to add top items above threshold\n        for item in sorted_items:\n            if combined_marginal[np.where(sorted_items == item)[0][0]] >= insertion_threshold and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Calculate impact scores for included items\n    if len(included_items) > 0:\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Dynamic threshold for removal (lower for solutions closer to frontier)\n        removal_threshold = 0.3 - 0.2 * (1 - min(frontier_dist / (np.max(frontier_dist) + 1e-6), 1))\n\n        # Probabilistic removal with dynamic threshold\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < removal_threshold and np.random.rand() < (1 - impact_scores[i]) * 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Targeted swapping for items with high potential\n    if len(available_items) > 0 and len(included_items) > 0:\n        # Find items to potentially swap (low impact in current solution but high potential in available items)\n        swap_candidates = []\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < removal_threshold * 0.5:\n                swap_candidates.append(item)\n\n        if swap_candidates:\n            # Find best available item to swap with\n            best_swap = None\n            best_gain = -np.inf\n            for item_out in swap_candidates:\n                for item_in in available_items:\n                    if current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity:\n                        # Calculate potential gain\n                        gain = (value1_lst[item_in] - value1_lst[item_out]) + (value2_lst[item_in] - value2_lst[item_out])\n                        if gain > best_gain:\n                            best_gain = gain\n                            best_swap = (item_out, item_in)\n\n            if best_swap and best_gain > 0:\n                item_out, item_in = best_swap\n                new_solution[item_out] = 0\n                new_solution[item_in] = 1\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 4 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n---\n**Final Answer**:\n```python\nimport numpy as np\nfrom random import choices, choice\n\ndef select_neighbor(archive, weights, capacity):\n    # Step 1: Select a promising solution near the frontier\n    objectives = np.array([s[1] for s in archive])\n    frontiers = np.percentile(objectives, 75, axis=0)  # Top 25% in either objective\n    candidates = [s for s in archive if any(s[1][i] >= frontiers[i] for i in range(2))]\n    if not candidates:\n        candidates = archive  # Fallback to all if no frontier solutions\n    selected = choice(candidates)\n    solution, (val1, val2) = selected\n\n    # Step 2: Hybrid local search with adaptive thresholds\n    items = list(range(len(solution)))\n    current_weight = sum(w for w, inc in zip(weights, solution) if inc)\n\n    # Option 1: Add a low-impact item (if space allows)\n    if current_weight < capacity:\n        marginal_impacts = [\n            (i, (val1 + w1, val2 + w2), w)\n            for i, (w1, w2, w) in enumerate(zip(weights, weights, weights))\n            if not solution[i] and current_weight + w <= capacity\n        ]\n        if marginal_impacts:\n            _, (new_val1, new_val2), _ = max(marginal_impacts, key=lambda x: (x[1][0] + x[1][1]))\n            if (new_val1 > val1 and new_val2 >= val2) or (new_val1 >= val1 and new_val2 > val2):\n                new_solution = solution.copy()\n                new_solution[choices([x[0] for x in marginal_impacts], k=1)[0]] = 1\n                return new_solution\n\n    # Option 2: Swap a high-impact item for a low-impact one\n    removable = [i for i in items if solution[i]]\n    addable = [i for i in items if not solution[i] and current_weight + weights[i] <= capacity]\n    if removable and addable:\n        swap_pairs = [\n            (out_i, in_i)\n            for out_i in removable\n            for in_i in addable\n            if current_weight - weights[out_i] + weights[in_i] <= capacity\n        ]\n        if swap_pairs:\n            out_i, in_i = choice(swap_pairs)\n            new_solution = solution.copy()\n            new_solution[out_i], new_solution[in_i] = 0, 1\n            return new_solution\n\n    # Option 3: Replace a low-utility item (if it dominates)\n    low_utility = [i for i in items if solution[i] and (weights[i] > 0.1 * capacity)]\n    if low_utility:\n        replace_i = choice(low_utility)\n        new_solution = solution.copy()\n        new_solution[replace_i] = 0\n        return new_solution\n\n    # Fallback: Random flip if no improvements found\n    return np.random.permutation(solution)\n```\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 111,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near the Pareto frontier\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = objectives[np.lexsort((-objectives[:, 0], -objectives[:, 1]))]\n    selected_idx = np.random.choice(np.where((objectives == pareto_front[0]).all(axis=1))[0])\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive item addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n        if len(strong_candidates) > 0:\n            selected_item = np.random.choice(strong_candidates)\n            new_solution[selected_item] = 1\n            current_weight += weight_lst[selected_item]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        utility_scores = combined_marginal[included] * (1 - (value1_lst[included]/(np.sum(value1_lst[included]) + 1e-6)) * (value2_lst[included]/(np.sum(value2_lst[included]) + 1e-6)))\n        worst_item = included[np.argmin(utility_scores)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8461463035993926,
            0.9317457973957062
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near the Pareto frontier\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = objectives[np.lexsort((-objectives[:, 0], -objectives[:, 1]))]\n    selected_idx = np.random.choice(np.where((objectives == pareto_front[0]).all(axis=1))[0])\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive item addition\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal[candidates], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n        if len(strong_candidates) > 0:\n            selected_item = np.random.choice(strong_candidates)\n            new_solution[selected_item] = 1\n            current_weight += weight_lst[selected_item]\n\n    # Phase 2: Capacity-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        utility_scores = combined_marginal[included] * (1 - (value1_lst[included]/(np.sum(value1_lst[included]) + 1e-6)) * (value2_lst[included]/(np.sum(value2_lst[included]) + 1e-6)))\n        worst_item = included[np.argmin(utility_scores)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n---\n**Final Answer**:\n```python\nimport numpy as np\nfrom random import choices, choice\n\ndef select_neighbor(archive, weights, capacity):\n    # Step 1: Select a promising solution near the frontier\n    objectives = np.array([s[1] for s in archive])\n    frontiers = np.percentile(objectives, 75, axis=0)  # Top 25% in either objective\n    candidates = [s for s in archive if any(s[1][i] >= frontiers[i] for i in range(2))]\n    if not candidates:\n        candidates = archive  # Fallback to all if no frontier solutions\n    selected = choice(candidates)\n    solution, (val1, val2) = selected\n\n    # Step 2: Hybrid local search with adaptive thresholds\n    items = list(range(len(solution)))\n    current_weight = sum(w for w, inc in zip(weights, solution) if inc)\n\n    # Option 1: Add a low-impact item (if space allows)\n    if current_weight < capacity:\n        marginal_impacts = [\n            (i, (val1 + w1, val2 + w2), w)\n            for i, (w1, w2, w) in enumerate(zip(weights, weights, weights))\n            if not solution[i] and current_weight + w <= capacity\n        ]\n        if marginal_impacts:\n            _, (new_val1, new_val2), _ = max(marginal_impacts, key=lambda x: (x[1][0] + x[1][1]))\n            if (new_val1 > val1 and new_val2 >= val2) or (new_val1 >= val1 and new_val2 > val2):\n                new_solution = solution.copy()\n                new_solution[choices([x[0] for x in marginal_impacts], k=1)[0]] = 1\n                return new_solution\n\n    # Option 2: Swap a high-impact item for a low-impact one\n    removable = [i for i in items if solution[i]]\n    addable = [i for i in items if not solution[i] and current_weight + weights[i] <= capacity]\n    if removable and addable:\n        swap_pairs = [\n            (out_i, in_i)\n            for out_i in removable\n            for in_i in addable\n            if current_weight - weights[out_i] + weights[in_i] <= capacity\n        ]\n        if swap_pairs:\n            out_i, in_i = choice(swap_pairs)\n            new_solution = solution.copy()\n            new_solution[out_i], new_solution[in_i] = 0, 1\n            return new_solution\n\n    # Option 3: Replace a low-utility item (if it dominates)\n    low_utility = [i for i in items if solution[i] and (weights[i] > 0.1 * capacity)]\n    if low_utility:\n        replace_i = choice(low_utility)\n        new_solution = solution.copy()\n        new_solution[replace_i] = 0\n        return new_solution\n\n    # Fallback: Random flip if no improvements found\n    return np.random.permutation(solution)\n```\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 112,
        "algorithm": "The algorithm employs cluster-based selection to identify under-explored solution regions, then performs a hybrid local search that adaptively removes low-contributing items and adds high-contributing items based on dynamic thresholds derived from cluster centroids, while strictly enforcing capacity constraints. The search prioritizes items with high individual or combined objective values, balancing exploration of diverse regions with exploitation of promising solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Cluster-based selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    k = min(3, len(archive))  # Number of clusters\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(normalized_obj)\n    cluster_counts = np.bincount(kmeans.labels_)\n    selected_cluster = np.argmin(cluster_counts)\n    candidate_indices = np.where(kmeans.labels_ == selected_cluster)[0]\n\n    if len(candidate_indices) == 0:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        # Select the most central solution in the selected cluster\n        cluster_points = normalized_obj[candidate_indices]\n        centroid = kmeans.cluster_centers_[selected_cluster]\n        distances = np.linalg.norm(cluster_points - centroid, axis=1)\n        selected_idx = candidate_indices[np.argmin(distances)]\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with cluster-based thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        # Calculate cluster-based dynamic thresholds\n        cluster_centroid = kmeans.cluster_centers_[selected_cluster]\n        threshold1 = cluster_centroid[0] * np.median(value1_lst[included_items])\n        threshold2 = cluster_centroid[1] * np.median(value2_lst[included_items])\n\n        # Remove items below both thresholds\n        for item in included_items:\n            if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above at least one threshold if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined contribution until feasible\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            contributions = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n            sorted_items = included_items[np.argsort(contributions)]\n            for item in sorted_items:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                if current_weight <= capacity:\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.6173539311268914,
            8.742191195487976
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Cluster-based selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    k = min(3, len(archive))  # Number of clusters\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(normalized_obj)\n    cluster_counts = np.bincount(kmeans.labels_)\n    selected_cluster = np.argmin(cluster_counts)\n    candidate_indices = np.where(kmeans.labels_ == selected_cluster)[0]\n\n    if len(candidate_indices) == 0:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        # Select the most central solution in the selected cluster\n        cluster_points = normalized_obj[candidate_indices]\n        centroid = kmeans.cluster_centers_[selected_cluster]\n        distances = np.linalg.norm(cluster_points - centroid, axis=1)\n        selected_idx = candidate_indices[np.argmin(distances)]\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with cluster-based thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        # Calculate cluster-based dynamic thresholds\n        cluster_centroid = kmeans.cluster_centers_[selected_cluster]\n        threshold1 = cluster_centroid[0] * np.median(value1_lst[included_items])\n        threshold2 = cluster_centroid[1] * np.median(value2_lst[included_items])\n\n        # Remove items below both thresholds\n        for item in included_items:\n            if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above at least one threshold if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined contribution until feasible\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            contributions = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n            sorted_items = included_items[np.argsort(contributions)]\n            for item in sorted_items:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                if current_weight <= capacity:\n                    break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n---\n**Final Answer**:\n```python\nimport numpy as np\nfrom random import choices, choice\n\ndef select_neighbor(archive, weights, capacity):\n    # Step 1: Select a promising solution near the frontier\n    objectives = np.array([s[1] for s in archive])\n    frontiers = np.percentile(objectives, 75, axis=0)  # Top 25% in either objective\n    candidates = [s for s in archive if any(s[1][i] >= frontiers[i] for i in range(2))]\n    if not candidates:\n        candidates = archive  # Fallback to all if no frontier solutions\n    selected = choice(candidates)\n    solution, (val1, val2) = selected\n\n    # Step 2: Hybrid local search with adaptive thresholds\n    items = list(range(len(solution)))\n    current_weight = sum(w for w, inc in zip(weights, solution) if inc)\n\n    # Option 1: Add a low-impact item (if space allows)\n    if current_weight < capacity:\n        marginal_impacts = [\n            (i, (val1 + w1, val2 + w2), w)\n            for i, (w1, w2, w) in enumerate(zip(weights, weights, weights))\n            if not solution[i] and current_weight + w <= capacity\n        ]\n        if marginal_impacts:\n            _, (new_val1, new_val2), _ = max(marginal_impacts, key=lambda x: (x[1][0] + x[1][1]))\n            if (new_val1 > val1 and new_val2 >= val2) or (new_val1 >= val1 and new_val2 > val2):\n                new_solution = solution.copy()\n                new_solution[choices([x[0] for x in marginal_impacts], k=1)[0]] = 1\n                return new_solution\n\n    # Option 2: Swap a high-impact item for a low-impact one\n    removable = [i for i in items if solution[i]]\n    addable = [i for i in items if not solution[i] and current_weight + weights[i] <= capacity]\n    if removable and addable:\n        swap_pairs = [\n            (out_i, in_i)\n            for out_i in removable\n            for in_i in addable\n            if current_weight - weights[out_i] + weights[in_i] <= capacity\n        ]\n        if swap_pairs:\n            out_i, in_i = choice(swap_pairs)\n            new_solution = solution.copy()\n            new_solution[out_i], new_solution[in_i] = 0, 1\n            return new_solution\n\n    # Option 3: Replace a low-utility item (if it dominates)\n    low_utility = [i for i in items if solution[i] and (weights[i] > 0.1 * capacity)]\n    if low_utility:\n        replace_i = choice(low_utility)\n        new_solution = solution.copy()\n        new_solution[replace_i] = 0\n        return new_solution\n\n    # Fallback: Random flip if no improvements found\n    return np.random.permutation(solution)\n```\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 113,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.8377091640717841,
            0.37413325905799866
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 114,
        "algorithm": "The algorithm selects a diverse solution from the archive using crowding distance, then applies a hybrid local search that prioritizes high-value-density swaps and capacity-aware perturbations to generate a high-quality neighbor while ensuring feasibility. It focuses on value-to-weight ratios, performs strategic item swaps, and includes probabilistic additions/removals to balance objectives and capacity constraints. The approach intelligently balances exploration (random additions) and exploitation (value-based swaps) to navigate the multi-objective solution space efficiently.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate crowding distance for each solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distance = np.zeros(len(archive))\n\n    for m in range(2):  # For each objective\n        sorted_idx = np.argsort(objectives[:, m])\n        sorted_obj = objectives[sorted_idx, m]\n        crowding_distance[sorted_idx[0]] = np.inf\n        crowding_distance[sorted_idx[-1]] = np.inf\n\n        for i in range(1, len(archive)-1):\n            if sorted_obj[-1] == sorted_obj[0]:\n                crowding_distance[sorted_idx[i]] += 0\n            else:\n                crowding_distance[sorted_idx[i]] += (sorted_obj[i+1] - sorted_obj[i-1]) / (sorted_obj[-1] - sorted_obj[0])\n\n    # Select solution with highest crowding distance (most diverse)\n    selected_idx = np.argmax(crowding_distance)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate value density (value per unit weight)\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n\n    # Hybrid operation: swap items based on value density and capacity\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Sort items by value density (descending)\n    sorted_included = included_items[np.argsort(-value_density1[included_items])]\n    sorted_excluded = excluded_items[np.argsort(-value_density1[excluded_items])]\n\n    # Swap low-density included items with high-density excluded items\n    for i in range(min(3, len(included_items), len(excluded_items))):\n        if value_density1[sorted_included[i]] < value_density1[sorted_excluded[i]]:\n            if current_weight - weight_lst[sorted_included[i]] + weight_lst[sorted_excluded[i]] <= capacity:\n                new_solution[sorted_included[i]] = 0\n                new_solution[sorted_excluded[i]] = 1\n                current_weight = current_weight - weight_lst[sorted_included[i]] + weight_lst[sorted_excluded[i]]\n\n    # Add random high-value items if capacity allows\n    for item in sorted_excluded:\n        if np.random.rand() < 0.3 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Remove random low-value items if over capacity\n    if current_weight > capacity:\n        over_items = np.where(new_solution == 1)[0]\n        sorted_over = over_items[np.argsort(value_density1[over_items])]\n        for item in sorted_over:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.728503501719628,
            4.841722190380096
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate crowding distance for each solution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distance = np.zeros(len(archive))\n\n    for m in range(2):  # For each objective\n        sorted_idx = np.argsort(objectives[:, m])\n        sorted_obj = objectives[sorted_idx, m]\n        crowding_distance[sorted_idx[0]] = np.inf\n        crowding_distance[sorted_idx[-1]] = np.inf\n\n        for i in range(1, len(archive)-1):\n            if sorted_obj[-1] == sorted_obj[0]:\n                crowding_distance[sorted_idx[i]] += 0\n            else:\n                crowding_distance[sorted_idx[i]] += (sorted_obj[i+1] - sorted_obj[i-1]) / (sorted_obj[-1] - sorted_obj[0])\n\n    # Select solution with highest crowding distance (most diverse)\n    selected_idx = np.argmax(crowding_distance)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate value density (value per unit weight)\n    value_density1 = value1_lst / (weight_lst + 1e-6)\n    value_density2 = value2_lst / (weight_lst + 1e-6)\n\n    # Hybrid operation: swap items based on value density and capacity\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Sort items by value density (descending)\n    sorted_included = included_items[np.argsort(-value_density1[included_items])]\n    sorted_excluded = excluded_items[np.argsort(-value_density1[excluded_items])]\n\n    # Swap low-density included items with high-density excluded items\n    for i in range(min(3, len(included_items), len(excluded_items))):\n        if value_density1[sorted_included[i]] < value_density1[sorted_excluded[i]]:\n            if current_weight - weight_lst[sorted_included[i]] + weight_lst[sorted_excluded[i]] <= capacity:\n                new_solution[sorted_included[i]] = 0\n                new_solution[sorted_excluded[i]] = 1\n                current_weight = current_weight - weight_lst[sorted_included[i]] + weight_lst[sorted_excluded[i]]\n\n    # Add random high-value items if capacity allows\n    for item in sorted_excluded:\n        if np.random.rand() < 0.3 and current_weight + weight_lst[item] <= capacity:\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n\n    # Remove random low-value items if over capacity\n    if current_weight > capacity:\n        over_items = np.where(new_solution == 1)[0]\n        sorted_over = over_items[np.argsort(value_density1[over_items])]\n        for item in sorted_over:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 115,
        "algorithm": "The algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.5004141164786036,
            0.34565797448158264
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined normalized objective values\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_scores = normalized_obj[:, 0] + normalized_obj[:, 1]\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic threshold-based item selection\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds (75th percentile of included items' values)\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Hybrid operation: remove low-value items and add high-value items\n    # Remove items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:  # 50% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final check for capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # If over capacity, remove random items until feasible\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n*(Word count: ~90)*\n\n---\n**Step-by-step reasoning**:\n1. **Keywords**: Identified core concepts (adaptive, hybrid, Pareto-aware) to guide design.\n2. **Advice**: Proposed hybrid operations (add/swap/replace) with dynamic thresholds to balance exploration/exploitation.\n3. **Avoid**: Excluded rigid single-phase operations and static criteria to prevent local optima.\n4. **Explanation**: Justified the approach with feasibility and multi-objective optimization goals.\n5. **Tip**: Emphasized dynamic selection (e.g., frontier solutions) for better neighbor quality.\n\n---\n**Final Answer**:\n```python\nimport numpy as np\nfrom random import choices, choice\n\ndef select_neighbor(archive, weights, capacity):\n    # Step 1: Select a promising solution near the frontier\n    objectives = np.array([s[1] for s in archive])\n    frontiers = np.percentile(objectives, 75, axis=0)  # Top 25% in either objective\n    candidates = [s for s in archive if any(s[1][i] >= frontiers[i] for i in range(2))]\n    if not candidates:\n        candidates = archive  # Fallback to all if no frontier solutions\n    selected = choice(candidates)\n    solution, (val1, val2) = selected\n\n    # Step 2: Hybrid local search with adaptive thresholds\n    items = list(range(len(solution)))\n    current_weight = sum(w for w, inc in zip(weights, solution) if inc)\n\n    # Option 1: Add a low-impact item (if space allows)\n    if current_weight < capacity:\n        marginal_impacts = [\n            (i, (val1 + w1, val2 + w2), w)\n            for i, (w1, w2, w) in enumerate(zip(weights, weights, weights))\n            if not solution[i] and current_weight + w <= capacity\n        ]\n        if marginal_impacts:\n            _, (new_val1, new_val2), _ = max(marginal_impacts, key=lambda x: (x[1][0] + x[1][1]))\n            if (new_val1 > val1 and new_val2 >= val2) or (new_val1 >= val1 and new_val2 > val2):\n                new_solution = solution.copy()\n                new_solution[choices([x[0] for x in marginal_impacts], k=1)[0]] = 1\n                return new_solution\n\n    # Option 2: Swap a high-impact item for a low-impact one\n    removable = [i for i in items if solution[i]]\n    addable = [i for i in items if not solution[i] and current_weight + weights[i] <= capacity]\n    if removable and addable:\n        swap_pairs = [\n            (out_i, in_i)\n            for out_i in removable\n            for in_i in addable\n            if current_weight - weights[out_i] + weights[in_i] <= capacity\n        ]\n        if swap_pairs:\n            out_i, in_i = choice(swap_pairs)\n            new_solution = solution.copy()\n            new_solution[out_i], new_solution[in_i] = 0, 1\n            return new_solution\n\n    # Option 3: Replace a low-utility item (if it dominates)\n    low_utility = [i for i in items if solution[i] and (weights[i] > 0.1 * capacity)]\n    if low_utility:\n        replace_i = choice(low_utility)\n        new_solution = solution.copy()\n        new_solution[replace_i] = 0\n        return new_solution\n\n    # Fallback: Random flip if no improvements found\n    return np.random.permutation(solution)\n```\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 116,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution from least crowded region in objective space (from Algorithm 2)\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate dynamic thresholds (from Algorithm 1)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Step 3: Hybrid local search with adaptive operations\n    # Option 1: Remove low-value items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Option 2: Add high-value items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Option 3: Multi-objective greedy insertion (from Algorithm 2)\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Option 4: Probabilistic removal of low-impact items (from Algorithm 2)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.40966811688624005,
            0.6984641849994659
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution from least crowded region in objective space (from Algorithm 2)\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Calculate dynamic thresholds (from Algorithm 1)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_values1 = value1_lst[included_items]\n        included_values2 = value2_lst[included_items]\n        threshold1 = np.percentile(included_values1, 75)\n        threshold2 = np.percentile(included_values2, 75)\n    else:\n        threshold1, threshold2 = 0, 0\n\n    # Step 3: Hybrid local search with adaptive operations\n    # Option 1: Remove low-value items below dynamic thresholds\n    for item in included_items:\n        if (value1_lst[item] < threshold1) and (value2_lst[item] < threshold2):\n            if np.random.rand() < 0.5:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Option 2: Add high-value items above dynamic thresholds if they fit\n    for item in excluded_items:\n        if (value1_lst[item] > threshold1) or (value2_lst[item] > threshold2):\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.7:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Option 3: Multi-objective greedy insertion (from Algorithm 2)\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Option 4: Probabilistic removal of low-impact items (from Algorithm 2)\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        over_items = np.where(new_solution == 1)[0]\n        while current_weight > capacity and len(over_items) > 0:\n            remove_idx = np.random.choice(over_items)\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n            over_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 117,
        "algorithm": "The algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.9390331106772647,
            0.5691756010055542
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 118,
        "algorithm": "The algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) it selects solutions using a metric balancing crowding distance and value dominance, (2) it prioritizes high-impact items and strategically replaces low-impact ones while maintaining feasibility, and (3) it refines solutions with adaptive thresholds based on value density, ensuring balanced exploration of the solution space. The code emphasizes combined marginal impact (equal weights for both objectives) and dynamic thresholds (30th and 25th percentiles) to guide the search.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8028906126666782,
            0.7339365482330322
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 119,
        "algorithm": "The algorithm combines marginal impact analysis with a three-phase hybrid local search: (1) adaptive addition of high-marginal-impact items, (2) capacity-aware swaps between complementary items, and (3) objective-aware replacements. It prioritizes items with high combined marginal value for both objectives, dynamically adjusts thresholds based on capacity, and ensures feasibility by validating weight constraints at each step. The selection metric balances objective dominance and diversity, favoring solutions with higher combined objective values and moderate diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining objective dominance and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        combined_obj = obj[0] + obj[1]\n        archive_with_metrics.append((sol, obj, combined_obj, diversity))\n\n    # Select solution with highest combined objective and moderate diversity\n    archive_with_metrics.sort(key=lambda x: (-x[2], x[3]))\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    available_items = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n\n    if len(available_items) > 0:\n        threshold = np.percentile(combined_marginal[available_items], 70)\n        strong_items = available_items[combined_marginal[available_items] >= threshold]\n\n        if len(strong_items) > 0:\n            selected_item = np.random.choice(strong_items)\n            new_solution[selected_item] = 1\n            current_weight += weight_lst[selected_item]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i] and combined_marginal[j] > combined_marginal[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i] and combined_marginal[j] > combined_marginal[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Objective-aware replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate current objective values\n        current_value1 = np.sum(value1_lst[included_items])\n        current_value2 = np.sum(value2_lst[included_items])\n\n        for i in included_items:\n            # Find potential replacements\n            potential_replacements = np.where((new_solution == 0) &\n                                             (weight_lst <= current_weight - weight_lst[i] + weight_lst))[0]\n\n            if len(potential_replacements) > 0:\n                # Evaluate potential improvements\n                improvements = []\n                for j in potential_replacements:\n                    new_value1 = current_value1 - value1_lst[i] + value1_lst[j]\n                    new_value2 = current_value2 - value2_lst[i] + value2_lst[j]\n                    improvement = (new_value1 - current_value1) + (new_value2 - current_value2)\n                    improvements.append((j, improvement))\n\n                improvements.sort(key=lambda x: -x[1])\n                best_replacement = improvements[0][0]\n\n                if improvements[0][1] > 0:\n                    new_solution[i], new_solution[best_replacement] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[best_replacement]\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.9330829634215005,
            1.6805499792099
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining objective dominance and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        combined_obj = obj[0] + obj[1]\n        archive_with_metrics.append((sol, obj, combined_obj, diversity))\n\n    # Select solution with highest combined objective and moderate diversity\n    archive_with_metrics.sort(key=lambda x: (-x[2], x[3]))\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    available_items = np.where((new_solution == 0) & (weight_lst <= remaining_capacity))[0]\n\n    if len(available_items) > 0:\n        threshold = np.percentile(combined_marginal[available_items], 70)\n        strong_items = available_items[combined_marginal[available_items] >= threshold]\n\n        if len(strong_items) > 0:\n            selected_item = np.random.choice(strong_items)\n            new_solution[selected_item] = 1\n            current_weight += weight_lst[selected_item]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i] and combined_marginal[j] > combined_marginal[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i] and combined_marginal[j] > combined_marginal[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Objective-aware replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate current objective values\n        current_value1 = np.sum(value1_lst[included_items])\n        current_value2 = np.sum(value2_lst[included_items])\n\n        for i in included_items:\n            # Find potential replacements\n            potential_replacements = np.where((new_solution == 0) &\n                                             (weight_lst <= current_weight - weight_lst[i] + weight_lst))[0]\n\n            if len(potential_replacements) > 0:\n                # Evaluate potential improvements\n                improvements = []\n                for j in potential_replacements:\n                    new_value1 = current_value1 - value1_lst[i] + value1_lst[j]\n                    new_value2 = current_value2 - value2_lst[i] + value2_lst[j]\n                    improvement = (new_value1 - current_value1) + (new_value2 - current_value2)\n                    improvements.append((j, improvement))\n\n                improvements.sort(key=lambda x: -x[1])\n                best_replacement = improvements[0][0]\n\n                if improvements[0][1] > 0:\n                    new_solution[i], new_solution[best_replacement] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[best_replacement]\n                    break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 120,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid metric combining value dominance, solution density, and diversity, then applies a three-stage local search: first adding high-marginal-impact items with adaptive weighting, second performing targeted item swaps based on value density and impact scores, and finally refining with probabilistic removal of low-impact items while ensuring feasibility. The selection prioritizes solutions with higher combined values, lower density (to avoid clustering), and higher diversity (to explore different regions), while the local search emphasizes marginal contributions, value density, and impact scores with adaptive thresholds.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining value dominance, solution density, and diversity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Value dominance component\n        value_sum = objectives.sum(axis=1)\n        value_rank = np.argsort(value_sum)[::-1]\n\n        # Solution density component\n        density = np.zeros(len(objectives))\n        for i in range(len(archive)):\n            for j in range(i+1, len(archive)):\n                dist = np.linalg.norm(objectives[i] - objectives[j])\n                density[i] += 1 / (dist + 1e-6)\n                density[j] += 1 / (dist + 1e-6)\n\n        # Diversity component\n        diversity = np.zeros(len(objectives))\n        for i in range(len(archive)):\n            for j in range(len(archive)):\n                if i != j:\n                    diversity[i] += np.linalg.norm(archive[i][0] - archive[j][0])\n\n        # Combine metrics with adaptive weights\n        total_value = np.max(value_sum)\n        hybrid_score = (0.4 * value_rank/len(value_rank) +\n                       0.3 * (1 - density/np.max(density)) +\n                       0.3 * diversity/np.max(diversity))\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Stage 1: Improve marginal contributions with adaptive weighting\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    # Dynamic weighting based on current solution's value balance\n    val1_total, val2_total = archive[selected_idx][1]\n    weight_factor = 0.5 + 0.3 * (val1_total / (val1_total + val2_total + 1e-6))\n    combined_marginal = weight_factor * marginal1 + (1 - weight_factor) * marginal2\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items[:3]:  # Consider top 3 items\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Stage 2: Targeted item swaps based on value density and impact scores\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density and impact scores\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        impact_scores = (value1_lst[included_items] * value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n\n        # Identify items for potential removal\n        density_threshold = np.percentile(value_density, 25)\n        impact_threshold = np.percentile(impact_scores, 25)\n\n        for i, item in enumerate(included_items):\n            if (value_density[i] < density_threshold or impact_scores[i] < impact_threshold):\n                # Find replacement candidates\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    # Calculate replacement potential\n                    replacement_potential = (value1_lst[replacement_candidates] + value2_lst[replacement_candidates]) / weight_lst[replacement_candidates]\n                    best_replacement = replacement_candidates[np.argmax(replacement_potential)]\n\n                    if (replacement_potential[np.argmax(replacement_potential)] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n\n    # Stage 3: Probabilistic refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized scores\n        norm_value1 = value1_lst[included_items] / (np.max(value1_lst) + 1e-6)\n        norm_value2 = value2_lst[included_items] / (np.max(value2_lst) + 1e-6)\n        norm_weight = weight_lst[included_items] / (np.max(weight_lst) + 1e-6)\n\n        # Calculate removal probability\n        removal_prob = (0.4 * norm_weight +\n                       0.3 * (1 - norm_value1) +\n                       0.3 * (1 - norm_value2))\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i] and np.random.rand() < 0.5:  # 50% chance to consider removal\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.4722135816612426,
            2.7874110341072083
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining value dominance, solution density, and diversity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Value dominance component\n        value_sum = objectives.sum(axis=1)\n        value_rank = np.argsort(value_sum)[::-1]\n\n        # Solution density component\n        density = np.zeros(len(objectives))\n        for i in range(len(archive)):\n            for j in range(i+1, len(archive)):\n                dist = np.linalg.norm(objectives[i] - objectives[j])\n                density[i] += 1 / (dist + 1e-6)\n                density[j] += 1 / (dist + 1e-6)\n\n        # Diversity component\n        diversity = np.zeros(len(objectives))\n        for i in range(len(archive)):\n            for j in range(len(archive)):\n                if i != j:\n                    diversity[i] += np.linalg.norm(archive[i][0] - archive[j][0])\n\n        # Combine metrics with adaptive weights\n        total_value = np.max(value_sum)\n        hybrid_score = (0.4 * value_rank/len(value_rank) +\n                       0.3 * (1 - density/np.max(density)) +\n                       0.3 * diversity/np.max(diversity))\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Stage 1: Improve marginal contributions with adaptive weighting\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    # Dynamic weighting based on current solution's value balance\n    val1_total, val2_total = archive[selected_idx][1]\n    weight_factor = 0.5 + 0.3 * (val1_total / (val1_total + val2_total + 1e-6))\n    combined_marginal = weight_factor * marginal1 + (1 - weight_factor) * marginal2\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items[:3]:  # Consider top 3 items\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Stage 2: Targeted item swaps based on value density and impact scores\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density and impact scores\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        impact_scores = (value1_lst[included_items] * value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n\n        # Identify items for potential removal\n        density_threshold = np.percentile(value_density, 25)\n        impact_threshold = np.percentile(impact_scores, 25)\n\n        for i, item in enumerate(included_items):\n            if (value_density[i] < density_threshold or impact_scores[i] < impact_threshold):\n                # Find replacement candidates\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    # Calculate replacement potential\n                    replacement_potential = (value1_lst[replacement_candidates] + value2_lst[replacement_candidates]) / weight_lst[replacement_candidates]\n                    best_replacement = replacement_candidates[np.argmax(replacement_potential)]\n\n                    if (replacement_potential[np.argmax(replacement_potential)] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n\n    # Stage 3: Probabilistic refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized scores\n        norm_value1 = value1_lst[included_items] / (np.max(value1_lst) + 1e-6)\n        norm_value2 = value2_lst[included_items] / (np.max(value2_lst) + 1e-6)\n        norm_weight = weight_lst[included_items] / (np.max(weight_lst) + 1e-6)\n\n        # Calculate removal probability\n        removal_prob = (0.4 * norm_weight +\n                       0.3 * (1 - norm_value1) +\n                       0.3 * (1 - norm_value2))\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i] and np.random.rand() < 0.5:  # 50% chance to consider removal\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 121,
        "algorithm": "This algorithm selects a balanced solution from the archive based on objective balance, then performs a four-phase optimization: adding high-contribution items, replacing low-impact items with better candidates, optimizing with dynamic objective weights that shift based on solution quality, and finally probabilistically removing low-impact items. The method prioritizes balanced solutions (Phase 1) and uses dynamic weights (Phase 3) to focus optimization where most needed, while also considering probabilistic removals (Phase 4) for refinement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using adaptive dominance metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Adaptive dominance: balance between objectives\n        norm_obj = objectives / (np.max(objectives, axis=0) + 1e-6)\n        balance_score = np.abs(norm_obj[:, 0] - norm_obj[:, 1])\n        selected_idx = np.argmin(balance_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Calculate normalized value contributions\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    combined_contribution = 0.5 * norm_value1 + 0.5 * norm_value2\n\n    # Add high-contribution items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_contribution[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Phase 3: Dynamic weight replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Dynamic weights based on current solution quality\n        current_value1 = np.sum(value1_lst[included_items])\n        current_value2 = np.sum(value2_lst[included_items])\n        total_value1 = np.sum(value1_lst)\n        total_value2 = np.sum(value2_lst)\n\n        # Adjust weights based on how close we are to optimal\n        weight1 = 0.7 if current_value1 < 0.8 * total_value1 else 0.3\n        weight2 = 0.3 if current_value2 < 0.8 * total_value2 else 0.7\n\n        dynamic_marginal = weight1 * norm_value1 + weight2 * norm_value2\n        threshold = np.percentile(dynamic_marginal[included_items], 25)\n\n        for i, item in enumerate(included_items):\n            if dynamic_marginal[item] < threshold:\n                replacement_candidates = np.where((new_solution == 0) &\n                                                 (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(dynamic_marginal[replacement_candidates])]\n                    if (dynamic_marginal[best_replacement] > dynamic_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 4: Probabilistic removal with objective consideration\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Combined impact score considering both objectives\n        impact_score = (0.6 * norm_value1 + 0.4 * norm_value2)[included_items]\n        threshold = np.percentile(impact_score, 15)\n\n        for i, item in enumerate(included_items):\n            if impact_score[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.5951399833255091,
            0.7207710146903992
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using adaptive dominance metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Adaptive dominance: balance between objectives\n        norm_obj = objectives / (np.max(objectives, axis=0) + 1e-6)\n        balance_score = np.abs(norm_obj[:, 0] - norm_obj[:, 1])\n        selected_idx = np.argmin(balance_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Calculate normalized value contributions\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    combined_contribution = 0.5 * norm_value1 + 0.5 * norm_value2\n\n    # Add high-contribution items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_contribution[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Phase 3: Dynamic weight replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Dynamic weights based on current solution quality\n        current_value1 = np.sum(value1_lst[included_items])\n        current_value2 = np.sum(value2_lst[included_items])\n        total_value1 = np.sum(value1_lst)\n        total_value2 = np.sum(value2_lst)\n\n        # Adjust weights based on how close we are to optimal\n        weight1 = 0.7 if current_value1 < 0.8 * total_value1 else 0.3\n        weight2 = 0.3 if current_value2 < 0.8 * total_value2 else 0.7\n\n        dynamic_marginal = weight1 * norm_value1 + weight2 * norm_value2\n        threshold = np.percentile(dynamic_marginal[included_items], 25)\n\n        for i, item in enumerate(included_items):\n            if dynamic_marginal[item] < threshold:\n                replacement_candidates = np.where((new_solution == 0) &\n                                                 (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(dynamic_marginal[replacement_candidates])]\n                    if (dynamic_marginal[best_replacement] > dynamic_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 4: Probabilistic removal with objective consideration\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Combined impact score considering both objectives\n        impact_score = (0.6 * norm_value1 + 0.4 * norm_value2)[included_items]\n        threshold = np.percentile(impact_score, 15)\n\n        for i, item in enumerate(included_items):\n            if impact_score[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 122,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid metric combining crowding distance and a weighted sum of objectives (60-40 split), then applies a four-phase local search: Phase 1 adds high-marginal-impact items, Phase 2 replaces low-value items with better candidates, Phase 3 probabilistically removes low-impact items, and Phase 4 performs final refinement with adaptive percentile-based thresholds. The approach prioritizes objective 1 (60%) over objective 2 (40%) in marginal contributions and uses dynamic thresholds to balance exploration and exploitation while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 0: Select solution using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate weighted value sum (60-40 split)\n        value_sum = 0.6 * objectives[:, 0] + 0.4 * objectives[:, 1]\n        # Combine metrics\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 1: Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = 0.6 * marginal1 + 0.4 * marginal2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Phase 2: Replace low-value items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidates\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    # Calculate weighted marginal for candidates\n                    marginal1 = value1_lst[replacement_candidates] / (weight_lst[replacement_candidates] + 1e-6)\n                    marginal2 = value2_lst[replacement_candidates] / (weight_lst[replacement_candidates] + 1e-6)\n                    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2\n\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal)]\n                    if (combined_marginal[np.argmax(combined_marginal)] > combined_marginal[i] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < (1 - impact_scores[i]) * 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 4: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value ratio and weight ratio\n        value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        weight_ratio = weight_lst[included_items] / (np.max(weight_lst) + 1e-6)\n\n        # Calculate combined score\n        combined_score = value_ratio - 0.5 * weight_ratio\n        threshold = np.percentile(combined_score, 25)  # Bottom 25% percentile\n\n        for i, item in enumerate(included_items):\n            if combined_score[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8575334153742468,
            0.9613925516605377
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 0: Select solution using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate weighted value sum (60-40 split)\n        value_sum = 0.6 * objectives[:, 0] + 0.4 * objectives[:, 1]\n        # Combine metrics\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 1: Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = 0.6 * marginal1 + 0.4 * marginal2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Phase 2: Replace low-value items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidates\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    # Calculate weighted marginal for candidates\n                    marginal1 = value1_lst[replacement_candidates] / (weight_lst[replacement_candidates] + 1e-6)\n                    marginal2 = value2_lst[replacement_candidates] / (weight_lst[replacement_candidates] + 1e-6)\n                    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2\n\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal)]\n                    if (combined_marginal[np.argmax(combined_marginal)] > combined_marginal[i] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < (1 - impact_scores[i]) * 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 4: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value ratio and weight ratio\n        value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        weight_ratio = weight_lst[included_items] / (np.max(weight_lst) + 1e-6)\n\n        # Calculate combined score\n        combined_score = value_ratio - 0.5 * weight_ratio\n        threshold = np.percentile(combined_score, 25)  # Bottom 25% percentile\n\n        for i, item in enumerate(included_items):\n            if combined_score[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 9 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 123,
        "algorithm": "This algorithm selects a promising solution from the archive based on crowding distance (prioritizing solutions in less crowded regions of the Pareto front) and applies a hybrid local search that dynamically combines item additions (focusing on high-marginal-value items with adaptive thresholds), Pareto-aware swaps (exploring trade-offs between objectives), and capacity-aware removals (removing low-contribution items when over capacity). The adaptive marginal impact calculation (weighted by the current capacity) ensures a balance between the two objectives, while the percentile-based thresholds (70th for additions, 30th for removals) provide dynamic control over the search intensity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-aware selection with adaptive marginal impact\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance (most promising for improvement)\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = alpha * marginal1 + (1-alpha) * marginal2\n\n    # Hybrid local search with dynamic thresholds\n    # Phase 1: Add high-marginal-value items\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        sorted_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        threshold = np.percentile(combined_marginal[available_items], 70)\n        for item in sorted_items:\n            if combined_marginal[item] >= threshold and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0 and len(available_items) > 0:\n        # Find items with complementary marginal improvements\n        for i in included_items:\n            for j in available_items:\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Capacity-aware removals\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0 and current_weight > capacity:\n        # Calculate contribution scores\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items]/total_value1 + value2_lst[included_items]/total_value2) / 2\n        threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.7174651852261066,
            1.6833925247192383
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-aware selection with adaptive marginal impact\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance (most promising for improvement)\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate adaptive marginal impacts\n    alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = alpha * marginal1 + (1-alpha) * marginal2\n\n    # Hybrid local search with dynamic thresholds\n    # Phase 1: Add high-marginal-value items\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        sorted_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        threshold = np.percentile(combined_marginal[available_items], 70)\n        for item in sorted_items:\n            if combined_marginal[item] >= threshold and current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0 and len(available_items) > 0:\n        # Find items with complementary marginal improvements\n        for i in included_items:\n            for j in available_items:\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Capacity-aware removals\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0 and current_weight > capacity:\n        # Calculate contribution scores\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items]/total_value1 + value2_lst[included_items]/total_value2) / 2\n        threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 124,
        "algorithm": "The algorithm selects a solution near the Pareto frontier by prioritizing high-frontier-proximity candidates, then generates neighbors through weighted marginal value insertion (favoring balanced objective contributions) and dynamic replacement of low-impact items (using a 25th-percentile threshold), while strictly enforcing capacity constraints. It balances exploration (frontier targeting) with exploitation (marginal prioritization and probabilistic replacement) to maintain diversity. The alpha parameter (0.6) slightly favors the first objective in marginal calculations, while the dynamic threshold adapts to solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n",
        "score": [
            -0.908409123489413,
            0.7198238968849182
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 125,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest adaptive diversity metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate adaptive diversity score\n        diversity = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(objectives - objectives[i], axis=1)\n            diversity[i] = np.percentile(distances, 75)  # Use 75th percentile to focus on well-spaced solutions\n\n        # Select solution with highest diversity\n        selected_idx = np.argmax(diversity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds and dynamic operations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Dynamic threshold for removal (percentile-based)\n    if len(included_items) > 0:\n        value_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        removal_threshold = np.percentile(value_scores, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if value_scores[i] < removal_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Adaptive insertion with dynamic value-weight balance\n    if len(excluded_items) > 0:\n        # Calculate adaptive value-weight ratios with dynamic balance\n        remaining_capacity = capacity - current_weight\n        weight_ratio = weight_lst[excluded_items] / remaining_capacity\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (np.max(value1_lst) + np.max(value2_lst) + 1e-6)\n\n        # Dynamic balance factor (higher when capacity is tight)\n        balance_factor = 0.7 if remaining_capacity < capacity * 0.3 else 0.3\n        combined_score = (1 - balance_factor) * value_ratio + balance_factor * (1 - weight_ratio)\n\n        # Select top candidate with probability\n        sorted_items = excluded_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity and np.random.rand() < 0.7:  # 70% chance to accept\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.8723084598824236,
            2.9246880412101746
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest adaptive diversity metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate adaptive diversity score\n        diversity = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(objectives - objectives[i], axis=1)\n            diversity[i] = np.percentile(distances, 75)  # Use 75th percentile to focus on well-spaced solutions\n\n        # Select solution with highest diversity\n        selected_idx = np.argmax(diversity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds and dynamic operations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Dynamic threshold for removal (percentile-based)\n    if len(included_items) > 0:\n        value_scores = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        removal_threshold = np.percentile(value_scores, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if value_scores[i] < removal_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Adaptive insertion with dynamic value-weight balance\n    if len(excluded_items) > 0:\n        # Calculate adaptive value-weight ratios with dynamic balance\n        remaining_capacity = capacity - current_weight\n        weight_ratio = weight_lst[excluded_items] / remaining_capacity\n        value_ratio = (value1_lst[excluded_items] + value2_lst[excluded_items]) / (np.max(value1_lst) + np.max(value2_lst) + 1e-6)\n\n        # Dynamic balance factor (higher when capacity is tight)\n        balance_factor = 0.7 if remaining_capacity < capacity * 0.3 else 0.3\n        combined_score = (1 - balance_factor) * value_ratio + balance_factor * (1 - weight_ratio)\n\n        # Select top candidate with probability\n        sorted_items = excluded_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity and np.random.rand() < 0.7:  # 70% chance to accept\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 126,
        "algorithm": "The algorithm selects a solution from the archive using a novelty-aware metric that balances hypervolume contribution (70%) and solution age (30%), then applies a hybrid local search that adaptively inserts high-value-weight trade-off items and probabilistically removes low-efficiency items based on dominance rank and weight efficiency, ensuring feasibility by dynamically adjusting the solution's weight. The insertion prioritizes items with the best combined value-to-weight ratio, while removal targets items with high dominance rank or poor weight efficiency, maintaining feasibility through constraint checks. The method combines multi-objective metrics with adaptive trade-offs to explore the solution space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novelty-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate hypervolume contribution\n        hv_contrib = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            temp_obj = np.delete(objectives, i, axis=0)\n            hv_contrib[i] = np.prod(objectives[i]) - np.prod(temp_obj.max(axis=0))\n\n        # Calculate solution age (simplified as position in archive)\n        age = np.arange(len(objectives)) / len(objectives)\n\n        # Combined novelty metric\n        novelty_metric = 0.7 * hv_contrib + 0.3 * (1 - age)\n        selected_idx = np.argmax(novelty_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight trade-off insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate trade-off scores\n        value_sum = value1_lst[available_items] + value2_lst[available_items]\n        weight_norm = weight_lst[available_items] / capacity\n        tradeoff_score = value_sum / (weight_norm + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(tradeoff_score)[::-1]]\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dominance rank and weight efficiency removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate dominance ranks (simplified)\n        value_matrix = np.column_stack((value1_lst[included_items], value2_lst[included_items]))\n        dominance_rank = np.zeros(len(included_items))\n        for i in range(len(included_items)):\n            dominance_rank[i] = np.sum(np.all(value_matrix > value_matrix[i], axis=1))\n\n        # Weight efficiency\n        weight_efficiency = weight_lst[included_items] / (value1_lst[included_items] + value2_lst[included_items] + 1e-6)\n\n        # Combined removal probability\n        removal_prob = (dominance_rank / (np.max(dominance_rank) + 1e-6)) * 0.5 + weight_efficiency * 0.5\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.9198401095543143,
            1.7535679340362549
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novelty-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate hypervolume contribution\n        hv_contrib = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            temp_obj = np.delete(objectives, i, axis=0)\n            hv_contrib[i] = np.prod(objectives[i]) - np.prod(temp_obj.max(axis=0))\n\n        # Calculate solution age (simplified as position in archive)\n        age = np.arange(len(objectives)) / len(objectives)\n\n        # Combined novelty metric\n        novelty_metric = 0.7 * hv_contrib + 0.3 * (1 - age)\n        selected_idx = np.argmax(novelty_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight trade-off insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate trade-off scores\n        value_sum = value1_lst[available_items] + value2_lst[available_items]\n        weight_norm = weight_lst[available_items] / capacity\n        tradeoff_score = value_sum / (weight_norm + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(tradeoff_score)[::-1]]\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dominance rank and weight efficiency removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate dominance ranks (simplified)\n        value_matrix = np.column_stack((value1_lst[included_items], value2_lst[included_items]))\n        dominance_rank = np.zeros(len(included_items))\n        for i in range(len(included_items)):\n            dominance_rank[i] = np.sum(np.all(value_matrix > value_matrix[i], axis=1))\n\n        # Weight efficiency\n        weight_efficiency = weight_lst[included_items] / (value1_lst[included_items] + value2_lst[included_items] + 1e-6)\n\n        # Combined removal probability\n        removal_prob = (dominance_rank / (np.max(dominance_rank) + 1e-6)) * 0.5 + weight_efficiency * 0.5\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 127,
        "algorithm": "The algorithm selects a solution from the archive using a hybrid metric combining dominance rank and entropy-based diversity, then applies a value-weighted insertion strategy followed by probabilistic removal based on item criticality scores, ensuring feasibility by adaptively handling constraints. The insertion prioritizes items with higher combined value-to-weight ratios (60% value1, 40% value2), while removal probabilistically removes less critical items (70% value contribution, 30% weight contribution). The selection metric (70% dominance, 30% entropy) ensures diverse and high-quality solutions are chosen for further improvement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining dominance rank and entropy\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate dominance ranks\n        dominance = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            dominated = 0\n            for j in range(len(objectives)):\n                if i != j and all(objectives[j] >= objectives[i]):\n                    dominated += 1\n            dominance[i] = dominated\n\n        # Calculate entropy-based diversity\n        entropy = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(objectives - objectives[i], axis=1)\n            distances[i] = np.inf  # exclude self\n            prob = distances / np.sum(distances)\n            entropy[i] = -np.sum(prob * np.log(prob + 1e-6))\n\n        # Combined selection metric\n        selection_metric = 0.7 * dominance + 0.3 * entropy\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Value-weighted insertion strategy\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate value-weighted scores\n        value_weight1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        value_weight2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_score = 0.6 * value_weight1 + 0.4 * value_weight2\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal based on item criticality\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate criticality scores\n        value_contribution = (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst[included_items]) + np.sum(value2_lst[included_items]) + 1e-6)\n        weight_contribution = weight_lst[included_items] / (current_weight + 1e-6)\n        criticality = 0.7 * value_contribution + 0.3 * weight_contribution\n\n        # Calculate removal probabilities\n        removal_prob = 1 - criticality\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8009507951152788,
            1.8946377635002136
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining dominance rank and entropy\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate dominance ranks\n        dominance = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            dominated = 0\n            for j in range(len(objectives)):\n                if i != j and all(objectives[j] >= objectives[i]):\n                    dominated += 1\n            dominance[i] = dominated\n\n        # Calculate entropy-based diversity\n        entropy = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(objectives - objectives[i], axis=1)\n            distances[i] = np.inf  # exclude self\n            prob = distances / np.sum(distances)\n            entropy[i] = -np.sum(prob * np.log(prob + 1e-6))\n\n        # Combined selection metric\n        selection_metric = 0.7 * dominance + 0.3 * entropy\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Value-weighted insertion strategy\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate value-weighted scores\n        value_weight1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        value_weight2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_score = 0.6 * value_weight1 + 0.4 * value_weight2\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal based on item criticality\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate criticality scores\n        value_contribution = (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst[included_items]) + np.sum(value2_lst[included_items]) + 1e-6)\n        weight_contribution = weight_lst[included_items] / (current_weight + 1e-6)\n        criticality = 0.7 * value_contribution + 0.3 * weight_contribution\n\n        # Calculate removal probabilities\n        removal_prob = 1 - criticality\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine marginal impact analysis with hybrid operations (add/swap/replace) and use dynamic thresholds (e.g., percentile-based) to balance exploration/exploitation.\n- **Avoid**: Static item selection criteria or rigid single-phase operations; ensure operations respect capacity constraints.\n- **Explanation**: Dynamic thresholds and hybrid operations ensure both objectives are optimized while maintaining feasibility, avoiding premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 128,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-aware selection with adaptive crowding and diversity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric with adaptive weights\n        selection_metric = crowding + 0.7 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive marginal impact\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Dynamic threshold based on 75th percentile\n        threshold = np.percentile(combined_score, 75)\n        candidate_items = available_items[combined_score >= threshold]\n\n        # Try to add items above threshold\n        for item in candidate_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact scores\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Dynamic removal probability based on impact and weight\n        removal_prob = (1 - value_impact) * 0.5 + weight_impact * 0.5\n\n        # Remove items with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i] * 0.4:  # Reduced base probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Additional hybrid operation: consider swapping items\n    if len(available_items) > 0 and len(included_items) > 0:\n        # Find items with highest marginal impact\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Find items with lowest impact in current solution\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Sort both lists\n        best_available = available_items[np.argsort(combined_marginal)[::-1]]\n        worst_included = included_items[np.argsort(impact_scores)]\n\n        # Try to swap worst included with best available\n        for item_in in worst_included[:2]:  # Consider top 2 worst items\n            for item_out in best_available[:2]:  # Consider top 2 best items\n                if current_weight - weight_lst[item_in] + weight_lst[item_out] <= capacity:\n                    new_solution[item_in] = 0\n                    new_solution[item_out] = 1\n                    current_weight = current_weight - weight_lst[item_in] + weight_lst[item_out]\n                    break\n            else:\n                continue\n            break\n\n    return new_solution\n\n",
        "score": [
            -0.8544602085314268,
            1.0492085814476013
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-aware selection with adaptive crowding and diversity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric with adaptive weights\n        selection_metric = crowding + 0.7 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive marginal impact\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Dynamic threshold based on 75th percentile\n        threshold = np.percentile(combined_score, 75)\n        candidate_items = available_items[combined_score >= threshold]\n\n        # Try to add items above threshold\n        for item in candidate_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact scores\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Dynamic removal probability based on impact and weight\n        removal_prob = (1 - value_impact) * 0.5 + weight_impact * 0.5\n\n        # Remove items with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i] * 0.4:  # Reduced base probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Additional hybrid operation: consider swapping items\n    if len(available_items) > 0 and len(included_items) > 0:\n        # Find items with highest marginal impact\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Find items with lowest impact in current solution\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Sort both lists\n        best_available = available_items[np.argsort(combined_marginal)[::-1]]\n        worst_included = included_items[np.argsort(impact_scores)]\n\n        # Try to swap worst included with best available\n        for item_in in worst_included[:2]:  # Consider top 2 worst items\n            for item_out in best_available[:2]:  # Consider top 2 best items\n                if current_weight - weight_lst[item_in] + weight_lst[item_out] <= capacity:\n                    new_solution[item_in] = 0\n                    new_solution[item_out] = 1\n                    current_weight = current_weight - weight_lst[item_in] + weight_lst[item_out]\n                    break\n            else:\n                continue\n            break\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 9 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 129,
        "algorithm": "The algorithm selects a solution from the least populated quadrant of the Pareto front, then applies a hybrid local search that first adds high-marginal-value items with dynamic objective weighting (prioritizing value1 when the knapsack is less full) and then performs targeted swaps to improve Pareto dominance, while ensuring feasibility through value-aware removal of items when necessary. The selection prioritizes underrepresented regions, while the local search balances exploration (adding items) and exploitation (swapping dominated items) to efficiently navigate the objective space.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most underrepresented quadrant\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Select solution with highest crowding distance in its quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic objective weighting\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Dynamic objective weighting based on current fill ratio\n        alpha = 0.7 - 0.4 * (current_weight / capacity)\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Add top items with probability based on marginal value\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Targeted swaps to improve Pareto frontier\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0 and len(available_items) > 0:\n        # Calculate Pareto dominance potential for included items\n        dominated = np.zeros(len(included_items), dtype=bool)\n        for i, item_i in enumerate(included_items):\n            for item_j in available_items:\n                if (value1_lst[item_j] > value1_lst[item_i] and value2_lst[item_j] >= value2_lst[item_i]) or \\\n                   (value2_lst[item_j] > value2_lst[item_i] and value1_lst[item_j] >= value1_lst[item_i]):\n                    dominated[i] = True\n                    break\n\n        # Perform swaps for dominated items\n        for i, dominated_item in enumerate(included_items[dominated]):\n            if np.random.rand() < 0.5:  # 50% chance to attempt swap\n                for item in available_items:\n                    if (current_weight - weight_lst[dominated_item] + weight_lst[item] <= capacity):\n                        new_solution[dominated_item] = 0\n                        new_solution[item] = 1\n                        current_weight = current_weight - weight_lst[dominated_item] + weight_lst[item]\n                        break\n\n    # Value-aware removal to maintain feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        # Calculate normalized value contributions\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        # Remove items with lowest contribution until feasible\n        while current_weight > capacity and len(included_items) > 0:\n            worst_item = included_items[np.argmin(contribution)]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) > 0:\n                contribution = (value1_lst[included_items] / np.sum(value1_lst[included_items]) +\n                               value2_lst[included_items] / np.sum(value2_lst[included_items])) / 2\n\n    return new_solution\n\n",
        "score": [
            -0.9043034664473747,
            1.9017133712768555
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most underrepresented quadrant\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Select solution with highest crowding distance in its quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic objective weighting\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Dynamic objective weighting based on current fill ratio\n        alpha = 0.7 - 0.4 * (current_weight / capacity)\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Add top items with probability based on marginal value\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Targeted swaps to improve Pareto frontier\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0 and len(available_items) > 0:\n        # Calculate Pareto dominance potential for included items\n        dominated = np.zeros(len(included_items), dtype=bool)\n        for i, item_i in enumerate(included_items):\n            for item_j in available_items:\n                if (value1_lst[item_j] > value1_lst[item_i] and value2_lst[item_j] >= value2_lst[item_i]) or \\\n                   (value2_lst[item_j] > value2_lst[item_i] and value1_lst[item_j] >= value1_lst[item_i]):\n                    dominated[i] = True\n                    break\n\n        # Perform swaps for dominated items\n        for i, dominated_item in enumerate(included_items[dominated]):\n            if np.random.rand() < 0.5:  # 50% chance to attempt swap\n                for item in available_items:\n                    if (current_weight - weight_lst[dominated_item] + weight_lst[item] <= capacity):\n                        new_solution[dominated_item] = 0\n                        new_solution[item] = 1\n                        current_weight = current_weight - weight_lst[dominated_item] + weight_lst[item]\n                        break\n\n    # Value-aware removal to maintain feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        # Calculate normalized value contributions\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        # Remove items with lowest contribution until feasible\n        while current_weight > capacity and len(included_items) > 0:\n            worst_item = included_items[np.argmin(contribution)]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n            included_items = np.where(new_solution == 1)[0]\n            if len(included_items) > 0:\n                contribution = (value1_lst[included_items] / np.sum(value1_lst[included_items]) +\n                               value2_lst[included_items] / np.sum(value2_lst[included_items])) / 2\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) it selects solutions using a metric balancing crowding distance and value dominance, (2) it prioritizes high-impact items and strategically replaces low-impact ones while maintaining feasibility, and (3) it refines solutions with adaptive thresholds based on value density, ensuring balanced exploration of the solution space. The code emphasizes combined marginal impact (equal weights for both objectives) and dynamic thresholds (30th and 25th percentiles) to guide the search.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 130,
        "algorithm": "The algorithm combines Pareto-aware selection with a dynamic, multi-phase local search that first identifies promising solutions using a hybrid metric of objective values and diversity, then performs targeted operations (addition, replacement, and removal) based on adaptive value density thresholds, prioritizing high-impact items while ensuring feasibility through capacity-aware swaps, and finally refines the solution with probabilistic removals of low-density items, dynamically adjusting thresholds to balance exploration and exploitation across both objectives. The code prioritizes objective 1 (60% weight) or objective 2 (40% weight) randomly, uses value density to guide replacements, and probabilistically removes low-value items, all while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using hybrid metric of objective values and diversity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        # Calculate diversity score\n        diversity = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.sqrt(np.sum((norm_obj - norm_obj[i])**2, axis=1))\n            diversity[i] = np.mean(distances)\n        # Hybrid score combines objective values and diversity\n        hybrid_score = 0.7 * (norm_obj[:, 0] + norm_obj[:, 1]) + 0.3 * diversity\n        selected_idx = np.argmax(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase operation with adaptive thresholds\n    # Calculate combined marginal impact with adaptive weights\n    obj1_weight = 0.6 if np.random.rand() < 0.5 else 0.4\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = obj1_weight * marginal1 + (1 - obj1_weight) * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items[:3]:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Replace low-impact items with better alternatives\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 20)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidates\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Probabilistic removal of low-density items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 15)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.26514272179366594,
            0.7676172256469727
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using hybrid metric of objective values and diversity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        # Calculate diversity score\n        diversity = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.sqrt(np.sum((norm_obj - norm_obj[i])**2, axis=1))\n            diversity[i] = np.mean(distances)\n        # Hybrid score combines objective values and diversity\n        hybrid_score = 0.7 * (norm_obj[:, 0] + norm_obj[:, 1]) + 0.3 * diversity\n        selected_idx = np.argmax(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase operation with adaptive thresholds\n    # Calculate combined marginal impact with adaptive weights\n    obj1_weight = 0.6 if np.random.rand() < 0.5 else 0.4\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = obj1_weight * marginal1 + (1 - obj1_weight) * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items[:3]:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Replace low-impact items with better alternatives\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 20)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidates\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Probabilistic removal of low-density items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 15)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) it selects solutions using a metric balancing crowding distance and value dominance, (2) it prioritizes high-impact items and strategically replaces low-impact ones while maintaining feasibility, and (3) it refines solutions with adaptive thresholds based on value density, ensuring balanced exploration of the solution space. The code emphasizes combined marginal impact (equal weights for both objectives) and dynamic thresholds (30th and 25th percentiles) to guide the search.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 131,
        "algorithm": "The algorithm combines frontier-aware selection with a multi-phase hybrid local search that prioritizes Pareto-frontier proximity and value density, using adaptive thresholds to balance exploration and exploitation. It first selects solutions near the estimated Pareto frontier, then applies a three-phase optimization: strategic item addition, replacement of low-value items, and probabilistic removal of low-contribution items, ensuring feasibility while progressively improving both objectives. The method dynamically adjusts selection criteria based on value density and contribution thresholds to navigate the trade-off space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Estimate Pareto frontier using k-nearest neighbors\n        k = min(3, len(objectives)-1)\n        frontier_indices = []\n        for i in range(len(objectives)):\n            # Check if current solution is not dominated by any other\n            dominated = False\n            for j in range(len(objectives)):\n                if i != j and objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1] and (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                    dominated = True\n                    break\n            if not dominated:\n                frontier_indices.append(i)\n\n        if not frontier_indices:\n            # If no non-dominated solutions, select based on combined value\n            combined_value = objectives[:, 0] + objectives[:, 1]\n            selected_idx = np.argmax(combined_value)\n        else:\n            # Select closest to frontier among non-dominated\n            selected_idx = frontier_indices[np.random.choice(len(frontier_indices))]\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined value density\n    combined_value = value1_lst + value2_lst\n    value_density = combined_value / (weight_lst + 1e-6)\n\n    # Add high-density items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(value_density[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-density items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        density_scores = value_density[included_items]\n        threshold = np.percentile(density_scores, 40)\n\n        for i, item in enumerate(included_items):\n            if density_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(value_density[replacement_candidates])]\n                    if (value_density[best_replacement] > value_density[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive probability\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized value contribution\n        total_combined = np.sum(combined_value[included_items])\n        contribution = combined_value[included_items] / total_combined\n        threshold = np.percentile(contribution, 30)\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold and np.random.rand() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.4796681532223995,
            0.572726309299469
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Estimate Pareto frontier using k-nearest neighbors\n        k = min(3, len(objectives)-1)\n        frontier_indices = []\n        for i in range(len(objectives)):\n            # Check if current solution is not dominated by any other\n            dominated = False\n            for j in range(len(objectives)):\n                if i != j and objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1] and (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                    dominated = True\n                    break\n            if not dominated:\n                frontier_indices.append(i)\n\n        if not frontier_indices:\n            # If no non-dominated solutions, select based on combined value\n            combined_value = objectives[:, 0] + objectives[:, 1]\n            selected_idx = np.argmax(combined_value)\n        else:\n            # Select closest to frontier among non-dominated\n            selected_idx = frontier_indices[np.random.choice(len(frontier_indices))]\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined value density\n    combined_value = value1_lst + value2_lst\n    value_density = combined_value / (weight_lst + 1e-6)\n\n    # Add high-density items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(value_density[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-density items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        density_scores = value_density[included_items]\n        threshold = np.percentile(density_scores, 40)\n\n        for i, item in enumerate(included_items):\n            if density_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(value_density[replacement_candidates])]\n                    if (value_density[best_replacement] > value_density[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive probability\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized value contribution\n        total_combined = np.sum(combined_value[included_items])\n        contribution = combined_value[included_items] / total_combined\n        threshold = np.percentile(contribution, 30)\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold and np.random.rand() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) it selects solutions using a metric balancing crowding distance and value dominance, (2) it prioritizes high-impact items and strategically replaces low-impact ones while maintaining feasibility, and (3) it refines solutions with adaptive thresholds based on value density, ensuring balanced exploration of the solution space. The code emphasizes combined marginal impact (equal weights for both objectives) and dynamic thresholds (30th and 25th percentiles) to guide the search.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 132,
        "algorithm": "This algorithm combines Pareto-aware selection with a four-phase hybrid local search: it first selects solutions using a metric balancing hypervolume contribution and objective dominance, then applies value-aware swapping, dynamic threshold replacement, and probabilistic refinement to improve both objectives while maintaining feasibility. The method prioritizes items with higher combined value-to-weight ratios and uses adaptive thresholds to maintain diversity, with probabilistic steps to further refine solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using hypervolume-aware metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate hypervolume contribution\n        ref_point = np.max(objectives, axis=0) * 1.1\n        hv_contribution = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            temp_obj = np.delete(objectives, i, axis=0)\n            if len(temp_obj) > 0:\n                sorted_obj = temp_obj[np.lexsort((temp_obj[:,1], temp_obj[:,0]))]\n                hv = 0\n                prev_point = ref_point\n                for point in sorted_obj:\n                    hv += (prev_point[0] - point[0]) * (prev_point[1] - point[1])\n                    prev_point = point\n                hv_contribution[i] = hv\n            else:\n                hv_contribution[i] = np.prod(ref_point - objectives[i])\n\n        # Calculate objective dominance score\n        dominance_score = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            dominated = 0\n            for j in range(len(objectives)):\n                if i != j and np.all(objectives[j] >= objectives[i]) and np.any(objectives[j] > objectives[i]):\n                    dominated += 1\n            dominance_score[i] = dominated\n\n        # Combine metrics with adaptive weights\n        alpha = 0.4 if len(objectives) > 10 else 0.6\n        hybrid_score = alpha * hv_contribution + (1 - alpha) * dominance_score\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Value-aware swapping mechanism\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value ratios for included items\n        value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n\n        # Find best candidate for removal\n        remove_candidate = included_items[np.argmin(value_ratio)]\n        remove_value1 = value1_lst[remove_candidate]\n        remove_value2 = value2_lst[remove_candidate]\n        remove_weight = weight_lst[remove_candidate]\n\n        # Find best candidate for addition\n        potential_add = excluded_items[(weight_lst[excluded_items] <= capacity - current_weight + remove_weight)]\n        if len(potential_add) > 0:\n            # Calculate combined value improvement\n            add_value1 = value1_lst[potential_add]\n            add_value2 = value2_lst[potential_add]\n            add_weight = weight_lst[potential_add]\n\n            # Calculate improvement score\n            improvement_score = (add_value1 - remove_value1) * (add_value2 - remove_value2) / (add_weight - remove_weight + 1e-6)\n\n            # Select best swap\n            best_swap = potential_add[np.argmax(improvement_score)]\n            if improvement_score[np.argmax(improvement_score)] > 0:\n                new_solution[remove_candidate] = 0\n                new_solution[best_swap] = 1\n                current_weight = current_weight - remove_weight + weight_lst[best_swap]\n\n    # Phase 3: Dynamic threshold replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n\n        # Calculate dynamic threshold\n        threshold = np.percentile(value_density, 15 + 10 * (current_weight / capacity))\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 4: Probabilistic refinement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate weight efficiency\n        weight_efficiency = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Find worst included item\n        worst_item = included_items[np.argmin(weight_efficiency[included_items])]\n\n        # Find best excluded item\n        best_excluded = excluded_items[np.argmax(weight_efficiency[excluded_items])]\n\n        # Probabilistic swap based on efficiency difference\n        efficiency_diff = weight_efficiency[best_excluded] - weight_efficiency[worst_item]\n        if efficiency_diff > 0 and np.random.rand() < np.tanh(efficiency_diff * 0.1):\n            if current_weight - weight_lst[worst_item] + weight_lst[best_excluded] <= capacity:\n                new_solution[worst_item] = 0\n                new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7063443045304177,
            3.1319501399993896
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using hypervolume-aware metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate hypervolume contribution\n        ref_point = np.max(objectives, axis=0) * 1.1\n        hv_contribution = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            temp_obj = np.delete(objectives, i, axis=0)\n            if len(temp_obj) > 0:\n                sorted_obj = temp_obj[np.lexsort((temp_obj[:,1], temp_obj[:,0]))]\n                hv = 0\n                prev_point = ref_point\n                for point in sorted_obj:\n                    hv += (prev_point[0] - point[0]) * (prev_point[1] - point[1])\n                    prev_point = point\n                hv_contribution[i] = hv\n            else:\n                hv_contribution[i] = np.prod(ref_point - objectives[i])\n\n        # Calculate objective dominance score\n        dominance_score = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            dominated = 0\n            for j in range(len(objectives)):\n                if i != j and np.all(objectives[j] >= objectives[i]) and np.any(objectives[j] > objectives[i]):\n                    dominated += 1\n            dominance_score[i] = dominated\n\n        # Combine metrics with adaptive weights\n        alpha = 0.4 if len(objectives) > 10 else 0.6\n        hybrid_score = alpha * hv_contribution + (1 - alpha) * dominance_score\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Value-aware swapping mechanism\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value ratios for included items\n        value_ratio = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n\n        # Find best candidate for removal\n        remove_candidate = included_items[np.argmin(value_ratio)]\n        remove_value1 = value1_lst[remove_candidate]\n        remove_value2 = value2_lst[remove_candidate]\n        remove_weight = weight_lst[remove_candidate]\n\n        # Find best candidate for addition\n        potential_add = excluded_items[(weight_lst[excluded_items] <= capacity - current_weight + remove_weight)]\n        if len(potential_add) > 0:\n            # Calculate combined value improvement\n            add_value1 = value1_lst[potential_add]\n            add_value2 = value2_lst[potential_add]\n            add_weight = weight_lst[potential_add]\n\n            # Calculate improvement score\n            improvement_score = (add_value1 - remove_value1) * (add_value2 - remove_value2) / (add_weight - remove_weight + 1e-6)\n\n            # Select best swap\n            best_swap = potential_add[np.argmax(improvement_score)]\n            if improvement_score[np.argmax(improvement_score)] > 0:\n                new_solution[remove_candidate] = 0\n                new_solution[best_swap] = 1\n                current_weight = current_weight - remove_weight + weight_lst[best_swap]\n\n    # Phase 3: Dynamic threshold replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n\n        # Calculate dynamic threshold\n        threshold = np.percentile(value_density, 15 + 10 * (current_weight / capacity))\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.4:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 4: Probabilistic refinement\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate weight efficiency\n        weight_efficiency = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n        # Find worst included item\n        worst_item = included_items[np.argmin(weight_efficiency[included_items])]\n\n        # Find best excluded item\n        best_excluded = excluded_items[np.argmax(weight_efficiency[excluded_items])]\n\n        # Probabilistic swap based on efficiency difference\n        efficiency_diff = weight_efficiency[best_excluded] - weight_efficiency[worst_item]\n        if efficiency_diff > 0 and np.random.rand() < np.tanh(efficiency_diff * 0.1):\n            if current_weight - weight_lst[worst_item] + weight_lst[best_excluded] <= capacity:\n                new_solution[worst_item] = 0\n                new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) it selects solutions using a metric balancing crowding distance and value dominance, (2) it prioritizes high-impact items and strategically replaces low-impact ones while maintaining feasibility, and (3) it refines solutions with adaptive thresholds based on value density, ensuring balanced exploration of the solution space. The code emphasizes combined marginal impact (equal weights for both objectives) and dynamic thresholds (30th and 25th percentiles) to guide the search.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 133,
        "algorithm": "This algorithm uses a three-phase approach to generate neighbor solutions from a Pareto archive: (1) it selects a base solution using weighted objective prioritization (60% value1, 40% value2), (2) it adaptively adds high-value-density items and replaces low-density items with better alternatives while maintaining feasibility, and (3) it probabilistically refines the solution by removing low-value, heavy items with a 40% chance. The method balances exploration and exploitation through normalized value densities and percentile-based thresholds.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using weighted objective prioritization\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective scores\n        norm_obj = objectives / (np.max(objectives, axis=0) + 1e-6)\n        # Weighted sum (prioritize objective 1 more)\n        weighted_scores = 0.6 * norm_obj[:, 0] + 0.4 * norm_obj[:, 1]\n        selected_idx = np.argmax(weighted_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Adaptive item selection based on value density\n    # Calculate normalized value density\n    value_density = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    value_density_norm = value_density / (np.max(value_density) + 1e-6)\n\n    # Add high-density items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(value_density_norm[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-density items with better alternatives\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        density_threshold = np.percentile(value_density_norm[included_items], 40)\n        for i, item in enumerate(included_items):\n            if value_density_norm[item] < density_threshold:\n                # Find replacement with higher density and feasible weight\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(value_density_norm[replacement_candidates])]\n                    if (value_density_norm[best_replacement] > value_density_norm[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Probabilistic refinement with adaptive weights\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate weighted value scores\n        weighted_values = 0.7 * value1_lst[included_items] + 0.3 * value2_lst[included_items]\n        weight_threshold = np.percentile(weight_lst[included_items], 60)\n\n        for i, item in enumerate(included_items):\n            if (weighted_values[i] < np.percentile(weighted_values, 35) and\n                weight_lst[item] > weight_threshold and\n                np.random.rand() < 0.4):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.35411393681455494,
            8.842253118753433
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using weighted objective prioritization\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective scores\n        norm_obj = objectives / (np.max(objectives, axis=0) + 1e-6)\n        # Weighted sum (prioritize objective 1 more)\n        weighted_scores = 0.6 * norm_obj[:, 0] + 0.4 * norm_obj[:, 1]\n        selected_idx = np.argmax(weighted_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Adaptive item selection based on value density\n    # Calculate normalized value density\n    value_density = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    value_density_norm = value_density / (np.max(value_density) + 1e-6)\n\n    # Add high-density items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(value_density_norm[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-density items with better alternatives\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        density_threshold = np.percentile(value_density_norm[included_items], 40)\n        for i, item in enumerate(included_items):\n            if value_density_norm[item] < density_threshold:\n                # Find replacement with higher density and feasible weight\n                replacement_candidates = np.where((new_solution == 0) &\n                                                (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(value_density_norm[replacement_candidates])]\n                    if (value_density_norm[best_replacement] > value_density_norm[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Probabilistic refinement with adaptive weights\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate weighted value scores\n        weighted_values = 0.7 * value1_lst[included_items] + 0.3 * value2_lst[included_items]\n        weight_threshold = np.percentile(weight_lst[included_items], 60)\n\n        for i, item in enumerate(included_items):\n            if (weighted_values[i] < np.percentile(weighted_values, 35) and\n                weight_lst[item] > weight_threshold and\n                np.random.rand() < 0.4):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto-aware selection with a three-phase hybrid local search: (1) it selects solutions using a metric balancing crowding distance and value dominance, (2) it prioritizes high-impact items and strategically replaces low-impact ones while maintaining feasibility, and (3) it refines solutions with adaptive thresholds based on value density, ensuring balanced exploration of the solution space. The code emphasizes combined marginal impact (equal weights for both objectives) and dynamic thresholds (30th and 25th percentiles) to guide the search.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution using Pareto-aware hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Calculate value dominance score\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item optimization\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.3:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier using a hybrid metric combining crowding distance and value dominance, then performs a three-phase optimization: first adding high-marginal-impact items, second replacing low-value items with better candidates while maintaining feasibility, and finally refining with adaptive thresholds to balance objectives. It prioritizes combined marginal contributions (60% value1, 40% value2) and dynamically adjusts selections based on value density and impact scores.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution near Pareto frontier using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Hybrid metric: combine crowding distance and value dominance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Value dominance: prefer solutions with higher combined value\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase item replacement with adaptive thresholds\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.6 * marginal1 + 0.4 * marginal2  # Weighted sum\n\n    # Add high-marginal-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-value items with high-marginal-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate value density\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 30)  # Bottom 30% percentile\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold:\n                # Find replacement candidate\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final optimization with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized impact scores\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 20)  # Bottom 20% percentile\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold and np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 134,
        "algorithm": "The algorithm first selects a solution from the least crowded region in the objective space using a hybrid metric combining crowding distance and value dominance, then applies a three-phase hybrid local search: adding high-marginal-impact items, replacing low-impact items with better candidates, and refining by probabilistically removing low-density items while maintaining feasibility. It prioritizes balanced marginal impact (equal weights) in both objectives and uses adaptive thresholds (30th/25th percentiles) for item selection/replacement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution from least crowded region using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase optimization\n    # Calculate combined marginal impact (equal weights)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.35:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.7834402547521307,
            0.7375106513500214
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Select solution from least crowded region using hybrid metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        value_sum = objectives.sum(axis=1)\n        hybrid_score = crowding + 0.3 * value_sum / np.max(value_sum)\n        selected_idx = np.argmin(hybrid_score)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 2: Multi-phase optimization\n    # Calculate combined marginal impact (equal weights)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = 0.5 * marginal1 + 0.5 * marginal2\n\n    # Add high-impact items\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        top_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        for item in top_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Replace low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        threshold = np.percentile(impact_scores, 30)\n\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                replacement_candidates = np.where((new_solution == 0) &\n                                                  (weight_lst <= current_weight - weight_lst[item] + weight_lst))[0]\n                if len(replacement_candidates) > 0:\n                    best_replacement = replacement_candidates[np.argmax(combined_marginal[replacement_candidates])]\n                    if (combined_marginal[best_replacement] > combined_marginal[item] and\n                        (current_weight - weight_lst[item] + weight_lst[best_replacement]) <= capacity):\n                        new_solution[item] = 0\n                        new_solution[best_replacement] = 1\n                        current_weight = current_weight - weight_lst[item] + weight_lst[best_replacement]\n                        break\n\n    # Phase 3: Final refinement with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        value_density = (value1_lst[included_items] + value2_lst[included_items]) / (weight_lst[included_items] + 1e-6)\n        threshold = np.percentile(value_density, 25)\n\n        for i, item in enumerate(included_items):\n            if value_density[i] < threshold and np.random.rand() < 0.35:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 9 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 135,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near the Pareto frontier using crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item addition based on combined marginal value\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamic weight for objective 1\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Pareto-aware swaps with dynamic threshold\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        threshold = 0.3 if len(archive) > 5 else 0.5\n        if np.random.random() < threshold:\n            item_out = np.random.choice(included_items)\n            item_in = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_out] + weight_lst[item_in]) <= capacity:\n                new_solution[item_out] = 0\n                new_solution[item_in] = 1\n\n    # Dynamic threshold-based removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.9633560529420893,
            0.5908704996109009
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near the Pareto frontier using crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item addition based on combined marginal value\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamic weight for objective 1\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Pareto-aware swaps with dynamic threshold\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        threshold = 0.3 if len(archive) > 5 else 0.5\n        if np.random.random() < threshold:\n            item_out = np.random.choice(included_items)\n            item_in = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_out] + weight_lst[item_in]) <= capacity:\n                new_solution[item_out] = 0\n                new_solution[item_in] = 1\n\n    # Dynamic threshold-based removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier by prioritizing high-frontier-proximity candidates, then generates neighbors through weighted marginal value insertion (favoring balanced objective contributions) and dynamic replacement of low-impact items (using a 25th-percentile threshold), while strictly enforcing capacity constraints. It balances exploration (frontier targeting) with exploitation (marginal prioritization and probabilistic replacement) to maintain diversity. The alpha parameter (0.6) slightly favors the first objective in marginal calculations, while the dynamic threshold adapts to solution quality.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 136,
        "algorithm": "This algorithm dynamically selects a solution from the archive based on combined objective contribution and frontier proximity, then generates a neighbor through a hybrid operation that prioritizes item additions and replacements based on weighted marginal contributions, while adaptively removing low-contribution items and ensuring feasibility through strict capacity constraints. It balances exploration and exploitation by using dynamic thresholds and probabilistic operations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution based on combined objective contribution and frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n        # Calculate combined objective contribution\n        combined_contribution = objectives[:, 0] + objectives[:, 1]\n        # Select solution with highest combined contribution and frontier proximity\n        selection_scores = combined_contribution * (1 + 0.2 * frontier_proximity)\n        selected_idx = np.argmax(selection_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid operation: Add, Replace, or Remove based on weighted marginal contribution\n    available_items = np.where(base_solution == 0)[0]\n    included_items = np.where(base_solution == 1)[0]\n\n    if len(available_items) > 0:\n        # Weighted marginal value calculation for addition\n        alpha = 0.5  # Equal weight for both objectives\n        marginal_add = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_add_items = available_items[np.argsort(marginal_add)[::-1]]\n\n        # Try to add top items\n        for item in sorted_add_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution for removal\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        # Remove low-contribution items and replace with best available\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold and np.random.rand() < 0.7:  # 70% chance to replace\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal_replace = (0.5 * value1_lst[available_items] + 0.5 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal_replace)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n",
        "score": [
            -0.29822559319181124,
            4.670660555362701
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution based on combined objective contribution and frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n        # Calculate combined objective contribution\n        combined_contribution = objectives[:, 0] + objectives[:, 1]\n        # Select solution with highest combined contribution and frontier proximity\n        selection_scores = combined_contribution * (1 + 0.2 * frontier_proximity)\n        selected_idx = np.argmax(selection_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid operation: Add, Replace, or Remove based on weighted marginal contribution\n    available_items = np.where(base_solution == 0)[0]\n    included_items = np.where(base_solution == 1)[0]\n\n    if len(available_items) > 0:\n        # Weighted marginal value calculation for addition\n        alpha = 0.5  # Equal weight for both objectives\n        marginal_add = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_add_items = available_items[np.argsort(marginal_add)[::-1]]\n\n        # Try to add top items\n        for item in sorted_add_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution for removal\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        # Remove low-contribution items and replace with best available\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold and np.random.rand() < 0.7:  # 70% chance to replace\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal_replace = (0.5 * value1_lst[available_items] + 0.5 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal_replace)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier by prioritizing high-frontier-proximity candidates, then generates neighbors through weighted marginal value insertion (favoring balanced objective contributions) and dynamic replacement of low-impact items (using a 25th-percentile threshold), while strictly enforcing capacity constraints. It balances exploration (frontier targeting) with exploitation (marginal prioritization and probabilistic replacement) to maintain diversity. The alpha parameter (0.6) slightly favors the first objective in marginal calculations, while the dynamic threshold adapts to solution quality.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 137,
        "algorithm": "The algorithm selects a solution near the Pareto frontier from the archive, then performs a two-phase local search: first removing low-contribution items and adding high-marginal-value items (prioritizing either objective with 70% probability), followed by balanced marginal insertion (60% probability). It dynamically adjusts replacement thresholds based on item contributions and ensures feasibility by strictly enforcing capacity constraints. The hybrid approach alternates between objective-specific and balanced operations while maintaining solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution1 = value1_lst[included_items] / total_value1\n        contribution2 = value2_lst[included_items] / total_value2\n        combined_contribution = (contribution1 + contribution2) / 2\n\n        threshold1 = np.percentile(contribution1, 30)\n        threshold2 = np.percentile(contribution2, 30)\n        combined_threshold = np.percentile(combined_contribution, 25)\n\n        for i, item in enumerate(included_items):\n            if contribution1[i] < threshold1 or contribution2[i] < threshold2 or combined_contribution[i] < combined_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    combined_marginal = (marginal1 + marginal2) / 2\n\n                    if np.random.rand() < 0.7:\n                        best_item = available_items[np.argmax(marginal1)]\n                    else:\n                        best_item = available_items[np.argmax(marginal2)]\n\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        if np.random.rand() < 0.6:\n            sorted_items = available_items[np.argsort(marginal1)[::-1]]\n        else:\n            sorted_items = available_items[np.argsort(marginal2)[::-1]]\n\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.592034071592212,
            1.5089989602565765
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution1 = value1_lst[included_items] / total_value1\n        contribution2 = value2_lst[included_items] / total_value2\n        combined_contribution = (contribution1 + contribution2) / 2\n\n        threshold1 = np.percentile(contribution1, 30)\n        threshold2 = np.percentile(contribution2, 30)\n        combined_threshold = np.percentile(combined_contribution, 25)\n\n        for i, item in enumerate(included_items):\n            if contribution1[i] < threshold1 or contribution2[i] < threshold2 or combined_contribution[i] < combined_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    combined_marginal = (marginal1 + marginal2) / 2\n\n                    if np.random.rand() < 0.7:\n                        best_item = available_items[np.argmax(marginal1)]\n                    else:\n                        best_item = available_items[np.argmax(marginal2)]\n\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        if np.random.rand() < 0.6:\n            sorted_items = available_items[np.argsort(marginal1)[::-1]]\n        else:\n            sorted_items = available_items[np.argsort(marginal2)[::-1]]\n\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution near the Pareto frontier by prioritizing high-frontier-proximity candidates, then generates neighbors through weighted marginal value insertion (favoring balanced objective contributions) and dynamic replacement of low-impact items (using a 25th-percentile threshold), while strictly enforcing capacity constraints. It balances exploration (frontier targeting) with exploitation (marginal prioritization and probabilistic replacement) to maintain diversity. The alpha parameter (0.6) slightly favors the first objective in marginal calculations, while the dynamic threshold adapts to solution quality.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 138,
        "algorithm": "The algorithm selects a solution from the archive using a diversity-aware metric that balances frontier proximity and crowding distance, then performs an adaptive local search by iteratively adjusting objective weights and swapping items based on their marginal contribution to the less dominant objective while ensuring feasibility. It prioritizes items with higher combined value-to-weight ratios, considering both objectives dynamically, and ensures solutions remain feasible by checking weight constraints during swaps.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate crowding distance\n        sorted_indices = np.argsort(norm_obj[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_indices[i]] = norm_obj[sorted_indices[i+1], 0] - norm_obj[sorted_indices[i-1], 0]\n\n        # Combine with frontier proximity\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n        selection_metric = 0.7 * frontier_proximity + 0.3 * crowding\n\n        selected_idx = np.argmax(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive objective balancing local search\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate dominance ratio for each objective\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        dominance_ratio = total_value1 / (total_value2 + 1e-6)\n\n        # Adjust objective weights based on dominance\n        if dominance_ratio > 1.2:\n            alpha = 0.3  # More weight to objective 2\n        elif dominance_ratio < 0.8:\n            alpha = 0.7  # More weight to objective 1\n        else:\n            alpha = 0.5  # Balanced\n\n        # Calculate marginal contribution considering both objectives\n        marginal = (alpha * value1_lst + (1-alpha) * value2_lst) / (weight_lst + 1e-6)\n\n        # Find items to potentially replace\n        candidate_items = included_items[np.argsort(marginal[included_items])[:max(1, len(included_items)//4)]]\n\n        for item in candidate_items:\n            # Remove the item\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n            # Find best replacement item\n            available_items = np.where(new_solution == 0)[0]\n            if len(available_items) > 0:\n                replacement_marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                best_replacement = available_items[np.argmax(replacement_marginal)]\n\n                if current_weight + weight_lst[best_replacement] <= capacity:\n                    new_solution[best_replacement] = 1\n                    current_weight += weight_lst[best_replacement]\n                else:\n                    # If replacement not possible, put item back\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.45797169289387407,
            6.299515217542648
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate crowding distance\n        sorted_indices = np.argsort(norm_obj[:, 0])\n        crowding = np.zeros(len(archive))\n        crowding[sorted_indices[0]] = crowding[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding[sorted_indices[i]] = norm_obj[sorted_indices[i+1], 0] - norm_obj[sorted_indices[i-1], 0]\n\n        # Combine with frontier proximity\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n        selection_metric = 0.7 * frontier_proximity + 0.3 * crowding\n\n        selected_idx = np.argmax(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive objective balancing local search\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate dominance ratio for each objective\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        dominance_ratio = total_value1 / (total_value2 + 1e-6)\n\n        # Adjust objective weights based on dominance\n        if dominance_ratio > 1.2:\n            alpha = 0.3  # More weight to objective 2\n        elif dominance_ratio < 0.8:\n            alpha = 0.7  # More weight to objective 1\n        else:\n            alpha = 0.5  # Balanced\n\n        # Calculate marginal contribution considering both objectives\n        marginal = (alpha * value1_lst + (1-alpha) * value2_lst) / (weight_lst + 1e-6)\n\n        # Find items to potentially replace\n        candidate_items = included_items[np.argsort(marginal[included_items])[:max(1, len(included_items)//4)]]\n\n        for item in candidate_items:\n            # Remove the item\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n            # Find best replacement item\n            available_items = np.where(new_solution == 0)[0]\n            if len(available_items) > 0:\n                replacement_marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                best_replacement = available_items[np.argmax(replacement_marginal)]\n\n                if current_weight + weight_lst[best_replacement] <= capacity:\n                    new_solution[best_replacement] = 1\n                    current_weight += weight_lst[best_replacement]\n                else:\n                    # If replacement not possible, put item back\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution near the Pareto frontier by prioritizing high-frontier-proximity candidates, then generates neighbors through weighted marginal value insertion (favoring balanced objective contributions) and dynamic replacement of low-impact items (using a 25th-percentile threshold), while strictly enforcing capacity constraints. It balances exploration (frontier targeting) with exploitation (marginal prioritization and probabilistic replacement) to maintain diversity. The alpha parameter (0.6) slightly favors the first objective in marginal calculations, while the dynamic threshold adapts to solution quality.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 139,
        "algorithm": "The algorithm selects a solution from the archive using crowding distance to promote diversity, then generates neighbors by prioritizing items with balanced marginal value contributions (product of value1 and value2 divided by weight) and adaptively removing low-contribution items (bottom 10% percentile) while ensuring feasibility. The method combines marginal value prioritization with adaptive replacement to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        sorted_obj = objectives[sorted_indices]\n        crowding = np.zeros(len(sorted_obj))\n        crowding[1:-1] = (sorted_obj[2:, 0] - sorted_obj[:-2, 0]) / (sorted_obj[-1, 0] - sorted_obj[0, 0] + 1e-6)\n        crowding[1:-1] += (sorted_obj[2:, 1] - sorted_obj[:-2, 1]) / (sorted_obj[-1, 1] - sorted_obj[0, 1] + 1e-6)\n        # Select solution with highest crowding distance\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Marginal value insertion with balanced prioritization\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Balanced marginal value calculation\n        marginal = (value1_lst[available_items] * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Adaptive replacement based on contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized contribution\n        contribution = (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst[included_items]) + np.sum(value2_lst[included_items]) + 1e-6)\n        adaptive_threshold = np.percentile(contribution, 10)  # Replace bottom 10%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (value1_lst[available_items] + value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7503408929086643,
            3.610493451356888
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection based on crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        sorted_indices = np.argsort(objectives[:, 0])\n        sorted_obj = objectives[sorted_indices]\n        crowding = np.zeros(len(sorted_obj))\n        crowding[1:-1] = (sorted_obj[2:, 0] - sorted_obj[:-2, 0]) / (sorted_obj[-1, 0] - sorted_obj[0, 0] + 1e-6)\n        crowding[1:-1] += (sorted_obj[2:, 1] - sorted_obj[:-2, 1]) / (sorted_obj[-1, 1] - sorted_obj[0, 1] + 1e-6)\n        # Select solution with highest crowding distance\n        selected_idx = sorted_indices[np.argmax(crowding)]\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Marginal value insertion with balanced prioritization\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Balanced marginal value calculation\n        marginal = (value1_lst[available_items] * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Adaptive replacement based on contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized contribution\n        contribution = (value1_lst[included_items] + value2_lst[included_items]) / (np.sum(value1_lst[included_items]) + np.sum(value2_lst[included_items]) + 1e-6)\n        adaptive_threshold = np.percentile(contribution, 10)  # Replace bottom 10%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (value1_lst[available_items] + value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution near the Pareto frontier by prioritizing high-frontier-proximity candidates, then generates neighbors through weighted marginal value insertion (favoring balanced objective contributions) and dynamic replacement of low-impact items (using a 25th-percentile threshold), while strictly enforcing capacity constraints. It balances exploration (frontier targeting) with exploitation (marginal prioritization and probabilistic replacement) to maintain diversity. The alpha parameter (0.6) slightly favors the first objective in marginal calculations, while the dynamic threshold adapts to solution quality.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive selection based on Pareto-frontier proximity\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Select solution with highest frontier proximity (near the frontier)\n        selected_idx = np.argmax(frontier_proximity)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic replacement\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.6  # Slightly more balanced weight for both objectives\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic replacement based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 25)  # Replace bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal = (0.6 * value1_lst[available_items] + 0.4 * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a dominance-aware metric that combines crowding distance and diversity, then applies a hybrid local search that adaptively inserts items based on value-weight ratios and probabilistically removes items based on multi-criteria impact, ensuring feasibility through dynamic constraint handling. It prioritizes items with high combined value-to-weight ratios for insertion and removes items with low value impact and high weight impact with higher probability.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel dominance-aware selection metric\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objective values\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate dominance score (combination of crowding and diversity)\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(norm_obj[:, i])\n            crowding[sorted_idx[0]] += 1\n            crowding[sorted_idx[-1]] += 1\n            for j in range(1, len(objectives)-1):\n                crowding[sorted_idx[j]] += (norm_obj[sorted_idx[j+1], i] - norm_obj[sorted_idx[j-1], i])\n\n        # Calculate diversity score (distance to k-nearest neighbors)\n        diversity = np.zeros(len(objectives))\n        k = min(3, len(objectives)-1)\n        for i in range(len(objectives)):\n            distances = np.linalg.norm(norm_obj - norm_obj[i], axis=1)\n            diversity[i] = np.mean(np.sort(distances)[1:k+1])\n\n        # Combined selection metric\n        selection_metric = crowding + 0.5 * diversity\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Adaptive value-weight ratio insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive ratios\n        weight_ratio = weight_lst[available_items] / capacity\n        value_ratio1 = value1_lst[available_items] / (np.max(value1_lst) + 1e-6)\n        value_ratio2 = value2_lst[available_items] / (np.max(value2_lst) + 1e-6)\n\n        # Combined score with adaptive weights\n        combined_score = (value_ratio1 + value_ratio2) / (weight_ratio + 1e-6)\n\n        # Sort and select top candidates\n        sorted_items = available_items[np.argsort(combined_score)[::-1]]\n        for item in sorted_items[:3]:  # Consider top 3 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Probabilistic removal with multi-criteria impact\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate multi-criteria impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        weight_impact = weight_lst[included_items] / capacity\n\n        # Combined removal probability\n        removal_prob = (1 - value_impact) * 0.4 + weight_impact * 0.6\n\n        for i, item in enumerate(included_items):\n            if np.random.rand() < removal_prob[i]:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 140,
        "algorithm": "The algorithm first selects a promising solution from the archive using a hybrid metric combining frontier proximity to the ideal point (70% weight) and crowding distance (30% weight), then applies a multi-step local search: 1) greedily adds top items based on combined marginal value (60% value1, 40% value2), 2) adaptively removes low-impact items (bottom 30% percentile) and replaces them with better alternatives, and 3) probabilistically removes low-impact items with adjusted probabilities. The method prioritizes value2 slightly more than value1 in marginal calculations and balances exploration/exploitation through dynamic thresholds and probabilistic removals.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining frontier proximity and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                             (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n\n        # Combined selection metric (favoring frontier proximity but also considering diversity)\n        selection_metric = 0.7 * frontier_proximity + 0.3 * (1 / (crowding + 1e-6))\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion with adaptive weights\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (0.6 * marginal1 + 0.4 * marginal2) / (weight_lst[available_items] + 1e-6)\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Adaptive threshold-based replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined value impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Dynamic threshold (bottom 30% percentile)\n        dynamic_threshold = np.percentile(value_impact, 30)\n\n        for i, item in enumerate(included_items):\n            if value_impact[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    combined_marginal = (0.6 * marginal1 + 0.4 * marginal2) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(combined_marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized value products\n        value_products = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Remove items with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - value_products[i]) * 0.4:  # 40% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.3847496400326914,
            1.138797253370285
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection metric combining frontier proximity and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Normalize objectives\n        norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier proximity (distance to ideal point)\n        ideal_point = np.ones(2)\n        frontier_proximity = np.linalg.norm(norm_obj - ideal_point, axis=1)\n\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                             (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n\n        # Combined selection metric (favoring frontier proximity but also considering diversity)\n        selection_metric = 0.7 * frontier_proximity + 0.3 * (1 / (crowding + 1e-6))\n        selected_idx = np.argmin(selection_metric)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion with adaptive weights\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate adaptive marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (0.6 * marginal1 + 0.4 * marginal2) / (weight_lst[available_items] + 1e-6)\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items[:2]:  # Consider top 2 candidates\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Adaptive threshold-based replacement\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined value impact\n        value_impact = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                      (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Dynamic threshold (bottom 30% percentile)\n        dynamic_threshold = np.percentile(value_impact, 30)\n\n        for i, item in enumerate(included_items):\n            if value_impact[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n                # Try to replace with best available item\n                available_items = np.where(new_solution == 0)[0]\n                if len(available_items) > 0:\n                    marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n                    combined_marginal = (0.6 * marginal1 + 0.4 * marginal2) / (weight_lst[available_items] + 1e-6)\n                    best_item = available_items[np.argmax(combined_marginal)]\n                    if current_weight + weight_lst[best_item] <= capacity:\n                        new_solution[best_item] = 1\n                        current_weight += weight_lst[best_item]\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized value products\n        value_products = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                        (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n\n        # Remove items with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - value_products[i]) * 0.4:  # 40% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 9 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 141,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined value ratio and diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_ratio = normalized_obj[:, 0] + normalized_obj[:, 1]\n    diversity = np.sum(np.abs(solutions - solutions.mean(axis=0)), axis=1)\n    scores = combined_ratio * (1 + diversity/len(weight_lst))\n    selected_idx = np.argmax(scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholding\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Phase 1: Add high-value items with probabilistic selection\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            if np.random.rand() < 0.7:  # Higher probability for high-value items\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Remove low-value items with adaptive threshold\n    for item in included:\n        if (value1_lst[item] < med_value1 * 0.8 or\n            value2_lst[item] < med_value2 * 0.8):\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Pareto-aware swaps with dynamic threshold\n    threshold = 0.4 if len(archive) > 5 else 0.6\n    if np.random.random() < threshold and len(included) > 0 and len(excluded) > 0:\n        # Select items for swap based on marginal improvements\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        if np.any(combined_marginal > 0):\n            best_add = excluded[np.argmax(combined_marginal)]\n            worst_remove = included[np.argmin((value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6))]\n\n            if (current_weight - weight_lst[worst_remove] + weight_lst[best_add] <= capacity):\n                new_solution[worst_remove] = 0\n                new_solution[best_add] = 1\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined value ratio\n        over_items = np.where(new_solution == 1)[0]\n        combined_ratio = (value1_lst[over_items] + value2_lst[over_items]) / (weight_lst[over_items] + 1e-6)\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8479435725186255,
            0.5349401235580444
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined value ratio and diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_ratio = normalized_obj[:, 0] + normalized_obj[:, 1]\n    diversity = np.sum(np.abs(solutions - solutions.mean(axis=0)), axis=1)\n    scores = combined_ratio * (1 + diversity/len(weight_lst))\n    selected_idx = np.argmax(scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholding\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Phase 1: Add high-value items with probabilistic selection\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            if np.random.rand() < 0.7:  # Higher probability for high-value items\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Remove low-value items with adaptive threshold\n    for item in included:\n        if (value1_lst[item] < med_value1 * 0.8 or\n            value2_lst[item] < med_value2 * 0.8):\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Pareto-aware swaps with dynamic threshold\n    threshold = 0.4 if len(archive) > 5 else 0.6\n    if np.random.random() < threshold and len(included) > 0 and len(excluded) > 0:\n        # Select items for swap based on marginal improvements\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        if np.any(combined_marginal > 0):\n            best_add = excluded[np.argmax(combined_marginal)]\n            worst_remove = included[np.argmin((value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6))]\n\n            if (current_weight - weight_lst[worst_remove] + weight_lst[best_add] <= capacity):\n                new_solution[worst_remove] = 0\n                new_solution[best_add] = 1\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined value ratio\n        over_items = np.where(new_solution == 1)[0]\n        combined_ratio = (value1_lst[over_items] + value2_lst[over_items]) / (weight_lst[over_items] + 1e-6)\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 142,
        "algorithm": "The algorithm selects a solution from the archive with the highest Pareto dominance (most extreme in either objective) and applies a multi-phase local search: first removing non-dominant items, then adding high-impact dominant items, followed by capacity-aware swaps, and finally refining the solution to lie on or near the Pareto frontier by removing items that least contribute to the trade-off between objectives. The method prioritizes items where one objective significantly outperforms the other, dynamically shifts between removal and addition phases, and ensures feasibility through probabilistic swaps and frontier-focused refinement.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest Pareto dominance (most extreme in either objective)\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    dominance_scores = np.max(normalized_obj, axis=1) - np.min(normalized_obj, axis=1)\n    selected_idx = np.argmax(dominance_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Value-dominance identification (items where one objective significantly outperforms the other)\n    ratio1 = value1_lst / (weight_lst + 1e-6)\n    ratio2 = value2_lst / (weight_lst + 1e-6)\n    dominance1 = ratio1 > 1.5 * ratio2\n    dominance2 = ratio2 > 1.5 * ratio1\n\n    # Phase 2: Dynamic removal of non-dominant items\n    included_items = np.where(new_solution == 1)[0]\n    for item in included_items:\n        if not (dominance1[item] or dominance2[item]):\n            if np.random.rand() < 0.7:  # Higher probability for non-dominant items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: High-impact addition of dominant items\n    excluded_items = np.where(new_solution == 0)[0]\n    for item in excluded_items:\n        if (dominance1[item] or dominance2[item]) and (current_weight + weight_lst[item] <= capacity):\n            if np.random.rand() < 0.8:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 4: Capacity-aware probabilistic swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                if (dominance1[j] and not dominance1[i]) or (dominance2[j] and not dominance2[i]):\n                    if np.random.rand() < 0.5:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 5: Frontier-focused refinement\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items that least contribute to the frontier\n        frontier_scores = (ratio1 + ratio2) / (np.abs(ratio1 - ratio2) + 1e-6)\n        worst_item = included_items[np.argmin(frontier_scores[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8762862304337273,
            1.761882096529007
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest Pareto dominance (most extreme in either objective)\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    dominance_scores = np.max(normalized_obj, axis=1) - np.min(normalized_obj, axis=1)\n    selected_idx = np.argmax(dominance_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Value-dominance identification (items where one objective significantly outperforms the other)\n    ratio1 = value1_lst / (weight_lst + 1e-6)\n    ratio2 = value2_lst / (weight_lst + 1e-6)\n    dominance1 = ratio1 > 1.5 * ratio2\n    dominance2 = ratio2 > 1.5 * ratio1\n\n    # Phase 2: Dynamic removal of non-dominant items\n    included_items = np.where(new_solution == 1)[0]\n    for item in included_items:\n        if not (dominance1[item] or dominance2[item]):\n            if np.random.rand() < 0.7:  # Higher probability for non-dominant items\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: High-impact addition of dominant items\n    excluded_items = np.where(new_solution == 0)[0]\n    for item in excluded_items:\n        if (dominance1[item] or dominance2[item]) and (current_weight + weight_lst[item] <= capacity):\n            if np.random.rand() < 0.8:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 4: Capacity-aware probabilistic swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                if (dominance1[j] and not dominance1[i]) or (dominance2[j] and not dominance2[i]):\n                    if np.random.rand() < 0.5:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 5: Frontier-focused refinement\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items that least contribute to the frontier\n        frontier_scores = (ratio1 + ratio2) / (np.abs(ratio1 - ratio2) + 1e-6)\n        worst_item = included_items[np.argmin(frontier_scores[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest combined value ratio and diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    combined_ratio = normalized_obj[:, 0] + normalized_obj[:, 1]\n    diversity = np.sum(np.abs(solutions - solutions.mean(axis=0)), axis=1)\n    scores = combined_ratio * (1 + diversity/len(weight_lst))\n    selected_idx = np.argmax(scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholding\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate dynamic thresholds\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Phase 1: Add high-value items with probabilistic selection\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            if np.random.rand() < 0.7:  # Higher probability for high-value items\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Remove low-value items with adaptive threshold\n    for item in included:\n        if (value1_lst[item] < med_value1 * 0.8 or\n            value2_lst[item] < med_value2 * 0.8):\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Phase 3: Pareto-aware swaps with dynamic threshold\n    threshold = 0.4 if len(archive) > 5 else 0.6\n    if np.random.random() < threshold and len(included) > 0 and len(excluded) > 0:\n        # Select items for swap based on marginal improvements\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        if np.any(combined_marginal > 0):\n            best_add = excluded[np.argmax(combined_marginal)]\n            worst_remove = included[np.argmin((value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6))]\n\n            if (current_weight - weight_lst[worst_remove] + weight_lst[best_add] <= capacity):\n                new_solution[worst_remove] = 0\n                new_solution[best_add] = 1\n\n    # Final capacity check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined value ratio\n        over_items = np.where(new_solution == 1)[0]\n        combined_ratio = (value1_lst[over_items] + value2_lst[over_items]) / (weight_lst[over_items] + 1e-6)\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 143,
        "algorithm": "The algorithm combines frontier-proximity selection with a three-phase hybrid local search: Phase 1 adds high-impact items using adaptive percentile thresholds, Phase 2 performs Pareto-aware swaps based on marginal improvements, and Phase 3 ensures feasibility by removing low-impact items while preserving capacity. It prioritizes objective diversity and frontier solutions, with probabilistic operations to refine solutions further.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest frontier proximity and objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    frontier_scores = np.max(normalized_obj, axis=1) * (1 - np.min(normalized_obj, axis=1))\n    diversity = np.sum(np.abs(solutions - solutions.mean(axis=0)), axis=1)\n    selection_scores = frontier_scores * (1 + diversity/len(weight_lst))\n    selected_idx = np.argmax(selection_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Add high-impact items with adaptive percentile thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        p1, p2 = np.percentile(value1_lst[included], 30), np.percentile(value2_lst[included], 30)\n        w_p = np.percentile(weight_lst[included], 40)\n    else:\n        p1, p2, w_p = 0, 0, 0\n\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > p1 and value2_lst[item] > p2 and\n            weight_lst[item] <= w_p * 1.2):\n            if np.random.rand() < 0.6:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps with dynamic thresholds\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate marginal improvements\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Find best candidate to add and worst to remove\n        if np.any(combined_marginal > 0):\n            best_add = excluded[np.argmax(combined_marginal)]\n            worst_remove = included[np.argmin((value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6))]\n\n            if (current_weight - weight_lst[worst_remove] + weight_lst[best_add] <= capacity):\n                new_solution[worst_remove] = 0\n                new_solution[best_add] = 1\n                current_weight = current_weight - weight_lst[worst_remove] + weight_lst[best_add]\n\n    # Phase 3: Capacity-preserving replacements with frontier-aware selection\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest combined normalized objective\n        combined_obj = (value1_lst[included_items] / value1_lst.max() + value2_lst[included_items] / value2_lst.max()) / 2\n        worst_item = included_items[np.argmin(combined_obj)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Final probabilistic flip of low-impact items\n    if np.random.rand() < 0.3 and len(included) > 0:\n        low_impact_items = np.where((value1_lst < p1 * 0.7) | (value2_lst < p2 * 0.7))[0]\n        if len(low_impact_items) > 0:\n            flip_item = np.random.choice(low_impact_items)\n            if new_solution[flip_item] == 1 and current_weight - weight_lst[flip_item] <= capacity:\n                new_solution[flip_item] = 0\n            elif new_solution[flip_item] == 0 and current_weight + weight_lst[flip_item] <= capacity:\n                new_solution[flip_item] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7176574987783993,
            6.642843246459961
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest frontier proximity and objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    frontier_scores = np.max(normalized_obj, axis=1) * (1 - np.min(normalized_obj, axis=1))\n    diversity = np.sum(np.abs(solutions - solutions.mean(axis=0)), axis=1)\n    selection_scores = frontier_scores * (1 + diversity/len(weight_lst))\n    selected_idx = np.argmax(selection_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 1: Add high-impact items with adaptive percentile thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        p1, p2 = np.percentile(value1_lst[included], 30), np.percentile(value2_lst[included], 30)\n        w_p = np.percentile(weight_lst[included], 40)\n    else:\n        p1, p2, w_p = 0, 0, 0\n\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > p1 and value2_lst[item] > p2 and\n            weight_lst[item] <= w_p * 1.2):\n            if np.random.rand() < 0.6:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps with dynamic thresholds\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate marginal improvements\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Find best candidate to add and worst to remove\n        if np.any(combined_marginal > 0):\n            best_add = excluded[np.argmax(combined_marginal)]\n            worst_remove = included[np.argmin((value1_lst[included] + value2_lst[included]) / (weight_lst[included] + 1e-6))]\n\n            if (current_weight - weight_lst[worst_remove] + weight_lst[best_add] <= capacity):\n                new_solution[worst_remove] = 0\n                new_solution[best_add] = 1\n                current_weight = current_weight - weight_lst[worst_remove] + weight_lst[best_add]\n\n    # Phase 3: Capacity-preserving replacements with frontier-aware selection\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest combined normalized objective\n        combined_obj = (value1_lst[included_items] / value1_lst.max() + value2_lst[included_items] / value2_lst.max()) / 2\n        worst_item = included_items[np.argmin(combined_obj)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Final probabilistic flip of low-impact items\n    if np.random.rand() < 0.3 and len(included) > 0:\n        low_impact_items = np.where((value1_lst < p1 * 0.7) | (value2_lst < p2 * 0.7))[0]\n        if len(low_impact_items) > 0:\n            flip_item = np.random.choice(low_impact_items)\n            if new_solution[flip_item] == 1 and current_weight - weight_lst[flip_item] <= capacity:\n                new_solution[flip_item] = 0\n            elif new_solution[flip_item] == 0 and current_weight + weight_lst[flip_item] <= capacity:\n                new_solution[flip_item] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 144,
        "algorithm": "The algorithm combines dynamic objective weighting with multi-phase local search, prioritizing high-marginal-impact items while ensuring feasibility through capacity-aware operations. It first selects a promising base solution, then uses weighted marginal impacts to greedily add items, probabilistically swaps items based on impact differences, and finally performs capacity adjustments and high-impact flips to refine the solution. The dynamic weights and probabilistic operations help balance exploration and exploitation while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high diversity potential (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic objective weights based on current archive distribution\n    obj1_weights = np.array([s[1][0] for s in archive])\n    obj2_weights = np.array([s[1][1] for s in archive])\n    avg_obj1 = np.mean(obj1_weights)\n    avg_obj2 = np.mean(obj2_weights)\n    obj1_ratio = avg_obj1 / (avg_obj1 + avg_obj2 + 1e-6)\n    obj2_ratio = avg_obj2 / (avg_obj1 + avg_obj2 + 1e-6)\n\n    # Calculate weighted marginal impact with dynamic objective weights\n    marginal_impact = (obj1_ratio * value1_lst + obj2_ratio * value2_lst) / (weight_lst + 1e-6)\n\n    # Phase 1: Capacity-aware greedy additions (top 30% marginal impact)\n    top_impact_items = np.argsort(marginal_impact)[::-1][:max(1, len(marginal_impact) // 3)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps with capacity-aware thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            weight_diff = weight_lst[j] - weight_lst[i]\n            if abs(weight_diff) <= remaining_capacity:\n                # Probabilistic swap based on marginal impact difference\n                impact_diff = marginal_impact[j] - marginal_impact[i]\n                swap_prob = min(1.0, max(0.1, impact_diff / (np.max(marginal_impact) + 1e-6)))\n                if np.random.rand() < swap_prob:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += weight_diff\n                    remaining_capacity -= weight_diff\n                    break\n\n    # Phase 3: Multi-phase capacity adjustment with randomized replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact with probability\n        for idx in included_items:\n            removal_prob = (np.max(marginal_impact) - marginal_impact[idx]) / (np.max(marginal_impact) + 1e-6)\n            if np.random.rand() < removal_prob:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                break\n\n    # Phase 4: High-impact item flips with capacity constraints\n    high_impact_items = np.argsort(marginal_impact)[::-1][:max(1, len(marginal_impact) // 4)]\n    num_to_flip = min(3, len(high_impact_items))\n    flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.5759009695301351,
            6.504892826080322
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high diversity potential (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic objective weights based on current archive distribution\n    obj1_weights = np.array([s[1][0] for s in archive])\n    obj2_weights = np.array([s[1][1] for s in archive])\n    avg_obj1 = np.mean(obj1_weights)\n    avg_obj2 = np.mean(obj2_weights)\n    obj1_ratio = avg_obj1 / (avg_obj1 + avg_obj2 + 1e-6)\n    obj2_ratio = avg_obj2 / (avg_obj1 + avg_obj2 + 1e-6)\n\n    # Calculate weighted marginal impact with dynamic objective weights\n    marginal_impact = (obj1_ratio * value1_lst + obj2_ratio * value2_lst) / (weight_lst + 1e-6)\n\n    # Phase 1: Capacity-aware greedy additions (top 30% marginal impact)\n    top_impact_items = np.argsort(marginal_impact)[::-1][:max(1, len(marginal_impact) // 3)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Probabilistic swaps with capacity-aware thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            weight_diff = weight_lst[j] - weight_lst[i]\n            if abs(weight_diff) <= remaining_capacity:\n                # Probabilistic swap based on marginal impact difference\n                impact_diff = marginal_impact[j] - marginal_impact[i]\n                swap_prob = min(1.0, max(0.1, impact_diff / (np.max(marginal_impact) + 1e-6)))\n                if np.random.rand() < swap_prob:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight += weight_diff\n                    remaining_capacity -= weight_diff\n                    break\n\n    # Phase 3: Multi-phase capacity adjustment with randomized replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest marginal impact with probability\n        for idx in included_items:\n            removal_prob = (np.max(marginal_impact) - marginal_impact[idx]) / (np.max(marginal_impact) + 1e-6)\n            if np.random.rand() < removal_prob:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n                break\n\n    # Phase 4: High-impact item flips with capacity constraints\n    high_impact_items = np.argsort(marginal_impact)[::-1][:max(1, len(marginal_impact) // 4)]\n    num_to_flip = min(3, len(high_impact_items))\n    flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n\n    for idx in flip_indices:\n        if new_solution[idx] == 1:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        else:\n            if current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 145,
        "algorithm": "This algorithm selects a promising solution from the top 30% of the archive (prioritizing solutions with higher combined objective values) and applies a hybrid local search strategy combining dynamic item additions, adaptive swaps, capacity-preserving replacements, and probabilistic flips. It prioritizes high-marginal-impact items for additions while using different thresholds and probabilities for swaps, replacements, and flips to balance both objectives. The method ensures feasibility by dynamically adjusting the solution to respect the weight capacity throughout the process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 * 0.7 + marginal_impact2 * 0.3)\n\n    # Phase 1: Dynamic high-impact additions (top 30% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 3)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 3)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items with different probabilities\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with different probabilities\n                if ((marginal_impact1[j] > marginal_impact1[i] * 1.2 and marginal_impact2[j] > marginal_impact2[i] * 1.2) or\n                    (marginal_impact1[j] > marginal_impact1[i] * 1.1 and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] * 1.1 and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with different thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact with different threshold\n        worst_item = included_items[np.argmin(combined_impact[included_items] * 0.9)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8858361763151679,
            1.4557276368141174
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] * 0.6 + x[1][1] * 0.4), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 * 0.7 + marginal_impact2 * 0.3)\n\n    # Phase 1: Dynamic high-impact additions (top 30% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 3)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 3)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items with different probabilities\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with different probabilities\n                if ((marginal_impact1[j] > marginal_impact1[i] * 1.2 and marginal_impact2[j] > marginal_impact2[i] * 1.2) or\n                    (marginal_impact1[j] > marginal_impact1[i] * 1.1 and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] * 1.1 and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with different thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact with different threshold\n        worst_item = included_items[np.argmin(combined_impact[included_items] * 0.9)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Dynamic high-impact additions (top 25% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 4)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 4)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.7) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(2, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined objective value) and applies a three-phase hybrid local search: (1) adds high-impact items (top 20% marginal impact) if feasible, (2) performs probabilistic swaps between included and excluded items based on marginal impact improvement, and (3) removes low-impact items if the solution exceeds capacity. It prioritizes items with high combined marginal impact for both objectives while maintaining feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact (combining both objectives)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]\n\n    # Phase 1: Dynamic high-impact additions (top 20% marginal impact)\n    top_impact_items = sorted_indices[:max(1, len(sorted_indices) // 5)]\n    remaining_capacity = capacity - current_weight\n\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Weight-balanced swaps (targeted high-impact replacements)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Probabilistic swap based on combined impact improvement\n                if (marginal_impact[j] > marginal_impact[i] and\n                    np.random.rand() < 0.5):  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements (if still over capacity)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 146,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9747520347546502,
            1.6449489295482635
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 9 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 147,
        "algorithm": "The algorithm selects a solution near the Pareto frontier from the archive, then applies a hybrid local search that first adds high-marginal-value items probabilistically and subsequently performs Pareto-aware swaps with dynamic probability thresholds, while ensuring feasibility through capacity-constrained operations. It prioritizes items with combined marginal value ratios and adjusts swap probabilities based on current capacity utilization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near the Pareto frontier using dynamic criterion\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objectives and find frontier solutions\n        normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        frontier_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced frontier\n        selected_idx = np.argmax(frontier_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate marginal value ratios\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Hybrid local search\n    # Phase 1: Adaptive item additions\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        # Add top items with probability based on marginal value\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, combined_marginal[item] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps with dynamic probability\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate dynamic swap probability based on current state\n        swap_prob = 0.4 + 0.3 * (current_weight / capacity)  # Higher probability when near capacity\n        if np.random.rand() < swap_prob:\n            # Select items for swap based on Pareto dominance\n            for i in included_items:\n                for j in excluded_items:\n                    if weight_lst[j] <= current_weight - weight_lst[i] + weight_lst[j]:\n                        # Check for Pareto improvement\n                        if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                           (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Ensure feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined marginal value\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(combined_marginal[included_items])]\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.2859523095756374,
            3.777890145778656
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near the Pareto frontier using dynamic criterion\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objectives and find frontier solutions\n        normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n        frontier_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced frontier\n        selected_idx = np.argmax(frontier_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate marginal value ratios\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Hybrid local search\n    # Phase 1: Adaptive item additions\n    available_items = np.where(new_solution == 0)[0]\n    if len(available_items) > 0:\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal[available_items])[::-1]]\n        # Add top items with probability based on marginal value\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, combined_marginal[item] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps with dynamic probability\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate dynamic swap probability based on current state\n        swap_prob = 0.4 + 0.3 * (current_weight / capacity)  # Higher probability when near capacity\n        if np.random.rand() < swap_prob:\n            # Select items for swap based on Pareto dominance\n            for i in included_items:\n                for j in excluded_items:\n                    if weight_lst[j] <= current_weight - weight_lst[i] + weight_lst[j]:\n                        # Check for Pareto improvement\n                        if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                           (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Ensure feasibility\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest combined marginal value\n        included_items = np.where(new_solution == 1)[0]\n        sorted_items = included_items[np.argsort(combined_marginal[included_items])]\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values and diversity, then generates a neighbor through three phases: adaptive addition of high-marginal-impact items, capacity-constrained hybrid swaps/replacements, and dynamic threshold-based rebalancing to maintain feasibility while optimizing both objectives. It prioritizes items with strong combined marginal impacts and performs targeted swaps to improve complementary objectives, while ensuring feasibility through iterative removal of low-utility items. The selection criterion favors solutions with higher combined objective scores and greater diversity, while the local search operations focus on marginal improvements and capacity-constrained transformations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 148,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid metric combining Pareto frontier proximity and diversity, then generates neighbors through three phases: adaptive addition of high-impact items, capacity-constrained item replacements prioritizing trade-offs between objectives, and iterative removal of low-utility items to maintain feasibility. The selection criterion balances both objectives and diversity, while the local search operations focus on dynamic marginal utility assessments and capacity-aware transformations to ensure high-quality, feasible neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto frontier proximity and diversity\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto frontier distance\n        dominated = False\n        for other_sol, other_obj in archive:\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if dominated:\n            continue\n\n        # Calculate diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Score combines objective values and diversity\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    if not archive_with_scores:\n        # Fallback to random selection if no non-dominated solutions\n        base_solution = archive[np.random.randint(0, len(archive))][0].copy()\n    else:\n        # Select solution with highest score\n        archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n        base_solution = archive_with_scores[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Adaptive addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Calculate objective-specific marginal utilities\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        # Set dynamic thresholds based on current solution's objective values\n        threshold1 = np.percentile(marginal1[candidates], 70)\n        threshold2 = np.percentile(marginal2[candidates], 70)\n\n        # Add items that exceed both thresholds\n        for idx in candidates:\n            if marginal1[idx] >= threshold1 and marginal2[idx] >= threshold2:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained item replacements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if replacement improves at least one objective\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Iterative removal of low-utility items\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores (harmonic mean of normalized values)\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            norm1 = value1_lst[idx] / (total_value1 + 1e-6)\n            norm2 = value2_lst[idx] / (total_value2 + 1e-6)\n            utility = 2 / ((1/(norm1 + 1e-6)) + (1/(norm2 + 1e-6)))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8027153254624773,
            2.732621490955353
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto frontier proximity and diversity\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto frontier distance\n        dominated = False\n        for other_sol, other_obj in archive:\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if dominated:\n            continue\n\n        # Calculate diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Score combines objective values and diversity\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    if not archive_with_scores:\n        # Fallback to random selection if no non-dominated solutions\n        base_solution = archive[np.random.randint(0, len(archive))][0].copy()\n    else:\n        # Select solution with highest score\n        archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n        base_solution = archive_with_scores[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Adaptive addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Calculate objective-specific marginal utilities\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n\n        # Set dynamic thresholds based on current solution's objective values\n        threshold1 = np.percentile(marginal1[candidates], 70)\n        threshold2 = np.percentile(marginal2[candidates], 70)\n\n        # Add items that exceed both thresholds\n        for idx in candidates:\n            if marginal1[idx] >= threshold1 and marginal2[idx] >= threshold2:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained item replacements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if replacement improves at least one objective\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Iterative removal of low-utility items\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores (harmonic mean of normalized values)\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            norm1 = value1_lst[idx] / (total_value1 + 1e-6)\n            norm2 = value2_lst[idx] / (total_value2 + 1e-6)\n            utility = 2 / ((1/(norm1 + 1e-6)) + (1/(norm2 + 1e-6)))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values and diversity, then generates a neighbor through three phases: adaptive addition of high-marginal-impact items, capacity-constrained hybrid swaps/replacements, and dynamic threshold-based rebalancing to maintain feasibility while optimizing both objectives. It prioritizes items with strong combined marginal impacts and performs targeted swaps to improve complementary objectives, while ensuring feasibility through iterative removal of low-utility items. The selection criterion favors solutions with higher combined objective scores and greater diversity, while the local search operations focus on marginal improvements and capacity-constrained transformations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 149,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Frontier-aware selection with dynamic quadrant emphasis\n    objectives = np.array([obj for _, obj in archive])\n    frontiers = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            frontiers[i] = 1\n\n    if np.sum(frontiers) > 0:\n        frontier_indices = np.where(frontiers == 1)[0]\n        selected_idx = np.random.choice(frontier_indices)\n    else:\n        # Fallback to diversity-aware selection if no frontier solutions\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n        selected_idx = np.random.choice(candidate_indices) if candidate_indices else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid frontier-aware operations with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Frontier-aware addition with dynamic utility\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        frontier_items = excluded[np.logical_and(\n            weight_lst[excluded] <= remaining_capacity,\n            (value1_lst[excluded] >= np.median(value1_lst)) | (value2_lst[excluded] >= np.median(value2_lst))\n        )]\n        if len(frontier_items) > 0:\n            utility = (value1_lst[frontier_items] + value2_lst[frontier_items]) / (weight_lst[frontier_items] + 1e-6)\n            top_items = frontier_items[np.argsort(utility)[-min(3, len(frontier_items)):]]\n            for item in top_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Phase 2: Capacity-constrained utility-based swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential utility gain\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                if (delta1 > 0 and delta2 >= 0) or (delta2 > 0 and delta1 >= 0):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (value1_lst[idx] + value2_lst[idx]) / (weight_lst[idx] + 1e-6)\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8553763105830408,
            3.5391503870487213
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Frontier-aware selection with dynamic quadrant emphasis\n    objectives = np.array([obj for _, obj in archive])\n    frontiers = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                          (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                dominated = True\n                break\n        if not dominated:\n            frontiers[i] = 1\n\n    if np.sum(frontiers) > 0:\n        frontier_indices = np.where(frontiers == 1)[0]\n        selected_idx = np.random.choice(frontier_indices)\n    else:\n        # Fallback to diversity-aware selection if no frontier solutions\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n        selected_idx = np.random.choice(candidate_indices) if candidate_indices else np.random.randint(len(archive))\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid frontier-aware operations with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Frontier-aware addition with dynamic utility\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        frontier_items = excluded[np.logical_and(\n            weight_lst[excluded] <= remaining_capacity,\n            (value1_lst[excluded] >= np.median(value1_lst)) | (value2_lst[excluded] >= np.median(value2_lst))\n        )]\n        if len(frontier_items) > 0:\n            utility = (value1_lst[frontier_items] + value2_lst[frontier_items]) / (weight_lst[frontier_items] + 1e-6)\n            top_items = frontier_items[np.argsort(utility)[-min(3, len(frontier_items)):]]\n            for item in top_items:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Phase 2: Capacity-constrained utility-based swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential utility gain\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                if (delta1 > 0 and delta2 >= 0) or (delta2 > 0 and delta1 >= 0):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (value1_lst[idx] + value2_lst[idx]) / (weight_lst[idx] + 1e-6)\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values and diversity, then generates a neighbor through three phases: adaptive addition of high-marginal-impact items, capacity-constrained hybrid swaps/replacements, and dynamic threshold-based rebalancing to maintain feasibility while optimizing both objectives. It prioritizes items with strong combined marginal impacts and performs targeted swaps to improve complementary objectives, while ensuring feasibility through iterative removal of low-utility items. The selection criterion favors solutions with higher combined objective scores and greater diversity, while the local search operations focus on marginal improvements and capacity-constrained transformations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 150,
        "algorithm": "The algorithm selects a diverse solution from the archive using a score combining objective values and diversity, then generates a neighbor through three phases: 1) adding high-impact items based on objective thresholds, 2) replacing items with better objective-specific alternatives, and 3) dynamically removing low-utility items to maintain feasibility. The marginal impact metric prioritizes items with balanced high values and low weights, while the utility score in phase 3 balances both objectives when adjusting capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Diversity-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        diversity = np.sum(sol != archive[0][0])\n        score = (obj[0] + obj[1]) * (1 + diversity / len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Novel combined marginal impact metric\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6) * (1 + value_ratio)\n\n    # Phase 1: Targeted additions based on objective thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        obj1_threshold = np.percentile(value1_lst[candidates], 75)\n        obj2_threshold = np.percentile(value2_lst[candidates], 75)\n\n        for idx in candidates:\n            if (value1_lst[idx] >= obj1_threshold) or (value2_lst[idx] >= obj2_threshold):\n                prob = min(0.8, marginal_impact[idx] / np.max(marginal_impact))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Objective-specific replacement operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Objective-specific replacement conditions\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic capacity adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Novel utility calculation combining both objectives\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n        utility_scores = []\n\n        for idx in included:\n            utility = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) * \\\n                     (1 - weight_lst[idx] / (current_weight + 1e-6))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8254042511252266,
            2.8379281163215637
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Diversity-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        diversity = np.sum(sol != archive[0][0])\n        score = (obj[0] + obj[1]) * (1 + diversity / len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Novel combined marginal impact metric\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6) * (1 + value_ratio)\n\n    # Phase 1: Targeted additions based on objective thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        obj1_threshold = np.percentile(value1_lst[candidates], 75)\n        obj2_threshold = np.percentile(value2_lst[candidates], 75)\n\n        for idx in candidates:\n            if (value1_lst[idx] >= obj1_threshold) or (value2_lst[idx] >= obj2_threshold):\n                prob = min(0.8, marginal_impact[idx] / np.max(marginal_impact))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity = capacity - current_weight\n\n    # Phase 2: Objective-specific replacement operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Objective-specific replacement conditions\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or \\\n                   (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic capacity adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Novel utility calculation combining both objectives\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n        utility_scores = []\n\n        for idx in included:\n            utility = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) * \\\n                     (1 - weight_lst[idx] / (current_weight + 1e-6))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values and diversity, then generates a neighbor through three phases: adaptive addition of high-marginal-impact items, capacity-constrained hybrid swaps/replacements, and dynamic threshold-based rebalancing to maintain feasibility while optimizing both objectives. It prioritizes items with strong combined marginal impacts and performs targeted swaps to improve complementary objectives, while ensuring feasibility through iterative removal of low-utility items. The selection criterion favors solutions with higher combined objective scores and greater diversity, while the local search operations focus on marginal improvements and capacity-constrained transformations.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 151,
        "algorithm": "This algorithm selects a diverse, high-quality solution from the archive and generates a neighbor through three phases: 1) adding high-marginal-impact items with adaptive thresholds, 2) swapping items to balance improvements across objectives, and 3) removing low-utility items while maintaining feasibility. It prioritizes combined marginal impact (70% value1, 30% value2) and dynamically adjusts thresholds based on current capacity usage.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Diversity-aware selection with objective balance\n    archive_with_scores = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - np.mean([s[0] for s in archive], axis=0)))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 * 0.7 + marginal2 * 0.3)\n\n    # Phase 1: Targeted addition with objective-aware thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 90 - (current_weight/capacity)*20)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob and (value1_lst[idx] > 0 or value2_lst[idx] > 0):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Objective-balanced swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Balanced improvement check\n                if ((marginal1[j] > marginal1[i] * 0.9 and marginal2[j] > marginal2[i] * 0.9) or\n                    (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i] * 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-aware removal with utility balance\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)) * 0.8))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7734108538331357,
            3.31136155128479
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Diversity-aware selection with objective balance\n    archive_with_scores = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - np.mean([s[0] for s in archive], axis=0)))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 * 0.7 + marginal2 * 0.3)\n\n    # Phase 1: Targeted addition with objective-aware thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 90 - (current_weight/capacity)*20)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob and (value1_lst[idx] > 0 or value2_lst[idx] > 0):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Objective-balanced swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Balanced improvement check\n                if ((marginal1[j] > marginal1[i] * 0.9 and marginal2[j] > marginal2[i] * 0.9) or\n                    (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i] * 0.7)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-aware removal with utility balance\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)) * 0.8))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values and diversity, then generates a neighbor through three phases: adaptive addition of high-marginal-impact items, capacity-constrained hybrid swaps/replacements, and dynamic threshold-based rebalancing to maintain feasibility while optimizing both objectives. It prioritizes items with strong combined marginal impacts and performs targeted swaps to improve complementary objectives, while ensuring feasibility through iterative removal of low-utility items. The selection criterion favors solutions with higher combined objective scores and greater diversity, while the local search operations focus on marginal improvements and capacity-constrained transformations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Combine objective values and diversity\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 152,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid Pareto-aware criterion that balances objective values, dominance, and diversity, then applies a three-phase local search: (1) adaptively adds high-marginal-impact items with dynamic thresholds, (2) performs capacity-constrained swaps prioritizing complementary improvements, and (3) dynamically rebalances by removing low-utility items to maintain feasibility. The selection favors solutions with higher combined objective scores, lower dominance, and greater diversity, while the local search operations focus on marginal improvements, capacity-constrained transformations, and probabilistic exploration to generate high-quality, feasible neighbors that balance both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection with dominance and diversity\n    archive_with_scores = []\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol)) / (1 + dominated)\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Final probabilistic operations for exploration\n    for i in range(min(5, len(weight_lst))):\n        if np.random.random() < 0.3:\n            j = np.random.choice(np.where(new_solution == 0)[0])\n            if current_weight + weight_lst[j] <= capacity:\n                new_solution[j] = 1\n                current_weight += weight_lst[j]\n\n    return new_solution\n\n",
        "score": [
            -0.8608735552396223,
            2.272821843624115
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid Pareto-aware selection with dominance and diversity\n    archive_with_scores = []\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol)) / (1 + dominated)\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive addition with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        for idx in strong_candidates:\n            prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n            if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity = capacity - current_weight\n\n    # Phase 2: Capacity-constrained hybrid operations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Complementary improvement check\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic threshold-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Final probabilistic operations for exploration\n    for i in range(min(5, len(weight_lst))):\n        if np.random.random() < 0.3:\n            j = np.random.choice(np.where(new_solution == 0)[0])\n            if current_weight + weight_lst[j] <= capacity:\n                new_solution[j] = 1\n                current_weight += weight_lst[j]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 153,
        "algorithm": "The algorithm selects a solution from the archive using a hybrid frontier-diversity criterion, then applies a three-phase local search: dynamically adding high-marginal-value items with adaptive objective weighting, performing Pareto-aware swaps between included/excluded items, and removing low-contribution items based on adaptive percentile thresholds to balance exploration and exploitation while ensuring feasibility. The selection prioritizes solutions closer to the ideal point (70% weight) while maintaining diversity (30% weight), and the local search phases emphasize first-objective improvements when capacity is tight, with progressive emphasis on both objectives as capacity allows.\n\nThe algorithm uses a hybrid selection criterion that combines frontier proximity (distance to ideal point) and diversity (distance to nearest neighbor) to identify promising solutions, then applies a three-phase local search that dynamically adjusts its focus between objectives based on remaining capacity. The selection prioritizes solutions that are both close to the ideal point and well-distributed in the objective space.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid frontier-diversity selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objectives\n        normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier scores (distance to ideal point)\n        ideal = np.ones(2)\n        frontier_scores = np.sqrt(np.sum((ideal - normalized)**2, axis=1))\n\n        # Calculate diversity scores (distance to nearest neighbor)\n        diversity_scores = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.sqrt(np.sum((objectives[i] - objectives)**2, axis=1))\n            diversity_scores[i] = np.min(distances[distances > 0])\n\n        # Combine scores (70% frontier, 30% diversity)\n        combined_scores = 0.7 * frontier_scores + 0.3 * diversity_scores\n        selected_idx = np.argmin(combined_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 1: Dynamic marginal-value additions\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Adaptive objective weighting (more emphasis on first objective when capacity is tight)\n        alpha = 0.6 + 0.3 * (current_weight / capacity)\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Add top items with probability based on marginal value\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate Pareto dominance relationships\n        for i in included_items:\n            for j in excluded_items:\n                if weight_lst[j] <= current_weight - weight_lst[i] + weight_lst[j]:\n                    # Check if swap improves both objectives\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n                    # Or if swap improves at least one objective significantly\n                    elif ((value1_lst[j] > 1.2 * value1_lst[i] and value2_lst[j] >= 0.8 * value2_lst[i]) or\n                          (value2_lst[j] > 1.2 * value2_lst[i] and value1_lst[j] >= 0.8 * value1_lst[i])):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Adaptive percentile-based removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized contributions\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n\n        # Dynamic threshold (25th percentile when capacity is tight, 35th when loose)\n        threshold_percentile = 25 + 10 * (1 - current_weight / capacity)\n        threshold = np.percentile(contribution, threshold_percentile)\n\n        # Remove items below threshold\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "score": [
            -0.8195144064382571,
            2.465260088443756
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid frontier-diversity selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate normalized objectives\n        normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n\n        # Calculate frontier scores (distance to ideal point)\n        ideal = np.ones(2)\n        frontier_scores = np.sqrt(np.sum((ideal - normalized)**2, axis=1))\n\n        # Calculate diversity scores (distance to nearest neighbor)\n        diversity_scores = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            distances = np.sqrt(np.sum((objectives[i] - objectives)**2, axis=1))\n            diversity_scores[i] = np.min(distances[distances > 0])\n\n        # Combine scores (70% frontier, 30% diversity)\n        combined_scores = 0.7 * frontier_scores + 0.3 * diversity_scores\n        selected_idx = np.argmin(combined_scores)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Phase 1: Dynamic marginal-value additions\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Adaptive objective weighting (more emphasis on first objective when capacity is tight)\n        alpha = 0.6 + 0.3 * (current_weight / capacity)\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Add top items with probability based on marginal value\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Phase 2: Pareto-aware swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate Pareto dominance relationships\n        for i in included_items:\n            for j in excluded_items:\n                if weight_lst[j] <= current_weight - weight_lst[i] + weight_lst[j]:\n                    # Check if swap improves both objectives\n                    if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n                    # Or if swap improves at least one objective significantly\n                    elif ((value1_lst[j] > 1.2 * value1_lst[i] and value2_lst[j] >= 0.8 * value2_lst[i]) or\n                          (value2_lst[j] > 1.2 * value2_lst[i] and value1_lst[j] >= 0.8 * value1_lst[i])):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Adaptive percentile-based removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized contributions\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n\n        # Dynamic threshold (25th percentile when capacity is tight, 35th when loose)\n        threshold_percentile = 25 + 10 * (1 - current_weight / capacity)\n        threshold = np.percentile(contribution, threshold_percentile)\n\n        # Remove items below threshold\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 154,
        "algorithm": "The algorithm selects a promising solution from the top 30% of the archive (sorted by combined objective values) and applies a hybrid local search: first adding high-impact frontier items, then removing worst-performing items if capacity is exceeded, followed by probabilistic flips of high-impact items, and finally randomized swaps to refine the solution while maintaining feasibility. The approach prioritizes items with high combined marginal impact (normalized by weight) and uses dynamic thresholds to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n",
        "score": [
            -0.45793351959214196,
            0.41745564341545105
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on normalized marginal impact scores for both objectives, then applies a three-tiered local search: first probabilistically adding high-impact items, then making targeted swaps between complementary items, and finally replacing low-impact items with higher-value candidates, while maintaining feasibility through continuous capacity checks and a final adjustment step. The selection prioritizes solutions with the highest combined marginal improvement potential, and the search tiers progressively refine the solution with decreasing probability thresholds to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 155,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined Pareto frontier dominance\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    dominance_scores = np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive value-to-weight ratios\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Identify frontier items (top 20% by combined ratio)\n    frontier_threshold = np.percentile(combined_ratio, 80)\n    frontier_items = np.where(combined_ratio >= frontier_threshold)[0]\n\n    # Phase 1: Expand frontier with high-ratio items\n    excluded_items = np.where(new_solution == 0)[0]\n    for item in excluded_items:\n        if combined_ratio[item] > frontier_threshold and current_weight + weight_lst[item] <= capacity:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Dynamic swaps between frontier and non-frontier items\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        if combined_ratio[i] < frontier_threshold:\n            for j in frontier_items:\n                if j not in included_items and weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                    if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and np.random.rand() < 0.5:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight += weight_lst[j] - weight_lst[i]\n                        break\n\n    # Phase 3: Value-aware compression (remove low-ratio items)\n    included_items = np.where(new_solution == 1)[0]\n    for item in included_items:\n        if combined_ratio[item] < np.percentile(combined_ratio[included_items], 30):  # Remove bottom 30%\n            if np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_ratio[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9611108820563109,
            4.970113784074783
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined Pareto frontier dominance\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    dominance_scores = np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive value-to-weight ratios\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Identify frontier items (top 20% by combined ratio)\n    frontier_threshold = np.percentile(combined_ratio, 80)\n    frontier_items = np.where(combined_ratio >= frontier_threshold)[0]\n\n    # Phase 1: Expand frontier with high-ratio items\n    excluded_items = np.where(new_solution == 0)[0]\n    for item in excluded_items:\n        if combined_ratio[item] > frontier_threshold and current_weight + weight_lst[item] <= capacity:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n\n    # Phase 2: Dynamic swaps between frontier and non-frontier items\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        if combined_ratio[i] < frontier_threshold:\n            for j in frontier_items:\n                if j not in included_items and weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                    if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and np.random.rand() < 0.5:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight += weight_lst[j] - weight_lst[i]\n                        break\n\n    # Phase 3: Value-aware compression (remove low-ratio items)\n    included_items = np.where(new_solution == 1)[0]\n    for item in included_items:\n        if combined_ratio[item] < np.percentile(combined_ratio[included_items], 30):  # Remove bottom 30%\n            if np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_ratio[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on normalized marginal impact scores for both objectives, then applies a three-tiered local search: first probabilistically adding high-impact items, then making targeted swaps between complementary items, and finally replacing low-impact items with higher-value candidates, while maintaining feasibility through continuous capacity checks and a final adjustment step. The selection prioritizes solutions with the highest combined marginal improvement potential, and the search tiers progressively refine the solution with decreasing probability thresholds to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 156,
        "algorithm": "The algorithm combines dynamic frontier-aware selection with a multi-phase neighborhood exploration, prioritizing Pareto-improving operations through probabilistic additions, targeted swaps between complementary items, and replacement-based improvements while maintaining feasibility. It uses adaptive percentile thresholds and angle-based metrics to balance objective improvements, with each phase dynamically adjusting based on the solution's position in the objective space. The algorithm emphasizes value ratios and relative improvements while ensuring capacity constraints are satisfied through final feasibility checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near Pareto frontier using angle-based metric\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    angles = np.arctan2(normalized[:, 1], normalized[:, 0])\n    frontier_scores = angles * (normalized[:, 0] * normalized[:, 1])  # Balance between angle and product\n    selected_idx = np.argmax(frontier_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Probabilistic additions based on value ratios\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        value_ratios = (value1_lst[excluded] / (value2_lst[excluded] + 1e-6)) + (value2_lst[excluded] / (value1_lst[excluded] + 1e-6))\n        sorted_indices = np.argsort(value_ratios)[::-1]\n        for idx in sorted_indices[:max(1, len(sorted_indices)//3)]:\n            item = excluded[idx]\n            if weight_lst[item] <= remaining_capacity:\n                new_solution[item] = 1\n                remaining_capacity -= weight_lst[item]\n\n    # Phase 2: Targeted swaps between complementary items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    for i in included:\n        for j in excluded:\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])):\n                if np.random.rand() < 0.5:  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n                    break\n\n    # Phase 3: Replacement based on relative objective improvements\n    included = np.where(new_solution == 1)[0]\n    for i in included:\n        excluded = np.where(new_solution == 0)[0]\n        if len(excluded) > 0:\n            # Calculate relative improvement potential\n            rel_improvement1 = (value1_lst[excluded] - value1_lst[i]) / (value1_lst[i] + 1e-6)\n            rel_improvement2 = (value2_lst[excluded] - value2_lst[i]) / (value2_lst[i] + 1e-6)\n            combined = rel_improvement1 + rel_improvement2\n            best_candidate = excluded[np.argmax(combined)]\n\n            if (weight_lst[best_candidate] <= remaining_capacity + weight_lst[i] and\n                (rel_improvement1[np.argmax(combined)] > 0 or rel_improvement2[np.argmax(combined)] > 0)):\n                new_solution[i], new_solution[best_candidate] = 0, 1\n                remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove item with smallest combined value ratio\n        value_ratios = (value1_lst[included] / (value2_lst[included] + 1e-6)) + (value2_lst[included] / (value1_lst[included] + 1e-6))\n        item_to_remove = included[np.argmin(value_ratios)]\n        new_solution[item_to_remove] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9749311414719832,
            1.941577136516571
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution near Pareto frontier using angle-based metric\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    angles = np.arctan2(normalized[:, 1], normalized[:, 0])\n    frontier_scores = angles * (normalized[:, 0] * normalized[:, 1])  # Balance between angle and product\n    selected_idx = np.argmax(frontier_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Phase 1: Probabilistic additions based on value ratios\n    excluded = np.where(new_solution == 0)[0]\n    if len(excluded) > 0:\n        value_ratios = (value1_lst[excluded] / (value2_lst[excluded] + 1e-6)) + (value2_lst[excluded] / (value1_lst[excluded] + 1e-6))\n        sorted_indices = np.argsort(value_ratios)[::-1]\n        for idx in sorted_indices[:max(1, len(sorted_indices)//3)]:\n            item = excluded[idx]\n            if weight_lst[item] <= remaining_capacity:\n                new_solution[item] = 1\n                remaining_capacity -= weight_lst[item]\n\n    # Phase 2: Targeted swaps between complementary items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    for i in included:\n        for j in excluded:\n            if (weight_lst[j] <= remaining_capacity + weight_lst[i] and\n                (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i])):\n                if np.random.rand() < 0.5:  # 50% chance to swap\n                    new_solution[i], new_solution[j] = 0, 1\n                    remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n                    break\n\n    # Phase 3: Replacement based on relative objective improvements\n    included = np.where(new_solution == 1)[0]\n    for i in included:\n        excluded = np.where(new_solution == 0)[0]\n        if len(excluded) > 0:\n            # Calculate relative improvement potential\n            rel_improvement1 = (value1_lst[excluded] - value1_lst[i]) / (value1_lst[i] + 1e-6)\n            rel_improvement2 = (value2_lst[excluded] - value2_lst[i]) / (value2_lst[i] + 1e-6)\n            combined = rel_improvement1 + rel_improvement2\n            best_candidate = excluded[np.argmax(combined)]\n\n            if (weight_lst[best_candidate] <= remaining_capacity + weight_lst[i] and\n                (rel_improvement1[np.argmax(combined)] > 0 or rel_improvement2[np.argmax(combined)] > 0)):\n                new_solution[i], new_solution[best_candidate] = 0, 1\n                remaining_capacity = capacity - np.sum(weight_lst[new_solution == 1])\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove item with smallest combined value ratio\n        value_ratios = (value1_lst[included] / (value2_lst[included] + 1e-6)) + (value2_lst[included] / (value1_lst[included] + 1e-6))\n        item_to_remove = included[np.argmin(value_ratios)]\n        new_solution[item_to_remove] = 0\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive based on normalized marginal impact scores for both objectives, then applies a three-tiered local search: first probabilistically adding high-impact items, then making targeted swaps between complementary items, and finally replacing low-impact items with higher-value candidates, while maintaining feasibility through continuous capacity checks and a final adjustment step. The selection prioritizes solutions with the highest combined marginal improvement potential, and the search tiers progressively refine the solution with decreasing probability thresholds to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 157,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid scoring of combined utility and diversity, then applies a four-stage local search: (1) probabilistically adding high-impact items based on utility scores, (2) adaptively swapping items to improve both objectives while balancing weight, (3) replacing low-utility items with high-value candidates, and (4) dynamically adjusting the solution to maintain feasibility by removing items with the smallest marginal contribution. The method prioritizes items with higher combined utility and value-to-weight ratios, while balancing exploration and exploitation through probabilistic decisions and adaptive weight considerations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection based on objective improvements and diversity\n    scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n\n        # Utility-based scoring\n        utility1 = np.where(included, value1_lst / weight_lst, 0)\n        utility2 = np.where(included, value2_lst / weight_lst, 0)\n        combined_utility = np.sum(utility1) + np.sum(utility2)\n\n        # Diversity component\n        diversity = np.sum(np.abs(utility1 - utility2))\n\n        # Combined score\n        scores.append(combined_utility + 0.5 * diversity)\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Stage 1: Probabilistic addition of high-impact items using utility scoring\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Calculate utility scores for candidates\n        utility_scores = (value1_lst[candidates] + value2_lst[candidates]) / weight_lst[candidates]\n        sorted_indices = np.argsort(utility_scores)[::-1]\n\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7 * (1 - current_weight/capacity):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Stage 2: Adaptive swap phase considering both objectives and weight balance\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Calculate normalized improvement\n                delta1 = (value1_lst[j] - value1_lst[i]) / (value1_lst.max() + 1e-6)\n                delta2 = (value2_lst[j] - value2_lst[i]) / (value2_lst.max() + 1e-6)\n                delta_weight = (weight_lst[j] - weight_lst[i]) / capacity\n\n                # Combined improvement score\n                improvement_score = delta1 + delta2 - 0.3 * np.abs(delta_weight)\n\n                if improvement_score > 0 and np.random.rand() < 0.6 * (1 - np.abs(delta_weight)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Stage 3: Targeted replacement of low-utility items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate utility scores for included items\n    utility_scores = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n    sorted_items = included_items[np.argsort(utility_scores)]\n\n    for i in sorted_items[:max(1, len(sorted_items)//3)]:\n        if np.random.rand() < 0.4:\n            candidates = np.where((new_solution == 0) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n\n            if len(candidates) > 0:\n                # Select candidate with highest combined value\n                candidate_scores = value1_lst[candidates] + value2_lst[candidates]\n                j = candidates[np.argmax(candidate_scores)]\n\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Stage 4: Dynamic adjustment phase\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n\n        if len(included_items) == 0:\n            break\n\n        # Remove item with smallest marginal contribution\n        marginal_contribution = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        i = included_items[np.argmin(marginal_contribution)]\n\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8832159161252763,
            6.518234699964523
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection based on objective improvements and diversity\n    scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n\n        # Utility-based scoring\n        utility1 = np.where(included, value1_lst / weight_lst, 0)\n        utility2 = np.where(included, value2_lst / weight_lst, 0)\n        combined_utility = np.sum(utility1) + np.sum(utility2)\n\n        # Diversity component\n        diversity = np.sum(np.abs(utility1 - utility2))\n\n        # Combined score\n        scores.append(combined_utility + 0.5 * diversity)\n\n    selected_idx = np.argmax(scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Stage 1: Probabilistic addition of high-impact items using utility scoring\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Calculate utility scores for candidates\n        utility_scores = (value1_lst[candidates] + value2_lst[candidates]) / weight_lst[candidates]\n        sorted_indices = np.argsort(utility_scores)[::-1]\n\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7 * (1 - current_weight/capacity):\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Stage 2: Adaptive swap phase considering both objectives and weight balance\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Calculate normalized improvement\n                delta1 = (value1_lst[j] - value1_lst[i]) / (value1_lst.max() + 1e-6)\n                delta2 = (value2_lst[j] - value2_lst[i]) / (value2_lst.max() + 1e-6)\n                delta_weight = (weight_lst[j] - weight_lst[i]) / capacity\n\n                # Combined improvement score\n                improvement_score = delta1 + delta2 - 0.3 * np.abs(delta_weight)\n\n                if improvement_score > 0 and np.random.rand() < 0.6 * (1 - np.abs(delta_weight)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Stage 3: Targeted replacement of low-utility items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate utility scores for included items\n    utility_scores = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n    sorted_items = included_items[np.argsort(utility_scores)]\n\n    for i in sorted_items[:max(1, len(sorted_items)//3)]:\n        if np.random.rand() < 0.4:\n            candidates = np.where((new_solution == 0) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n\n            if len(candidates) > 0:\n                # Select candidate with highest combined value\n                candidate_scores = value1_lst[candidates] + value2_lst[candidates]\n                j = candidates[np.argmax(candidate_scores)]\n\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Stage 4: Dynamic adjustment phase\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n\n        if len(included_items) == 0:\n            break\n\n        # Remove item with smallest marginal contribution\n        marginal_contribution = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        i = included_items[np.argmin(marginal_contribution)]\n\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive based on normalized marginal impact scores for both objectives, then applies a three-tiered local search: first probabilistically adding high-impact items, then making targeted swaps between complementary items, and finally replacing low-impact items with higher-value candidates, while maintaining feasibility through continuous capacity checks and a final adjustment step. The selection prioritizes solutions with the highest combined marginal improvement potential, and the search tiers progressively refine the solution with decreasing probability thresholds to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 158,
        "algorithm": "The algorithm selects a non-dominated solution from the archive based on dominance scores, then applies a four-stage local search: first removing low-contribution items probabilistically, followed by inserting high-value items with a probability, then swapping items with complementary value profiles, and finally replacing items with significant marginal value improvements, while ensuring feasibility through continuous weight checks and a final adjustment step. The method prioritizes value-to-weight ratios and complementary value profiles, with probabilistic decisions to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest dynamic Pareto front dominance score\n    dominance_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        total1, total2 = obj\n        # Calculate dominance score based on how many other solutions it dominates\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if other_sol is sol:\n                continue\n            if (total1 >= other_obj[0] and total2 > other_obj[1]) or (total1 > other_obj[0] and total2 >= other_obj[1]):\n                dominates += 1\n        dominance_scores.append(dominates)\n    selected_idx = np.argmax(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Stage 1: Remove low-contribution items with high probability\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        if np.random.rand() < 0.5:  # 50% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Stage 2: Probabilistic insertion of high-ratio items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        value_ratios = (value1_lst + value2_lst) / weight_lst\n        sorted_indices = np.argsort(value_ratios)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Stage 3: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if items have complementary value profiles\n                if (value1_lst[i] < value1_lst[j] and value2_lst[i] > value2_lst[j]) or \\\n                   (value1_lst[i] > value1_lst[j] and value2_lst[i] < value2_lst[j]):\n                    if np.random.rand() < 0.5:  # 50% chance to swap\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Stage 4: Replace items with high marginal value improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i] * 1.2) &\n                                 (value2_lst > value2_lst[i] * 1.2) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax((value1_lst[candidates] - value1_lst[i]) + (value2_lst[candidates] - value2_lst[i]))]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8301209603550017,
            4.023605585098267
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest dynamic Pareto front dominance score\n    dominance_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        total1, total2 = obj\n        # Calculate dominance score based on how many other solutions it dominates\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if other_sol is sol:\n                continue\n            if (total1 >= other_obj[0] and total2 > other_obj[1]) or (total1 > other_obj[0] and total2 >= other_obj[1]):\n                dominates += 1\n        dominance_scores.append(dominates)\n    selected_idx = np.argmax(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Stage 1: Remove low-contribution items with high probability\n    included_items = np.where(new_solution == 1)[0]\n    for i in included_items:\n        if np.random.rand() < 0.5:  # 50% chance to remove\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Stage 2: Probabilistic insertion of high-ratio items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        value_ratios = (value1_lst + value2_lst) / weight_lst\n        sorted_indices = np.argsort(value_ratios)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Stage 3: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if items have complementary value profiles\n                if (value1_lst[i] < value1_lst[j] and value2_lst[i] > value2_lst[j]) or \\\n                   (value1_lst[i] > value1_lst[j] and value2_lst[i] < value2_lst[j]):\n                    if np.random.rand() < 0.5:  # 50% chance to swap\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Stage 4: Replace items with high marginal value improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i] * 1.2) &\n                                 (value2_lst > value2_lst[i] * 1.2) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax((value1_lst[candidates] - value1_lst[i]) + (value2_lst[candidates] - value2_lst[i]))]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin((value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive based on normalized marginal impact scores for both objectives, then applies a three-tiered local search: first probabilistically adding high-impact items, then making targeted swaps between complementary items, and finally replacing low-impact items with higher-value candidates, while maintaining feasibility through continuous capacity checks and a final adjustment step. The selection prioritizes solutions with the highest combined marginal improvement potential, and the search tiers progressively refine the solution with decreasing probability thresholds to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest normalized marginal impact sum\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        total1, total2 = obj\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    selected_idx = np.argmax(marginal_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Tier 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = value1_lst - value1_lst[base_solution == 1].sum()\n        marginal2 = value2_lst - value2_lst[base_solution == 1].sum()\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Tier 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:  # 60% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Tier 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.4:  # 40% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                 (value1_lst > value1_lst[i]) &\n                                 (value2_lst > value2_lst[i]) &\n                                 (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection with a dynamic hybrid local search that prioritizes solutions near the Pareto frontier, then applies a three-phase improvement strategy (probabilistic addition, targeted swaps, and rebalancing replacements) while ensuring feasibility through continuous weight checks and dynamic adjustments. It uses combined marginal impact to guide item selection, with higher priority given to items that improve both objectives, and balances exploration with exploitation through probabilistic thresholds and adaptive adjustments.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solutions near the Pareto frontier (top 25% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 25)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n    combined_marginal = marginal1 + marginal2\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on combined marginal impact\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = combined_marginal[j] - combined_marginal[i]\n                if delta_marginal > 0 and np.random.rand() < 0.5:  # 50% chance to swap if beneficial\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Rebalance by replacing low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        if np.random.rand() < 0.3:  # 30% chance to consider replacement\n            candidates = np.where((new_solution == 0) &\n                                (combined_marginal > combined_marginal[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(combined_marginal[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm combines marginal impact analysis with adaptive local search by first selecting promising solutions near the Pareto frontier (top 30% by dominance), then applying a hybrid operator that probabilistically swaps items based on their marginal contribution to both objectives while ensuring feasibility, and finally performs targeted replacements of low-impact items with high-impact candidates, prioritizing solutions with higher combined marginal impact in both objectives. The algorithm maintains feasibility through continuous weight checks and dynamic adjustments, ensuring the generated neighbor solution remains valid.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Identify solutions near the Pareto frontier (top 30% by dominance)\n    dominance_scores = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = sum(1 for (_, other_obj) in archive if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]))\n        dominance_scores.append(dominated)\n    threshold = np.percentile(dominance_scores, 30)\n    candidate_indices = [i for i, score in enumerate(dominance_scores) if score <= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.random.randint(0, len(archive))\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal contributions for both objectives\n    marginal1 = np.where(base_solution == 1, -value1_lst, value1_lst)\n    marginal2 = np.where(base_solution == 1, -value2_lst, value2_lst)\n\n    # Probabilistic swaps based on combined marginal impact\n    for i in range(len(weight_lst)):\n        if np.random.random() < 0.4:  # 40% chance for swap attempt\n            j = np.random.randint(len(weight_lst))\n            if i != j:\n                delta_weight = (weight_lst[j] - weight_lst[i]) if new_solution[i] == 1 else (weight_lst[i] - weight_lst[j])\n                if current_weight + delta_weight <= capacity:\n                    # Accept swap if it improves at least one objective\n                    if (marginal1[i] + marginal1[j] > 0) or (marginal2[i] + marginal2[j] > 0):\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight += delta_weight\n\n    # Targeted replacements of low-impact items with high-impact candidates\n    for i in np.where(new_solution == 1)[0]:\n        if np.random.random() < 0.3:  # 30% chance to consider replacement\n            # Find items not in solution with high marginal impact\n            candidates = np.where((new_solution == 0) &\n                                ((marginal1 > marginal1[i]) | (marginal2 > marginal2[i])) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                # Select candidate with highest combined marginal impact\n                combined_marginal = marginal1[candidates] + marginal2[candidates]\n                j = candidates[np.argmax(combined_marginal)]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Final feasibility check and adjustment\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove item with lowest combined marginal impact\n        combined_marginal = marginal1[excess_items] + marginal2[excess_items]\n        i = excess_items[np.argmin(combined_marginal)]\n        new_solution[i] = 0\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 159,
        "algorithm": "The algorithm selects a promising solution from the top 20% of the archive based on combined normalized marginal impact for both objectives, then applies a four-phase hybrid local search: (1) probabilistically adds high-impact items (70% chance), (2) performs targeted swaps between complementary items (60% chance for beneficial swaps), (3) replaces low-impact items with superior candidates (50% chance), and (4) rebalances by flipping Pareto-dominated items (40% chance for low-dominance items). It prioritizes solutions with high marginal gains in both objectives while ensuring feasibility through continuous capacity checks and dynamic adjustments. The algorithm emphasizes balanced exploration of high-impact items and complementary swaps, with targeted replacements and final rebalancing to refine solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution from top 20% by combined normalized marginal impact\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    threshold = np.percentile(marginal_scores, 80)\n    candidate_indices = [i for i, score in enumerate(marginal_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.argmax(marginal_scores)\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = np.where(new_solution == 1, -value1_lst, value1_lst)\n        marginal2 = np.where(new_solution == 1, -value2_lst, value2_lst)\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.5:\n            candidates = np.where((new_solution == 0) &\n                                (value1_lst > value1_lst[i]) &\n                                (value2_lst > value2_lst[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Phase 4: Final rebalancing based on Pareto dominance\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate Pareto dominance scores\n        dominance_scores = []\n        for i in included_items:\n            dominates = 0\n            for j in included_items:\n                if i != j and (value1_lst[i] >= value1_lst[j] and value2_lst[i] >= value2_lst[j] and\n                              (value1_lst[i] > value1_lst[j] or value2_lst[i] > value2_lst[j])):\n                    dominates += 1\n            dominance_scores.append(dominates)\n        # Flip items with low dominance score (potential for improvement)\n        for i, score in zip(included_items, dominance_scores):\n            if score == 0 and np.random.rand() < 0.4:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8019608012489852,
            2.1817129850387573
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution from top 20% by combined normalized marginal impact\n    marginal_scores = []\n    for sol, obj in archive:\n        included = sol == 1\n        excluded = sol == 0\n        marginal1 = np.where(included, -value1_lst, value1_lst)\n        marginal2 = np.where(included, -value2_lst, value2_lst)\n        norm_marginal = (marginal1 / value1_lst.max()) + (marginal2 / value2_lst.max())\n        marginal_scores.append(np.sum(norm_marginal))\n    threshold = np.percentile(marginal_scores, 80)\n    candidate_indices = [i for i, score in enumerate(marginal_scores) if score >= threshold]\n\n    if not candidate_indices:\n        selected_idx = np.argmax(marginal_scores)\n    else:\n        selected_idx = np.random.choice(candidate_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Phase 1: Probabilistic addition of high-impact items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        marginal1 = np.where(new_solution == 1, -value1_lst, value1_lst)\n        marginal2 = np.where(new_solution == 1, -value2_lst, value2_lst)\n        combined_marginal = marginal1 + marginal2\n        sorted_indices = np.argsort(combined_marginal)[::-1]\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.7:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                delta_marginal = (value1_lst[j] - value1_lst[i]) + (value2_lst[j] - value2_lst[i])\n                if delta_marginal > 0 and np.random.rand() < 0.6:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Replace low-impact items with high-impact candidates\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    for i in included_items:\n        if np.random.rand() < 0.5:\n            candidates = np.where((new_solution == 0) &\n                                (value1_lst > value1_lst[i]) &\n                                (value2_lst > value2_lst[i]) &\n                                (weight_lst <= capacity - current_weight + weight_lst[i]))[0]\n            if len(candidates) > 0:\n                j = candidates[np.argmax(value1_lst[candidates] + value2_lst[candidates])]\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Phase 4: Final rebalancing based on Pareto dominance\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate Pareto dominance scores\n        dominance_scores = []\n        for i in included_items:\n            dominates = 0\n            for j in included_items:\n                if i != j and (value1_lst[i] >= value1_lst[j] and value2_lst[i] >= value2_lst[j] and\n                              (value1_lst[i] > value1_lst[j] or value2_lst[i] > value2_lst[j])):\n                    dominates += 1\n            dominance_scores.append(dominates)\n        # Flip items with low dominance score (potential for improvement)\n        for i, score in zip(included_items, dominance_scores):\n            if score == 0 and np.random.rand() < 0.4:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        i = included_items[np.argmin(value1_lst[included_items] + value2_lst[included_items])]\n        new_solution[i] = 0\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 160,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion that balances objective values and diversity, then applies a three-phase hybrid local search: (1) Pareto-aware additions of high-marginal-value items with adaptive weights, (2) targeted swaps to improve both objectives with probabilistic acceptance, and (3) dynamic removal of low-contribution items below a percentile threshold while ensuring feasibility. The algorithm prioritizes items with higher combined marginal value (weighted by a dynamic alpha) and uses probabilistic selection to explore high-potential regions of the solution space.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_diversity[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impacts with adaptive weights\n    alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (alpha * marginal1 + (1-alpha) * marginal2)\n\n    # Phase 1: Pareto-aware additions of high-marginal-value items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Sort candidates by combined marginal value\n        candidate_scores = combined_marginal[candidates]\n        sorted_candidates = candidates[np.argsort(candidate_scores)[::-1]]\n\n        # Add top items with probability based on marginal value\n        for idx in sorted_candidates:\n            prob = min(0.9, candidate_scores[idx] / np.max(candidate_scores))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2: Targeted swaps to improve both objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and np.random.rand() < 0.6) or \\\n                   (marginal2[j] > marginal2[i] and np.random.rand() < 0.6):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic removal of low-contribution items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate contribution scores\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n\n        # Remove items below the 30th percentile\n        threshold = np.percentile(contribution, 30)\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Final capacity check\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove item with lowest combined marginal value\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8276809545910737,
            1.7288326621055603
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_diversity[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impacts with adaptive weights\n    alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (alpha * marginal1 + (1-alpha) * marginal2)\n\n    # Phase 1: Pareto-aware additions of high-marginal-value items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Sort candidates by combined marginal value\n        candidate_scores = combined_marginal[candidates]\n        sorted_candidates = candidates[np.argsort(candidate_scores)[::-1]]\n\n        # Add top items with probability based on marginal value\n        for idx in sorted_candidates:\n            prob = min(0.9, candidate_scores[idx] / np.max(candidate_scores))\n            if np.random.rand() < prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                if remaining_capacity <= 0:\n                    break\n\n    # Phase 2: Targeted swaps to improve both objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] > marginal1[i] and np.random.rand() < 0.6) or \\\n                   (marginal2[j] > marginal2[i] and np.random.rand() < 0.6):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic removal of low-contribution items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate contribution scores\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n\n        # Remove items below the 30th percentile\n        threshold = np.percentile(contribution, 30)\n        for i, item in enumerate(included_items):\n            if contribution[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Final capacity check\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove item with lowest combined marginal value\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the top 30% of the archive (sorted by combined objective values) and applies a hybrid local search: first adding high-impact frontier items, then removing worst-performing items if capacity is exceeded, followed by probabilistic flips of high-impact items, and finally randomized swaps to refine the solution while maintaining feasibility. The approach prioritizes items with high combined marginal impact (normalized by weight) and uses dynamic thresholds to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 161,
        "algorithm": "The algorithm selects a high-potential solution from the top 25% of the archive (sorted by combined objective values) and applies a three-phase hybrid local search: it first adds items with high marginal impact for both objectives using a Pareto-inspired heuristic, then performs capacity-preserving replacements by removing least impactful items, and finally conducts probabilistic flips of high-impact items with dynamic thresholds. The method balances exploration and exploitation by dynamically adjusting selection criteria to ensure high-quality, feasible neighbors while avoiding premature convergence.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts and combined impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Pareto-inspired additions (items that improve both objectives)\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements (remove worst items)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 4 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(4, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6887704142880026,
            0.3003746271133423
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts and combined impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Pareto-inspired additions (items that improve both objectives)\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements (remove worst items)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 4 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(4, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the top 30% of the archive (sorted by combined objective values) and applies a hybrid local search: first adding high-impact frontier items, then removing worst-performing items if capacity is exceeded, followed by probabilistic flips of high-impact items, and finally randomized swaps to refine the solution while maintaining feasibility. The approach prioritizes items with high combined marginal impact (normalized by weight) and uses dynamic thresholds to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 162,
        "algorithm": "The algorithm selects a promising solution from the archive (top 3 by combined objective values), then applies a three-phase local search: 1) greedily adds high-impact items (top 20% combined marginal value), 2) performs targeted swaps to balance objectives while maintaining feasibility, and 3) probabilistically flips items (with 60% chance) based on combined impact, prioritizing higher-value items. It always ensures feasibility through adaptive capacity adjustments by removing low-impact items when needed. The combined marginal impact metric (value1+value2/weight) guides all operations, with percentile thresholds ensuring high-quality neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high combined objective potential\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(2, len(archive) - 1)  # Select from top 3 solutions\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Add high-impact items above 80th percentile\n    high_impact_items = np.where(combined_impact > np.percentile(combined_impact, 80))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps to balance objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive probabilistic flips\n    if np.random.rand() < 0.6:  # 60% chance to perform flips\n        flip_candidates = np.argsort(combined_impact)[::-1][:max(3, len(combined_impact)//4)]\n        if len(flip_candidates) > 0:\n            num_flips = min(2, len(flip_candidates))\n            flip_indices = np.random.choice(flip_candidates, size=num_flips, replace=False)\n            for idx in flip_indices:\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    # Phase 4: Capacity adjustment\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9125738628847365,
            0.5887700319290161
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high combined objective potential\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(2, len(archive) - 1)  # Select from top 3 solutions\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Add high-impact items above 80th percentile\n    high_impact_items = np.where(combined_impact > np.percentile(combined_impact, 80))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps to balance objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive probabilistic flips\n    if np.random.rand() < 0.6:  # 60% chance to perform flips\n        flip_candidates = np.argsort(combined_impact)[::-1][:max(3, len(combined_impact)//4)]\n        if len(flip_candidates) > 0:\n            num_flips = min(2, len(flip_candidates))\n            flip_indices = np.random.choice(flip_candidates, size=num_flips, replace=False)\n            for idx in flip_indices:\n                if new_solution[idx] == 1:\n                    if current_weight - weight_lst[idx] <= capacity:\n                        new_solution[idx] = 0\n                        current_weight -= weight_lst[idx]\n                else:\n                    if current_weight + weight_lst[idx] <= capacity:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n\n    # Phase 4: Capacity adjustment\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the top 30% of the archive (sorted by combined objective values) and applies a hybrid local search: first adding high-impact frontier items, then removing worst-performing items if capacity is exceeded, followed by probabilistic flips of high-impact items, and finally randomized swaps to refine the solution while maintaining feasibility. The approach prioritizes items with high combined marginal impact (normalized by weight) and uses dynamic thresholds to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 163,
        "algorithm": "The algorithm selects a high-potential solution from the top 30% of the archive, then applies a three-phase local search: first clustering items by value to identify high-impact groups, followed by removing low-utility items and finally performing probabilistic swaps of high-utility items while ensuring feasibility through capacity checks. The method prioritizes combined marginal impact (sum of normalized value1 and value2 impacts) for both selection and improvement, using clustering to focus on promising item groups and dynamic thresholding to maintain balance between objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Value-based clustering and selective additions\n    from sklearn.cluster import KMeans\n    X = np.column_stack((value1_lst, value2_lst))\n    k = min(3, len(X))\n    if k > 1:\n        kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n        cluster_centers = kmeans.cluster_centers_\n        cluster_distances = np.linalg.norm(X[:, np.newaxis] - cluster_centers, axis=2)\n        closest_clusters = np.argmin(cluster_distances, axis=1)\n\n        for cluster_idx in range(k):\n            cluster_items = np.where(closest_clusters == cluster_idx)[0]\n            cluster_impact = combined_impact[cluster_items]\n            top_items = cluster_items[np.argsort(cluster_impact)[-min(3, len(cluster_items)):]]\n\n            for idx in top_items:\n                if new_solution[idx] == 0 and weight_lst[idx] <= (capacity - current_weight):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic thresholding for item removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        impact_threshold = np.percentile(combined_impact[included_items], 30)\n        for idx in included_items:\n            if combined_impact[idx] < impact_threshold:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 3: Probabilistic swap with utility-aware selection\n    for _ in range(3):\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Select items with high utility for removal\n            utility_scores = combined_impact[included_items]\n            prob_remove = utility_scores / (utility_scores.sum() + 1e-6)\n            i = np.random.choice(included_items, p=prob_remove)\n\n            # Select items with high utility for addition\n            utility_scores = combined_impact[excluded_items]\n            prob_add = utility_scores / (utility_scores.sum() + 1e-6)\n            j = np.random.choice(excluded_items, p=prob_add)\n\n            if (current_weight - weight_lst[i] + weight_lst[j]) <= capacity:\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    return new_solution\n\n",
        "score": [
            -0.8669925322332948,
            3.6928852796554565
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Value-based clustering and selective additions\n    from sklearn.cluster import KMeans\n    X = np.column_stack((value1_lst, value2_lst))\n    k = min(3, len(X))\n    if k > 1:\n        kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n        cluster_centers = kmeans.cluster_centers_\n        cluster_distances = np.linalg.norm(X[:, np.newaxis] - cluster_centers, axis=2)\n        closest_clusters = np.argmin(cluster_distances, axis=1)\n\n        for cluster_idx in range(k):\n            cluster_items = np.where(closest_clusters == cluster_idx)[0]\n            cluster_impact = combined_impact[cluster_items]\n            top_items = cluster_items[np.argsort(cluster_impact)[-min(3, len(cluster_items)):]]\n\n            for idx in top_items:\n                if new_solution[idx] == 0 and weight_lst[idx] <= (capacity - current_weight):\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 2: Dynamic thresholding for item removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        impact_threshold = np.percentile(combined_impact[included_items], 30)\n        for idx in included_items:\n            if combined_impact[idx] < impact_threshold:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 3: Probabilistic swap with utility-aware selection\n    for _ in range(3):\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            # Select items with high utility for removal\n            utility_scores = combined_impact[included_items]\n            prob_remove = utility_scores / (utility_scores.sum() + 1e-6)\n            i = np.random.choice(included_items, p=prob_remove)\n\n            # Select items with high utility for addition\n            utility_scores = combined_impact[excluded_items]\n            prob_add = utility_scores / (utility_scores.sum() + 1e-6)\n            j = np.random.choice(excluded_items, p=prob_add)\n\n            if (current_weight - weight_lst[i] + weight_lst[j]) <= capacity:\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the top 30% of the archive (sorted by combined objective values) and applies a hybrid local search: first adding high-impact frontier items, then removing worst-performing items if capacity is exceeded, followed by probabilistic flips of high-impact items, and finally randomized swaps to refine the solution while maintaining feasibility. The approach prioritizes items with high combined marginal impact (normalized by weight) and uses dynamic thresholds to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 164,
        "algorithm": "The algorithm selects a solution from the bottom 40% of the archive (sorted by combined objective values) and applies a four-phase local search: first removing low-impact items to free capacity, then adding high-impact items from a dynamic frontier, flipping boundary items near capacity, and finally performing probabilistic swaps with adaptive thresholds. It prioritizes objective 1 (60%) over objective 2 (40%) in impact calculations and ensures feasibility by strictly enforcing capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution from the bottom 40% for potential improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]))\n    selected_idx = max(0, int(0.6 * len(archive)) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact with different weighting\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6) ** 0.7\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6) ** 0.7\n    combined_impact = marginal_impact1 * 0.6 + marginal_impact2 * 0.4\n\n    # Phase 1: Remove low-impact items first to free capacity\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        low_impact_items = included_items[np.argsort(combined_impact[included_items])][:max(1, len(included_items)//4)]\n        for idx in low_impact_items:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 2: Add high-impact items from dynamic frontier (top 60%)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 40))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Targeted flips of boundary items (items near capacity)\n    boundary_items = np.where((weight_lst * (new_solution == 1)) > 0.8 * remaining_capacity)[0]\n    if len(boundary_items) > 0:\n        num_to_flip = min(3, len(boundary_items))\n        flip_indices = np.random.choice(boundary_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Probabilistic swaps with adaptive thresholds\n    for _ in range(3):\n        if np.random.rand() < 0.6:\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n",
        "score": [
            -0.3385225519690334,
            0.5407408475875854
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution from the bottom 40% for potential improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]))\n    selected_idx = max(0, int(0.6 * len(archive)) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact with different weighting\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6) ** 0.7\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6) ** 0.7\n    combined_impact = marginal_impact1 * 0.6 + marginal_impact2 * 0.4\n\n    # Phase 1: Remove low-impact items first to free capacity\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        low_impact_items = included_items[np.argsort(combined_impact[included_items])][:max(1, len(included_items)//4)]\n        for idx in low_impact_items:\n            if current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 2: Add high-impact items from dynamic frontier (top 60%)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 40))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 3: Targeted flips of boundary items (items near capacity)\n    boundary_items = np.where((weight_lst * (new_solution == 1)) > 0.8 * remaining_capacity)[0]\n    if len(boundary_items) > 0:\n        num_to_flip = min(3, len(boundary_items))\n        flip_indices = np.random.choice(boundary_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Probabilistic swaps with adaptive thresholds\n    for _ in range(3):\n        if np.random.rand() < 0.6:\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the top 30% of the archive (sorted by combined objective values) and applies a hybrid local search: first adding high-impact frontier items, then removing worst-performing items if capacity is exceeded, followed by probabilistic flips of high-impact items, and finally randomized swaps to refine the solution while maintaining feasibility. The approach prioritizes items with high combined marginal impact (normalized by weight) and uses dynamic thresholds to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Frontier-aware additions (items near Pareto frontier)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 70))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 5 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 3)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(5, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized local search with dynamic thresholds\n    for _ in range(2):\n        if np.random.rand() < 0.7:  # 70% chance to perform a swap\n            included_items = np.where(new_solution == 1)[0]\n            excluded_items = np.where(new_solution == 0)[0]\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                i = np.random.choice(included_items)\n                j = np.random.choice(excluded_items)\n                if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 165,
        "algorithm": "The algorithm selects a high-potential solution from the top 30% of the archive (sorted by combined objective values) and applies a four-phase hybrid local search: Phase 1 greedily adds high-impact items (top 60% marginal impact), Phase 2 removes worst-performing items if over capacity, Phase 3 probabilistically flips high-impact items (top 20%) with a 60% chance, and Phase 4 performs randomized swaps between high-impact included and excluded items (top 40%). The method prioritizes items with combined high marginal impact for both objectives while ensuring feasibility through adaptive capacity checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Add high-impact frontier items (top 60%)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 60))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Remove worst-performing items if over capacity\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (top 20%)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0 and np.random.rand() < 0.6:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized swaps between high-impact included and excluded items (top 40%)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 2)]\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    high_included = np.intersect1d(included_items, high_impact_items)\n    high_excluded = np.intersect1d(excluded_items, high_impact_items)\n\n    if len(high_included) > 0 and len(high_excluded) > 0:\n        for _ in range(2):\n            i = np.random.choice(high_included)\n            j = np.random.choice(high_excluded)\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n",
        "score": [
            -0.7006110676870887,
            0.5865888595581055
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impact for both objectives\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Add high-impact frontier items (top 60%)\n    frontier_items = np.where(combined_impact > np.percentile(combined_impact, 60))[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in frontier_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Remove worst-performing items if over capacity\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (top 20%)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0 and np.random.rand() < 0.6:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Phase 4: Randomized swaps between high-impact included and excluded items (top 40%)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 2)]\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    high_included = np.intersect1d(included_items, high_impact_items)\n    high_excluded = np.intersect1d(excluded_items, high_impact_items)\n\n    if len(high_included) > 0 and len(high_excluded) > 0:\n        for _ in range(2):\n            i = np.random.choice(high_included)\n            j = np.random.choice(high_excluded)\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                remaining_capacity = capacity - current_weight\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects the most balanced solution from the archive (based on harmonic mean of normalized objectives) and applies a hybrid local search that aggressively removes low-value items (below 80% of the median ratio) and adds high-value items (above 120% of the median ratio) while ensuring feasibility through probabilistic selection and value-aware removal. It prioritizes items with combined value-to-weight ratios from both objectives, dynamically adjusting thresholds based on included items.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest harmonic mean of normalized objectives\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    harmonic_scores = 2 / (1/normalized_obj[:, 0] + 1/normalized_obj[:, 1])\n    selected_idx = np.argmax(harmonic_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value-to-weight ratios for both objectives\n    ratio1 = value1_lst / weight_lst\n    ratio2 = value2_lst / weight_lst\n    combined_ratio = ratio1 + ratio2\n\n    # Dynamic threshold selection (median of included items' ratios)\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0:\n        included_ratios = combined_ratio[included_items]\n        threshold_ratio = np.median(included_ratios)\n    else:\n        threshold_ratio = np.median(combined_ratio)\n\n    # Hybrid operation: remove items below threshold and add high-ratio items\n    for item in included_items:\n        if combined_ratio[item] < threshold_ratio * 0.8:  # More aggressive removal\n            if np.random.rand() < 0.6:  # Higher removal probability\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Add items with high ratios if they fit\n    for item in excluded_items:\n        if combined_ratio[item] > threshold_ratio * 1.2:  # Higher addition threshold\n            if current_weight + weight_lst[item] <= capacity:\n                if np.random.rand() < 0.8:  # Higher addition probability\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Final capacity check with value-aware removal\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    if current_weight > capacity:\n        # Remove items with lowest value-to-weight ratio first\n        over_items = np.where(new_solution == 1)[0]\n        sorted_items = sorted(over_items, key=lambda x: combined_ratio[x])\n        for item in sorted_items:\n            if current_weight <= capacity:\n                break\n            new_solution[item] = 0\n            current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 166,
        "algorithm": "The algorithm combines Pareto dominance and crowding distance to select a promising solution from the archive, then applies a three-phase hybrid local search: Phase 1 adds high-utility items, Phase 2 performs Pareto-aware swaps with dynamic thresholds, and Phase 3 removes low-utility items while dynamically adjusting weights between objectives to maintain feasibility. The utility scores balance both objectives, with higher priority given to the first objective when over capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto dominance and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Identify Pareto frontier\n        is_dominated = np.zeros(len(objectives), dtype=bool)\n        for i in range(len(objectives)):\n            for j in range(len(objectives)):\n                if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                              (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                    is_dominated[i] = True\n                    break\n        frontier_indices = np.where(~is_dominated)[0]\n\n        if len(frontier_indices) > 0:\n            # Calculate crowding distance for frontier solutions\n            frontier_obj = objectives[frontier_indices]\n            crowding = np.zeros(len(frontier_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(frontier_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(frontier_obj)-1):\n                    if frontier_obj[sorted_idx[-1], i] != frontier_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (frontier_obj[sorted_idx[j+1], i] - frontier_obj[sorted_idx[j-1], i]) / \\\n                                                  (frontier_obj[sorted_idx[-1], i] - frontier_obj[sorted_idx[0], i])\n            # Select solution with highest crowding distance\n            selected_idx = frontier_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate marginal values and utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    utility_scores = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions of high-marginal-value items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% marginal items\n        sorted_candidates = candidates[np.argsort(utility_scores[candidates])[::-1]]\n        top_items = sorted_candidates[:max(1, len(sorted_candidates) // 5)]\n\n        for idx in top_items:\n            if np.random.rand() < 0.8:  # 80% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-aware swaps with dynamic thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with dynamic probability\n                prob_threshold = 0.5 + 0.3 * (utility_scores[j] / (utility_scores[i] + 1e-6))\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > marginal1[i] and np.random.rand() < prob_threshold) or\n                    (marginal2[j] > marginal2[i] and np.random.rand() < prob_threshold)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with adaptive utility scores\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate adaptive utility scores (weighted by current load)\n        alpha = 0.5 + 0.4 * (current_weight / capacity)  # More weight to first objective when over capacity\n        adaptive_utilities = alpha * marginal1[included_items] + (1-alpha) * marginal2[included_items]\n\n        # Remove items with lowest adaptive utility\n        worst_item = included_items[np.argmin(adaptive_utilities)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.882788127345195,
            2.2635218799114227
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine Pareto dominance and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Identify Pareto frontier\n        is_dominated = np.zeros(len(objectives), dtype=bool)\n        for i in range(len(objectives)):\n            for j in range(len(objectives)):\n                if i != j and (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                              (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                    is_dominated[i] = True\n                    break\n        frontier_indices = np.where(~is_dominated)[0]\n\n        if len(frontier_indices) > 0:\n            # Calculate crowding distance for frontier solutions\n            frontier_obj = objectives[frontier_indices]\n            crowding = np.zeros(len(frontier_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(frontier_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(frontier_obj)-1):\n                    if frontier_obj[sorted_idx[-1], i] != frontier_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (frontier_obj[sorted_idx[j+1], i] - frontier_obj[sorted_idx[j-1], i]) / \\\n                                                  (frontier_obj[sorted_idx[-1], i] - frontier_obj[sorted_idx[0], i])\n            # Select solution with highest crowding distance\n            selected_idx = frontier_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate marginal values and utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    utility_scores = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions of high-marginal-value items\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% marginal items\n        sorted_candidates = candidates[np.argsort(utility_scores[candidates])[::-1]]\n        top_items = sorted_candidates[:max(1, len(sorted_candidates) // 5)]\n\n        for idx in top_items:\n            if np.random.rand() < 0.8:  # 80% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-aware swaps with dynamic thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with dynamic probability\n                prob_threshold = 0.5 + 0.3 * (utility_scores[j] / (utility_scores[i] + 1e-6))\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > marginal1[i] and np.random.rand() < prob_threshold) or\n                    (marginal2[j] > marginal2[i] and np.random.rand() < prob_threshold)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with adaptive utility scores\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Calculate adaptive utility scores (weighted by current load)\n        alpha = 0.5 + 0.4 * (current_weight / capacity)  # More weight to first objective when over capacity\n        adaptive_utilities = alpha * marginal1[included_items] + (1-alpha) * marginal2[included_items]\n\n        # Remove items with lowest adaptive utility\n        worst_item = included_items[np.argmin(adaptive_utilities)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a high-potential solution from the top 25% of the archive (sorted by combined objective values) and applies a three-phase hybrid local search: it first adds items with high marginal impact for both objectives using a Pareto-inspired heuristic, then performs capacity-preserving replacements by removing least impactful items, and finally conducts probabilistic flips of high-impact items with dynamic thresholds. The method balances exploration and exploitation by dynamically adjusting selection criteria to ensure high-quality, feasible neighbors while avoiding premature convergence.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts and combined impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Pareto-inspired additions (items that improve both objectives)\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements (remove worst items)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 4 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(4, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic algorithm prioritizes solutions near the Pareto frontier (top 1/3 of the archive) and employs a three-phase local search: (1) greedily adding high-impact items (top 30% marginal value), (2) adaptively swapping items with complementary marginal gains, and (3) capacity-preserving removals of low-impact items, while dynamically adjusting selection thresholds based on Pareto dominance and marginal value. The algorithm ensures feasibility through probabilistic capacity checks and balances exploration of both objectives by combining marginal impacts.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 167,
        "algorithm": "The algorithm dynamically selects promising solutions from the archive's frontier (top 25%), calculates marginal impacts for both objectives, and applies a three-phase local search: 1) probabilistically adds high-impact items, 2) performs capacity-aware swaps prioritizing complementary gains, and 3) adaptively removes low-impact items while ensuring feasibility. It balances exploration and exploitation by dynamically adjusting selection probabilities and thresholds based on combined marginal impacts. The solution prioritizes items with high combined marginal gains and ensures feasibility through continuous capacity checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic frontier selection (top 1/4 of archive)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 4)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic additions with adaptive thresholds\n    dynamic_threshold = np.percentile(combined_marginal, 60)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary gains\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary gains\n                if ((marginal1[j] > 0.9 * marginal1[i] and marginal2[j] > 1.1 * marginal2[i]) or\n                    (marginal2[j] > 0.9 * marginal2[i] and marginal1[j] > 1.1 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 2)]]\n        if np.random.rand() < 0.8:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8323386679960927,
            2.344767540693283
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic frontier selection (top 1/4 of archive)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 4)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic additions with adaptive thresholds\n    dynamic_threshold = np.percentile(combined_marginal, 60)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary gains\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary gains\n                if ((marginal1[j] > 0.9 * marginal1[i] and marginal2[j] > 1.1 * marginal2[i]) or\n                    (marginal2[j] > 0.9 * marginal2[i] and marginal1[j] > 1.1 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 2)]]\n        if np.random.rand() < 0.8:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic algorithm prioritizes solutions near the Pareto frontier (top 1/3 of the archive) and employs a three-phase local search: (1) greedily adding high-impact items (top 30% marginal value), (2) adaptively swapping items with complementary marginal gains, and (3) capacity-preserving removals of low-impact items, while dynamically adjusting selection thresholds based on Pareto dominance and marginal value. The algorithm ensures feasibility through probabilistic capacity checks and balances exploration of both objectives by combining marginal impacts.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 168,
        "algorithm": "The algorithm dynamically selects promising solutions from the archive using a combination of frontier proximity and crowding distance, then applies a three-phase hybrid local search that adaptively replaces items based on Pareto dominance and marginal gains, performs capacity-aware swaps between complementary items, and uses dynamic thresholding to remove low-impact items while maintaining feasibility through probabilistic adjustments. The method prioritizes solutions near the Pareto frontier with good diversification, and the local search emphasizes both balanced improvements and complementary gains across objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection combining frontier proximity and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Combine with frontier proximity (top 25%)\n        frontier_threshold = np.percentile(objectives.sum(axis=1), 75)\n        frontier_mask = objectives.sum(axis=1) >= frontier_threshold\n        if np.any(frontier_mask):\n            candidates = np.where(frontier_mask)[0]\n            selected_idx = candidates[np.argmax(crowding[candidates])]\n        else:\n            selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive replacement with Pareto consideration\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.3 * marginal1[i] and marginal2[j] > 0.7 * marginal2[i]) or\n                    (marginal2[j] > 1.3 * marginal2[i] and marginal1[j] > 0.7 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 2: Capacity-aware swaps with complementary gains\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check for complementary gains (one objective improves significantly while the other doesn't worsen too much)\n                if ((marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.9 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.9 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic thresholding removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate dynamic threshold (median of included items' combined marginal)\n        dynamic_threshold = np.median(combined_marginal[included_items])\n        for item in included_items:\n            if combined_marginal[item] < dynamic_threshold and np.random.rand() < 0.6:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Ensure feasibility\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8650236762180816,
            5.764221698045731
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection combining frontier proximity and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Combine with frontier proximity (top 25%)\n        frontier_threshold = np.percentile(objectives.sum(axis=1), 75)\n        frontier_mask = objectives.sum(axis=1) >= frontier_threshold\n        if np.any(frontier_mask):\n            candidates = np.where(frontier_mask)[0]\n            selected_idx = candidates[np.argmax(crowding[candidates])]\n        else:\n            selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive replacement with Pareto consideration\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.3 * marginal1[i] and marginal2[j] > 0.7 * marginal2[i]) or\n                    (marginal2[j] > 1.3 * marginal2[i] and marginal1[j] > 0.7 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 2: Capacity-aware swaps with complementary gains\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check for complementary gains (one objective improves significantly while the other doesn't worsen too much)\n                if ((marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.9 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.9 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Dynamic thresholding removal\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate dynamic threshold (median of included items' combined marginal)\n        dynamic_threshold = np.median(combined_marginal[included_items])\n        for item in included_items:\n            if combined_marginal[item] < dynamic_threshold and np.random.rand() < 0.6:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    # Ensure feasibility\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic algorithm prioritizes solutions near the Pareto frontier (top 1/3 of the archive) and employs a three-phase local search: (1) greedily adding high-impact items (top 30% marginal value), (2) adaptively swapping items with complementary marginal gains, and (3) capacity-preserving removals of low-impact items, while dynamically adjusting selection thresholds based on Pareto dominance and marginal value. The algorithm ensures feasibility through probabilistic capacity checks and balances exploration of both objectives by combining marginal impacts.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 169,
        "algorithm": "The algorithm employs a multi-phase local search strategy that first inserts high-potential items based on normalized marginal gains, then adaptively swaps items to improve Pareto efficiency, and finally refines the solution by balancing capacity constraints and objective improvements, prioritizing items with high combined efficiency and probabilistic selection to ensure feasibility. The solution selection focuses on top-performing archive entries, while the local search emphasizes balanced improvements in both objectives through adaptive thresholds and probabilistic decisions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    selected_idx = np.random.randint(0, min(3, len(archive)))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal gains\n    weight_efficiency = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n\n    # Phase 1: High-potential insertions with adaptive threshold\n    threshold = np.percentile(weight_efficiency, 60)\n    candidate_items = np.where((weight_efficiency >= threshold) & (new_solution == 0))[0]\n\n    for idx in candidate_items:\n        if weight_lst[idx] <= (capacity - current_weight):\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive removals and insertions\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= (capacity - current_weight + weight_lst[i]):\n                # Check for Pareto improvement or significant marginal gain\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > 1.1 * value1_lst[i] and value2_lst[j] > 0.9 * value2_lst[i]) or \\\n                   (value2_lst[j] > 1.1 * value2_lst[i] and value1_lst[j] > 0.9 * value1_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity adjustment with probabilistic selection\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest combined efficiency or probabilistic selection\n        removal_candidates = included_items[np.argsort(weight_efficiency[included_items])[:max(1, len(included_items) // 4)]]\n        if np.random.rand() < 0.6:\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Final refinement: Objective-balanced swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= (capacity - current_weight + weight_lst[i]):\n                # Check for balanced improvement\n                if (value1_lst[j] > 0.95 * value1_lst[i] and value2_lst[j] > 0.95 * value2_lst[i] and\n                    (value1_lst[j] + value2_lst[j]) > (value1_lst[i] + value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.9114704002839041,
            4.020656108856201
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]) / (np.sum(weight_lst[x[0] == 1]) + 1e-6), reverse=True)\n    selected_idx = np.random.randint(0, min(3, len(archive)))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal gains\n    weight_efficiency = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n\n    # Phase 1: High-potential insertions with adaptive threshold\n    threshold = np.percentile(weight_efficiency, 60)\n    candidate_items = np.where((weight_efficiency >= threshold) & (new_solution == 0))[0]\n\n    for idx in candidate_items:\n        if weight_lst[idx] <= (capacity - current_weight):\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Phase 2: Adaptive removals and insertions\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= (capacity - current_weight + weight_lst[i]):\n                # Check for Pareto improvement or significant marginal gain\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > 1.1 * value1_lst[i] and value2_lst[j] > 0.9 * value2_lst[i]) or \\\n                   (value2_lst[j] > 1.1 * value2_lst[i] and value1_lst[j] > 0.9 * value1_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Capacity adjustment with probabilistic selection\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest combined efficiency or probabilistic selection\n        removal_candidates = included_items[np.argsort(weight_efficiency[included_items])[:max(1, len(included_items) // 4)]]\n        if np.random.rand() < 0.6:\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Final refinement: Objective-balanced swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= (capacity - current_weight + weight_lst[i]):\n                # Check for balanced improvement\n                if (value1_lst[j] > 0.95 * value1_lst[i] and value2_lst[j] > 0.95 * value2_lst[i] and\n                    (value1_lst[j] + value2_lst[j]) > (value1_lst[i] + value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic algorithm prioritizes solutions near the Pareto frontier (top 1/3 of the archive) and employs a three-phase local search: (1) greedily adding high-impact items (top 30% marginal value), (2) adaptively swapping items with complementary marginal gains, and (3) capacity-preserving removals of low-impact items, while dynamically adjusting selection thresholds based on Pareto dominance and marginal value. The algorithm ensures feasibility through probabilistic capacity checks and balances exploration of both objectives by combining marginal impacts.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 170,
        "algorithm": "The algorithm prioritizes solutions near the Pareto frontier (top 1/4 of the archive) and employs a four-phase local search: (1) greedy additions of high-impact items, (2) adaptive swaps between complementary items, (3) capacity-preserving removals of low-impact items, and (4) trade-off exploration based on objective correlations, ensuring feasibility while dynamically adjusting selection thresholds. The method balances exploitation of high-marginal-value items with exploration of trade-offs, particularly when objective correlations are weak.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier (top 1/4)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 4)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 40% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 60)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.3 * marginal1[i] and marginal2[j] > 0.7 * marginal2[i]) or\n                    (marginal2[j] > 1.3 * marginal2[i] and marginal1[j] > 0.7 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 4)]]\n        if np.random.rand() < 0.8:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Trade-off exploration by selectively adjusting based on objective correlations\n    correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if abs(correlation) < 0.3:  # Weak correlation, explore trade-offs\n        for idx in np.random.permutation(len(new_solution)):\n            if new_solution[idx] == 1 and np.random.rand() < 0.2:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n            elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.1:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8374965770327357,
            2.819192558526993
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier (top 1/4)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 4)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 40% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 60)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.3 * marginal1[i] and marginal2[j] > 0.7 * marginal2[i]) or\n                    (marginal2[j] > 1.3 * marginal2[i] and marginal1[j] > 0.7 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 4)]]\n        if np.random.rand() < 0.8:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Trade-off exploration by selectively adjusting based on objective correlations\n    correlation = np.corrcoef(value1_lst, value2_lst)[0, 1]\n    if abs(correlation) < 0.3:  # Weak correlation, explore trade-offs\n        for idx in np.random.permutation(len(new_solution)):\n            if new_solution[idx] == 1 and np.random.rand() < 0.2:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n            elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity and np.random.rand() < 0.1:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe heuristic algorithm prioritizes solutions near the Pareto frontier (top 1/3 of the archive) and employs a three-phase local search: (1) greedily adding high-impact items (top 30% marginal value), (2) adaptively swapping items with complementary marginal gains, and (3) capacity-preserving removals of low-impact items, while dynamically adjusting selection thresholds based on Pareto dominance and marginal value. The algorithm ensures feasibility through probabilistic capacity checks and balances exploration of both objectives by combining marginal impacts.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-aware selection: prioritize solutions near the frontier\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 3)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy additions with dynamic threshold (top 30% marginal impact)\n    dynamic_threshold = np.percentile(combined_marginal, 70)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive swaps between complementary items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement or significant marginal gain\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.2 * marginal1[i] and marginal2[j] > 0.8 * marginal2[i]) or\n                    (marginal2[j] > 1.2 * marginal2[i] and marginal1[j] > 0.8 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving removals with dynamic criteria\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact or probabilistic selection\n        removal_candidates = included_items[np.argsort(combined_marginal[included_items])[:max(1, len(included_items) // 3)]]\n        if np.random.rand() < 0.7:  # Higher chance to remove worst items\n            worst_item = removal_candidates[0]\n        else:\n            worst_item = np.random.choice(removal_candidates)\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm selects the best solution from the archive (based on combined objective values) and applies a three-phase local search: first greedily adds high-value items, then performs objective-aware swaps between included and excluded items, and finally removes low-value items to maintain feasibility. It prioritizes items with high combined normalized value-weight ratios and uses adaptive weight adjustments to ensure feasibility while improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized value-weight ratios for both objectives\n    norm1 = value1_lst / (weight_lst + 1e-6)\n    norm2 = value2_lst / (weight_lst + 1e-6)\n    combined_norm = (norm1 + norm2) / 2\n    sorted_indices = np.argsort(combined_norm)[::-1]\n\n    # Phase 1: Greedy addition of high-value items\n    remaining_capacity = capacity - current_weight\n    for idx in sorted_indices:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-aware swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate potential improvement for each objective\n                delta1 = value1_lst[j] - value1_lst[i]\n                delta2 = value2_lst[j] - value2_lst[i]\n                # Accept swap if both objectives improve or one improves significantly\n                if (delta1 > 0 and delta2 > 0) or (delta1 > 0.1 * value1_lst[i] and delta2 > 0.1 * value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with value-weight priority\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        # Remove items with lowest combined normalized value\n        worst_item = included[np.argmin(combined_norm[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 171,
        "algorithm": "The algorithm selects a high-performing solution from the top 25% of the archive (prioritizing combined objective values) and applies a four-phase local search: (1) greedily adds high-impact items (top 25% marginal impact), (2) performs Pareto-improving swaps between included/excluded items, (3) adaptively replaces items with significantly better marginal contributions, and (4) removes low-impact items to ensure feasibility. The method balances exploration and exploitation by dynamically adjusting selection criteria based on Pareto dominance and marginal impact, while maintaining feasibility through weight-sensitive rebalancing.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution from the top 25% of the archive\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 4)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy addition of high-impact items (top 25%)\n    dynamic_threshold = np.percentile(combined_marginal, 75)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-improving swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.1 * marginal1[i] and marginal2[j] > 0.9 * marginal2[i]) or\n                    (marginal2[j] > 1.1 * marginal2[i] and marginal1[j] > 0.9 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive replacements\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Replace if marginal impact is significantly better\n                if (combined_marginal[j] > 1.2 * combined_marginal[i] and\n                    current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 4: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8180366051428287,
            3.6570158302783966
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution from the top 25% of the archive\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    frontier_size = max(1, len(archive) // 4)\n    selected_idx = np.random.randint(0, frontier_size)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Greedy addition of high-impact items (top 25%)\n    dynamic_threshold = np.percentile(combined_marginal, 75)\n    high_impact_items = np.where(combined_marginal >= dynamic_threshold)[0]\n    remaining_capacity = capacity - current_weight\n\n    for idx in high_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-improving swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (marginal1[j] > 1.1 * marginal1[i] and marginal2[j] > 0.9 * marginal2[i]) or\n                    (marginal2[j] > 1.1 * marginal2[i] and marginal1[j] > 0.9 * marginal1[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive replacements\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Replace if marginal impact is significantly better\n                if (combined_marginal[j] > 1.2 * combined_marginal[i] and\n                    current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 4: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the top 25% of the archive (sorted by combined objective values) and applies a three-phase hybrid local search: it first adds items with high marginal impact for both objectives using a Pareto-inspired heuristic, then performs capacity-preserving replacements by removing least impactful items, and finally conducts probabilistic flips of high-impact items with dynamic thresholds. The method balances exploration and exploitation by dynamically adjusting selection criteria to ensure high-quality, feasible neighbors while avoiding premature convergence.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts and combined impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Pareto-inspired additions (items that improve both objectives)\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements (remove worst items)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 4 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(4, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 172,
        "algorithm": "The algorithm selects solutions from crowded regions of the objective space, then applies a hybrid local search combining Pareto-aware additions of high-marginal-value items, dynamic threshold-based replacements of low-value items, and probabilistic flips of high-impact items to balance exploration and exploitation while ensuring feasibility. It prioritizes items with combined marginal benefits and dynamically adjusts thresholds to maintain solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined = marginal1 + marginal2\n\n    # Phase 1: Pareto-aware additions\n    available = np.where(new_solution == 0)[0]\n    if len(available) > 0:\n        pareto_front = []\n        for idx in available:\n            if weight_lst[idx] <= capacity - current_weight:\n                pareto_front.append(idx)\n                for i in range(len(pareto_front)-1, 0, -1):\n                    if (value1_lst[idx] >= value1_lst[pareto_front[i-1]] and\n                        value2_lst[idx] >= value2_lst[pareto_front[i-1]]):\n                        if (value1_lst[idx] > value1_lst[pareto_front[i-1]] or\n                            value2_lst[idx] > value2_lst[pareto_front[i-1]]):\n                            pareto_front.pop(i-1)\n                    elif (value1_lst[idx] <= value1_lst[pareto_front[i-1]] and\n                          value2_lst[idx] <= value2_lst[pareto_front[i-1]]):\n                        pareto_front.pop(-1)\n                        break\n\n        if pareto_front:\n            selected = np.random.choice(pareto_front)\n            new_solution[selected] = 1\n            current_weight += weight_lst[selected]\n\n    # Phase 2: Dynamic threshold replacements\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0:\n        threshold1 = np.percentile(value1_lst[included], 30)\n        threshold2 = np.percentile(value2_lst[included], 30)\n        for idx in included:\n            if (value1_lst[idx] < threshold1 and value2_lst[idx] < threshold2 and\n                current_weight - weight_lst[idx] <= capacity):\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 3: Probabilistic flips of high-impact items\n    high_impact = np.argsort(combined)[-max(1, len(combined)//4):]\n    for idx in high_impact:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            if np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            if np.random.rand() < 0.3:  # 30% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.47868338384909276,
            0.8228515982627869
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined = marginal1 + marginal2\n\n    # Phase 1: Pareto-aware additions\n    available = np.where(new_solution == 0)[0]\n    if len(available) > 0:\n        pareto_front = []\n        for idx in available:\n            if weight_lst[idx] <= capacity - current_weight:\n                pareto_front.append(idx)\n                for i in range(len(pareto_front)-1, 0, -1):\n                    if (value1_lst[idx] >= value1_lst[pareto_front[i-1]] and\n                        value2_lst[idx] >= value2_lst[pareto_front[i-1]]):\n                        if (value1_lst[idx] > value1_lst[pareto_front[i-1]] or\n                            value2_lst[idx] > value2_lst[pareto_front[i-1]]):\n                            pareto_front.pop(i-1)\n                    elif (value1_lst[idx] <= value1_lst[pareto_front[i-1]] and\n                          value2_lst[idx] <= value2_lst[pareto_front[i-1]]):\n                        pareto_front.pop(-1)\n                        break\n\n        if pareto_front:\n            selected = np.random.choice(pareto_front)\n            new_solution[selected] = 1\n            current_weight += weight_lst[selected]\n\n    # Phase 2: Dynamic threshold replacements\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0:\n        threshold1 = np.percentile(value1_lst[included], 30)\n        threshold2 = np.percentile(value2_lst[included], 30)\n        for idx in included:\n            if (value1_lst[idx] < threshold1 and value2_lst[idx] < threshold2 and\n                current_weight - weight_lst[idx] <= capacity):\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 3: Probabilistic flips of high-impact items\n    high_impact = np.argsort(combined)[-max(1, len(combined)//4):]\n    for idx in high_impact:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            if np.random.rand() < 0.4:  # 40% chance to remove\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            if np.random.rand() < 0.3:  # 30% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 173,
        "algorithm": "The algorithm combines dynamic Pareto-frontier selection with a multi-phase local search strategy: it first expands the frontier by adding high-impact items, then performs balanced swaps between included and excluded items, followed by adaptive rebalancing to maintain feasibility, and finally probabilistically adds high-impact items. Key variables are prioritized based on combined marginal utility, with a focus on maintaining objective trade-offs and feasibility throughout the process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-frontier selection (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized objective impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-expansion additions (items above 75th percentile)\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 75)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-balanced swaps (items with complementary improvements)\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for balanced improvements (one objective improves while the other doesn't worsen significantly)\n                if (marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i] * 0.9) or \\\n                   (marginal2[j] > marginal2[i] and marginal1[j] >= marginal1[i] * 0.9):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive rebalancing with dynamic utility function\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Dynamic utility function: prioritize items with low marginal impact but high contribution to objectives\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            # Utility considers both marginal impact and normalized contribution to objectives\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(2, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8483099124291444,
            1.6519125401973724
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-frontier selection (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized objective impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-expansion additions (items above 75th percentile)\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 75)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-balanced swaps (items with complementary improvements)\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for balanced improvements (one objective improves while the other doesn't worsen significantly)\n                if (marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i] * 0.9) or \\\n                   (marginal2[j] > marginal2[i] and marginal1[j] >= marginal1[i] * 0.9):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive rebalancing with dynamic utility function\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Dynamic utility function: prioritize items with low marginal impact but high contribution to objectives\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            # Utility considers both marginal impact and normalized contribution to objectives\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(2, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 174,
        "algorithm": "The algorithm combines dynamic Pareto-frontier selection with a three-phase hybrid local search: (1) It first identifies promising solutions near the knee-point of the Pareto frontier, (2) then performs frontier-aware item additions with adaptive thresholds, (3) followed by capacity-constrained swaps between complementary items, and finally (4) dynamically removes low-utility items to maintain feasibility. The algorithm prioritizes items with high combined marginal impact while ensuring feasibility through strict capacity enforcement and adaptive threshold adjustments.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 0: Dynamic selection near Pareto frontier using knee-point detection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate knee-point (solution with maximum perpendicular distance to the line connecting extremes)\n        min_obj = np.min(objectives, axis=0)\n        max_obj = np.max(objectives, axis=0)\n        line = max_obj - min_obj\n        distances = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            vec = objectives[i] - min_obj\n            if line[0] == 0 or line[1] == 0:\n                distances[i] = np.linalg.norm(vec)\n            else:\n                # Perpendicular distance to the line\n                distances[i] = np.abs(np.cross(vec, line)) / np.linalg.norm(line)\n        selected_idx = np.argmax(distances)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-aware addition with adaptive percentile thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive percentile based on current solution quality\n        percentile = 90 - (current_weight / capacity) * 20\n        threshold = np.percentile(combined_marginal, percentile)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on normalized marginal impact\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-constrained swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    # Only swap if at least one objective improves\n                    if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        remaining_capacity = capacity - current_weight\n                        break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8352553463435567,
            1.2753121852874756
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 0: Dynamic selection near Pareto frontier using knee-point detection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate knee-point (solution with maximum perpendicular distance to the line connecting extremes)\n        min_obj = np.min(objectives, axis=0)\n        max_obj = np.max(objectives, axis=0)\n        line = max_obj - min_obj\n        distances = np.zeros(len(objectives))\n        for i in range(len(objectives)):\n            vec = objectives[i] - min_obj\n            if line[0] == 0 or line[1] == 0:\n                distances[i] = np.linalg.norm(vec)\n            else:\n                # Perpendicular distance to the line\n                distances[i] = np.abs(np.cross(vec, line)) / np.linalg.norm(line)\n        selected_idx = np.argmax(distances)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-aware addition with adaptive percentile thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive percentile based on current solution quality\n        percentile = 90 - (current_weight / capacity) * 20\n        threshold = np.percentile(combined_marginal, percentile)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on normalized marginal impact\n            for idx in strong_candidates:\n                prob = min(0.8, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-constrained swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    # Only swap if at least one objective improves\n                    if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]):\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        remaining_capacity = capacity - current_weight\n                        break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 175,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with a Pareto-adaptive threshold, (2) performing capacity-aware swaps between items with complementary marginal improvements, and (3) dynamically rebalancing by removing low-utility items with a diversity-preserving mechanism. The algorithm prioritizes items with high combined marginal impact (value1 + value2) and ensures feasibility by dynamically adjusting the solution when the capacity is exceeded.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with Pareto-adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Pareto-adaptive threshold\n        pareto_front = []\n        for idx in candidates:\n            dominated = False\n            for j in candidates:\n                if idx != j and value1_lst[idx] <= value1_lst[j] and value2_lst[idx] <= value2_lst[j] and (value1_lst[idx] < value1_lst[j] or value2_lst[idx] < value2_lst[j]):\n                    dominated = True\n                    break\n            if not dominated:\n                pareto_front.append(idx)\n\n        if len(pareto_front) > 0:\n            threshold = np.percentile(combined_marginal[pareto_front], 100 - (current_weight/capacity)*40)\n            strong_candidates = [idx for idx in pareto_front if combined_marginal[idx] >= threshold]\n\n            if len(strong_candidates) > 0:\n                for idx in strong_candidates:\n                    prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                    if np.random.rand() < prob:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n                        remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Enhanced complementary check with trade-off consideration\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i] and\n                    (value1_lst[j]/value2_lst[j] > value1_lst[i]/value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity-preserving removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Diversity-preserving utility scores\n        utility_scores = []\n        for idx in included:\n            # Utility considers marginal impact, objective trade-offs, and diversity\n            diversity_contribution = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))) *\n                      (1 + diversity_contribution))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.768187213219621,
            0.7769441306591034
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with Pareto-adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Pareto-adaptive threshold\n        pareto_front = []\n        for idx in candidates:\n            dominated = False\n            for j in candidates:\n                if idx != j and value1_lst[idx] <= value1_lst[j] and value2_lst[idx] <= value2_lst[j] and (value1_lst[idx] < value1_lst[j] or value2_lst[idx] < value2_lst[j]):\n                    dominated = True\n                    break\n            if not dominated:\n                pareto_front.append(idx)\n\n        if len(pareto_front) > 0:\n            threshold = np.percentile(combined_marginal[pareto_front], 100 - (current_weight/capacity)*40)\n            strong_candidates = [idx for idx in pareto_front if combined_marginal[idx] >= threshold]\n\n            if len(strong_candidates) > 0:\n                for idx in strong_candidates:\n                    prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                    if np.random.rand() < prob:\n                        new_solution[idx] = 1\n                        current_weight += weight_lst[idx]\n                        remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Enhanced complementary check with trade-off consideration\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i] and\n                    (value1_lst[j]/value2_lst[j] > value1_lst[i]/value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with diversity-preserving removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Diversity-preserving utility scores\n        utility_scores = []\n        for idx in included:\n            # Utility considers marginal impact, objective trade-offs, and diversity\n            diversity_contribution = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))) *\n                      (1 + diversity_contribution))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 176,
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9686882459758612,
            1.19118270277977
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 177,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion that combines objective values, diversity, and Pareto frontier proximity, then generates a neighbor through three phases: (1) adaptive item addition based on percentile thresholds of combined marginal impacts, (2) targeted swaps that improve at least one objective while maintaining feasibility, and (3) utility-based removal of low-impact items to rebalance the solution. The selection prioritizes frontier-proximate solutions (60% weight) over diverse ones (40% weight), while the neighbor generation focuses on high-marginal-impact items and Pareto improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values, diversity, and frontier proximity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        frontier_score = obj[0] + obj[1]\n        score = (frontier_score * 0.6) + (diversity * 0.4)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Dynamic addition with adaptive percentile thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive percentile based on current solution quality\n        percentile = 100 - (current_weight/capacity)*40\n        threshold = np.percentile(combined_marginal, percentile)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with Pareto improvement\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Pareto-aware rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9860152765024062,
            2.8089873492717743
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values, diversity, and frontier proximity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        frontier_score = obj[0] + obj[1]\n        score = (frontier_score * 0.6) + (diversity * 0.4)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Dynamic addition with adaptive percentile thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive percentile based on current solution quality\n        percentile = 100 - (current_weight/capacity)*40\n        threshold = np.percentile(combined_marginal, percentile)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps with Pareto improvement\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto improvement\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Pareto-aware rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes underrepresented quadrants in the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items (weighted by an adaptive balance between objectives) and removes low-contribution items (based on normalized combined utility), while adjusting thresholds to balance exploration and exploitation. The method ensures feasibility by only adding items that fit within the remaining capacity and removing items below a dynamically calculated contribution threshold.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Diversity-aware selection based on objective quadrants\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with adaptive thresholds\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate weighted marginal value with adaptive weights\n        alpha = 0.6 + 0.2 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items with adaptive probability\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                prob = min(0.9, marginal[item] / np.max(marginal))\n                if np.random.rand() < prob:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n\n    # Dynamic removal based on adaptive contribution thresholds\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution with adaptive weights\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        beta = 0.5 + 0.3 * (current_weight / capacity)  # Dynamically adjust weight for first objective\n        contribution = (beta * value1_lst[included_items] / total_value1 + (1-beta) * value2_lst[included_items] / total_value2) / 2\n        adaptive_threshold = np.percentile(contribution, 25)  # Remove bottom 25%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < adaptive_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the top 25% of the archive (sorted by combined objective values) and applies a three-phase hybrid local search: it first adds items with high marginal impact for both objectives using a Pareto-inspired heuristic, then performs capacity-preserving replacements by removing least impactful items, and finally conducts probabilistic flips of high-impact items with dynamic thresholds. The method balances exploration and exploitation by dynamically adjusting selection criteria to ensure high-quality, feasible neighbors while avoiding premature convergence.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts and combined impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Pareto-inspired additions (items that improve both objectives)\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements (remove worst items)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 4 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(4, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 178,
        "algorithm": "The algorithm dynamically selects a solution from the Pareto frontier using crowding distance to prioritize less-crowded regions, then applies a hybrid local search that combines Pareto-aware item additions (weighted by objective importance), capacity-preserving replacements (removing worst-marginal items), and probabilistic flips of high-impact items (adjusting 25-75% of the most valuable items). The dynamic weight *alpha* balances exploration/exploitation based on solution position, ensuring both objectives are improved while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection from Pareto frontier using crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate dynamic crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Adjust weights based on current solution's position\n        frontier_idx = np.argmax(crowding)\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic thresholds based on current solution's position\n    current_obj = objectives[selected_idx]\n    obj_range = objectives.max(axis=0) - objectives.min(axis=0) + 1e-6\n    position = (current_obj - objectives.min(axis=0)) / obj_range\n    alpha = 0.5 + 0.3 * position[0]  # Dynamic weight for objective 1\n\n    # Phase 1: Pareto-aware additions (items that improve both objectives)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (alpha * marginal1 + (1-alpha) * marginal2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_marginal)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal1[idx] > marginal1.mean() and marginal2[idx] > marginal2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7443222026795734,
            0.7958133518695831
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic selection from Pareto frontier using crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate dynamic crowding distance\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Adjust weights based on current solution's position\n        frontier_idx = np.argmax(crowding)\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate dynamic thresholds based on current solution's position\n    current_obj = objectives[selected_idx]\n    obj_range = objectives.max(axis=0) - objectives.min(axis=0) + 1e-6\n    position = (current_obj - objectives.min(axis=0)) / obj_range\n    alpha = 0.5 + 0.3 * position[0]  # Dynamic weight for objective 1\n\n    # Phase 1: Pareto-aware additions (items that improve both objectives)\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (alpha * marginal1 + (1-alpha) * marginal2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_marginal)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal1[idx] > marginal1.mean() and marginal2[idx] > marginal2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_marginal[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items (based on combined marginal impact) through three phases: adding top 15% of high-impact excluded items, performing targeted swaps between included and excluded items that improve at least one objective while respecting capacity, and dynamically removing low-impact included items when capacity is exceeded. It intelligently selects solutions from the top 15% of the archive for improvement, using adaptive thresholds and dynamic probabilities to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 179,
        "algorithm": "The algorithm combines dynamic Pareto-frontier selection with a hybrid local search strategy that prioritizes high-impact items (based on combined marginal value/weight ratios) while ensuring feasibility. It interleaves targeted item additions, Pareto-aware swaps, and capacity-constrained removals, dynamically adjusting exploration-exploitation balance by favoring top-percentile items and removing low-impact items when capacity is exceeded. The method balances both objectives by evaluating swaps only when they improve at least one objective while respecting capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    archive_sorted = sorted(archive, key=lambda x: -(x[1][0] + x[1][1]))\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive_sorted[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate Pareto-aware marginal impacts\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with adaptive probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Pareto-aware swaps with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value2_lst[j] > value2_lst[i] and value1_lst[j] >= value1_lst[i])) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 20)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.9563223766465291,
            2.032307356595993
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    archive_sorted = sorted(archive, key=lambda x: -(x[1][0] + x[1][1]))\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive_sorted[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate Pareto-aware marginal impacts\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with adaptive probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Pareto-aware swaps with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value2_lst[j] > value2_lst[i] and value1_lst[j] >= value1_lst[i])) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 20)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items (based on combined marginal impact) through three phases: adding top 15% of high-impact excluded items, performing targeted swaps between included and excluded items that improve at least one objective while respecting capacity, and dynamically removing low-impact included items when capacity is exceeded. It intelligently selects solutions from the top 15% of the archive for improvement, using adaptive thresholds and dynamic probabilities to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 180,
        "algorithm": "The algorithm combines dynamic solution selection based on combined objective scores, entropy, and diversity with a three-phase local search: frontier-focused additions (prioritizing high-marginal-value items), capacity-aware swaps (exchanging items for Pareto improvements), and adaptive replacements (removing low-utility items while maintaining feasibility). It balances exploration and exploitation through probabilistic selection and entropy-aware utility metrics, with objective weights (0.7 for value1, 0.3 for value2) guiding the search toward Pareto-efficient solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic solution selection based on combined objective and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        diversity = np.sum(np.abs(sol - archive[0][0])) / len(sol)\n        score = (obj[0] * 0.7 + obj[1] * 0.3) * (1 + entropy/len(sol)) * (1 + diversity)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate multi-objective metrics\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n\n    # Phase 1: Frontier-focused additions with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        frontier_threshold = np.percentile(pareto_front[:,0], 80)\n        strong_candidates = candidates[combined_marginal[candidates] >= frontier_threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in np.random.permutation(strong_candidates):\n                prob = min(0.9, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.3))\n                if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware multi-objective swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving or orthogonal swaps\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive item replacements with entropy consideration\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate replacement utility with entropy\n        replacement_scores = []\n        for idx in included:\n            entropy_contribution = -new_solution[idx] * np.log(new_solution[idx] + 1e-6) - (1 - new_solution[idx]) * np.log(1 - new_solution[idx] + 1e-6)\n            utility = (combined_marginal[idx] * 0.6 + entropy_contribution * 0.4) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            replacement_scores.append((idx, utility))\n\n        replacement_scores.sort(key=lambda x: x[1])\n        worst_idx = replacement_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9655742855863559,
            3.8685154020786285
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic solution selection based on combined objective and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        diversity = np.sum(np.abs(sol - archive[0][0])) / len(sol)\n        score = (obj[0] * 0.7 + obj[1] * 0.3) * (1 + entropy/len(sol)) * (1 + diversity)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate multi-objective metrics\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n\n    # Phase 1: Frontier-focused additions with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        frontier_threshold = np.percentile(pareto_front[:,0], 80)\n        strong_candidates = candidates[combined_marginal[candidates] >= frontier_threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in np.random.permutation(strong_candidates):\n                prob = min(0.9, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.3))\n                if np.random.rand() < prob and current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware multi-objective swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving or orthogonal swaps\n                if ((value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]) or\n                    (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i])):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive item replacements with entropy consideration\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate replacement utility with entropy\n        replacement_scores = []\n        for idx in included:\n            entropy_contribution = -new_solution[idx] * np.log(new_solution[idx] + 1e-6) - (1 - new_solution[idx]) * np.log(1 - new_solution[idx] + 1e-6)\n            utility = (combined_marginal[idx] * 0.6 + entropy_contribution * 0.4) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            replacement_scores.append((idx, utility))\n\n        replacement_scores.sort(key=lambda x: x[1])\n        worst_idx = replacement_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items (based on combined marginal impact) through three phases: adding top 15% of high-impact excluded items, performing targeted swaps between included and excluded items that improve at least one objective while respecting capacity, and dynamically removing low-impact included items when capacity is exceeded. It intelligently selects solutions from the top 15% of the archive for improvement, using adaptive thresholds and dynamic probabilities to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 181,
        "algorithm": "The algorithm implements a hybrid local search strategy that combines adaptive item selection with dynamic objective-driven swaps, using weighted impact metrics to balance both objectives while maintaining feasibility. It first selects a promising solution from the archive, then probabilistically adds high-impact items, performs objective-driven swaps, and finally adjusts the solution to stay within capacity through dynamic rebalancing. The weighted impact metric prioritizes items based on their combined value-to-weight ratio for both objectives, with periodic adjustments to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 20% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact with dynamic objective weights\n    alpha = 0.6  # Initial weight for objective 1\n    beta = 0.4   # Initial weight for objective 2\n    if np.random.rand() < 0.3:  # 30% chance to adjust weights\n        alpha, beta = beta, alpha\n\n    weighted_impact = alpha * (value1_lst / (weight_lst + 1e-6)) + beta * (value2_lst / (weight_lst + 1e-6))\n    sorted_indices = np.argsort(weighted_impact)[::-1]\n\n    # Phase 1: Add high-impact items with probabilistic acceptance\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates:\n                # Acceptance probability based on impact and remaining capacity\n                accept_prob = min(1.0, 0.5 + 0.5 * (weighted_impact[idx] / np.max(weighted_impact)) *\n                                (remaining_capacity / capacity))\n                if np.random.rand() < accept_prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Objective-driven swaps with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate weighted improvement\n                improvement1 = value1_lst[j] - value1_lst[i]\n                improvement2 = value2_lst[j] - value2_lst[i]\n                weighted_improvement = alpha * improvement1 + beta * improvement2\n\n                # Accept swap if it improves at least one objective and meets capacity\n                if weighted_improvement > 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with probabilistic removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest 20% weighted impact with probability\n        remove_threshold = np.percentile(weighted_impact[included_items], 20)\n        worst_items = included_items[weighted_impact[included_items] <= remove_threshold]\n\n        if len(worst_items) > 0:\n            # Remove with probability based on impact and capacity pressure\n            remove_prob = 0.7 * (1 - remaining_capacity / capacity)\n            if np.random.rand() < remove_prob:\n                worst_item = worst_items[np.random.choice(len(worst_items))]\n                new_solution[worst_item] = 0\n                current_weight -= weight_lst[worst_item]\n                remaining_capacity += weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.8870798550396024,
            1.0617228746414185
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential (top 20% by combined objective value)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact with dynamic objective weights\n    alpha = 0.6  # Initial weight for objective 1\n    beta = 0.4   # Initial weight for objective 2\n    if np.random.rand() < 0.3:  # 30% chance to adjust weights\n        alpha, beta = beta, alpha\n\n    weighted_impact = alpha * (value1_lst / (weight_lst + 1e-6)) + beta * (value2_lst / (weight_lst + 1e-6))\n    sorted_indices = np.argsort(weighted_impact)[::-1]\n\n    # Phase 1: Add high-impact items with probabilistic acceptance\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates:\n                # Acceptance probability based on impact and remaining capacity\n                accept_prob = min(1.0, 0.5 + 0.5 * (weighted_impact[idx] / np.max(weighted_impact)) *\n                                (remaining_capacity / capacity))\n                if np.random.rand() < accept_prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n                    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                    if len(candidates) == 0:\n                        break\n\n    # Phase 2: Objective-driven swaps with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Calculate weighted improvement\n                improvement1 = value1_lst[j] - value1_lst[i]\n                improvement2 = value2_lst[j] - value2_lst[i]\n                weighted_improvement = alpha * improvement1 + beta * improvement2\n\n                # Accept swap if it improves at least one objective and meets capacity\n                if weighted_improvement > 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with probabilistic removal\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n\n        # Remove items with lowest 20% weighted impact with probability\n        remove_threshold = np.percentile(weighted_impact[included_items], 20)\n        worst_items = included_items[weighted_impact[included_items] <= remove_threshold]\n\n        if len(worst_items) > 0:\n            # Remove with probability based on impact and capacity pressure\n            remove_prob = 0.7 * (1 - remaining_capacity / capacity)\n            if np.random.rand() < remove_prob:\n                worst_item = worst_items[np.random.choice(len(worst_items))]\n                new_solution[worst_item] = 0\n                current_weight -= weight_lst[worst_item]\n                remaining_capacity += weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items (based on combined marginal impact) through three phases: adding top 15% of high-impact excluded items, performing targeted swaps between included and excluded items that improve at least one objective while respecting capacity, and dynamically removing low-impact included items when capacity is exceeded. It intelligently selects solutions from the top 15% of the archive for improvement, using adaptive thresholds and dynamic probabilities to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 182,
        "algorithm": "The algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items through a combination of greedy addition (Phase 1), targeted swaps (Phase 2), and dynamic rebalancing (Phase 3), using adaptive thresholds and marginal impact calculations to ensure feasible solutions while improving both objectives. It selects promising solutions from the top 25% of the archive and applies probabilistic and heuristic-based operations to explore the solution space efficiently.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 25% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 4)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives and respects capacity\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 25% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 25)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.6948206889323701,
            1.839159369468689
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 4, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 25% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 4)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives and respects capacity\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 25% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 25)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm implements a multi-phase adaptive local search for the BI-KP, prioritizing high-impact items (based on combined marginal impact) through three phases: adding top 15% of high-impact excluded items, performing targeted swaps between included and excluded items that improve at least one objective while respecting capacity, and dynamically removing low-impact included items when capacity is exceeded. It intelligently selects solutions from the top 15% of the archive for improvement, using adaptive thresholds and dynamic probabilities to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impact for each item\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n    sorted_indices = np.argsort(combined_impact)[::-1]\n\n    # Phase 1: Add high-impact items not in the solution with dynamic probability\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with dynamic probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items with adaptive thresholds\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with adaptive percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(combined_impact[included_items], 15)\n        worst_items = included_items[combined_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Dynamic selection, hybrid operations, Pareto-aware, adaptive thresholds\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 183,
        "algorithm": "The algorithm selects a high-potential solution from the top 15% of the archive (by combined marginal impact) and generates a neighbor through three phases: 1) adding high-impact items (60% probability), 2) Pareto-aware swaps where excluded items dominate included ones in at least one objective, and 3) dynamically removing low-impact items if capacity is exceeded. It prioritizes frontier-focused selection and adaptive thresholds to balance both objectives while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with 60% probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Pareto-aware swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 15)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7586102921623332,
            0.5313935875892639
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 15% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 7, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 15% of high-impact items to add with 60% probability\n        num_to_add = max(1, len(sorted_indices) // 7)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.6:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Pareto-aware swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves at least one objective and respects capacity\n                if (value1_lst[j] > value1_lst[i] or value2_lst[j] > value2_lst[i]) and \\\n                   (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 15% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 15)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the top 25% of the archive (sorted by combined objective values) and applies a three-phase hybrid local search: it first adds items with high marginal impact for both objectives using a Pareto-inspired heuristic, then performs capacity-preserving replacements by removing least impactful items, and finally conducts probabilistic flips of high-impact items with dynamic thresholds. The method balances exploration and exploitation by dynamically adjusting selection criteria to ensure high-quality, feasible neighbors while avoiding premature convergence.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impacts and combined impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Pareto-inspired additions (items that improve both objectives)\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-preserving replacements (remove worst items)\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic flips of high-impact items (up to 4 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(4, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution with high potential for improvement\n    objectives = np.array([obj for _, obj in archive])\n    normalized = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    potential_scores = normalized[:, 0] * normalized[:, 1]  # Geometric mean for balanced potential\n    selected_idx = np.argmax(potential_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Calculate current state\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    current_value1 = np.sum(value1_lst[base_solution == 1])\n    current_value2 = np.sum(value2_lst[base_solution == 1])\n\n    # Step 3: Adaptive threshold-based item selection\n    included = np.where(base_solution == 1)[0]\n    excluded = np.where(base_solution == 0)[0]\n\n    # Calculate dynamic thresholds (median of included items)\n    if len(included) > 0:\n        med_value1 = np.median(value1_lst[included])\n        med_value2 = np.median(value2_lst[included])\n        med_weight = np.median(weight_lst[included])\n    else:\n        med_value1, med_value2, med_weight = 0, 0, 0\n\n    # Step 4: Hybrid operation - prioritize items that improve both objectives\n    new_solution = base_solution.copy()\n    improved = False\n\n    # First pass: Try to add items that improve both objectives\n    for item in excluded:\n        if (current_weight + weight_lst[item] <= capacity and\n            value1_lst[item] > med_value1 and\n            value2_lst[item] > med_value2):\n            new_solution[item] = 1\n            current_weight += weight_lst[item]\n            improved = True\n            break\n\n    # Second pass: If no improvement, try to remove items that are below median in both objectives\n    if not improved and len(included) > 0:\n        for item in included:\n            if (value1_lst[item] < med_value1 and\n                value2_lst[item] < med_value2):\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n                improved = True\n                break\n\n    # Third pass: If still no improvement, perform a random swap\n    if not improved and len(included) > 0 and len(excluded) > 0:\n        item_out = np.random.choice(included)\n        item_in = np.random.choice(excluded)\n        if (current_weight - weight_lst[item_out] + weight_lst[item_in] <= capacity):\n            new_solution[item_out] = 0\n            new_solution[item_in] = 1\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 184,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then applies a three-phase local search: adaptive Pareto-aware additions prioritize high-marginal-value items, capacity-constrained swaps dynamically balance trade-offs, and probabilistic flips target high-impact items while ensuring feasibility. The method emphasizes multi-objective improvement with dynamic thresholds and probabilistic decisions, favoring complementary improvements and high-marginal-value items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Pareto-aware additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-constrained swaps with dynamic thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    swap_threshold = 0.5 + (current_weight/capacity)*0.5  # Higher threshold when near capacity\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements with dynamic threshold\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (np.random.rand() < swap_threshold and\n                     ((marginal1[j] > marginal1[i]) or (marginal2[j] > marginal2[i])))):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic flips of high-impact items\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal)//4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Ensure feasibility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        worst_item = included[np.argmin(combined_marginal[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.7948668185133845,
            2.3232329189777374
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate weighted marginal contributions\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Adaptive Pareto-aware additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-constrained swaps with dynamic thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    swap_threshold = 0.5 + (current_weight/capacity)*0.5  # Higher threshold when near capacity\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements with dynamic threshold\n                if ((marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or\n                    (np.random.rand() < swap_threshold and\n                     ((marginal1[j] > marginal1[i]) or (marginal2[j] > marginal2[i])))):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic flips of high-impact items\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal)//4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Ensure feasibility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        worst_item = included[np.argmin(combined_marginal[included])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 7 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic function selects a solution from the archive with high potential for improvement (prioritizing less-explored solutions) and applies a hybrid local search combining random swaps (exploration) with targeted item replacements (exploitation) to generate neighbors while ensuring feasibility. It intelligently selects items to swap based on their potential to improve both objectives, prioritizing higher-value items while maintaining capacity constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Intelligent selection: choose a solution with high potential for improvement\n    # We select a solution that is not too crowded in the archive (less likely to be explored)\n    # and has a high potential for improvement (high value but not fully packed)\n    selected_idx = np.random.choice(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search: combine random swaps with targeted item replacements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Step 1: Random swaps (exploration)\n    for _ in range(min(3, n_items // 2)):\n        i, j = np.random.choice(n_items, size=2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            temp_weight = current_weight - weight_lst[i] + weight_lst[j] if new_solution[i] == 1 else current_weight + weight_lst[i] - weight_lst[j]\n            if temp_weight <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = temp_weight\n\n    # Step 2: Targeted item replacements (exploitation)\n    # Identify items that could improve both objectives when swapped\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            # Try to replace with an item not in the solution\n            candidates = np.where((weight_lst <= capacity - current_weight + weight_lst[i]) &\n                                (new_solution == 0) &\n                                ((value1_lst > value1_lst[i]) | (value2_lst > value2_lst[i])))[0]\n            if len(candidates) > 0:\n                j = np.random.choice(candidates)\n                new_solution[i], new_solution[j] = 0, 1\n                current_weight += weight_lst[j] - weight_lst[i]\n\n    # Ensure feasibility (in case of any numerical precision issues)\n    while np.sum(weight_lst[new_solution == 1]) > capacity:\n        # Randomly remove items until feasible\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) == 0:\n            break\n        i = np.random.choice(excess_items)\n        new_solution[i] = 0\n\n    return new_solution\n\nNo. 3 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on the sum of both objectives) and applies a hybrid local search that alternates between item swaps and reinsertions, ensuring feasibility by checking weight constraints. It prioritizes solutions that improve at least one objective while limiting iterations to balance exploration and computational effort. The key design ideas are prioritizing high-potential solutions, combining swaps and reinsertions, and maintaining feasibility through weight checks.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty.\")\n\n    # Select the most promising solution (e.g., highest sum of objectives)\n    archive.sort(key=lambda x: sum(x[1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: swap or reinsert items to improve both objectives\n    for _ in range(10):  # Limit iterations to avoid excessive computation\n        # Randomly select two items to swap or reinsert\n        indices = np.where(base_solution == 1)[0]\n        if len(indices) < 2:\n            break\n\n        # Choose two items to swap\n        i, j = np.random.choice(indices, size=2, replace=False)\n\n        # Calculate new weight if we swap i and j\n        new_weight = current_weight - weight_lst[i] + weight_lst[j]\n        if new_weight <= capacity:\n            # Accept the swap if it improves at least one objective\n            new_value1 = sum(value1_lst * new_solution) - value1_lst[i] + value1_lst[j]\n            new_value2 = sum(value2_lst * new_solution) - value2_lst[i] + value2_lst[j]\n            if new_value1 > sum(value1_lst * base_solution) or new_value2 > sum(value2_lst * base_solution):\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight = new_weight\n                base_solution = new_solution.copy()\n\n        # Reinsertion: remove i and add a new item\n        if np.sum(base_solution == 1) < len(weight_lst):\n            # Find an item not in the solution\n            candidates = np.where(base_solution == 0)[0]\n            if len(candidates) > 0:\n                k = np.random.choice(candidates)\n                if current_weight - weight_lst[i] + weight_lst[k] <= capacity:\n                    new_solution[i] = 0\n                    new_solution[k] = 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[k]\n\n    return new_solution\n\nNo. 4 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining item swapping and probabilistic flipping, ensuring feasibility by reverting infeasible moves. It prioritizes adding items (30% chance) and removing items (20% chance) while checking weight constraints, with swaps only occurring between already included items. The method balances exploration (random selection) and exploitation (targeted flips) to generate diverse feasible neighbors.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a promising solution (high crowding distance or low dominance rank)\n    selected_idx = np.random.choice(len(archive))\n    base_solution, _ = archive[selected_idx]\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: item swapping and probabilistic flipping\n    num_items = len(base_solution)\n    if num_items < 2:\n        return new_solution\n\n    # Step 1: Swap two items (if feasible)\n    swap_candidates = np.where(base_solution == 1)[0]\n    if len(swap_candidates) >= 2:\n        i, j = np.random.choice(swap_candidates, 2, replace=False)\n        temp = new_solution[i]\n        new_solution[i] = new_solution[j]\n        new_solution[j] = temp\n        # Check feasibility\n        total_weight = np.sum(weight_lst[new_solution == 1])\n        if total_weight > capacity:\n            # Revert if infeasible\n            new_solution[j] = new_solution[i]\n            new_solution[i] = temp\n\n    # Step 2: Probabilistic flipping (add or remove an item)\n    flip_candidates = np.where(base_solution == 0)[0]\n    if len(flip_candidates) > 0:\n        i = np.random.choice(flip_candidates)\n        if np.random.rand() < 0.3:  # 30% chance to add\n            new_solution[i] = 1\n            # Check feasibility\n            total_weight = np.sum(weight_lst[new_solution == 1])\n            if total_weight > capacity:\n                new_solution[i] = 0\n\n    # Step 3: Remove a random item (if any)\n    remove_candidates = np.where(base_solution == 1)[0]\n    if len(remove_candidates) > 0:\n        i = np.random.choice(remove_candidates)\n        if np.random.rand() < 0.2:  # 20% chance to remove\n            new_solution[i] = 0\n\n    return new_solution\n\nNo. 5 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the top 20% of the archive (prioritizing higher combined objective values) and applies a hybrid local search combining item swaps and path-relinking to generate a neighbor solution, ensuring feasibility by checking weight constraints. It first swaps an item from the current solution with one not included, then greedily adds additional items if capacity allows. The selection prioritizes solutions with higher potential for improvement, while the local search balances exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution (e.g., top 20% by objective values)\n    archive_sorted = sorted(archive, key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    top_solutions = archive_sorted[:max(1, len(archive) // 5)]\n    selected = random.choice(top_solutions)\n    base_solution = selected[0].copy()\n    current_weight = np.sum(weight_lst * base_solution)\n    current_value1 = selected[1][0]\n    current_value2 = selected[1][1]\n\n    # Hybrid local search: item swap with path-relinking\n    new_solution = base_solution.copy()\n    candidates = np.where(new_solution == 1)[0]\n    non_candidates = np.where(new_solution == 0)[0]\n\n    if len(candidates) > 0 and len(non_candidates) > 0:\n        # Swap one item from candidates with one from non-candidates\n        swap_out = random.choice(candidates)\n        swap_in = random.choice(non_candidates)\n\n        if (current_weight - weight_lst[swap_out] + weight_lst[swap_in]) <= capacity:\n            new_solution[swap_out] = 0\n            new_solution[swap_in] = 1\n\n        # Path-relinking: try to include additional items if possible\n        remaining_weight = capacity - np.sum(weight_lst * new_solution)\n        for item in non_candidates:\n            if new_solution[item] == 0 and weight_lst[item] <= remaining_weight:\n                new_solution[item] = 1\n                remaining_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 6 algorithm and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 7 algorithm and the corresponding code are:\nThe algorithm selects a solution from the least crowded region in the objective space to focus on underrepresented trade-offs, then applies a hybrid local search that combines multi-objective greedy insertion (prioritizing items with high combined marginal value for both objectives) with probabilistic removal of low-impact items (based on normalized value products and adjusted removal probabilities). The method ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from least crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Probabilistic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with lowest impact with probability proportional to their low impact\n        for i, item in enumerate(included_items):\n            if np.random.rand() < (1 - impact_scores[i]) * 0.3:  # 30% base chance adjusted by impact\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 185,
        "algorithm": "The algorithm first identifies under-explored regions in the objective space by selecting a solution with the lowest crowding distance from the archive. It then applies a hybrid local search: probabilistic item swaps between included and excluded items to improve combined objective values, followed by a greedy diversification step that removes low-marginal-value items and adds high-potential items while ensuring feasibility. The approach balances exploration and exploitation by dynamically adjusting swap probabilities and prioritizing items based on their combined marginal value for both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Identify least explored region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: probabilistic swaps followed by greedy diversification\n    # Phase 1: Probabilistic swaps between included and excluded items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate potential improvement for each swap\n        for _ in range(min(5, len(included))):  # Limit number of swaps\n            i = np.random.choice(included)\n            j = np.random.choice(excluded)\n\n            # Calculate potential new weight\n            new_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n            if new_weight <= capacity:\n                # Calculate potential improvement in combined objectives\n                current_value = (sum(value1_lst[new_solution == 1]), sum(value2_lst[new_solution == 1]))\n                new_value1 = current_value[0] - value1_lst[i] + value1_lst[j]\n                new_value2 = current_value[1] - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > current_value[0] or new_value2 > current_value[1]:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n                    included = np.where(new_solution == 1)[0]\n                    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 2: Greedy diversification - remove low-impact items and add high-potential items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        # Calculate marginal contributions for included items\n        marginal1 = value1_lst[included] / (weight_lst[included] + 1e-6)\n        marginal2 = value2_lst[included] / (weight_lst[included] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Remove items with lowest combined marginal contribution\n        if len(included) > 1:  # Keep at least one item\n            sort_idx = np.argsort(combined_marginal)\n            for i in sort_idx[:max(1, len(included)//3)]:  # Remove up to 1/3 of items\n                item = included[i]\n                if np.random.rand() < 0.5:  # 50% chance to remove\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Add high-potential items from excluded\n    if len(excluded) > 0:\n        # Calculate potential marginal contributions for excluded items\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = excluded[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                if np.random.rand() < 0.3:  # 30% chance to stop adding after each successful addition\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.320928401274164,
            0.5104221403598785
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Identify least explored region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with lowest crowding distance\n        selected_idx = np.argmin(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search: probabilistic swaps followed by greedy diversification\n    # Phase 1: Probabilistic swaps between included and excluded items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0 and len(excluded) > 0:\n        # Calculate potential improvement for each swap\n        for _ in range(min(5, len(included))):  # Limit number of swaps\n            i = np.random.choice(included)\n            j = np.random.choice(excluded)\n\n            # Calculate potential new weight\n            new_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n            if new_weight <= capacity:\n                # Calculate potential improvement in combined objectives\n                current_value = (sum(value1_lst[new_solution == 1]), sum(value2_lst[new_solution == 1]))\n                new_value1 = current_value[0] - value1_lst[i] + value1_lst[j]\n                new_value2 = current_value[1] - value2_lst[i] + value2_lst[j]\n\n                # Accept if at least one objective improves\n                if new_value1 > current_value[0] or new_value2 > current_value[1]:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n                    included = np.where(new_solution == 1)[0]\n                    excluded = np.where(new_solution == 0)[0]\n\n    # Phase 2: Greedy diversification - remove low-impact items and add high-potential items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(included) > 0:\n        # Calculate marginal contributions for included items\n        marginal1 = value1_lst[included] / (weight_lst[included] + 1e-6)\n        marginal2 = value2_lst[included] / (weight_lst[included] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Remove items with lowest combined marginal contribution\n        if len(included) > 1:  # Keep at least one item\n            sort_idx = np.argsort(combined_marginal)\n            for i in sort_idx[:max(1, len(included)//3)]:  # Remove up to 1/3 of items\n                item = included[i]\n                if np.random.rand() < 0.5:  # 50% chance to remove\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n\n    # Add high-potential items from excluded\n    if len(excluded) > 0:\n        # Calculate potential marginal contributions for excluded items\n        marginal1 = value1_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        marginal2 = value2_lst[excluded] / (weight_lst[excluded] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = excluded[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                if np.random.rand() < 0.3:  # 30% chance to stop adding after each successful addition\n                    break\n\n    return new_solution\n\n",
        "operation": "e1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the Pareto frontier using weighted crowding distance, then applies a hybrid local search that first performs adaptive item replacement (prioritizing high marginal value ratios) and then, with a dynamic threshold, attempts Pareto-aware swaps to explore trade-offs while maintaining feasibility. The threshold adjusts based on archive size, and the method ensures solutions remain feasible by checking weight constraints during swaps.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate weighted crowding distance to select frontier solutions\n        crowding = np.zeros(len(objectives))\n        for i in range(2):\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Weight crowding by solution's position relative to frontier\n        frontier_idx = np.argmax(np.sum(crowding))\n        selected_idx = frontier_idx\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive item replacement (prioritize high marginal value ratios)\n    included_items = np.where(base_solution == 1)[0]\n    excluded_items = np.where(base_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate marginal value ratios for included items\n        marginal1_in = value1_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        marginal2_in = value2_lst[included_items] / (weight_lst[included_items] + 1e-6)\n        combined_in = (marginal1_in + marginal2_in) / 2\n\n        # Calculate marginal value ratios for excluded items\n        marginal1_out = value1_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        marginal2_out = value2_lst[excluded_items] / (weight_lst[excluded_items] + 1e-6)\n        combined_out = (marginal1_out + marginal2_out) / 2\n\n        # Find best item to remove (lowest combined marginal)\n        remove_idx = np.argmin(combined_in)\n        item_to_remove = included_items[remove_idx]\n\n        # Find best item to add (highest combined marginal)\n        add_idx = np.argmax(combined_out)\n        item_to_add = excluded_items[add_idx]\n\n        # Perform swap if feasible\n        if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n            new_solution[item_to_remove] = 0\n            new_solution[item_to_add] = 1\n\n    # Dynamic threshold for Pareto-aware swaps\n    threshold = 0.3 if len(archive) > 5 else 0.5  # Adjust threshold based on archive size\n    if np.random.random() < threshold:\n        # Try a random swap to explore trade-offs\n        included_items = np.where(new_solution == 1)[0]\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(included_items) > 0 and len(excluded_items) > 0:\n            item_to_remove = np.random.choice(included_items)\n            item_to_add = np.random.choice(excluded_items)\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 186,
        "algorithm": "The algorithm combines a multi-phase approach to generate high-quality neighbors by first selecting promising solutions based on weighted objectives and diversity, then performing cluster-based additions of items with orthogonal improvements, followed by capacity-aware swaps prioritizing marginal value ratios, and finally probabilistic removals of low-utility items while maintaining feasibility through strict capacity checks. The selection and neighborhood generation are dynamically adjusted based on the current archive state, with value-weighted clustering and Pareto-aware swaps as key design features.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Solution selection based on weighted objectives and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0])) / len(sol)\n        score = (obj[0] * 0.7 + obj[1] * 0.3) * (1 + diversity)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate value-weighted item clusters\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 2: Cluster-based addition with orthogonal improvements\n    remaining_capacity = capacity - current_weight\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(excluded) > 0:\n        # Dynamic threshold based on value distribution\n        threshold = np.percentile(combined_marginal, 60)\n        strong_candidates = excluded[combined_marginal[excluded] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Find items with orthogonal improvements\n            for i in included:\n                for j in strong_candidates:\n                    if weight_lst[j] <= remaining_capacity:\n                        if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                           (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                            new_solution[j] = 1\n                            current_weight += weight_lst[j]\n                            remaining_capacity -= weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware swaps with dynamic Pareto thresholds\n    if len(included) > 0 and len(excluded) > 0:\n        # Dynamic threshold based on archive size\n        swap_threshold = 0.4 if len(archive) > 5 else 0.6\n        if np.random.random() < swap_threshold:\n            # Find items with highest marginal value ratios\n            marginal_ratios = combined_marginal[included] / (weight_lst[included] + 1e-6)\n            item_to_remove = included[np.argmin(marginal_ratios)]\n            marginal_ratios_out = combined_marginal[excluded] / (weight_lst[excluded] + 1e-6)\n            item_to_add = excluded[np.argmax(marginal_ratios_out)]\n\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    # Phase 4: Probabilistic removal with diversity-aware utility\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0:\n        # Calculate utility with diversity consideration\n        utility_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        # Remove items with low utility with higher probability\n        for idx, utility in utility_scores:\n            if np.random.random() < (1 - utility) * 0.7:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6194264706428535,
            5.566417932510376
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Phase 1: Solution selection based on weighted objectives and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0])) / len(sol)\n        score = (obj[0] * 0.7 + obj[1] * 0.3) * (1 + diversity)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate value-weighted item clusters\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 2: Cluster-based addition with orthogonal improvements\n    remaining_capacity = capacity - current_weight\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    if len(excluded) > 0:\n        # Dynamic threshold based on value distribution\n        threshold = np.percentile(combined_marginal, 60)\n        strong_candidates = excluded[combined_marginal[excluded] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Find items with orthogonal improvements\n            for i in included:\n                for j in strong_candidates:\n                    if weight_lst[j] <= remaining_capacity:\n                        if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                           (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                            new_solution[j] = 1\n                            current_weight += weight_lst[j]\n                            remaining_capacity -= weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware swaps with dynamic Pareto thresholds\n    if len(included) > 0 and len(excluded) > 0:\n        # Dynamic threshold based on archive size\n        swap_threshold = 0.4 if len(archive) > 5 else 0.6\n        if np.random.random() < swap_threshold:\n            # Find items with highest marginal value ratios\n            marginal_ratios = combined_marginal[included] / (weight_lst[included] + 1e-6)\n            item_to_remove = included[np.argmin(marginal_ratios)]\n            marginal_ratios_out = combined_marginal[excluded] / (weight_lst[excluded] + 1e-6)\n            item_to_add = excluded[np.argmax(marginal_ratios_out)]\n\n            if (current_weight - weight_lst[item_to_remove] + weight_lst[item_to_add]) <= capacity:\n                new_solution[item_to_remove] = 0\n                new_solution[item_to_add] = 1\n\n    # Phase 4: Probabilistic removal with diversity-aware utility\n    included = np.where(new_solution == 1)[0]\n    if len(included) > 0:\n        # Calculate utility with diversity consideration\n        utility_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        # Remove items with low utility with higher probability\n        for idx, utility in utility_scores:\n            if np.random.random() < (1 - utility) * 0.7:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the most crowded region in the objective space to focus on well-represented trade-offs, then applies a hybrid local search combining multi-objective greedy insertion (prioritizing items with high combined marginal value) with deterministic removal of low-impact items (based on a fixed threshold). It ensures feasibility by strictly enforcing weight constraints during both insertion and removal operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution from most crowded region in objective space\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Calculate crowding distance for each solution\n        crowding = np.zeros(len(objectives))\n        for i in range(2):  # For both objectives\n            sorted_idx = np.argsort(objectives[:, i])\n            crowding[sorted_idx[0]] = np.inf\n            crowding[sorted_idx[-1]] = np.inf\n            for j in range(1, len(objectives)-1):\n                if objectives[sorted_idx[-1], i] != objectives[sorted_idx[0], i]:\n                    crowding[sorted_idx[j]] += (objectives[sorted_idx[j+1], i] - objectives[sorted_idx[j-1], i]) / \\\n                                              (objectives[sorted_idx[-1], i] - objectives[sorted_idx[0], i])\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-objective greedy insertion\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Calculate normalized marginal contributions\n        marginal1 = value1_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        marginal2 = value2_lst[available_items] / (weight_lst[available_items] + 1e-6)\n        combined_marginal = (marginal1 + marginal2) / 2\n\n        # Sort by combined marginal value\n        sorted_items = available_items[np.argsort(combined_marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break  # Add only one item per iteration\n\n    # Deterministic removal of low-impact items\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate impact score (product of normalized values)\n        impact_scores = (value1_lst[included_items] / (np.max(value1_lst) + 1e-6)) * \\\n                       (value2_lst[included_items] / (np.max(value2_lst) + 1e-6))\n        # Remove items with impact below threshold\n        threshold = 0.2  # Fixed threshold for removal\n        for i, item in enumerate(included_items):\n            if impact_scores[i] < threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence.\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors. \ud83d\ude80\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 187,
        "algorithm": "The algorithm selects a promising solution from the archive by combining objective values and entropy metrics, then applies a three-phase local search: frontier-focused additions of high-impact items, capacity-constrained swaps prioritizing orthogonal improvements, and entropy-aware removals to maintain feasibility while preserving solution diversity. It dynamically adjusts search focus based on solution quality and archive diversity, balancing exploration and exploitation through marginal value analysis and iterative feasibility checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection: Combine objective values and entropy for dynamic focus\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.5 + obj[1] * 0.5) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic Pareto threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution's position\n        frontier_threshold = np.percentile(combined_marginal, 80)\n        strong_candidates = candidates[combined_marginal[candidates] >= frontier_threshold]\n\n        if len(strong_candidates) > 0:\n            # Select top candidate with probability based on marginal value\n            probs = combined_marginal[strong_candidates] / np.sum(combined_marginal[strong_candidates])\n            selected_idx = np.random.choice(strong_candidates, p=probs)\n            new_solution[selected_idx] = 1\n            current_weight += weight_lst[selected_idx]\n\n    # Phase 2: Capacity-constrained swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Entropy-aware removals with feasibility checks\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate entropy contribution of each item\n        entropy_contributions = []\n        for idx in included:\n            temp_sol = new_solution.copy()\n            temp_sol[idx] = 0\n            temp_entropy = np.sum(-temp_sol * np.log(temp_sol + 1e-6) - (1 - temp_sol) * np.log(1 - temp_sol + 1e-6))\n            entropy_contributions.append((idx, temp_entropy))\n\n        # Remove item with least entropy contribution\n        entropy_contributions.sort(key=lambda x: x[1])\n        worst_idx = entropy_contributions[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9435766417353224,
            2.084515005350113
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection: Combine objective values and entropy for dynamic focus\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.5 + obj[1] * 0.5) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic Pareto threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on current solution's position\n        frontier_threshold = np.percentile(combined_marginal, 80)\n        strong_candidates = candidates[combined_marginal[candidates] >= frontier_threshold]\n\n        if len(strong_candidates) > 0:\n            # Select top candidate with probability based on marginal value\n            probs = combined_marginal[strong_candidates] / np.sum(combined_marginal[strong_candidates])\n            selected_idx = np.random.choice(strong_candidates, p=probs)\n            new_solution[selected_idx] = 1\n            current_weight += weight_lst[selected_idx]\n\n    # Phase 2: Capacity-constrained swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Entropy-aware removals with feasibility checks\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate entropy contribution of each item\n        entropy_contributions = []\n        for idx in included:\n            temp_sol = new_solution.copy()\n            temp_sol[idx] = 0\n            temp_entropy = np.sum(-temp_sol * np.log(temp_sol + 1e-6) - (1 - temp_sol) * np.log(1 - temp_sol + 1e-6))\n            entropy_contributions.append((idx, temp_entropy))\n\n        # Remove item with least entropy contribution\n        entropy_contributions.sort(key=lambda x: x[1])\n        worst_idx = entropy_contributions[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 188,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid metric combining Pareto dominance and diversity, then generates a neighbor through three phases: Pareto-frontier guided additions prioritize high-utility items, value-ratio balanced swaps enforce trade-off preservation, and entropy-aware removals maintain solution diversity while ensuring feasibility. The method emphasizes utility-based selection and entropy-aware adjustments to balance exploration and exploitation across objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection metric combining Pareto dominance and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        # Calculate solution diversity\n        diversity = np.mean([np.sum(np.abs(sol - arch_sol)) for arch_sol, _ in archive]) if len(archive) > 1 else 0\n        # Calculate Pareto dominance score\n        dominance = sum(1 for arch_sol, arch_obj in archive if (arch_obj[0] > obj[0] and arch_obj[1] >= obj[1]) or (arch_obj[0] >= obj[0] and arch_obj[1] > obj[1]))\n        score = (obj[0] * 0.5 + obj[1] * 0.5) * (1 + diversity) / (1 + dominance)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate value ratios and utility scores\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    utility_score = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n    # Phase 1: Pareto-frontier guided additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Identify Pareto frontier items\n        pareto_front = np.argsort(-utility_score)[::-1]\n        pareto_threshold = np.percentile(utility_score[pareto_front], 80)\n        strong_candidates = candidates[utility_score[candidates] >= pareto_threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.9, (utility_score[idx] / np.max(utility_score)) * (1 + np.random.rand() * 0.3))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Value-ratio balanced swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for balanced improvements\n                if (value_ratio[i] * 0.8 <= value_ratio[j] <= value_ratio[i] * 1.25) and \\\n                   (utility_score[j] > utility_score[i] * 0.9):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Entropy-aware probabilistic removals\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate entropy-based utility\n        entropy_scores = []\n        for idx in included:\n            # Solution entropy\n            sol_entropy = -np.sum(new_solution * np.log(new_solution + 1e-6) + (1 - new_solution) * np.log(1 - new_solution + 1e-6))\n            # Utility score with entropy adjustment\n            utility = utility_score[idx] * (1 - sol_entropy/np.log(len(new_solution)))\n            entropy_scores.append((idx, utility))\n\n        entropy_scores.sort(key=lambda x: x[1])\n        worst_idx = entropy_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8758284663315947,
            8.694425135850906
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection metric combining Pareto dominance and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        # Calculate solution diversity\n        diversity = np.mean([np.sum(np.abs(sol - arch_sol)) for arch_sol, _ in archive]) if len(archive) > 1 else 0\n        # Calculate Pareto dominance score\n        dominance = sum(1 for arch_sol, arch_obj in archive if (arch_obj[0] > obj[0] and arch_obj[1] >= obj[1]) or (arch_obj[0] >= obj[0] and arch_obj[1] > obj[1]))\n        score = (obj[0] * 0.5 + obj[1] * 0.5) * (1 + diversity) / (1 + dominance)\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate value ratios and utility scores\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    utility_score = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n\n    # Phase 1: Pareto-frontier guided additions\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Identify Pareto frontier items\n        pareto_front = np.argsort(-utility_score)[::-1]\n        pareto_threshold = np.percentile(utility_score[pareto_front], 80)\n        strong_candidates = candidates[utility_score[candidates] >= pareto_threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.9, (utility_score[idx] / np.max(utility_score)) * (1 + np.random.rand() * 0.3))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Value-ratio balanced swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for balanced improvements\n                if (value_ratio[i] * 0.8 <= value_ratio[j] <= value_ratio[i] * 1.25) and \\\n                   (utility_score[j] > utility_score[i] * 0.9):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Entropy-aware probabilistic removals\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate entropy-based utility\n        entropy_scores = []\n        for idx in included:\n            # Solution entropy\n            sol_entropy = -np.sum(new_solution * np.log(new_solution + 1e-6) + (1 - new_solution) * np.log(1 - new_solution + 1e-6))\n            # Utility score with entropy adjustment\n            utility = utility_score[idx] * (1 - sol_entropy/np.log(len(new_solution)))\n            entropy_scores.append((idx, utility))\n\n        entropy_scores.sort(key=lambda x: x[1])\n        worst_idx = entropy_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 189,
        "algorithm": "The algorithm selects a solution from an archive using a dynamic weighted combination of objectives and diversity, then generates neighbors through three phases: 1) adding high-utility items via Pareto-dominance filtering, 2) capacity-constrained swaps for complementary improvements, and 3) probabilistic removal of low-contribution items weighted by entropy-aware utility. It prioritizes balanced utility (combined from both objectives) while ensuring feasibility through capacity checks and entropy-based removal. The selection emphasizes both objective performance and solution diversity, with probabilistic elements in item addition and removal.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on dynamic weighted objectives and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - np.mean(archive[0][0]))) / len(sol)\n        obj_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n        score = (obj[0] * obj_weight + obj[1] * (1 - obj_weight)) * (1 + diversity/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized utility scores with diversity consideration\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n    combined_utility = (utility1 + utility2) / 2\n\n    # Phase 1: Selective addition with Pareto-dominance filter\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic Pareto filter\n        pareto_mask = np.zeros(len(candidates), dtype=bool)\n        for i, idx in enumerate(candidates):\n            dominates = True\n            for j, jdx in enumerate(candidates):\n                if i != j and (utility1[jdx] >= utility1[idx] and utility2[jdx] >= utility2[idx] and\n                              (utility1[jdx] > utility1[idx] or utility2[jdx] > utility2[idx])):\n                    dominates = False\n                    break\n            pareto_mask[i] = dominates\n\n        pareto_candidates = candidates[pareto_mask]\n        if len(pareto_candidates) > 0:\n            for idx in pareto_candidates:\n                prob = min(0.7, (combined_utility[idx] / np.max(combined_utility)) * (1 + np.random.rand() * 0.3))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-constrained complementary swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (utility1[j] > utility1[i] and utility2[j] < utility2[i]) or \\\n                   (utility1[j] < utility1[i] and utility2[j] > utility2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with entropy-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate entropy-aware utility\n        entropy = -np.sum(new_solution * np.log(new_solution + 1e-6) + (1 - new_solution) * np.log(1 - new_solution + 1e-6))\n        utility_scores = []\n        for idx in included:\n            item_utility = combined_utility[idx] * (1 - entropy/len(new_solution))\n            utility_scores.append((idx, item_utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.901988624980256,
            2.124980181455612
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on dynamic weighted objectives and diversity\n    archive_with_metrics = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - np.mean(archive[0][0]))) / len(sol)\n        obj_weight = 0.7 if np.random.rand() < 0.5 else 0.3\n        score = (obj[0] * obj_weight + obj[1] * (1 - obj_weight)) * (1 + diversity/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized utility scores with diversity consideration\n    utility1 = value1_lst / (weight_lst + 1e-6)\n    utility2 = value2_lst / (weight_lst + 1e-6)\n    combined_utility = (utility1 + utility2) / 2\n\n    # Phase 1: Selective addition with Pareto-dominance filter\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic Pareto filter\n        pareto_mask = np.zeros(len(candidates), dtype=bool)\n        for i, idx in enumerate(candidates):\n            dominates = True\n            for j, jdx in enumerate(candidates):\n                if i != j and (utility1[jdx] >= utility1[idx] and utility2[jdx] >= utility2[idx] and\n                              (utility1[jdx] > utility1[idx] or utility2[jdx] > utility2[idx])):\n                    dominates = False\n                    break\n            pareto_mask[i] = dominates\n\n        pareto_candidates = candidates[pareto_mask]\n        if len(pareto_candidates) > 0:\n            for idx in pareto_candidates:\n                prob = min(0.7, (combined_utility[idx] / np.max(combined_utility)) * (1 + np.random.rand() * 0.3))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-constrained complementary swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (utility1[j] > utility1[i] and utility2[j] < utility2[i]) or \\\n                   (utility1[j] < utility1[i] and utility2[j] > utility2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with entropy-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate entropy-aware utility\n        entropy = -np.sum(new_solution * np.log(new_solution + 1e-6) + (1 - new_solution) * np.log(1 - new_solution + 1e-6))\n        utility_scores = []\n        for idx in included:\n            item_utility = combined_utility[idx] * (1 - entropy/len(new_solution))\n            utility_scores.append((idx, item_utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    }
]