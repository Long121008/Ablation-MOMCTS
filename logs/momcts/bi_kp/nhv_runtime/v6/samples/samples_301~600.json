[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm integrates a three-phase approach: (1) frontier-aware selection with diversity clustering to prioritize high-utility solutions, (2) a utility frontier swap operation that replaces low-utility items with high-utility ones while maintaining feasibility and Pareto dominance, and (3) capacity-aware probabilistic additions to explore high-utility items with adaptive thresholds. It balances objective trade-offs through marginal utility scoring, percentile-based thresholding, and continuous feasibility checks, ensuring high-quality neighbors while avoiding standard local search methods like 2-opt.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Frontier-aware selection with diversity clustering\n    archive_with_frontier = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_frontier.append((sol, obj, score))\n\n    archive_with_frontier.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_frontier[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Utility frontier swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    frontier_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < frontier_threshold:\n            for j in excluded:\n                if utility_scores[j] > frontier_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check if item j is near the Pareto frontier\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= frontier_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence. \ud83d\ude80\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 301,
        "algorithm": "The algorithm combines Pareto-aware selection, utility-driven swaps, and capacity-preserving additions to generate high-quality neighbors by prioritizing solutions near the frontier, replacing low-utility items with high-utility ones while ensuring Pareto improvements, and probabilistically adding high-utility items while maintaining feasibility through adaptive adjustments. It dynamically balances exploration and exploitation using marginal utility scores and adaptive thresholds, with a three-phase structure that emphasizes Pareto dominance, utility maximization, and capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-aware selection with adaptive utility scoring\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Calculate utility score\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        utility = np.sum(marginal1[sol == 1]) + np.sum(marginal2[sol == 1])\n        score = dominates * utility * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_scores[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility thresholds\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    high_utility_threshold = np.percentile(combined_marginal, 75)\n    low_utility_threshold = np.percentile(combined_marginal, 25)\n\n    # Phase 2: Utility-driven swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        if combined_marginal[i] < low_utility_threshold:\n            for j in excluded:\n                if combined_marginal[j] > high_utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check for Pareto improvement\n                        if (marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-preserving probabilistic additions\n    high_utility_items = np.where(combined_marginal > high_utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0 and np.random.rand() < 0.7:  # 70% probability\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = combined_marginal[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.5668587740812068,
            1.2534626424312592
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-aware selection with adaptive utility scoring\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Calculate utility score\n        marginal1 = value1_lst / (weight_lst + 1e-6)\n        marginal2 = value2_lst / (weight_lst + 1e-6)\n        utility = np.sum(marginal1[sol == 1]) + np.sum(marginal2[sol == 1])\n        score = dominates * utility * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_scores[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility thresholds\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    high_utility_threshold = np.percentile(combined_marginal, 75)\n    low_utility_threshold = np.percentile(combined_marginal, 25)\n\n    # Phase 2: Utility-driven swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        if combined_marginal[i] < low_utility_threshold:\n            for j in excluded:\n                if combined_marginal[j] > high_utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check for Pareto improvement\n                        if (marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-preserving probabilistic additions\n    high_utility_items = np.where(combined_marginal > high_utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0 and np.random.rand() < 0.7:  # 70% probability\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = combined_marginal[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm integrates a three-phase approach: (1) frontier-aware selection with diversity clustering to prioritize high-utility solutions, (2) a utility frontier swap operation that replaces low-utility items with high-utility ones while maintaining feasibility and Pareto dominance, and (3) capacity-aware probabilistic additions to explore high-utility items with adaptive thresholds. It balances objective trade-offs through marginal utility scoring, percentile-based thresholding, and continuous feasibility checks, ensuring high-quality neighbors while avoiding standard local search methods like 2-opt.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Frontier-aware selection with diversity clustering\n    archive_with_frontier = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_frontier.append((sol, obj, score))\n\n    archive_with_frontier.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_frontier[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Utility frontier swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    frontier_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < frontier_threshold:\n            for j in excluded:\n                if utility_scores[j] > frontier_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check if item j is near the Pareto frontier\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= frontier_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence. \ud83d\ude80\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 302,
        "algorithm": "The algorithm combines Pareto-frontier biased selection with a hybrid local search that prioritizes high-utility items while dynamically balancing objective trade-offs through adaptive utility scoring, percentile-based thresholds, and capacity-aware probabilistic operations, ensuring feasibility through continuous constraint checks and marginal impact analysis. It first selects near-Pareto solutions, then removes low-utility items, swaps them with high-utility items that improve objective balance, and probabilistically adds high-utility items while maintaining capacity constraints, with utility scores favoring items that improve both objectives proportionally.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-frontier biased selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Hybrid local search with dynamic thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Remove low-utility items\n    removal_threshold = np.percentile(utility_scores[included], 30)\n    for i in included:\n        if utility_scores[i] < removal_threshold:\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Swap with high-utility items\n    swap_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < swap_threshold:\n            for j in excluded:\n                if utility_scores[j] > swap_threshold:\n                    if weight_lst[j] <= current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= swap_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(2, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.841395243898046,
            1.8470323085784912
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto-frontier biased selection\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Hybrid local search with dynamic thresholds\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Remove low-utility items\n    removal_threshold = np.percentile(utility_scores[included], 30)\n    for i in included:\n        if utility_scores[i] < removal_threshold:\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Swap with high-utility items\n    swap_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < swap_threshold:\n            for j in excluded:\n                if utility_scores[j] > swap_threshold:\n                    if weight_lst[j] <= current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= swap_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(2, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm integrates a three-phase approach: (1) frontier-aware selection with diversity clustering to prioritize high-utility solutions, (2) a utility frontier swap operation that replaces low-utility items with high-utility ones while maintaining feasibility and Pareto dominance, and (3) capacity-aware probabilistic additions to explore high-utility items with adaptive thresholds. It balances objective trade-offs through marginal utility scoring, percentile-based thresholding, and continuous feasibility checks, ensuring high-quality neighbors while avoiding standard local search methods like 2-opt.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Frontier-aware selection with diversity clustering\n    archive_with_frontier = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_frontier.append((sol, obj, score))\n\n    archive_with_frontier.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_frontier[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Utility frontier swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    frontier_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < frontier_threshold:\n            for j in excluded:\n                if utility_scores[j] > frontier_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check if item j is near the Pareto frontier\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= frontier_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 303,
        "algorithm": "This algorithm combines adaptive Pareto frontier selection with a novel multi-objective utility-driven swap mechanism and probabilistic item additions. It prioritizes high-utility items, dynamically adjusts selection thresholds based on capacity and marginal utility, and ensures feasibility through capacity-aware removals. The approach balances exploration (via utility scores) and exploitation (via swaps and additions), with special attention to Pareto improvement in both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive Pareto frontier selection with utility-based clustering\n    archive_with_utility = []\n    for sol, obj in archive:\n        # Calculate combined utility score\n        utility = (obj[0] * obj[1]) / (np.sum(weight_lst[sol == 1]) + 1e-6)\n        # Calculate diversity score based on solution difference\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = utility * (1 + diversity/len(sol))\n        archive_with_utility.append((sol, obj, score))\n\n    archive_with_utility.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_utility[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate dynamic utility scores with adaptive weights\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    weight_factor = 0.5 + 0.5 * (current_weight / capacity)\n    utility_scores = (marginal1 * weight_factor + marginal2 * (1 - weight_factor)) * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Multi-objective utility-driven swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    dynamic_threshold = np.percentile(utility_scores, 80)\n    for i in included:\n        if utility_scores[i] < dynamic_threshold:\n            for j in excluded:\n                if utility_scores[j] > dynamic_threshold:\n                    weight_diff = weight_lst[j] - weight_lst[i]\n                    new_weight = current_weight + weight_diff\n                    if 0 <= new_weight <= capacity:\n                        # Check for potential Pareto improvement\n                        if (marginal1[j] > marginal1[i] * 1.2 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.2 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = new_weight\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions with adaptive selection pressure\n    high_utility_items = np.where(utility_scores >= dynamic_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            selection_pressure = min(0.3, 0.1 + (remaining_capacity / capacity) * 0.2)\n            num_to_add = max(1, int(len(candidates) * selection_pressure))\n            add_indices = np.random.choice(candidates, size=min(num_to_add, len(candidates)), replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment with utility-aware removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity)) * (1 - (value1_lst[idx] / (np.sum(value1_lst) + 1e-6)) * (value2_lst[idx] / (np.sum(value2_lst) + 1e-6)))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7911155913570715,
            1.1377308368682861
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Adaptive Pareto frontier selection with utility-based clustering\n    archive_with_utility = []\n    for sol, obj in archive:\n        # Calculate combined utility score\n        utility = (obj[0] * obj[1]) / (np.sum(weight_lst[sol == 1]) + 1e-6)\n        # Calculate diversity score based on solution difference\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = utility * (1 + diversity/len(sol))\n        archive_with_utility.append((sol, obj, score))\n\n    archive_with_utility.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_utility[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate dynamic utility scores with adaptive weights\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    weight_factor = 0.5 + 0.5 * (current_weight / capacity)\n    utility_scores = (marginal1 * weight_factor + marginal2 * (1 - weight_factor)) * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Multi-objective utility-driven swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    dynamic_threshold = np.percentile(utility_scores, 80)\n    for i in included:\n        if utility_scores[i] < dynamic_threshold:\n            for j in excluded:\n                if utility_scores[j] > dynamic_threshold:\n                    weight_diff = weight_lst[j] - weight_lst[i]\n                    new_weight = current_weight + weight_diff\n                    if 0 <= new_weight <= capacity:\n                        # Check for potential Pareto improvement\n                        if (marginal1[j] > marginal1[i] * 1.2 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.2 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = new_weight\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions with adaptive selection pressure\n    high_utility_items = np.where(utility_scores >= dynamic_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            selection_pressure = min(0.3, 0.1 + (remaining_capacity / capacity) * 0.2)\n            num_to_add = max(1, int(len(candidates) * selection_pressure))\n            add_indices = np.random.choice(candidates, size=min(num_to_add, len(candidates)), replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment with utility-aware removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity)) * (1 - (value1_lst[idx] / (np.sum(value1_lst) + 1e-6)) * (value2_lst[idx] / (np.sum(value2_lst) + 1e-6)))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm integrates a three-phase approach: (1) frontier-aware selection with diversity clustering to prioritize high-utility solutions, (2) a utility frontier swap operation that replaces low-utility items with high-utility ones while maintaining feasibility and Pareto dominance, and (3) capacity-aware probabilistic additions to explore high-utility items with adaptive thresholds. It balances objective trade-offs through marginal utility scoring, percentile-based thresholding, and continuous feasibility checks, ensuring high-quality neighbors while avoiding standard local search methods like 2-opt.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Frontier-aware selection with diversity clustering\n    archive_with_frontier = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_frontier.append((sol, obj, score))\n\n    archive_with_frontier.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_frontier[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Utility frontier swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    frontier_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < frontier_threshold:\n            for j in excluded:\n                if utility_scores[j] > frontier_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check if item j is near the Pareto frontier\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= frontier_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 304,
        "algorithm": "The algorithm implements a multi-phase heuristic for the BI-KP, prioritizing Pareto-ranked solutions with crowding distance selection, then applying marginal utility-driven swaps, capacity-aware removals, and adaptive threshold-based additions to generate high-quality neighbor solutions while maintaining feasibility. It balances objective trade-offs through normalized marginal utility scoring and adaptive percentile thresholds, focusing on items with high utility and low weight while ensuring solutions stay within capacity constraints. The critical design ideas are the normalized marginal utility scoring, adaptive threshold-based selection, and capacity-aware removal phases.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto rank and crowding distance selection\n    archive_with_rank = []\n    for sol, obj in archive:\n        rank = 1\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                rank += 1\n        archive_with_rank.append((sol, obj, rank))\n\n    archive_with_rank.sort(key=lambda x: x[2])\n    selected = archive_with_rank[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal utility scores\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    norm_weight = weight_lst / (np.max(weight_lst) + 1e-6)\n    marginal_scores = (norm_value1 + norm_value2) / (2 * norm_weight + 1e-6)\n\n    # Phase 2: Marginal utility-driven swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    utility_threshold = np.percentile(marginal_scores, 60)\n    for i in included:\n        if marginal_scores[i] < utility_threshold:\n            for j in excluded:\n                if marginal_scores[j] > utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        if (norm_value1[j] > norm_value1[i] and norm_value2[j] >= norm_value2[i] * 0.9) or \\\n                           (norm_value2[j] > norm_value2[i] and norm_value1[j] >= norm_value1[i] * 0.9):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic removal\n    if current_weight > capacity * 0.9:\n        removal_candidates = np.where(new_solution == 1)[0]\n        if len(removal_candidates) > 0:\n            removal_scores = []\n            for idx in removal_candidates:\n                score = marginal_scores[idx] * (1 - (weight_lst[idx] / (capacity + 1e-6)))\n                removal_scores.append((idx, score))\n\n            removal_scores.sort(key=lambda x: x[1])\n            num_to_remove = min(2, len(removal_scores))\n            for idx, _ in removal_scores[:num_to_remove]:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 4: Adaptive threshold-based addition\n    remaining_capacity = capacity - current_weight\n    high_utility_items = np.where(marginal_scores >= utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(2, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Final feasibility check\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = marginal_scores[idx] * (1 - (weight_lst[idx] / (capacity + 1e-6)))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8882193711350764,
            0.8024208247661591
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Pareto rank and crowding distance selection\n    archive_with_rank = []\n    for sol, obj in archive:\n        rank = 1\n        for other_sol, other_obj in archive:\n            if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                rank += 1\n        archive_with_rank.append((sol, obj, rank))\n\n    archive_with_rank.sort(key=lambda x: x[2])\n    selected = archive_with_rank[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal utility scores\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    norm_weight = weight_lst / (np.max(weight_lst) + 1e-6)\n    marginal_scores = (norm_value1 + norm_value2) / (2 * norm_weight + 1e-6)\n\n    # Phase 2: Marginal utility-driven swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    utility_threshold = np.percentile(marginal_scores, 60)\n    for i in included:\n        if marginal_scores[i] < utility_threshold:\n            for j in excluded:\n                if marginal_scores[j] > utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        if (norm_value1[j] > norm_value1[i] and norm_value2[j] >= norm_value2[i] * 0.9) or \\\n                           (norm_value2[j] > norm_value2[i] and norm_value1[j] >= norm_value1[i] * 0.9):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic removal\n    if current_weight > capacity * 0.9:\n        removal_candidates = np.where(new_solution == 1)[0]\n        if len(removal_candidates) > 0:\n            removal_scores = []\n            for idx in removal_candidates:\n                score = marginal_scores[idx] * (1 - (weight_lst[idx] / (capacity + 1e-6)))\n                removal_scores.append((idx, score))\n\n            removal_scores.sort(key=lambda x: x[1])\n            num_to_remove = min(2, len(removal_scores))\n            for idx, _ in removal_scores[:num_to_remove]:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    # Phase 4: Adaptive threshold-based addition\n    remaining_capacity = capacity - current_weight\n    high_utility_items = np.where(marginal_scores >= utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(2, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Final feasibility check\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = marginal_scores[idx] * (1 - (weight_lst[idx] / (capacity + 1e-6)))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 8 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm integrates a three-phase approach: (1) frontier-aware selection with diversity clustering to prioritize high-utility solutions, (2) a utility frontier swap operation that replaces low-utility items with high-utility ones while maintaining feasibility and Pareto dominance, and (3) capacity-aware probabilistic additions to explore high-utility items with adaptive thresholds. It balances objective trade-offs through marginal utility scoring, percentile-based thresholding, and continuous feasibility checks, ensuring high-quality neighbors while avoiding standard local search methods like 2-opt.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Frontier-aware selection with diversity clustering\n    archive_with_frontier = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        score = dominates * (1 + diversity/len(sol))\n        archive_with_frontier.append((sol, obj, score))\n\n    archive_with_frontier.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_frontier[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Utility frontier swap operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    frontier_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < frontier_threshold:\n            for j in excluded:\n                if utility_scores[j] > frontier_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        # Check if item j is near the Pareto frontier\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= frontier_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Feasibility check and adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic algorithm employs a multi-phase approach to generate high-quality neighbors: first clustering and removing low-utility items, then performing dynamic marginal utility swaps, followed by capacity-aware probabilistic additions, and finally utility-based capacity adjustments. It prioritizes items with high combined marginal utility scores while ensuring feasibility through continuous weight checks and targeted removals. The algorithm balances exploration (random additions) with exploitation (utility-driven swaps) to navigate the trade-off between the two objectives effectively.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Tiered selection (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 1: Cluster items and remove low-utility clusters\n    cluster_threshold = np.percentile(utility_scores, 50)\n    low_utility_clusters = np.where((utility_scores <= cluster_threshold) & (new_solution == 1))[0]\n\n    for idx in low_utility_clusters:\n        new_solution[idx] = 0\n        current_weight -= weight_lst[idx]\n\n    # Phase 2: Dynamic marginal utility swaps\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    utility_threshold = np.percentile(utility_scores, 75)\n    for i in included:\n        if utility_scores[i] < utility_threshold:\n            for j in excluded:\n                if utility_scores[j] > utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        if (marginal1[j] > marginal1[i] * 1.2 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.2 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Phase 3: Capacity-aware probabilistic additions\n    high_utility_items = np.where(utility_scores >= utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        num_to_add = min(2, len(high_utility_items))\n        add_indices = np.random.choice(high_utility_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= capacity - current_weight:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 4: Utility-based capacity adjustment\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        removal_scores = []\n        for idx in included:\n            score = utility_scores[idx] * (1 - (weight_lst[idx] / capacity))\n            removal_scores.append((idx, score))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThis algorithm employs a tiered local search approach that first removes low-utility items, performs targeted item swaps between high-utility items and excluded ones, probabilistically adds high-impact items, and finally refines the solution through dynamic utility-based rebalancing. It prioritizes high-marginal-value items (combined utility) while ensuring feasibility through adaptive capacity checks and utility-based removals. The method balances exploration (random additions) with exploitation (targeted swaps and removals) to improve multi-objective solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Tiered selection (top 30% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized objective impacts with weight consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Remove low-utility items (below 25th percentile)\n    low_utility_threshold = np.percentile(combined_marginal, 25)\n    low_utility_items = np.where((combined_marginal <= low_utility_threshold) & (new_solution == 1))[0]\n\n    for idx in low_utility_items:\n        new_solution[idx] = 0\n        current_weight -= weight_lst[idx]\n\n    # Phase 2: Targeted swaps between high-utility included and excluded items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    high_utility_threshold = np.percentile(combined_marginal, 75)\n    high_utility_included = np.where((combined_marginal >= high_utility_threshold) & (new_solution == 1))[0]\n    high_utility_excluded = np.where((combined_marginal >= high_utility_threshold) & (new_solution == 0))[0]\n\n    for i in high_utility_included:\n        for j in high_utility_excluded:\n            if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                   (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Probabilistic additions of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(3, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= capacity - current_weight:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n\n    # Phase 4: Dynamic utility-based rebalancing\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6)))) * (1 + weight_lst[idx]/capacity)\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines dynamic Pareto-frontier selection with a multi-phase local search strategy: it first expands the frontier by adding high-impact items, then performs balanced swaps between included and excluded items, followed by adaptive rebalancing to maintain feasibility, and finally probabilistically adds high-impact items. Key variables are prioritized based on combined marginal utility, with a focus on maintaining objective trade-offs and feasibility throughout the process.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Dynamic Pareto-frontier selection (top 25% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.25 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate normalized objective impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-expansion additions (items above 75th percentile)\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 75)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Objective-balanced swaps (items with complementary improvements)\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for balanced improvements (one objective improves while the other doesn't worsen significantly)\n                if (marginal1[j] > marginal1[i] and marginal2[j] >= marginal2[i] * 0.9) or \\\n                   (marginal2[j] > marginal2[i] and marginal1[j] >= marginal1[i] * 0.9):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Adaptive rebalancing with dynamic utility function\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Dynamic utility function: prioritize items with low marginal impact but high contribution to objectives\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            # Utility considers both marginal impact and normalized contribution to objectives\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items (up to 2 items)\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(2, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 20% by combined marginal impact), then generates a neighbor through a three-phase process: 1) adding high-impact items (70% chance for top 20% marginal items), 2) targeted swaps where excluded items dominate included ones in at least one objective while respecting capacity, and 3) dynamic rebalancing by removing low-marginal items if capacity is exceeded. The method prioritizes high-marginal items and ensures feasibility through capacity-aware operations.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined marginal impact)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(len(archive) // 5, len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Add high-impact items not in the solution\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top 20% of high-impact items to add\n        num_to_add = max(1, len(sorted_indices) // 5)\n        for idx in sorted_indices[:num_to_add]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps of high-impact items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with percentile-based thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest 20% marginal impact\n        remove_threshold = np.percentile(marginal_impact[included_items], 20)\n        worst_items = included_items[marginal_impact[included_items] <= remove_threshold]\n        if len(worst_items) > 0:\n            worst_item = worst_items[0]\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm selects a high-potential solution from the archive, prioritizes items with high marginal impact (combining both objectives) for addition, then performs adaptive swaps to improve both objectives while ensuring feasibility, and finally rebalances the solution by removing low-impact items if the weight exceeds capacity. It balances exploration (via marginal impact) and exploitation (via adaptive swaps) while maintaining feasibility through weight-sensitive rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact for each item\n    marginal_impact = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(marginal_impact)[::-1]  # Descending order\n\n    # Phase 1: Dynamic item selection based on marginal impact\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Select top items with highest marginal impact\n        for idx in sorted_indices:\n            if idx in candidates and np.random.rand() < 0.6:  # 60% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Adaptive flipping based on current solution's characteristics\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate potential improvements for each swap\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing with dynamic threshold\n    while current_weight > capacity:\n        # Remove items with lowest marginal impact first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(marginal_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and generates a neighbor by flipping a subset of high-impact items (top 20% by marginal contribution to both objectives) while ensuring feasibility. It prioritizes items that improve both objectives and flips up to 3 of them randomly, adjusting weights accordingly. The heuristic balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Generate a neighbor using a hybrid local search: flip a subset of items with high marginal impact\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate marginal impact of each item (difference in objective values if flipped)\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    # Identify items with high marginal impact (top 20%)\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 5):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 5):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    # Randomly select a subset of top items to flip\n    if len(top_items) > 0:\n        num_to_flip = min(3, len(top_items))  # Flip up to 3 items\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        # Flip selected items and ensure feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence. \ud83d\ude80\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors.\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 305,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid scoring mechanism that combines objective values, diversity, and Pareto dominance, then applies a multi-phase local search to generate neighbors. It prioritizes high-utility items (based on combined marginal values) through targeted swaps, probabilistic additions, and utility-based replacements, while dynamically adjusting thresholds and ensuring feasibility through weight rebalancing. The utility scores are calculated as a product of marginal values and their normalized contributions to both objectives, with adaptive thresholds based on current solution quality and capacity utilization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Hybrid selection combining objectives, diversity, and Pareto dominance\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Hybrid score combining objectives, diversity, and Pareto dominance\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol)) * (1 + dominates/len(archive))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Hybrid local search operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Dynamic threshold based on current solution quality\n    utility_threshold = np.percentile(utility_scores, 100 - (current_weight/capacity)*40)\n\n    # Targeted swaps between high-utility included and excluded items\n    for i in included:\n        if utility_scores[i] < utility_threshold:\n            for j in excluded:\n                if utility_scores[j] > utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Probabilistic additions of high-utility items\n    high_utility_items = np.where(utility_scores >= utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0 and np.random.rand() < 0.7:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Utility-based replacements for low-utility items\n    low_utility_items = np.where(utility_scores < utility_threshold)[0]\n    if len(low_utility_items) > 0:\n        for i in included:\n            if utility_scores[i] < utility_threshold:\n                for j in excluded:\n                    if utility_scores[j] > utility_threshold and weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        removal_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                      (value2_lst[idx]/(total_value2 + 1e-6))))\n            removal_scores.append((idx, utility))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8424819417336967,
            1.1606015861034393
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Phase 1: Hybrid selection combining objectives, diversity, and Pareto dominance\n    archive_with_scores = []\n    for sol, obj in archive:\n        # Calculate Pareto dominance score\n        dominates = 0\n        for other_sol, other_obj in archive:\n            if (obj[0] >= other_obj[0] and obj[1] > other_obj[1]) or (obj[0] > other_obj[0] and obj[1] >= other_obj[1]):\n                dominates += 1\n        # Calculate diversity score\n        diversity = np.sum(np.abs(sol - archive[0][0]))\n        # Hybrid score combining objectives, diversity, and Pareto dominance\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol)) * (1 + dominates/len(archive))\n        archive_with_scores.append((sol, obj, score))\n\n    archive_with_scores.sort(key=lambda x: x[2], reverse=True)\n    base_solution = archive_with_scores[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive utility scores\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n    utility_scores = combined_marginal * (1 + (value1_lst / (np.sum(value1_lst) + 1e-6)) * (value2_lst / (np.sum(value2_lst) + 1e-6)))\n\n    # Phase 2: Hybrid local search operation\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Dynamic threshold based on current solution quality\n    utility_threshold = np.percentile(utility_scores, 100 - (current_weight/capacity)*40)\n\n    # Targeted swaps between high-utility included and excluded items\n    for i in included:\n        if utility_scores[i] < utility_threshold:\n            for j in excluded:\n                if utility_scores[j] > utility_threshold:\n                    if weight_lst[j] <= current_weight + weight_lst[j] - weight_lst[i] <= capacity:\n                        if (marginal1[j] > marginal1[i] * 1.1 and marginal2[j] >= marginal2[i]) or \\\n                           (marginal2[j] > marginal2[i] * 1.1 and marginal1[j] >= marginal1[i]):\n                            new_solution[i], new_solution[j] = 0, 1\n                            current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                            break\n\n    # Probabilistic additions of high-utility items\n    high_utility_items = np.where(utility_scores >= utility_threshold)[0]\n    if len(high_utility_items) > 0:\n        remaining_capacity = capacity - current_weight\n        candidates = high_utility_items[weight_lst[high_utility_items] <= remaining_capacity]\n        if len(candidates) > 0:\n            num_to_add = min(3, len(candidates))\n            add_indices = np.random.choice(candidates, size=num_to_add, replace=False)\n            for idx in add_indices:\n                if new_solution[idx] == 0 and np.random.rand() < 0.7:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    # Utility-based replacements for low-utility items\n    low_utility_items = np.where(utility_scores < utility_threshold)[0]\n    if len(low_utility_items) > 0:\n        for i in included:\n            if utility_scores[i] < utility_threshold:\n                for j in excluded:\n                    if utility_scores[j] > utility_threshold and weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                        new_solution[i], new_solution[j] = 0, 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                        break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        removal_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                      (value2_lst[idx]/(total_value2 + 1e-6))))\n            removal_scores.append((idx, utility))\n\n        removal_scores.sort(key=lambda x: x[1])\n        worst_idx = removal_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 10 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a hybrid criterion combining objective values and diversity, then generates a neighbor through three phases: (1) probabilistically adding high-impact items with adaptive thresholds, (2) capacity-aware swaps between items with complementary marginal improvements, and (3) dynamic rebalancing by removing low-utility items with a novel utility-based criterion that balances marginal impact and objective trade-offs. The algorithm prioritizes items with high combined marginal impact for objectives 1 and 2, while ensuring feasibility through capacity-aware operations and rebalancing.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Hybrid selection: combine objective values and diversity\n    archive_with_diversity = []\n    for sol, obj in archive:\n        diversity = np.sum(np.abs(sol - archive[0][0]))  # Simple diversity measure\n        score = (obj[0] + obj[1]) * (1 + diversity/len(sol))\n        archive_with_diversity.append((sol, obj, score))\n\n    archive_with_diversity.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_diversity[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[selected == 1])\n\n    # Calculate normalized marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Probabilistic addition with adaptive thresholds\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Adaptive threshold based on current solution quality\n        threshold = np.percentile(combined_marginal, 100 - (current_weight/capacity)*30)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            # Add with probability based on marginal impact\n            for idx in strong_candidates:\n                prob = min(0.9, combined_marginal[idx] / np.max(combined_marginal))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with complementary improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for complementary improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] < marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] > marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores for removal\n        utility_scores = []\n        for idx in included:\n            # Utility considers both marginal impact and objective trade-offs\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                      (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6))))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm randomly selects a solution from the archive and generates a neighbor by flipping up to 5 high-impact items (top 30% by marginal contribution to either objective), prioritizing those that improve both objectives while ensuring feasibility. It balances exploration (random selection) and exploitation (targeted flips) to balance the bi-objective trade-off. The critical design idea is the selection of high-impact items based on marginal contributions, ensuring the neighbor remains feasible while potentially improving both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    selected_idx = np.random.randint(0, len(archive))\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    total_weight = np.sum(weight_lst[base_solution == 1])\n\n    marginal_impact1 = value1_lst - value1_lst[base_solution == 1].sum()\n    marginal_impact2 = value2_lst - value2_lst[base_solution == 1].sum()\n\n    num_items = len(weight_lst)\n    top_items1 = np.argsort(marginal_impact1)[-max(1, num_items // 3):]\n    top_items2 = np.argsort(marginal_impact2)[-max(1, num_items // 3):]\n    top_items = np.union1d(top_items1, top_items2)\n\n    if len(top_items) > 0:\n        num_to_flip = min(5, len(top_items))\n        flip_indices = np.random.choice(top_items, size=num_to_flip, replace=False)\n\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if total_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    total_weight -= weight_lst[idx]\n            else:\n                if total_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    total_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of objective values and solution entropy, then generates a neighbor through three phases: aggressive addition of high-impact items based on Pareto dominance, capacity-aware swaps between items with orthogonal improvements, and probabilistic removal of low-utility items weighted by diversity-aware metrics. The selection prioritizes solutions with higher combined objective scores and higher entropy, while the local search emphasizes Pareto-optimal additions, orthogonal swaps, and diversity-preserving removals to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Selection based on weighted objectives and entropy\n    archive_with_metrics = []\n    for sol, obj in archive:\n        entropy = np.sum(-sol * np.log(sol + 1e-6) - (1 - sol) * np.log(1 - sol + 1e-6))\n        score = (obj[0] * 0.6 + obj[1] * 0.4) * (1 + entropy/len(sol))\n        archive_with_metrics.append((sol, obj, score))\n\n    archive_with_metrics.sort(key=lambda x: x[2], reverse=True)\n    selected = archive_with_metrics[0][0].copy()\n    new_solution = selected.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate normalized marginal impacts with Pareto consideration\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    pareto_front = np.array([marginal1, marginal2]).T\n    pareto_front = pareto_front[np.lexsort((pareto_front[:,1], -pareto_front[:,0]))]\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Aggressive addition with Pareto-based threshold\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n\n    if len(candidates) > 0:\n        # Dynamic threshold based on Pareto front\n        threshold = np.percentile(pareto_front[:,0], 70)\n        strong_candidates = candidates[combined_marginal[candidates] >= threshold]\n\n        if len(strong_candidates) > 0:\n            for idx in strong_candidates:\n                prob = min(0.8, (combined_marginal[idx] / np.max(combined_marginal)) * (1 + np.random.rand() * 0.2))\n                if np.random.rand() < prob:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware swaps with orthogonal improvements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for orthogonal improvements\n                if (marginal1[j] > marginal1[i] and marginal2[j] > marginal2[i]) or \\\n                   (marginal1[j] < marginal1[i] and marginal2[j] < marginal2[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Probabilistic removal with diversity-aware utility\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility with diversity consideration\n        diversity_scores = []\n        for idx in included:\n            diversity = np.sum(np.abs(new_solution - archive[0][0])) / len(new_solution)\n            utility = (combined_marginal[idx] * (1 - diversity)) * \\\n                     (1 - (value1_lst[idx]/(np.sum(value1_lst[included]) + 1e-6)) *\n                     (value2_lst[idx]/(np.sum(value2_lst[included]) + 1e-6)))\n            diversity_scores.append((idx, utility))\n\n        diversity_scores.sort(key=lambda x: x[1])\n        worst_idx = diversity_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a diversity-aware mechanism that prioritizes solutions in underrepresented quadrants of the objective space, then applies a hybrid local search that dynamically adds high-marginal-value items and removes low-contribution items based on a weighted combination of the two objectives. The weighted marginal value calculation (with \u03b1=0.7 favoring the first objective) guides item additions, while dynamic removal thresholds (30th percentile of normalized contributions) ensure feasible solutions. The approach balances exploration (quadrant-based selection) and exploitation (marginal value prioritization).\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Novel diversity-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    if len(objectives) > 1:\n        # Partition objective space into quadrants\n        median1 = np.median(objectives[:, 0])\n        median2 = np.median(objectives[:, 1])\n        quadrant_counts = np.zeros(4)\n        for obj in objectives:\n            if obj[0] > median1 and obj[1] > median2:\n                quadrant_counts[0] += 1\n            elif obj[0] <= median1 and obj[1] > median2:\n                quadrant_counts[1] += 1\n            elif obj[0] <= median1 and obj[1] <= median2:\n                quadrant_counts[2] += 1\n            else:\n                quadrant_counts[3] += 1\n\n        # Select from least populated quadrant\n        selected_quadrant = np.argmin(quadrant_counts)\n        candidate_indices = []\n        for i, obj in enumerate(objectives):\n            if (selected_quadrant == 0 and obj[0] > median1 and obj[1] > median2) or \\\n               (selected_quadrant == 1 and obj[0] <= median1 and obj[1] > median2) or \\\n               (selected_quadrant == 2 and obj[0] <= median1 and obj[1] <= median2) or \\\n               (selected_quadrant == 3 and obj[0] > median1 and obj[1] <= median2):\n                candidate_indices.append(i)\n\n        if candidate_indices:\n            # Calculate crowding distance within selected quadrant\n            quadrant_obj = objectives[candidate_indices]\n            crowding = np.zeros(len(quadrant_obj))\n            for i in range(2):\n                sorted_idx = np.argsort(quadrant_obj[:, i])\n                crowding[sorted_idx[0]] = np.inf\n                crowding[sorted_idx[-1]] = np.inf\n                for j in range(1, len(quadrant_obj)-1):\n                    if quadrant_obj[sorted_idx[-1], i] != quadrant_obj[sorted_idx[0], i]:\n                        crowding[sorted_idx[j]] += (quadrant_obj[sorted_idx[j+1], i] - quadrant_obj[sorted_idx[j-1], i]) / \\\n                                                  (quadrant_obj[sorted_idx[-1], i] - quadrant_obj[sorted_idx[0], i])\n            selected_idx = candidate_indices[np.argmax(crowding)]\n        else:\n            selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Hybrid local search with dynamic threshold\n    available_items = np.where(base_solution == 0)[0]\n    if len(available_items) > 0:\n        # Weighted marginal value calculation\n        alpha = 0.7  # Weight for first objective\n        marginal = (alpha * value1_lst[available_items] + (1-alpha) * value2_lst[available_items]) / (weight_lst[available_items] + 1e-6)\n        sorted_items = available_items[np.argsort(marginal)[::-1]]\n\n        # Try to add top items\n        for item in sorted_items:\n            if current_weight + weight_lst[item] <= capacity:\n                new_solution[item] = 1\n                current_weight += weight_lst[item]\n                break\n\n    # Dynamic removal based on combined objective contribution\n    included_items = np.where(new_solution == 1)[0]\n    if len(included_items) > 0:\n        # Calculate normalized combined contribution\n        total_value1 = np.sum(value1_lst[included_items])\n        total_value2 = np.sum(value2_lst[included_items])\n        contribution = (value1_lst[included_items] / total_value1 + value2_lst[included_items] / total_value2) / 2\n        dynamic_threshold = np.percentile(contribution, 30)  # Remove bottom 30%\n\n        for i, item in enumerate(included_items):\n            if contribution[i] < dynamic_threshold:\n                new_solution[item] = 0\n                current_weight -= weight_lst[item]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate weighted marginal impact for each objective and combined\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = (marginal_impact1 + marginal_impact2) / 2\n\n    # Phase 1: Frontier-focused additions (top 20% marginal impact for each objective)\n    top_impact_items1 = np.argsort(marginal_impact1)[::-1][:max(1, len(marginal_impact1) // 5)]\n    top_impact_items2 = np.argsort(marginal_impact2)[::-1][:max(1, len(marginal_impact2) // 5)]\n    top_impact_items = np.union1d(top_impact_items1, top_impact_items2)\n\n    remaining_capacity = capacity - current_weight\n    for idx in top_impact_items:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Adaptive hybrid swaps between included and excluded items\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check if swap improves both objectives or at least one with high probability\n                if ((marginal_impact1[j] > marginal_impact1[i] and marginal_impact2[j] > marginal_impact2[i]) or\n                    (marginal_impact1[j] > marginal_impact1[i] and np.random.rand() < 0.6) or\n                    (marginal_impact2[j] > marginal_impact2[i] and np.random.rand() < 0.6)):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Capacity-preserving replacements with dynamic thresholds\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        # Remove items with lowest combined marginal impact\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 4: Probabilistic flips of high-impact items (up to 3 items)\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Pareto-aware selection: prioritize solutions near the frontier with high combined objective values\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts considering both objectives and weights\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 80 - (current_weight/capacity)*10)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Hybrid local search with targeted swaps and replacements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Sort items by their marginal impact ratios\n    marginal_ratio = marginal1 / (marginal2 + 1e-6)\n    sorted_included = included[np.argsort(marginal_ratio[included])]\n    sorted_excluded = excluded[np.argsort(marginal_ratio[excluded])[::-1]]\n\n    # Perform swaps between high-marginal included and excluded items\n    for i in sorted_included[:min(3, len(sorted_included))]:\n        for j in sorted_excluded[:min(5, len(sorted_excluded))]:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving swaps\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores considering both objectives and weight distribution\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))) *\n                      (1 + weight_lst[idx]/capacity))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items with adaptive thresholds\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(2, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts considering both objectives and weights\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 80 - (current_weight/capacity)*10)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Hybrid local search with targeted swaps and replacements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Sort items by their marginal impact ratios\n    marginal_ratio = marginal1 / (marginal2 + 1e-6)\n    sorted_included = included[np.argsort(marginal_ratio[included])]\n    sorted_excluded = excluded[np.argsort(marginal_ratio[excluded])[::-1]]\n\n    # Perform swaps between high-marginal included and excluded items\n    for i in sorted_included[:min(3, len(sorted_included))]:\n        for j in sorted_excluded[:min(5, len(sorted_excluded))]:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving swaps\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores considering both objectives and weight distribution\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))) *\n                      (1 + weight_lst[idx]/capacity))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items with adaptive thresholds\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(2, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm combines intelligent solution selection with a three-phase local search: first exploring high-value items probabilistically, then performing targeted swaps to improve both objectives, and finally rebalancing by removing low-value items to maintain feasibility. It prioritizes items with high value ratios while dynamically adjusting the solution to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high value ratio potential\n    archive.sort(key=lambda x: (x[1][0] / (x[1][1] + 1e-6), sum(x[1])), reverse=True)\n    base_solution = archive[0][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Calculate value ratios for each item\n    value_ratios = (value1_lst + 1e-6) / (value2_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)\n\n    # Phase 1: Probabilistic item additions (exploration)\n    remaining_capacity = capacity - current_weight\n    candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n    if len(candidates) > 0:\n        # Select items with high value ratios first\n        for idx in sorted_indices[::-1]:\n            if idx in candidates and np.random.rand() < 0.7:  # 70% chance to add\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n                candidates = np.where((weight_lst <= remaining_capacity) & (new_solution == 0))[0]\n                if len(candidates) == 0:\n                    break\n\n    # Phase 2: Targeted swaps based on objective improvements\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for i in included_items:\n        for j in excluded_items:\n            if weight_lst[j] <= capacity - current_weight + weight_lst[i]:\n                # Check if swap improves both objectives\n                if (value1_lst[j] > value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity) or \\\n                   (value2_lst[j] > value2_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    break\n\n    # Phase 3: Weight-sensitive rebalancing\n    while current_weight > capacity:\n        # Remove items with lowest value ratio first\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(value_ratios[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n\nNo. 9 algorithm's description and the corresponding code are:\nThe algorithm combines frontier-focused selection with a multi-phase local search: it first prioritizes high-impact items for addition, then performs capacity-aware replacements to maintain feasibility, and finally conducts probabilistic exploration of high-impact items to balance exploration and exploitation. The solution is selected from the top 30% of the archive, and marginal impacts for both objectives guide the search, ensuring high-quality neighbors while respecting the knapsack capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution from the frontier (top 30% by sum of objectives)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.3 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate combined marginal impact\n    marginal_impact1 = value1_lst / (weight_lst + 1e-6)\n    marginal_impact2 = value2_lst / (weight_lst + 1e-6)\n    combined_impact = marginal_impact1 + marginal_impact2\n\n    # Phase 1: Add high-impact items that improve both objectives\n    remaining_capacity = capacity - current_weight\n    for idx in np.argsort(combined_impact)[::-1]:\n        if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            if marginal_impact1[idx] > marginal_impact1.mean() and marginal_impact2[idx] > marginal_impact2.mean():\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Capacity-aware replacements\n    while current_weight > capacity:\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) == 0:\n            break\n        worst_item = included_items[np.argmin(combined_impact[included_items])]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    # Phase 3: Probabilistic exploration of high-impact items\n    high_impact_items = np.argsort(combined_impact)[::-1][:max(1, len(combined_impact) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_flip = min(3, len(high_impact_items))\n        flip_indices = np.random.choice(high_impact_items, size=num_to_flip, replace=False)\n        for idx in flip_indices:\n            if new_solution[idx] == 1:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n            else:\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n\n    return new_solution\n\n\nNo. 10 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select a solution with high potential for improvement (top 20% by combined objective)\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    selected_idx = min(int(0.2 * len(archive)), len(archive) - 1)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts considering both objectives and weights\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic thresholds\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 80 - (current_weight/capacity)*10)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Hybrid local search with targeted swaps and replacements\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Sort items by their marginal impact ratios\n    marginal_ratio = marginal1 / (marginal2 + 1e-6)\n    sorted_included = included[np.argsort(marginal_ratio[included])]\n    sorted_excluded = excluded[np.argsort(marginal_ratio[excluded])[::-1]]\n\n    # Perform swaps between high-marginal included and excluded items\n    for i in sorted_included[:min(3, len(sorted_included))]:\n        for j in sorted_excluded[:min(5, len(sorted_excluded))]:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving swaps\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores considering both objectives and weight distribution\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))) *\n                      (1 + weight_lst[idx]/capacity))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items with adaptive thresholds\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 4)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(2, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                if np.random.rand() < 0.7:  # 70% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design in 4 bullet points:\n\n- **Keywords**: Adaptive marginal impact, hybrid local search, dynamic thresholds, Pareto-aware selection.\n- **Advice**: Combine add/swap/replace operations with frontier-focused selection and percentile-based thresholds to balance exploration/exploitation.\n- **Avoid**: Static item selection or rigid single-phase operations; enforce capacity constraints via feasibility checks.\n- **Explanation**: Dynamic criteria ensure both objectives are optimized while maintaining feasibility, preventing premature convergence. \ud83d\ude80\n\n**Tip**: Focus on dynamic criteria (e.g., solutions near the frontier) and hybrid operations to ensure high-quality, feasible neighbors.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 306,
        "algorithm": "The algorithm combines adaptive marginal impact selection with a hybrid local search that prioritizes frontier items (Phase 1), performs Pareto-aware swaps (Phase 2), dynamically rebalances solutions (Phase 3), and probabilistically adds high-impact items (Phase 4), ensuring feasibility at each step. It emphasizes combined marginal values (value1 + value2) for selection and prioritizes items with high utility scores during rebalancing.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic threshold\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 90 - (current_weight/capacity)*15)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-aware swaps between included and excluded items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving swaps\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores considering both objectives and weight distribution\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))) *\n                      (1 + weight_lst[idx]/capacity))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(3, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                if np.random.rand() < 0.8:  # 80% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9482289716695371,
            3.4790739119052887
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest combined objective value\n    archive.sort(key=lambda x: (x[1][0] + x[1][1]), reverse=True)\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Calculate adaptive marginal impacts\n    marginal1 = value1_lst / (weight_lst + 1e-6)\n    marginal2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal1 + marginal2) / 2\n\n    # Phase 1: Frontier-focused additions with dynamic threshold\n    remaining_capacity = capacity - current_weight\n    frontier_threshold = np.percentile(combined_marginal, 90 - (current_weight/capacity)*15)\n    frontier_items = np.where((combined_marginal >= frontier_threshold) & (new_solution == 0))[0]\n\n    for idx in frontier_items:\n        if weight_lst[idx] <= remaining_capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            remaining_capacity -= weight_lst[idx]\n\n    # Phase 2: Pareto-aware swaps between included and excluded items\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    for i in included:\n        for j in excluded:\n            if weight_lst[j] <= remaining_capacity + weight_lst[i]:\n                # Check for Pareto-improving swaps\n                if (value1_lst[j] >= value1_lst[i] and value2_lst[j] > value2_lst[i]) or \\\n                   (value1_lst[j] > value1_lst[i] and value2_lst[j] >= value2_lst[i]):\n                    new_solution[i], new_solution[j] = 0, 1\n                    current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                    remaining_capacity = capacity - current_weight\n                    break\n\n    # Phase 3: Dynamic rebalancing with utility-based removal\n    while current_weight > capacity:\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n\n        # Calculate utility scores considering both objectives and weight distribution\n        utility_scores = []\n        total_value1 = np.sum(value1_lst[included])\n        total_value2 = np.sum(value2_lst[included])\n\n        for idx in included:\n            utility = (combined_marginal[idx] *\n                      (1 - (value1_lst[idx]/(total_value1 + 1e-6)) *\n                       (value2_lst[idx]/(total_value2 + 1e-6))) *\n                      (1 + weight_lst[idx]/capacity))\n            utility_scores.append((idx, utility))\n\n        utility_scores.sort(key=lambda x: x[1])\n        worst_idx = utility_scores[0][0]\n        new_solution[worst_idx] = 0\n        current_weight -= weight_lst[worst_idx]\n\n    # Phase 4: Probabilistic additions of high-impact items\n    high_impact_items = np.argsort(combined_marginal)[::-1][:max(1, len(combined_marginal) // 5)]\n    if len(high_impact_items) > 0:\n        num_to_add = min(3, len(high_impact_items))\n        add_indices = np.random.choice(high_impact_items, size=num_to_add, replace=False)\n        for idx in add_indices:\n            if new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n                if np.random.rand() < 0.8:  # 80% chance to add\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n                    remaining_capacity -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    }
]